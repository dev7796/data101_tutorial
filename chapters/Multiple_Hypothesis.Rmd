# üîñ Multiple Hypothesis Testing {#Mtest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

We often consider multiple possible hypotheses in our search for discovery  to find one with lowest possible p-value. Consciously or subconsciously we are engaging, what is often called,  p-value hunting. We have to be very careful! We may ‚Äúdiscover‚Äù what is simply random even if we correctly calculate p-value and compare it with the significance level. It is very important to learn about multiple hypothesis traps very early in the process of learning data science.  

For example assume that we are looking for associations between sales of individual items in a supermarket.  Does bread sell with butter? Does coffee sell with spring water?  There is an exponential number of possible combinations (N choose 2 to be exact, where N is the number of items).  For each such pair we perform hypothesis testing. If one test is performed at the 5% significance level and the corresponding null hypothesis is true, there is only a 5% chance of incorrectly rejecting the null hypothesis. However, if 100 tests are each conducted at the 5% significance level and all corresponding null hypotheses are true, the expected number of incorrect rejections (also known as false positives or Type I errors) is 5. If the tests are statistically independent from each other, the probability of at least one incorrect rejection is approximately 99.4%.  Thus, we will almost surely find one false positive! In other words we will be fooled by data. 

Bonferroni correction is a method to counteract the multiple hypothesis (often called multiple comparison problem. Make it harder to reject null hypotheses by dividing the significance level by number of hypotheses. The Bonferroni correction compensates for that increase by testing each individual hypothesis at a significance level of **Œ± / m**  where  m is the number of hypotheses. For example, if a trial is testing me = 20 hypotheses with a desired Œ± = 0.05, then the Bonferroni correction would test each individual hypothesis at

\begin{equation}
\alpha = \frac{0.05} {20} 
       = 0.0025
\end{equation}

Thus, there is a very simple remedy for multiple hypothesis traps. Just divide the significance level by the number of (potential) hypotheses tested.  This will make it harder, often much much harder to reject the null hypothesis and yell Eureka! Critics say that in fact Bonferroni correction is too conservative and too ‚Äúpro-null‚Äù  and tough on alternative hypotheses to be acceptable.  

The unwanted side effect of Bonferroni correction is that  we may  fail to reject the null hypothesis too often. Bonferroni correction makes discovery sometimes too hard, making data scientists too conservative and accepting null hypothesis when they should be rejected.  It may also be the case that even Bonferroni correction will not protect us, as we will show in our example below.  But at least we will be much less likely to  make fools of ourselves coming with false discoveries leading potentially to very wrong business decisions. 

There are other less conservative methods of correcting for multiple hypotheses - such as the Benjamini-Hochberg method described in the attached slides.

We illustrate the p-value hunt in snippet 8.1 below. It is based on synthetic data set showing summer temperatures in New Jersey townships.
Table below describes the data set based on hypothetical temperature readings in various municipalities of New Jersey over summer. Is one city experiencing higher average temperatures than another? Can we find such a pair of cities? This is the ultimate p-value hunt. Let‚Äôs compare townshiships pair by pair, until we find a pair with sufficiently large differences of mean temperatures and sufficiently low p-value. Careful! You may come up with false discovery if you do not correct for multiple hypotheses!

The \@ref(snippet1)  shows several permutation tests for different pairs of townships and difference of means of temperatures hypothesis test. Two of four pairs show p‚Äìvalues less than customary significance level of 5%. Should we then reject the null hypothesis and conclude that indeed Ocean Grove is warmer than New Brunswick and that New Brunswick is warmer than Holmdel? Indeed, both pairs result in p-values significantly lower than 5%. If we incorrectly disregard the number of hypotheses considered, we may come to wrong conclusions supporting these two alternative hypotheses. But there are around 20 townships in the Temp data set. Thus there are around 200 possible hypotheses (200 pairs of townships) which we may consider in our p-value hunt. If we apply Bonferroni correction for N=200, the significance level will be 200 times lower, instead of 5%, it will be 0.025%. None of the two hypotheses (Ocean Grove vs New Brunswick and New Brunswick vs Holmdel) meets the new significance level. Indeed in both cases p-values are significantly larger than 0.025%. Thus, for none of the four pairs we can reject null hypotheses.

Now we can disclose that we have created our Temp data set completely randomly - assigning random temperatures between 50 and 100 degrees to each township. Thus, without Bonferroni coefficient we would be fooled by data, not once, but twice in our four tests. We would find a trend when it does not exist - it is simply random deviation.

It turns out however that even Bonferroni correction is not sufficient to protect us against incorrectly rejecting null hypothesis. Indeed, for Red Bank and Holmdel, we conclude that Red Band is warmer than Holmdel with p-value of 0.01%! (see the last permutation test in the snippet 1). This p-value falls even below significance level adjusted with Bonferroni correction (0.025%). It only shows that dealing with multiple hypotheses is a risky adventure. We may end up being fooled by data even when we apply Bonferroni correction. But at least we are less likely to fall into the trap of multiple hypotheses when we apply Bonferroni Correction.

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Tempratures.csv" download="Tempratures.csv">Tempratures.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
hindex<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Tempratures.csv") #web load
# head(moody)
temp<-knitr::kable(
  hindex[sample(1:nrow(hindex),10), ], caption = 'Snippet of Temperature Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

The Temp data set assigned random temperatures between 50 and 100 degrees to around 20 townships in New Jersey. 

Without Bonferroni correction we would have to incorrectly reject the null hypothesis in the last three permutation tests  (3rd, 4th and 5th) tests. We will still reject null hypothesis (and have false positive discovery) in the last, 5th case. Indeed, the 5th p-value is 0.0002 which is less than the significance level after Bonferroni correction (0.00025).  

### Multiple Permutation Tests {#snippet1}

```{r,tut=TRUE,ex="permutationtestfunction10",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction10",type="sample-code",height=400}

Temp <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Tempratures.csv") #web load

Permutation(Temp, "Township", "Temprature",10000, "Princeton", "Trenton")
Permutation(Temp, "Township", "Temprature",10000, "Passaic", "Newark")
Permutation(Temp, "Township", "Temprature",10000, "Ocean Grove", "New Brunswick")
Permutation(Temp, "Township", "Temprature",10000, "New Brunswick", "Holmdel")
Permutation(Temp, "Township", "Temprature",10000, "Red Bank", "Holmdel")
```
<!--
## Snippet 1 - Benjamini-Hochberg Algorithm {#Snippet1}

```{r,tut=TRUE,height=450}
p<-sort(round(runif(100, min=0, max=0.05), 4))
p
p<-p+0.0003
p
#implement Benjamini-Hochberg formula
q<-rep(0.05,100)
q
r=c(1:100)
q<-round(q*r/100,4)
temp<-p<q
#Select p-values which correspond to discoveries (reject NULL)
maxindex<-max(which(temp=='TRUE'))
p[1:maxindex]
```

## Snippet 2

Happiness Index synthetic data set which is used in my slides for multiple hypotheses testing

- How to order by aggregate?

- First make a data frame out of tapply? Use  aggregate  and list functions.

```{r,tut=TRUE,height=450}
Hindex <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv") #web load

Hindex<-aggregate(Hindex$HAPPINESS, list(Hindex$COUNTRY), mean)
colnames(Hindex)<- c("Country","AverageH")
#renames columns of the Hindex data frame
colnames(Hindex)

Hindex[order(Hindex$AverageH),]
```
-->

## Additional References 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MPT12"> Multiple Hypothesis Testing</button> 
<div id="MPT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1dCbhnuGMsXYJEltQXUfPhl8dCTAqJWCZ0xI-0IhnxAc/edit?usp=sharing" width="100%" height="500px"></embed>
</div>


https://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/
