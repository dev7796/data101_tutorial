# üîñ Test of Independence {#chitest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

We would like to test if a student's final grade in Professor Moody‚Äôs class  depends  on the student's major.  The null hypothesis in this case is the hypothesis of independence.  Independence means that the distribution of final grades is the same no matter what the major is. The alternative hypothesis is that the distribution of final grades  changes from major to major, rejecting the null hypothesis of independence. 

Notice that we do not specify how the grades depend on students' majors.  Do CS students get better grades than Psychology  majors? Do Economics majors get lower grades than Statistics majors?  We do not care about this. We are only testing here whether there is a relationship between Major and the final grade distribution.

We will describe the permutation test which scrambles our data randomly in such a way that any relationship between grades and major  (if it ever existed) is broken. We will run a permutation test a large number of times - possibly tens of thousands of times. Then we will determine how likely it is to randomly obtain the observed result.  But what is the observed result? It is a bit more complex than the difference of means which were observing the difference of means hypothesis testing. 

The observed result in our case is calculated by so called chi-square statistic which is calculated on the contingency table,  ```table(moody$Grade, moody$Major)```

To explain it, let us first define two tables: The observed contingency table and the expected contingency table. It is calculated by table() function.


**OBSERVED CONTINGENCY TABLE**
<table>
<tr>
<td>Grade/Major</td>
<td>CS</td>
<td>Economics</td>
<td>Psychology</td>
<td>Statistics</td>
</tr>

<tr>
<td>A</td>
<td>46</td>
<td>54</td>
<td>69</td>
<td>42</td>
</tr>

<tr>
<td>B</td>
<td>46</td>
<td>12</td>
<td>2</td>
<td>35</td>
</tr>

<tr>
<td>C</td>
<td>51</td>
<td>33</td>
<td>30</td>
<td>34</td>
</tr>

<tr>
<td>D</td>
<td>41</td>
<td>37</td>
<td>29</td>
<td>34</td>
</tr>

<tr>
<td>F</td>
<td>108</td>
<td>99</td>
<td>99</td>
<td>99</td>
</tr>

</table>


The second table we need is called the expected table. This is the hypothetical table of the relationship between grade and major,  assuming grade and major are completely independent.  Such a table  would be the result of  the same distribution of grades for each of the majors.  Notice that we 1000 students in the data set the expected table (i.e. table which have grades completely independent from majors) would  have the same distribution of grades for each major that over all students - which is shown by the **TOTAL** column.


**EXPECTED CONTINGENCY TABLE**
<table>
<tr>
<td>Grade/Major</td>
<td>CS</td>
<td>Economics</td>
<td>Psychology</td>
<td>Statistics</td>
<td>TOTAL</td>
</tr>

<tr>
<td>A</td>
<td>61</td>
<td>50</td>
<td>48</td>
<td>51</td>
</tr>

<tr>
<td>B</td>
<td>28</td>
<td>22</td>
<td>22</td>
<td>23</td>
</tr>

<tr>
<td>C</td>
<td>43</td>
<td>35</td>
<td>34</td>
<td>36</td>
</tr>

<tr>
<td>D</td>
<td>41</td>
<td>33</td>
<td>32</td>
<td>34</td>
</tr>

<tr>
<td>F</td>
<td>118</td>
<td>96</td>
<td>93</td>
<td>99</td>
</tr>

<tr>
<td>TOTAL</td>
<td>292</td>
<td>236</td>
<td>229</td>
<td>244</td>
<td>1000</td>
</tr>
</table>

We kept fractions - although these are number of students - therefore would have to be rounded up to integers 

The chi-square formula calculates the **‚Äúdistance‚Äù** between the observed contingency table and the expected contingency table.

\begin{equation}

\sum \frac{(O_i - E_i)^2}{E_i}\\


\text{where:}\\
\text{O = observed values}\\
\text{E = expected values}\\

\end{equation}

For the two tables above the, 

\begin{equation}

\sum \frac{(O_i - E_i)^2}{E_i}  = 60.03\\

\end{equation}

To evaluate how far off is this observed result assuming that Grades are independent from Major, we run a permutation test which scrambles Grades and Majors randomly and every time computes the chi-square formula with the new observed table (the expected table is always the same).  Then, we see how many times out of, say 10,000 iterations of permutation test we obtain a result larger than the observed result of 60.03? This is the p-value. 

Permutation test for independence hypothesis gives us again a better feeling about the impact of randomness and whether the observed chi-square result for ‚Äúsimilarity‚Äù of grade distributions for different majors can be obtained randomly. 
In the following snippet we run the chisq test which is based on the so-called chi square distribution. Here we simply show you a function which can calculate p-value, just like the z-test function does.  The explanation of the chi-square test is provided in attached link to the excellent Khan Academy video.

Permutation tests in both cases of difference of means and independence hypotheses give a better intuitive sense of how we answer the question - can the observed result be obtained randomly? 

Notice that the independence test is looking globally at two vectors and whether one depends on another. If we wanted to be more specific and know if psychology majors are more likely to get an A than CS majors, we can frame this as a difference of means hypothesis. Testing this hypothesis will be using the difference of means of frequencies of A‚Äôs among CS majors and psychology majors.  This could be done again by permutation test in section 8 or the z-test. 



## Snippet 1

```{r,tut=TRUE,height=300}
Expected <-matrix(c(200,420,180, 40,120,40), nrow=3, ncol=2)
Observed<-matrix(c(200,420,180,35,120,45), nrow=3, ncol=2)
Expected
Observed
chisq.test(Observed)
```

## Snippet 2

```{r,tut=TRUE,ex="chisquarefunction",type="pre-exercise-code"}
library(dplyr)
options(warn=-1)
chi_test <- function(data,col1,col2,iter) {
  
  df <- data.frame(data)
  vals<- unique(df[[col2]])
  no_rows <- nrow(df)
  dt <- table(df[[col1]], df[[col2]])
  res <- chisq.test(dt)
  real_ans <- res$statistic
  p_value <- res$p.value
  ans_vec <- vector()
  for (x in 1:iter){

    new_data <- sample(x=vals,size=no_rows,replace = TRUE)

    dt_new <- table(df[[col1]], new_data)

    res_new <- chisq.test(dt_new)

    ans_vec <- append(ans_vec,res_new$statistic)
  }
  hist(ans_vec,main="Permutation Test for Chi-Square",xlab="Chi-Square Values",breaks = 100)
  print(real_ans)
  abline(v=real_ans,col="blue",lwd=2)
  return (p_value)
}

```


```{r,tut=TRUE,ex="chisquarefunction",type="sample-code",height=500}

d<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")
head(d)

chi_test(d,"Major","Grade",1000)

```

## Snippet 3

```{r,tut=TRUE,height=300}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")
moody$IN<-'Out_Slice'
moody[moody$DOZES_OFF=='never' & moody$TEXTING_IN_CLASS=='always', ]$IN<-'In_Slice'
d<-table(moody$GRADE, moody$IN)
d
chisq.test(d)
```

## Snippet 4


```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")
data<-table(movies$content, movies$genre)
chisq.test(data)
```

## Additional Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#CS12">Chi Square</button> 

<button class="btn btn-primary" data-toggle="collapse" data-target="#KH13">Khan Academy Video</button>

<div id="CS12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1h-h2S5lW6ReFwdeJKNflPpE0iCjoS08T0mpgSLiFv88/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<div id="KH13" class="collapse">https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-goodness-fit/v/chi-square-statistic
</div>
