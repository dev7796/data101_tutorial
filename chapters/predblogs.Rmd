# Prediction Challenges {#predblogs}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

Each prediction challenge is based on the training data generated synthetically with some embedded patterns. Students can either build their own prediction models from scratch (coding their own prediction models without using R libraries, “free style”) or utilize R libraries for a multitude of machine learning methods. These methods range from decision trees (rpart, recursive partitioning) through linear regression (lm), svm and even neural networks. Students  test their prediction models on the testing data. We use Kaggle to automatically calculate the prediction errors and rank student solutions by prediction accuracy. Depending on the type of  independent, target variable we use either prediction accuracy or MSE. 

As data creators we know the method of data generation, therefore we can construct the *perfect* prediction model, Accuracy of the perfect prediction model provides a soft upper bound for prediction accuracy of all possible prediction models, created without knowing how data was generated. Thus, there is no general, absolute,  notion of a “good” prediction model. It all depends on the data. Sometimes prediction accuracy of 60% is excellent. For other data sets, accuracy of 95% may not be good enough.  

If the accuracy of a prediction model is near the accuracy of a perfect prediction model, such a model is definitely good. Turns out that, to our initial surprise, for some data sets, students created prediction models with better accuracy than corresponding perfect prediction models. We will discuss it below.


<!--Until we have studied multiple methods of data analysis in sections \@ref(freestyle),\@ref(datatransformation), statistical testing in sections \@ref(stateval), &   building prediction models for both classification \@ref(classification) and regression \@ref(regression) along with advanced ML models \@ref(models).

Now its time to utilize them in various ways for  analysis and prediction of data.

To do this, in this course, we have designed few prediction challenges, which test your ability to implement skills learnt in the course until now. 

First challenge is a basic prediction challenge using only data analysis using the freestyle techniques from section \@ref(freestyle). 

Then onwards, prediction challenges used multitude of modeling techniques which were studied in \@ref(classification) and \@ref(regression).

---

## General Structure of the Prediction Challenges.

Usually there is a task to be performed in each prediction challenge. 

Either predicting a numerical of categorical values is the task of each challenge.

The way to perform those task are constrained differently for different prediction challenges based on levels of difficulty and ML models to be used.

The submission will take place on **Kaggle** which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submission.

The datasets provided for each prediction challenge is as follows:

1. Training Dataset.
    - It is used for training and cross-validation purpose in the prediction challenge. 
    - This data has all the training attributes along and the ideal values of the prediction attribute.
    - Models for prediction are to be trained using this dataset only.
2. Testing Dataset.
    - It is used for prediction only.
    - It consists of all the attributes that were used for training, but it does not contain any values of the actual prediction attributes, which is actually the attribute that the prediction challenge predicts.
    - Since its only used for prediction purpose and is not involved in training of the models, it is thus not involved in the cross-validation phase too.
3. Submission Dataset.
    - After prediction using the "testing" dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g. ID or name,etc) and second the predicted attribute column.
    - Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle.

- To read the datasets use the *read.csv()* function and for writing the dataset to the file, use the *write.csv()* function.
  - Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle.
  - To avoid this, you can add the parameter, `row.names = F` in the `write.csv()` function. e.g. `write.csv(*dataframe*,*fileaddress*,row.names = F)`.

Now lets look at the prediction challenges that took place in this course along with the top submissions by students.

---
-->

## Prediction Challenge 1.

For this prediction challenge we used our favorite dataset, the Professor Moody dataset, and predicted the Grade category of all students. The Grade category had only 2 factors: Pass OR Fail.

Let's look at a snippet of the moody dataset used for training in this challenge.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021train.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Moody Dataset(TRAINING) for Prediction Challenge 1',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")

```

<!--We can see that there are multiple attributes like *Score, Attendence, Major, etc.* that can be used as predictors, and then there is *Grade* attribute with ideal values for each record of student which will be used while training and then will be predicted on the testing dataset.

Lets look at the snippet of the moody dataset for testing. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test-students.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Moody Dataset(TESTING) for Prediction Challenge 1', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see that the *Grade* attribute is not present in this dataset, since it is the attribute that will be predicted using our analysis of the training dataset. -->

<!-- Also, lets look at the submission file for prediction challenge 1. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test-submission-file.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Submission file for Prediction Challenge 1', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see there are only 2 columns *Studentid* and *Grade*. *Studentid* column's entries corresponds/are similar to the *Studentid* column of the testing data. Thus we need to just fill the *Grade* column with appropriate grade predicted by our analysis, corresponding to the same *Studentid* values in both test and submission data. -->

<!-- Now that we have seen the data, feel free to go to the Kaggle site of this prediction challenge and take the challenge yourself. The link for challenge: [Prediction Challenge 1](https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d){target="_blank"} -->

---

### How the data was generated for Challenge 1

Professor Moody's data set has been synthetically generated using a random generator which follows probabilistic rules implementing “secret patterns” which we embedded in the data.

These patterns are presented below in the form of a decision tree. For example, a rule that Statistics  majors with scores over 60, pass the class - reflects the generated data in which a high percentage (but not 100%) of  students majoring in Statistics indeed pass Moody’s class. There will always be random exceptions to these rules. But majority of Statistics students with score above 60 will pass the class

The data is based on a tree given by the following conditions:


``` text
Tree which is embedded in the data (secret pattern for Moody -challenge1/2)

Major
   Stat
      Score > 60 Pass
      Score <= 60  Fail
   Comm
      Score >40  Pass
      Score <=40
         Texting = Rarely   Fail
          Texting = Always  PAss
   Polsci
      Score >50  Pass
      Score <=50  
         Questions = rare    Fail   
         Questions = always  Pass
   Cs
      Score >70      Pass
      Score <=70
        Seniority= Freshman 
             Score >50 Pass
             Score <=50 
               Attendance >=60
                    Score > 40   Pass
                    Score <=40   Fail
               Attendance <60    Fail  
         Seniority= Sophomore
               Score >50         Pass
               Score <=50        Fail
         Seniority= Junior       Fail
         Seniority = Senior Fail 
```

We can see this in pictorial representation below based on each subset of Majors. Clearly rules differ for different Majors. In other words each Major has a different grade prediction model.

- For Stats Major:
  - We can see that the rule was very simple, with the final grade decided.
  - ![The tree used for predicting Stats Major students grade](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred1stat.svg)

- For Communication Major Students:
  - The grade prediction model  for Communication majors depended not only on the Score attribute but also on the Texting attribute of the students records.
  - ![The tree used for predicting Communications Major students grade](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred1comm.svg)

- For Political Science Major Students:
  - The grade prediction model  for students majoring in  Political Science Major is based not only on the Score attribute but also based on the Questions attribute of the students records.
  - ![The tree used for predicting Political Science Major students grade](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred1polsc.svg)
  
- For Computer Science Major Students:
  - The grade prediction model for students  majoring in  Computer Science  was the most complex .
  - Attributes like *Score, Seniority and Attendance* were involved in prediction model and the subsetting conditions were quite complex. 
  - ![The tree used for predicting Communications Major students grade](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred1cs.svg)
  
- **How the data was generated using R**
  - The code below implements  perfect prediction model using the above data generation rules.
  
```{r,height=700}
# Load Data
dat<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test-students.csv",stringsAsFactors = T)

# Create an all Fail Category/Predicted Value Vector and append it to the testing dataset.
dat$Grade<-rep('Fail', nrow(dat))

# The various conditions used to predict grade attribute.

# For Major = Stat
dat[dat$Major=='Stat' & dat$Score>60,]$Grade <-'Pass'

# For Major = Comm
dat[dat$Major=='Communication' & dat$Score>40,]$Grade <-'Pass'
dat[dat$Major=='Communication' & dat$Score<=40 & dat$Texting=='Always',]$Grade <-'Pass'

# For Major = Polsci
dat[dat$Major=='Polsci' & dat$Score>50,]$Grade <-'Pass'
dat[dat$Major=='Polsci' & dat$Score<=50 & dat$Questions=='Always',]$Grade <-'Pass'

# For Major = Cs
dat[dat$Major=='Cs' & dat$Score>70,]$Grade <-'Pass'
dat[dat$Major=='Cs' & dat$Score<=70 & dat$Seniority=='Freshman' & dat$Score>50,]$Grade <-'Pass'
dat[dat$Major=='Cs' & dat$Score<=70 & dat$Seniority=='Freshman' & dat$Attendance >=60 & dat$Score>40,]$Grade <-'Pass'
dat[dat$Major=='Cs' & dat$Score<=70 & dat$Seniority=='Sophomore' & dat$Score>50,]$Grade <-'Pass'


# Compare it with the ideal predictions for checking accuracy of our predictions.
answers<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test_answer.csv",stringsAsFactors = T)
mean(dat$Grade==answers$Grade) # Accuracy
```

Accuracy of the perfect prediction model defined above  was around 83% on the testing data set.
In the next section we review some top student submissions. Surprisingly all top 10 student solutions achieve accuracy beating the ideal 83% accuracy by a solid  few percentage points. In other words, these students do better than the prediction model which is fully aware of the rules used in data generation!  How is this possible?

There may be several reasons for this. First,the synthetically generated data satisfies some spurious, random patterns, in addition to the patterns built into the data generation process. The smaller the generated data set, the more likely these spurious patterns are. Perfect prediction model will not incorporate these spurious patterns. Prediction models built by students on the basis of the training data set may take advantage of these spurious patterns. Summarizing, the actual data set has some additional noise, and some random patterns which are generated as unintended side effects of our generation procedure. 

The larger our generated data sets are, the less prominent these spurious patterns are. Typically our data sets have a few thousand tuples. This is not large enough to combat the law of small numbers - some extreme patterns appearing randomly. 

Another reason could be  getting away with overfitting - since even though we have used testing data which is different from training data, we only tested the student prediction models once, against  one specific test data set. The students' models may have  overfit the training data and testing data as well (since only one testing data set was used).   If we tested against multiple testing data sets, this overfitting effect may have vanished. 


---

### Top Submissions for Challenge 1.

Students with accuracy over 60% were considered passed for this prediction challenge.

1. *Jeremy Prasad*     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred11">Jeremy's PPT</button>
<div id="pred11" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/jeremypred1.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - Jeremy performed exceptionally well in this prediction challenge.
  - His approach was an iterative learning process, where at each step after performing analysis he tried to decrease the error more and more.
  - He started with a very basic model, of using just the score attribute with a hard threshold for pass or fail grade based on the score value. 
  - After this, to increase accuracy, he analysed the data more found which attributes effect the prediction of the data, and which are not really useful
  - After finding these highly effective attributes, he wrote concrete set of attributs that can be used to assign the grade. Most of them were dependent on 2-3 attributes like Major-Senioriy-Score, Major-Score, or Major-Questions-Score,etc. 
  - Jeremy’s model has achieved accuracy of nearly 87% - beating the perfect prediction model by 4 percentage points. The reasons for these unexpected results were discussed earlier in this section.
 


2. *Rohit  Manjunath*    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred12">Rohit's PPT</button>
<div id="pred12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/rohitpred1.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - Rohit performed well in this prediction challenge, and has a different approach than that of Jeremy's.
  - In Rohit's approach, instead of finding the minimum global threshold of pass or fail based on score, he found the threshold for the maximum score, above which every student passed the class.
  - He then analysed the data based on the Majors first and then found interval threshold for each Majors scores.
  - For some Majors, to increase accuracy, he further explored other attributes in detail to find which effects the final grade.
  - Rohit obtained accuracy of almost 85%, beating the accuracy of perfect model by 2%.
  


---

## Prediction Challenge 2.

Both the training and testing data sets  for this prediction challenge were the same as those in the prediction challenge 1.  Students were simply allowed to use the rpart  library  in R to build their prediction models. This provided the opportunity to assess the value of pre-packaged libraries such as rpart as opposed to manual coding the prediction models.  

With rpart() doing most work of prediction in this task, the students were also asked to provide validation for their models prediction power/accuracy. This involved use of cross-validation techinques, which for the ease of this course level was provided in a custom function, see \@ref(crossvalidation).

In general, rpart() offered great help to the vast majority of students. In fact around 70% of all students achieved identical accuracy on the testing data set, which was a few points shy of the ideal 83%. This was the case because  they used the same prediction model - a simple default rpart model. Interestingly rpart()  accuracy did not  beat the few top prediction model solutions for challenge 1. These were coded manually without using any R libraries. In fact nobody has beat the top accuracy achieved for the prediction challenge 1 by Jeremy Prasad!  However he spent a lot of time building his prediction model from the ground. And got away with some overfitting for sure.

To perform this challenge yourself please visit the kaggle site of this prediction challenge. Link to Kaggle Site: [Prediction Challenge 2](https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b){target="_blank"}

---

### How the data was generated to Challenge 2

As we saw that the prediction task and the datasets in challenge 2 are similar to that of challenge 1. Thus the data analysis of the challenge 1 would applicable in this case too.

<!-- But here we can use the rpart() function of creating the decision tree and predicting on the testing dataset. -->

<!-- - **How the data was generated using R for prediction challenge 2** -->

<!-- ```{r} -->
<!-- library(rpart) -->

<!-- # Load Data -->
<!-- dat<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test_answer.csv",stringsAsFactors = T) -->

<!-- # Using  rpart() for prediction -->
<!-- tree <- rpart(Grade ~ Attendance+Major+Questions+Score+Seniority+Texting, data=dat,method = "class") -->

<!-- # Predict using the built decision tree. -->
<!-- pred <- predict(tree, newdata = dat, type = "class") -->

<!-- # Confusion matrix for prediction vs actual values. -->
<!-- table(actual = dat[,8], predicted = pred) -->

<!-- # Accuracy of the prediction -->
<!-- mean(pred==dat$Grade) -->

<!-- # Code to display the tree. Cannot be used in this interactive box. -->
<!-- # library(rpart.plot) -->
<!-- # rpart.plot(tree) -->
<!-- ``` -->
<!-- ![Tree Predicted above using the rpart function.](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/predblog/pred2.svg) -->

<!-- - As we can see, the prediction accuracy is near 83%, using just the simple rpart model without any control parameters. -->
<!--   - Students can use the control parameters for creating decision tree with more accuracy. -->
<!-- - **Note**, that the decision tree generated using the rpart() function will be different than that of the ideal trees shown in prediction challenge 1. -->

---

### Top Submissions for Challenge 2

Since rpart() is a very powerful function to find patterns with higher accuracy, the passing criteria for this challenge was above 80% accuracy score.

1. Kevin Larkin    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred21">Kevin's PPT</button>
<div id="pred21" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/kevinpred2.pdf&embedded=true" width="100%" height="500px"></embed>
</div>  
  
  - This was the top submission in terms of accuracy score on Kaggle.
  - Kevin used the rpart() function, for modeling, with all the attributes of the training dataset except *Studentid*.
  - To increase the accuracy of his model, he used the `rpart.control()` function parameters, especially the `cp` parameter of the function, which increased the splitting accuracy.
  - Kevin achieved an accuracy score of over 86% on the test dataset for this challenge.
  


2. Michael Ryvin    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred22">Michael's PPT</button>
<div id="pred22" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/michaelpred2.pdf&embedded=true" width="100%" height="500px"></embed>
</div>
 
  - This was the second best submission as per accuracy score on Kaggle.
  - Michael used the rpart() function, along with some control parameters for creating the decision tree.
  - Michael achieved an accuracy score of over 86% on the test dataset.
 
  
  
3. Shuohao Ping    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred23">Shuohao's PPT</button>
<div id="pred23" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/shuohaopred2.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was the third best submission as per accuracy score on Kaggle.
  - Shuohao used multiple iterations to create his final model.
  - In each iteration, Shuohao tried to vary the control parameters and its values to find the best fit model after cross-validation.
  - Shuohao, achieved an accuracy score of over 86% on the test dataset.
  



---

## Prediction Challenge 3.

In  prediction challenge 3, the task was to predict Earnings as a numerical variable, using any ML algorithm.

Earnings variable is part of the Earnings dataset which is a synthetic data set relating earnings of a person to different attributes such as GPA in college, Major, as well as number of personal connections and several other attributes. 

Lets look at a snippet of the Earnings dataset used for training the models below.


```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/Earnings_Train2021.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Earnings Dataset(TRAINING) for Prediction Challenge 3',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")

```

<!-- We can see that there are multiple attributes like *GPA,Major,Graduation_Year,Height,etc.* that can be used as predictors, and then there is *Earnings* attribute with ideal values for each record of student which will be used while training and then will be predicted on the testing dataset.

 Lets look at the snippet of the earnings dataset for testing. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/Earnings_Test.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Earnings Dataset(TESTING) for Prediction Challenge 3', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see that the *Earnings* attribute is not present in this dataset, since it is the attribute that will be predicted using our analysis of the training dataset. -->

<!-- Also, lets look at the submission file for prediction challenge 3. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/earning_submission.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Submission file for Prediction Challenge 3', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see there are only 2 columns *ID* and *Earnings*. *ID* column's entries corresponds/are similar to the *ID* column of the testing data. Thus we need to just fill the *Earnings* column with appropriate earning value predicted by our analysis. -->

<!-- Now that we have seen the data, feel free to go to the Kaggle site of this prediction challenge and take the challenge yourself. The link for challenge: [Prediction Challenge 3](https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd){target="_blank"} -->


---

### How the data was generated for Challenge 3

The data was generated differently for different values of Education attribute - which described several classes of majors such as STEM, Humanities, Professional etc.

The relationship between earnings and other attributes was generated by different formulas for different values of Education attribute as follows:


``` text

   Stem                 earn = -100 * gpa +10000
   Humanities           earn =  100*  gpa + 10000
   Vocational           earn =  100 * gpa + 13000
   Professional         earn =  -100gpa +12000
   other                earn =  connection ^2 +5000
   business             earn =  gpa  * 100 * parity +10000
                                where parity   = 1 if graduation year = even
                                                 0 if graduation year = odd
```

As we can see, these formulas are mostly linear, while the formula for “other” education attributes is quadratic. Also, for “Business” education attribute subjects, the formula is dependent on an additional attribute. Notice that the rule for business majors was quite tricky - making the earning formula dependent on the student’s graduation year being even or odd!

For more detailed data analysis please view the document attached here. <button class="btn btn-primary" data-toggle="collapse" data-target="#pred3"> Pred 3 Analysis</button>
<div id="pred3" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/sarahpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

- **How the data was modeled in R **

```{r,height=800}
# Load the dataset
dat<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/Earnings_Test_answer.csv",stringsAsFactors = T)

# Create a prediction column to store predicted values and append that column to dataset.
dat$predEarnings <- rep(0,nrow(dat))


# Predict Earnings of subjects with Education in STEM field.
dat[dat$Major=='STEM',]$predEarnings <- (-100*dat[dat$Major=='STEM',]$GPA + 10000)

# Predict Earnings of subjects with Education in Humanities field.
dat[dat$Major=='Humanities',]$predEarnings <- (100*dat[dat$Major=='Humanities',]$GPA + 10000)

# Predict Earnings of subjects with Education in Vocational field.
dat[dat$Major=='Vocational',]$predEarnings <- (100*dat[dat$Major=='Vocational',]$GPA + 13000)

# Predict Earnings of subjects with Education in Professional field.
dat[dat$Major=='Professional',]$predEarnings <- (-100*dat[dat$Major=='Professional',]$GPA + 12000)

# Predict Earnings of subjects with Education in Other fields.
dat[dat$Major=='Other',]$predEarnings <- (dat[dat$Major=='Other',]$Number_Of_Professional_Connections^2 + 5000)

# Predict Earnings of subjects with Education in Business field.
dat[dat$Major=='Buisness',]$predEarnings <- (100*dat[dat$Major=='Buisness',]$GPA*((dat[dat$Major=='Buisness',]$Graduation_Year+1)%%2) + 10000)


# Compare the predicted Earnings values with the ideal Earnings values in test data.
library(ModelMetrics)
mse(dat$Earnings,dat$predEarnings)


```

We can see that the perfect model, based on knowledge of the way data was generated, achieved  MSE of around 3300. Certainly prediction models with MSE close to 3300 would be considered very good.  Again, at first quite surprisingly, there were quite a number of submissions with MSE of prediction models being far less than 3300.

Explanation of how the perfect model could be so soundly beaten is “getting away” with overfitting models for the case of just single testing data set.


---


### Top Submissions for Challenge 3

For this prediction challenge, the MSE score below 30000 was considered a Passing score.

1. Seok Yim     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred31">Seok's PPT</button>
<div id="pred31" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/seokpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was the top submission based on MSE score, with a final score less than 100.
  - Seok’s analyzed data using plots first and quickly discovered that subsetting data based on Education is the way to go
  - For each subset (based on a different value of Education attribute)  he subsequently built a different model
  - In comparison to the perfect model accuracy, this MSE was over one order of magnitude smaller, just like Nick Whelan’s solution below.  
  
  
2. Nick Whelan     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred32">Nick's PPT</button>
<div id="pred32" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/nickpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was another top submission based on MSE score, with final score less than 100.
  - The approach to solving the task was different compared to Seok's implementation, but was equally good, with nearly the same prediction power/accuracy.
    - Nick tried to use the randomForest algorithm on the whole dataset as the initial model, but the MSE turned out to be near 25,000.
    - Then he did some free-style analysis and found the linear relationship between various subsets of dataset with the *earnings* value.
    - To implement this he used the fundamentals of linear regression very well while creating a learning model, and also used a quadratic model where needed.
  - This resulted in a very accurate model with low MSE score.


3. Bennett Garcia     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred33">Bennett's PPT</button>
<div id="pred33" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/bennetpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>
    
  - Bennett had a final MSE score of below 100 and was one of the top submissions for this challenge.
  - A significantly different learning model was used by Bennett to achieve this low MSE.
    - He first analyzed the data, and found attributes on which the dataset can be subsetted on.
    - Then, he here used Neural Networks as models for prediction on those subsets.
    - This Neural Network approach was very well implemented.
    
---

    
## Prediction Challenge 4.

The challenge 4 involved “hidden” derived attributes. We have hoped that students who combined prior data inspection (plotting) with machine learning packages from R library will be able to achieve very high accuracy, certainly over 90%.  Unfortunately, none of the student solutions came even close to the perfect model accuracy, which was 92%.  Pretty much all student solutions centered around the upper 60s percentile. Even the top 3 solutions barely exceeded 68%! 

Let us first describe the challenge and the way data was generated.


*Mysterious box was found on the beach. *

*Despite spending probably years in the water, it still works! *

*But what does it do? *

*It has four inputs (electric) & a switch. Setting these inputs and different switch positions emits various weird and scary sounds as output in response to the electric signals. *

*It sizzles, gurgles, hisses, ominously tics like a bomb,etc.....but nothing happens - just sounds. So no harm will happen to surroundings.*

*Predict the output sound of the box based on one of the four inputs INPUTs, INPUT1,...INPUT4 (numerical values) and SWITCH which has five discrete positions.*

Let's look at a snippet of the Box on the Beach  dataset used for training the models below. The training describes which sounds have been noted in the laboratory in nearly 20,000 experiments combining different input signals and switch positions.


```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxtrainApril22.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Black Box Dataset(TRAINING) for Prediction Challenge 4',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")

```


<!-- Lets look at the snippet of the black box dataset for testing. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22-students.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Black Box Dataset(TESTING) for Prediction Challenge 4', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see that the *Sound* attribute is not present in this dataset, since it is the attribute that will be predicted using our analysis of the training dataset. -->

<!-- Also, lets look at the submission file for prediction challenge 4. -->

<!-- ```{r,echo=FALSE} -->
<!-- realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22-submission.csv") #web load -->
<!-- temp<-knitr::kable( -->
<!--   head(realestate, 10), caption = 'Snippet of Submission file for Prediction Challenge 4', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- library(kableExtra) -->
<!-- kableExtra::scroll_box(temp,width = "100%") -->
<!-- ``` -->

<!-- We can see there are only 2 columns *ID* and *Sound*. *ID* column's entries corresponds/are similar to the *ID* column of the testing data. Thus we need to just fill the *Sound* column with appropriate earning value predicted by our analysis. -->

<!-- Now that we have seen the data, feel free to go to the Kaggle site of this prediction challenge and take the challenge yourself. The link for challenge: [Prediction Challenge 4](https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe){target="_blank"} -->

---

### How the data was generated for Challenge 4

The value of Sound attribute (the output sound of the box) was dependent on the four input attributes: INPUT1,...INPUT4  and one of the five switch positions: Low, Minimum, Medium, Maximum and High. 

Data was generated in two phases. In the first phase a derived attribute called OUTPUT was defined. In the second phase a simple decision tree using just OUTPUT was created which defined the output sound of the box.

The derived attribute OUTPUT was created as different linear combinations of INPUT1...INPUT4 attributes depending on the value of SWITCH.

First we ordered the values of SWITCH attribute as follows


``` text

The ordering of Switch position is given as:
  Low = 1
  Minimum = 2
  Medium = 3
  Maximum = 4
  High = 5

if Switch == 1 i.e. "Low" 
    then OUTPUT = Input 1+ 5 * Input 2  - 2 * Input3  + sample(2:5,1)
if Switch == 2  i.e. "Minimum"  
    then OUTPUT = 3* Input 2 - 2 * Input 4  + sample(2:3,1)
else  i.e. Position other than "Low" and "Minimum"
    then OUTPUT =  Input1 ^2 -1.5 * Input 3 + sample(5:10,1)


Then SOUND totally depends on OUTPUT attribute, but is distributed probabilistically over all possible sound.

For example, the SOUND when OUTPUT>150 is distributed as 0, 0, 10, 0, 10, 60, 20.
This number list corresponds to Gargle, Tick, Beep, Kaboom, Rumble, Sizzle, Hiss. And thus we can see that "Sizzle" sound has the max probability of 60%, and is this the most likely sound when the OUTPUT value is above 150.


      OUTPUT > 150 -> Max Probability of finding "Sizzle"
100 < OUTPUT < 150 -> Max Probability of finding "Rumble"
 70 < OUTPUT < 100 -> Max Probability of finding "Kaboom"
 50 < OUTPUT < 70  -> Max Probability of finding "Hiss"
 20 < OUTPUT < 50  -> Max Probability of finding "Tick"
      OUTPUT < 20  -> Max Probability of finding "Gargle"

```

The OUTPUT  variable is created by three different linear combinations of INPUT1,...INPUT4 variables   depending on different values of SWITCH attributes. Notice that the same formula is used for  “Medium”, “Maximum” and “High” positions of the SWITCH. The only two SWITCH positions which have different linear combinations of INPUT1, INPUT2, INPUT3 and INPUT4  defining OUTPUT are “Low” and “Minimum”.

- **How the data was generated for Challenge 4 in R**

```text

# Load The Data
dat<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22_answer.csv",stringsAsFactors = T)

dat$OUTPUT <- rep(0,nrow(dat))

dat$OUTPUT <- ((dat$INPUT1^2) - (1.5*dat$INPUT3) + sample(5:10,1))

dat[dat$SWITCH == "Low",]$OUTPUT <- (dat[dat$SWITCH == "Low",]$INPUT1 + (5*dat[dat$SWITCH == "Low",]$INPUT2) - (2*dat[dat$SWITCH == "Low",]$INPUT3) + sample(2:5,1))

dat[dat$SWITCH == "Minimum",]$OUTPUT <- ((3*dat[dat$SWITCH == "Minimum",]$INPUT2) - (2*dat[dat$SWITCH == "Minimum",]$INPUT4) + sample(2:3,1))


dat$predSound <- rep('Empty',nrow(dat))
dat[dat$OUTPUT>150,]$predSound<-'Sizzle'
dat[dat$OUTPUT>=100 & dat$OUTPUT<150,]$predSound<-'Rumble'
dat[dat$OUTPUT>=70 & dat$OUTPUT<100,]$predSound<-'Kaboom'
dat[dat$OUTPUT>=50 & dat$OUTPUT<70,]$predSound<-'Hiss'
dat[dat$OUTPUT>=20 & dat$OUTPUT<50,]$predSound<-'Tick'
dat[dat$OUTPUT<20,]$predSound<-'Gargle'


mean(dat$SOUND==dat$predSound)
```

The accuracy of the perfect model created using the above rules, was 92%.  

To our disappointment this  challenge was not a success. And this is by no means the students fault. It was our fault - since the challenge was really not fair. None of our teaching assistants was able to better the student solutions and therefore each solution had accuracy almost 25% below the accuracy of the perfect model.  In fact almost everyone’s model made the range of 65% to 68%. 

Why such a huge discrepancy? Especially that for the first two challenges, students actually got away with some overfitting and even managed to beat the perfect model by a few percentage points. 

We suspect that adding noise to the linear formulas defining OUTPUT for different SWITCH positions may have created problems for simple rpart() application based just on five independent variables INPUT1...INPUT4, SWITCH.  One certainly could not expect from someone who has no hints about the manner the derived attribute OUTPUT was defined, to be able to find the definition (not to mention even guess the existence of)  of OUTPUT. Certainly, if OUTPUT’s definition was known, a simple rpart prediction model  using just OUTPUT would achieve accuracy exceeding 90%.

---

### Top Submissions for Challenge 4

Since this challenge involved stochastically generated data, the prediction accuracy required for passing this challenge was above 60%.

1. Nicole Coria     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred41">Nicole's PPT</button>
<div id="pred41" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/nicolepred41.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was the top submission based on accuracy score, with a final score more than 68.7%
  - The approach to solving this challenge was iterative and trail and error based. 
    - First, since the task is to predict categorical data, she decided to use rpart(directly).
    - Then, over iteration, by varying the control parameters of rpart, she tried to find the model with the highest accuracy.
  - Use of cross-validation also helped in finding the best fit model.
  
  
2. Atharva Patil     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred42">Atharva's PPT</button>
<div id="pred42" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/atharvapred41.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was another top submission based on accuracy score, with final score above 68%
  - The approach to solving the task was very well implemented, using external resources too.
    - Atharva tried to analyze the data first. To do this, he used Prof. Imielinski's online platform called [Boundless Analytics](http://www.foreveranalytics.com){target="_blank"}.
      - This online platform has ability to analyze the data automatically, and create plots which only matter or provide more information about the data.
      - It eliminates the need to perform the data analysis manually.
  - Then, he proceeded by building the model using the rpart() function and control parameters.


3. Andrew Scovell     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred43">Andrew's PPT</button>
<div id="pred43" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/andrewpred4.pdf&embedded=true" width="100%" height="500px"></embed>
</div>
    
  - Bennett had a final accuracy score of above 68% and was one of the top submissions for this challenge.
  - He did a very extensive data analysis using all the attributes of the dataset.
    - He also tried analyzing using mean, sums, standard deviation, etc of the numerical inputs.
  - Using the control parameters of the rpart() function he tried to find the best fitting model, and used cross-validation to avoid overfitting.
  
---

To perform any of the above challenges yourself, visit the appropriate links.

1. Prediction Challenge 1 [https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d](https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d){target="_blank"}
1. Prediction Challenge 2 [https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b](https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b){target="_blank"}
1. Prediction Challenge 3 [https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd](https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd){target="_blank"}
1. Prediction Challenge 4 [https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe](https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe){target="_blank"}
    
