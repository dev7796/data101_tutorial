# p-value: Simple Statistical Evaluation {#stateval}

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE,height=700)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

Randomness is the biggest enemy of your findings. In order to convince your audience that you have found something worthwhile in the data  you need to address the question “how do you know that your result is simply not sheer luck, that  it is not random?”

This is where the concept of p-value enters the picture.

What is the p-value?

First,  there must be a hypothesis.  In fact, at least a pair of them - one which is called the **NULL hypothesis** and another (the one which you hope you can defend), the **ALTERNATE hypothesis**. 

Informally, NULL hypothesis  is what a SKEPTIC would argue.  Skeptic, who says, “Data does not support your findings” in fact, your findings are random. Some call the NULL hypothesis - the boring one. Nothing really happens in your data. 

You, the data scientist, hope you can reject the NULL hypothesis! Show that it is unlikely to see the observed data  by random chance  UNDER NULL HYPOTHESIS. 
Here are some examples of NULL and ALTERNATIVE hypotheses: 

**Examples of NULL hypotheses**

Traffic in Holland Tunnel is same as in Lincoln Tunnel

French Wines are equally priced as Californian Wines

Crime has not changed as result of Gun laws

Unemployment has not changed as result of tax reductions

Democrat  candidate is tied with  GOP candidate

**Examples of ALTERNATE hypotheses**

Traffic in Holland Tunnel is higher as in Lincoln Tunnel

French Wines are less expensive than  Californian Wines

Crime changed as result of Gun laws

Unemployment has decreased  as result of tax reductions

Democrat candidate is leading GOP candidate

Notice that the alternate hypothesis is not simply the negation of the null hypothesis. It is mutually exclusive with the NULL hypothesis though.  Some of the alternate hypotheses above are two tied, others are one-tied. For example Crime changed as result of Gun laws is an example of two tied alternate hypotheses.  No statement is made about how the crime has moved, has it increased, or has it decreased?  The alternate hypothesis  simply states that Crime has changed.  Compare it with alternate hypothesis that Unemployment has decreased  as a result of tax reductions.  This is a one-tied alternate hypothesis, which is stronger than the Two-tied hypothesis, stating  that unemployment has decreased.  Thus, it states that unemployment was affected by tax reductions and that it has decreased.

Let us now focus on one of these examples. 

**NULL HYPOTHESIS  (“The Skeptic”)**

There is no traffic difference  between Lincoln  and Holland tunnels

**ALTERNATE  HYPOTHESIS   (Data Scientist)**

Lincoln is busier than Holland

The p-value is defined as the probability of obtaining a result equal to or "more extreme" than what was actually observed, when the null hypothesis is true. 

Thus, p-value is the CONDITIONAL probability. Conditional upon null hypothesis being true

In other words: showing that obtained result could have been generated by RANDOM CHANCE under assumption of null hypothesis with probability exceeding the critical value alpha.  It was just a “fluke” - which can be attributed to randomness and cannot be used to support the alternate hypothesis.  In the case of our example, There is no traffic difference  between Lincoln  and Holland tunnels, even though the data seems to indicate that Lincoln is busier than Holland.  But evidence for the latter is not stored enough.  There is, say, p chance  (say 0.10) that observed data was obtained as a result of random fluke and tunnels are equally busy. Generally, if p is larger than 0.05, the observed data is not strong enough to reject the NULL.

Now we discuss the permutation test which helps us to understand the role of randomness in producing seemingly meaningful evidence. 

---

```{r child="./chapters/permutationtest.Rmd"}

```

---


## Multiple Hypothesis - Bonferroni Correction. {#bonferroni}

```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE,height=700)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

<script src="files/js/dcl.js"></script>

Multiple hypothesis testing refers to  simultaneously  testing  more than one alternate hypothesis against the same NULL hypothesis. This often occurs in the process of so-called p-value chasing. In p-value chasing, a number of alternative hypotheses  are tested until one with acceptable p-value is found. The process of p-value chasing is inherently flawed. Since it is prone to simply randomly finding an alternate hypothesis with p-value below the critical value alpha. Thus, we need to make our critical value requirements much stricker. Depending how many hypotheses are tested, we may require a much smaller p-value to reject the NULL.

Say, we have tested 100 alternate hypotheses. The probability of getting at least one significant result with $\alpha$ = 0.05 can be computed as follows:

$$P(\text{at least one significant result}) = 1- (1-0.05)^{100} ≈ 0.99$$

This means that if we continue to consider 0.05 as the critical value for p-value, then the probability of getting at least one significant result will be about 99%, which leads to  false rejection of the NULL. The more hypotheses we test, we randomly will find one which will reject the NULL. Does it mean that testing multiple alternate hypotheses is forbidden?  No, it cannot be. We do it all the time. We just have to change our p-value requirements. It has to be much harder to reject the NULL, when testing multiple alternate hypotheses.  

One such method for adjusting $\alpha$ is *BONFERRONI CORRECTION!*

The Bonferroni correction sets the significance cut-off at $\alpha / N$ where N is the number of possible hypotheses. 

For example, in the example above, with 100 tests and $\alpha = 0.05$, you’d only reject a null hypothesis if the p-value is less than $\alpha/N = 0.05/100 = 0.0005$

Thus, the value of $\alpha$ after Bonferroni correction would be $0.0005$.

Again, let’s calculate the probability of observing at least one significant result when using the correction just described:

$$P(\text{at least one significant result}) = 1 − P(\text{no significant results}) \\
= 1 − (1 − 0.0005)^{100} ≈ 0.048$$

This gives us a 4.8% probability of getting at least one significant result randomly under NULL hypothesis.  It is less than 5% and we can reject the NULL, after Bonferroni correction.

As we can see this value of probability using Bonferroni correction is much better than the 99% which we saw before when we did not use correction for performing multiple hypothesis testing.

The challenge in calculating the Bonferroni correction is correct estimation of the number of multiple hypotheses tested. How do we know what this number really is? For example, if we want to find two items in a supermarket which sell together more often than expected, we may find such a pair after testing all possible pairs of items. With a total number of items in thousands, this would lead to millions of possible pairs and millions of hypotheses tested. Bonferroni correction would then exceed million, and possibly even tens of millions. This would lead to miniscule critical value of 10^-7 or less for the p-value to meet.  

One may consider it too strict. In fact, the data scientist performing these tests may argue that s/he found the correlated pair of items after only 10 tests, not several millions of tests. Should one trust the data scientist? Or should the worst case number of necessary tests be considered?  Both solutions are problematic. The former one relies on trust, why should we trust the data scientist? The latter one seems to conservative and stringent, making it virtually impossible to reject the NULL. This applies in particular to multidimensional data with a large number of dimensions.  

The larger number of dimensions, the larger (and exponentially so) the potential number of multiple hypotheses. Thus, even though there is more potential for exciting associations, the threshold for p-value is also more and more unlikely to be met. Thus rejection of NULL is exponentially harder. Dimensions (variables) may be correlated, and often are, but Bonferoni protects us assuming all hypotheses are independent. Too conservative and too pessimistic. These are concerns about the Bonferroni coefficient. 

For sure, however, the Bonferoni coefficient protects us against false rejection of NULL. It may prevent us from correct rejection of NULL, though.

---

### Examples for Multiple hypothesis testing.

Let’s consider the Happiness dataset as an example.
<script src="files/js/dcl.js"></script>


```{r,echo=FALSE,error=FALSE,warning=FALSE,tut=FALSE}
library(kableExtra)
earningdata<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/HAPPINESS2017.csv") #web load
temp<-knitr::kable(earningdata[sample(nrow(earningdata),10),], caption = 'Snippet of Happiness Dataset',booktabs=TRUE) 
kableExtra::scroll_box(temp,width = "100%")
```

There are 156 unique countries in the dataset. 
This can be checked using the unique() function – `unique(indiv_happiness$country)`

Since there are 156 distinct countries, we have ${{n}\choose{2}} = {156\choose2}=(156 * 155)/2 = 12090$ different hypotheses. Let’s call this value N. 

Using this N, the P-value cutoff after Bonferroni correction will be, $α = 0.05 / 12090 ≈ 4.13 *10^{-6}$

#### Example 1 

Let’s calculate the P-value for the following hypotheses from the dataset.

- Our hypothesis: People from Canada are happier than people from Iceland.
- Null hypothesis: There is no difference in happiness levels of people from Canada and people from Iceland.

<script src="files/js/dcl.js"></script>

```{r,tut=TRUE}
# Load dataset
happiness <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/HAPPINESS2017.csv", stringsAsFactors = T) #web load

# Two subsets of Canada and Iceland 
happiness.canada <- subset(happiness$HAPPINESS, happiness$COUNTRY =="Canada")
happiness.iceland <- subset(happiness$HAPPINESS, happiness$COUNTRY == "Iceland")

# Mean of subsets.
mean.canada <- mean(happiness.canada)
mean.iceland <- mean(happiness.iceland)

mean.canada
mean.iceland

# Length of subsets
len.canada <- length(happiness.canada)
len.iceland <- length(happiness.iceland)

# Standard Deviation of Subsets.
sd.canada <- sd(happiness.canada)
sd.iceland <- sd(happiness.iceland)

# Calculating Z-score 
zeta <- (mean.canada - mean.iceland)/ sqrt((sd.canada^2)/len.canada + (sd.iceland^2)/len.iceland)
zeta

# Calculate p-value from Z-score
p_value <- pnorm(-zeta)
p_value
```

In this case, after applying Bonferroni Correction we get the value of $α = 0.05/12090 ≈ 4.14 * 10^{-06}$
Here, we get the p-value of 0.25 which is much higher than the value of our α.
Based on this we fail reject our null hypothesis.

#### Example 2
<script src="files/js/dcl.js"></script>

Let’s consider the following hypotheses from the dataset.

- Our hypothesis: People from Italy are happier than people from Afghanistan.
- Null hypothesis: There is no difference in happiness levels of people from Italy and people from Afghanistan.

```{r,tut=TRUE}
# Load dataset
happiness <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/HAPPINESS2017.csv", stringsAsFactors = T) #web load

# Two subsets of Italy and Afghanistan 
happiness.italy <- subset(happiness$HAPPINESS, happiness$COUNTRY =="Italy")
happiness.afghanistan <- subset(happiness$HAPPINESS, happiness$COUNTRY == "Afghanistan")

# Mean of subsets.
mean.italy <- mean(happiness.italy)
mean.afghanistan <- mean(happiness.afghanistan)

mean.italy
mean.afghanistan

# Length of subsets
len.italy <- length(happiness.italy)
len.afghanistan <- length(happiness.afghanistan)

# Standard Deviation of Subsets.
sd.italy <- sd(happiness.italy)
sd.afghanistan <- sd(happiness.afghanistan)

# Calculating Z-score 
zeta <- (mean.italy - mean.afghanistan)/ sqrt((sd.italy^2)/len.italy + (sd.afghanistan^2)/len.afghanistan)
zeta

# Calculate p-value from Z-score
p_value <- pnorm(-zeta)
p_value
```
In this case, after applying Bonferroni Correction we get the value of $α = 0.05/12090 ≈ 4.14 * 10^{-06}$

Here, we get the p-value of 0.00364 which is lower than the value of default p-value cutoff $α = 0.05$, but this obtained p-value is higher than our Bonferroni correction cutoff.

So, based on the results, we fail to reject our null hypothesis even though the obtained p-value is less than 0.05.

---

**EOC**
