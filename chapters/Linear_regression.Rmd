# ðŸ”– Linear Regression {#lr}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#lr12"> Linear Regression </button> 
<div id="lr12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1RuAidmZGDoTRYMjONLEsw3bfx_aQb-f3C0V4Jjk4Vzs/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

Linear regression is a linear approach to modeling the relationship between a numerical response ($Y$) and one or more independent variables ($X_i$).

Usually in linear regression, models are used to predict only one scalar variable. But there are two subtype if these models:
- First when there is only one explanatory variable and one output variable. This type of linear regression model known as simple linear regression.
- Second, when there are multiple predictors, i.e. explanatory/dependent variables for the output variable. This type of linear regression model known as multiple linear regression.

![Linear models fitted to various different type of data spread. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. Credits: Wikipedia.](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling2/lmvariants.svg)

## Linear regression using lm() function {#lm}

Syntax for building the regression model using the *lm()* function is as follows:

- `lm(formula, data, ...)`
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *data*: here we provide the dataset on which the linear regression model is to be trained.
  
For more info on the *lm()* function visit [lm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm)

Lets look at the example on the Moody dataset.

```{r,echo=FALSE}
moodyNUM<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv')
temp<-knitr::kable(
  head(moodyNUM, 10), caption = 'Snippet of Moody Num Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

Now we can build a simple linear regression model to predict the ClassScore attribute based on the various other attributes present in the dataset, as shown above.

Since we will be predicting only one attribute values, this model will be called simple linear regression model.

### Snippet 1: How much do Midterm, Project and Final Exam count?

```{r,tut=TRUE,height=500}
moodyNUM<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv')
split<-0.7*nrow(moodyNUM)
split
moodyNUMTr<-moodyNUM[1:split,]
moodyNUMTr
moodyNUMTs<-moodyNUM[split:nrow(moodyNUM),]
#We use linear regression to #find out the weights of #Midterm, Project and Final #Exam in calculation of the #final class score. Each of #them are scored out of 100 and #the final class score is also #scored out of 100 as weighted #sum of Midterm, Project and #Final Exam scores.
train <- lm(ClassScore~.,  data=moodyNUMTr)
train
pred <- predict(train,newdata=moodyNUMTs)
mean((pred - moodyNUMTs$ClassScore)^2)
```

We can see that, 

- The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc.
- The predicted values are obtained from the predict function using the trained model and the test data.

## Calculating the Error using mse() {#mse}

As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test won't prove useful now in our numerical predictions scenario.

We don't want to eyeball every time we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique.

To do this we will use the Mean Squared Error(MSE).

- The MSE is a measure of the quality of an predictor/estimator
- It is always non-negative
- Values closer to zero are better.

The equation to calculate the MSE is as follows:

\begin{equation}
MSE=\frac{1}{n} \sum_{i=1}^{n}{(Y_i - \hat{Y_i})^2}
\\ \text{where $n$ is the number of data points, $Y_i$ are the observed value}\\ \text{and $\hat{Y_i}$ are the predicted values}
\end{equation}

To implement this, we will use the *mse()* function present in the Metrics Package, so remember to install the Metrics package and use `library(Metrics)` in the code for local use.

The syntax for *mse()* function is very simple:

- `mse(actual,predicted)`
  - *actual*: vector of the actual values of the attribute we want to predict.
  - *predicted*: vector of the predicted values obtained using our model.


## Snippet 2: Cross Validate your prediction

```{r,tut=TRUE,height=600}
library(ModelMetrics)

train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")
#scramble the train frame
v<-sample(1:nrow(train))
v[1:5]
trainScrambled<-train[v, ]

#one step crossvalidation
n <- 100
trainSample<-trainScrambled[nrow(trainScrambled)-n:nrow(trainScrambled), ]
testSample <- trainScrambled[1:n,]

lm.tree <- lm(ClassScore~.,  data=trainSample)
lm.tree

pred <- predict(lm.tree,newdata=testSample)
pred

mse(testSample$ClassScore,pred)

  
```

We can see that,

- The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc.
- The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the previous model we are using the cross validation technique to check that if we have more accurate predictions, thus increasing the overall accuracy of the model.

## Snippet 3: Submission with lm

```{r,tut=TRUE,height=600}
library(rpart)
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM_test.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')
train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")

tree <- lm(ClassScore~.,  data=train)
tree

prediction <- predict(tree, newdata=test)

#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-prediction 

# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle
```