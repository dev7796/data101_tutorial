[["index.html", "DATA 101 Shortest Textbook How To Accomplish More With Less ", " DATA 101 Shortest Textbook How To Accomplish More With Less Tomasz Imielinski 2022-01-17 This is a textbook based on the DATA 101 course thought by Prof. Tomasz Imielinski at Computer Science Department at Rutgers University, New Brunswick. Data 101 is an introductory course for beginners from any field of study, interested in the field of Data Science. This book’s pages are created using Rmarkdown from Rstudio (similar to jupyter notebook) and the book is compiled using “bookdown” package. The most important aspect of this interactive book are the interactive code chunks for running code, which are powered by a minimal version of Datacamp’s learning interface called “Datacamp Light”. For more info visit Datacamp Light Note1: This book is undergoing constant updates following along with the course thought in Spring 2021. Topics under progress are marked with a \" * \". Note2: This book uses Datacamp Light for supporting runnable code chunks. In case the code chunks do not connect to run-time session, please copy the code and run in RStudio. Also, please report if facing this issue to the instructor via email. "],["intro.html", "Chapter 1 Introduction 1.1 Setting Up R", " Chapter 1 Introduction The objective of this textbook is to provide you with the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in a statistically convincing manner, perhaps in a colorful and visually appealing way. Questions which you will have to anticipate and you will have to answer are - How do you know that your findings are not random? - And fundamental of all questions: - So what? Even the most impressing looking results may come up randomly. And you will be asked this question along with the question “what was your p-value and how did you compute it” And even if you convince your audience that your results are not random, you will have to be ready to explain why your audience should care about the results you reported. In other words, is there any actionable value in your results? Or they are just simply interesting, good to know, but no one really needs to care much about them otherwise? Hopefully it is the former not the latter. In the following sections we will address these questions and go through the process of data exploration, validation, and presentation. We will start with making plots, follow with free style data exploration – which allows us to form the leads, that is hypotheses. Then we will follow with simple statistical tests which will allow us to validate these hypothesis and defend our findings against randomness claims. - We will learn how to calculate p-values and how to use them to defend our findings. We will use as few R commands as possible and reach our goal in the shortest possible path. In fact we will demonstrate how using just 7 R commands we can perform quite sophisticated data exploration. In the appendix, we show many more useful commands of R which eventually you would have to use. However, our goal in this short textbook, is to present the shortest path to data analysis which will let you import the data, plot it, make some analysis yourself and use R-libraries. In this textbook and in this class we do not teach how to clean the data (data wrangling) and how to deal with a wide variety of data types. We also do not address complex data transformations such as multi-frame operations like merge (we show them in appendix). We also do not explain how different machine learning methods work, we only show you how to use them. It is similar to teaching one how to drive a car without knowing how a car engine works. 1.1 Setting Up R Important Instructions Installation of R is required before installing RStudio “R” is a programming language, and, “RStudio” is an Integrated Development Environment (IDE) which provides you a platform to code in R. How to download and install R &amp; RStudio? Downloading and installing R. For Windows Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cran.r-project.org/bin/windows/base/ Click on the link at top left where it says “Download R 4.0.3 for windows” or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. For MAC Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cloud.r-project.org/bin/macosx/ Under “Latest release”, click on “R-4.0.3.pkg” or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. Downloading and installing RStudio. For Windows Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named “All Installers”. Click on the download link beside “Windows 10/8/7” to download the windows version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. For MAC Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named “All Installers”. Click on the link beside “macOS 10.13+” to start your download the MAC version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. How to upload a data set? To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon. Let us look at the example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFJlYWQgaW4gdGhlIGRhdGFcbmRmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMGIuY3N2XCIpXG5cbiMgUHJpbnQgb3V0IGBkZmBcbmhlYWQoZGYpIn0= "],["dataexp.html", "Chapter 2 Data Exploration 2.1 Plots 2.2 Free Style data exploration with just seven R commands \" R.7 \" 2.3 Professor Moody Puzzle", " Chapter 2 Data Exploration 2.1 Plots Table 2.1: Snippet of Moody Dataset score grade texting questions participation 26.89 F never never 0.41 71.57 B always rarely 0.00 90.11 A always never 0.27 31.52 D sometimes rarely 0.68 95.94 A always rarely 0.09 45.72 D always rarely 0.19 90.82 A always always 0.25 75.52 B sometimes never 0.28 52.31 C never never 0.67 39.57 D always always 0.40 2.1.1 Scatter Plot Scatter Plot are used to plot points on the Cartesian plane (X-Y Plane) Hence it is used when both the labels are numerical values. Lets look at example of scatter plot using Moody. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExldCdzIGxvb2sgYXQgYSAyIGF0dHJpYnV0ZSBzY2F0dGVyIHBsb3QuXG4jIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5wbG90KG1vb2R5JHBhcnRpY2lwYXRpb24sbW9vZHkkc2NvcmUseWxhYj1cInNjb3JlXCIseGxhYj1cInBhcnRpY2lwYXRpb25cIixtYWluPVwiIFBhcnRpY2lwYXRpb24gdnMgU2NvcmVcIixjb2w9XCJyZWRcIikifQ== 2.1.2 Bar Plot A bar plot is used to plot rectangular bars proportional to the values present in a numerical vector. This rectangle height is proportional to the value of the variable in the vector. Barplots are also used to graphically represent the distribution of a categorical variable, after converting the categorical vector into a table(i.e. frequency distribution table) In a bar plot, you can also give different colors to each bar. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cbiNsZXRzIG1ha2UgYSB0YWJsZSBmb3IgdGhlIGdyYWRlcyBvZiBzdHVkZW50cyBhbmQgY291bnRzIG9mIHN0dWRlbnRzIGZvciBlYWNoIEdyYWRlLiBcblxudDwtdGFibGUobW9vZHkkZ3JhZGUpXG5cbiNvbmNlIHdlIGhhdmUgdGhlIHRhYmxlIGxldHMgY3JlYXRlIGEgYmFycGxvdCBmb3IgaXQuXG5cbmJhcnBsb3QodCx4bGFiPVwiR3JhZGVcIix5bGFiPVwiTnVtYmVyIG9mIFN0dWRlbnRzXCIsY29sPWNvbG9ycyxcbm1haW49XCJCYXJwbG90IGZvciBzdHVkZW50IGdyYWRlIGRpc3RyaWJ1dGlvblwiLGJvcmRlcj1cImJsYWNrXCIpIn0= 2.1.3 Box Plot A boxplot shows the distribution of data in a dataset. A boxplot shows the following things: Minimum Maximum Median First quartile Third quartile Outliers You can create a single boxplot using just a vector or a multiple boxplot using a formula. When you write a formula, you should use the Tilde (~) operator. This column name on the left side of this operator goes on the y axis and the column name on the right side of this operator goes on the x axis. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cblxuI1N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCB0aGUgZGlzdHJpYnV0aW9uIG9mIHN0dWRlbnRzIHNjb3JlIHBlciBHcmFkZS4gV2UgdXNlIGJveCBwbG90IGZvciBnZXR0aW5nIHRoYXQuIFxuYm94cGxvdChzY29yZX5ncmFkZSxkYXRhPW1vb2R5LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJTY29yZVwiLCBtYWluPVwiQm94cGxvdCBvZiBncmFkZSB2cyBzY29yZVwiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIilcblxuIyB0aGUgY2lyY2xlcyByZXByZXNlbnQgb3V0bGllcnMuIn0= 2.1.4 Mosaic Plot Mosaic plot is a graphical method for visualizing data from two or more qualitative variables. The length of the rectangles in the mosaic plot represents the frequency of that particular value. The width and length of the mosaic plot can be used to interpret the frequencies of the elements. For example, if you want to plot the number of individuals per letter grade using a smartphone, you want to look at a mosaic plot. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cbiNzdXBwb3NlIHlvdSB3YW50IHRvIGZpbmQgbnVtYmVycyBvZiBzdHVkZW50cyB3aXRoIGEgcGFydGljdWxhciBncmFkZSBiYXNlZCBvbiB0aGVpciB0ZXh0aW5nIGhhYml0cy4gVXNlIE1vc2lhYy1wbG90LlxuXG5tb3NhaWNwbG90KG1vb2R5JGdyYWRlfm1vb2R5JHRleHRpbmcseGxhYiA9ICdHcmFkZScseWxhYiA9ICdUZXh0aW5nIGhhYml0JywgbWFpbiA9IFwiTW9zaWFjIG9mIGdyYWRlIHZzIHRleGluZyBoYWJpdCBpbiBjbGFzc1wiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIikifQ== 2.2 Free Style data exploration with just seven R commands \" R.7 \" We have to know 7 commands in R which we call R.7. 4 commands to plot (barplot, boxplot, plot and histogram) These we have already discussed. One command to slice and dice data: subset() or df[condition,]$variable 2.2.1 Subset eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG4jU3Vic2V0IG9mIHJvd3Ncbm1vb2R5X25ldmVyX3NtYXJ0cGhvbmU8LXN1YnNldChtb29keSxPTl9TTUFSVFBIT05FPT1cIm5ldmVyXCIpXG5ucm93KG1vb2R5KVxubnJvdyhtb29keV9uZXZlcl9zbWFydHBob25lKVxudGFibGUobW9vZHlfbmV2ZXJfc21hcnRwaG9uZSRPTl9TTUFSVFBIT05FKSAjIFlvdSBjYW4gc2VlIG9ubHkgc3R1ZGVudCBuZXZlciBvbiBzbWFydHBob25lIGFyZSBpbiB0aGUgc3Vic2V0LlxuXG4jQWx0ZXJuYXRlIHdheSB0byBzdWJzZXQuXG5tb29keV9uZXZlcl9zbWFydHBob25lX2FsdDwtbW9vZHlbbW9vZHkkT05fU01BUlRQSE9ORT09XCJuZXZlclwiLCBdXG50YWJsZShtb29keV9uZXZlcl9zbWFydHBob25lX2FsdCRPTl9TTUFSVFBIT05FKSAjIFlvdSBjYW4gc2VlIGEgc2ltaWxhciB0YWJsZSBhcyBhYm92ZS5cblxuXG4jc3Vic2V0IG9mIGNvbHVtbnNcbm1vb2R5X2V4Y2VwdDg8LXN1YnNldChtb29keSwgc2VsZWN0ID0gLWMoOCkpXG5uY29sKG1vb2R5KVxubmNvbChtb29keV9leGNlcHQ4KSAjIFlvdSBjYW4gc2VlIHRoZSBudW1iZXIgb2YgY29sdW1ucyBoYXMgYmVlbiByZWR1Y2VkIGJ5IDEsIGR1ZSB0byBzdWJzZXR0aW5nIHdpdGhvdXQgY29sdW1uIDhcblxuI1N1YnNldCBvZiBSb3dzIGFuZCBDb2x1bW5zXG5tb29keV9leGNlcHQ4X25ldmVyPC1zdWJzZXQobW9vZHksIHNlbGVjdCA9IC1jKDgpLCBPTl9TTUFSVFBIT05FID09IFwibmV2ZXJcIilcbnRhYmxlKG1vb2R5X2V4Y2VwdDhfbmV2ZXIkT05fU01BUlRQSE9ORSlcbmRpbShtb29keSlcbmRpbShtb29keV9leGNlcHQ4X25ldmVyKSMgWW91IGNhbiBzZWUgb25seSBzdHVkZW50IG5ldmVyIG9uIHNtYXJ0cGhvbmVzIHdpdGhvdXQgY29sdW1uIDggZGF0YSBhcmUgcHJlc2VudCBpbiB0aGUgc3Vic2V0LiJ9 2.2.1.1 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxubW9vZHlbbW9vZHkkU0NPUkU+PTkwLDNdXG4jIFdoYXQgd2lsbCBSIHNheT9cblxuXG4jIEEuIEdldCBzdWJzZXQgb2YgYWxsIGNvbHVtbnMgd2hpY2ggY29udGFpbnMgc3R1ZGVudHMgd2hvIHNjb3JlZCBtb3JlIHRoYW4gZXF1YWwgdG8gOTBcbiMgQi4gZXJyb3JcbiMgQy4gZ2V0IGFsbCBzY29yZSB2YWx1ZXMgd2hpY2ggYXJlIG1vcmUgdGhhbiBlcXVhbCB0byA5MFxuIyBELiBnZXQgc3Vic2V0IG9mIG9ubHkgdGhlIGdyYWRlcyBvZiBzdHVkZW50cyB3aXRoIHNjb3JlIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA5MCJ9 2.2.1.2 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxubW9vZHlbbW9vZHkkU0NPUkU+PTgwLjAgJiBtb29keSRHUkFERSA9PSdCJyxdIFxuIyBXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gc3Vic2V0IG9mIG1vb2R5IGRhdGEgZnJhbWUgd2hvIGdvdCBCIGdyYWRlLlxuIyBCLiBlcnJvci5cbiMgQy4gc3Vic2V0IG9mIG1vb2R5IGRhdGEgZnJhbWUgd2l0aCBzY29yZSBncmVhdGVyIHRoYW4gODAuXG4jIEQuIHN1YnNldCBvZiBtb29keSBkYXRhIGZyYW1lIHdpdGggc2NvcmUgbW9yZSB0aGFuIDgwIGFuZCBnb3QgQiBncmFkZS4ifQ== Two commands to aggregate data: table() and tapply() 2.2.2 Table eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG50YWJsZWV4MTwtIHRhYmxlKG1vb2R5JEdSQURFKSAjVXNlIG9mIHRhYmxlICBmdW5jdGlvbiBvbiB0aGUgbmV3IGNvbHVtbi5cbnRhYmxlZXgxXG5iYXJwbG90KHRhYmxlZXgxLGNvbCA9YyhcInJlZFwiLFwicHVycGxlXCIsXCJjeWFuXCIsXCJ5ZWxsb3dcIixcImdyZWVuXCIpLHhsYWIgPSBcIkxhYmVsc1wiLCB5bGFiID0gXCJGcmVxdWVuY3lcIixtYWluID0gXCJ0YWJsZSgpIGV4YW1wbGUgMVwiKSAjcGxvdC5cblxuXG50YWJsZWV4MjwtdGFibGUobW9vZHkkR1JBREUsbW9vZHkkQVNLU19RVUVTVElPTlMpXG50YWJsZWV4MlxubW9zYWljcGxvdCh0YWJsZWV4Mixjb2wgPWMoXCJyZWRcIixcInB1cnBsZVwiLFwiY3lhblwiLFwieWVsbG93XCIsXCJncmVlblwiKSxtYWluID0gXCJ0YWJsZSgpIGV4YW1wbGUgMlwiKSJ9 2.2.2.1 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFibGUobW9vZHlbbW9vZHkkQVNLU19RVUVTVElPTlMhPSdhbHdheXMnLF0kR1JBREUpXG4jV2hhdCB3aWxsIFIgc2F5P1xuXG4jIEEuIGVycm9yXG4jIEIuIGRpc3RyaWJ1dGlvbiBvZiBncmFkZXMgZm9yIHN0dWRlbnRzIHdobyBhbHdheXMgYXNrIHF1ZXN0aW9uc1xuIyBDLiBkaXN0cmlidXRpb24gb2YgZ3JhZGVzIGZvciBzdHVkZW50cyB3aG8gZG8gbm90IGFsd2F5cyBhc2sgcXVlc3Rpb25zICJ9 2.2.2.2 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFibGUobW9vZHlbbW9vZHkkQVNLU19RVUVTVElPTlM9PWxpc3QoJ2Fsd2F5cycsJ25ldmVyJyksXSRHUkFERSlcbiNXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gZXJyb3IuXG4jIEIuIGRpc3RyaWJ1dGlvbiBvZiBncmFkZXMgZm9yIHN0dWRlbnRzIHdobyBhbHdheXMgb3IgbmV2ZXIgYXNrIHF1ZXN0aW9ucy4gIFxuIyBDLiBkaXN0cmlidXRpb24gb2YgZ3JhZGVzIGZvciBzdHVkZW50cyB3aG8gZG8gbm90IGFzayBxdWVzdGlvbnMgYWx3YXlzIG9yIG5ldmVyLiAifQ== 2.2.3 tapply eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG4jIFRvIGFwcGx5IHRhcHBseSgpIG9uIFNDT1JFIGZhY3RvcmVkIG9uIE9OX1NNQVJUUEhPTkVcblxubW9vZHlfc2NvcmVhdmc8LXRhcHBseShtb29keSRTQ09SRSxtb29keSRPTl9TTUFSVFBIT05FLG1lYW4pXG5tb29keV9zY29yZWF2ZyAjIFdlIGNhbiBzZWUgaXQgY2FsY3VsYXRlZCBtZWFuIHZhbHVlIG9mIHRoZSBzY29yZSBieSBzdHVkZW50cyB3aXRoIHJlc3BlY3QgdG8gdGhlaXIgdXNlIG9mIHBob25lIGluIGNsYXNzLlxuXG5iYXJwbG90KG1vb2R5X3Njb3JlYXZnLGNvbCA9IFwiY3lhblwiLHhsYWIgPSBcIkxhYmVsc1wiLCB5bGFiID0gXCJtZWFuX3ZhbFwiLG1haW4gPSBcInRhcHBseSgpIGV4YW1wbGUgMVwiLGxhcyA9IDIsIGNleC5uYW1lcyA9IDAuNzUpI3Bsb3RcblxuI0xldHMgZmFjdG9yIHRoZSBncmFkZXMgb24gb25fc21hcnRwaG9uZSBhcyB3ZWxsIGFzIGdyYWRlIGNhdGVnb3J5LlxuXG5tb29keS5zY29yZWF2ZzJkPC10YXBwbHkobW9vZHkkR1JBREUsbGlzdChtb29keSRPTl9TTUFSVFBIT05FLG1vb2R5JEdSQURFKSxsZW5ndGgpXG5tb29keS5zY29yZWF2ZzJkW2lzLm5hKG1vb2R5LnNjb3JlYXZnMmQpXTwtMFxubW9vZHkuc2NvcmVhdmcyZCMgV2UgY2FuIHNlZSBpdCBjYWxjdWxhdGVkIGNvdW50IG9mIHRoZSBncmFkZSBvZiBzdHVkZW50IHdpdGggcmVzcGVjdCB0byB0aGVpciBpbi1jbGFzcyBzbWFydHBob25lIHVzYWdlICBhbmQgZ3JhZGUgY2F0ZWdvcnkuXG5iYXJwbG90KG1vb2R5LnNjb3JlYXZnMmQsY29sPWMoXCJyZWRcIixcImN5YW5cIixcIm9yYW5nZVwiLFwiYmx1ZVwiKSxtYWluID0gXCJ0YXBwbHkoKSBleGFtcGxlIDJcIixiZXNpZGUgPSBUUlVFLGxlZ2VuZD1yb3duYW1lcyhtb29keS5zY29yZWF2ZzJkKSkifQ== 2.2.3.1 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFwcGx5KG1vb2R5LCBHUkFERSwgU0NPUkUsIG1pbilcbiMgV2hhdCB3aWxsIFIgc2F5P1xuXG4jIEEuIG1pbmltdW0gc2NvcmUgZm9yIGVhY2ggZ3JhZGVcbiMgQi4gbWluaW11bSBncmFkZSBmb3IgZWFjaCBzY29yZVxuIyBDLiBtaW5pbXVtIGdyYWRlIG9ubHkgXG4jIEQuIEVycm9yLiJ9 2.2.3.2 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFwcGx5KG1vb2R5JEFTS19RVUVTVElPTlMsIG1vb2R5JEdSQURFLCBtZWFuKVxuIyBXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gbWVhbiBncmFkZSBmb3IgZWFjaCB2YWx1ZXMgb2YgYXNrX3F1ZXN0aW9uIGF0dHJpYnV0ZVxuIyBCLiBtZWFuIHZhbHVlIG9mIGFza19xdWVzdGlvbnMgYXR0cmlidXRlIGZvciBlYWNoIGdyYWRlXG4jIEMuIG1lYW4gY2F0ZWdvcnkgb2YgYXNrX3F1ZXN0aW9ucyBvbmx5IFxuIyBELiBlcnJvci4ifQ== One data structure: Data frame 2.2.4 data.frame() eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgMyB2ZWN0b3JzIHdpdGggdGl0bGUsIGF1dGhvciBhbmQgeWVhci5cbnRpdGxlIDwtIGMoJ0RhdGEgU21hcnQnLCdPcmllbnRhbGlzbScsJ0ZhbHNlIEltcHJlc3Npb25zJywnTWFraW5nIFNvZnR3YXJlJylcbmF1dGhvciA8LSBjKCdGb3JlbWFuLCBKb2huJywnU2FpZCwgRWR3YXJkJywnQXJjaGVyLCBKZWZmZXJ5JywnT3JhbSwgQW5keScpXG55ZWFyIDwtIGMoMjAxMCwyMDExLDIwMTIsMTk5OClcblxuI0xldHMgbG9vayBhdCBob3cgdGhlIGNyZWF0ZWQgdmVjdG9ycyBsb29rLlxudGl0bGVcbmF1dGhvclxueWVhclxuXG4jIEFsc28gbGV0cyBsb29rIGF0IHRoZWlyIHR5cGVzIHVzaW5nIHRoZSBjbGFzcyBmdW5jdGlvbi5cbmNsYXNzKHRpdGxlKVxuY2xhc3MoYXV0aG9yKVxuY2xhc3MoeWVhcilcblxuXG4jIE5vdyBsZXRzIGNyZWF0ZSBhIGRhdGFmcmFtZSB1c2luZyB0aGUgYWJvdmUgY29sdW1uIHZlY3RvcnMuXG5cbmRmIDwtIGRhdGEuZnJhbWUodGl0bGUsIGF1dGhvciwgeWVhcilcbmRmICMgTGV0cyBsb29rIGF0IGhvdyB0aGUgZGF0YWZyYW1lIGxvb2tzLiJ9 And with these seven commands of R.7 we will be able to do quite a bit of data exploration. Manipulate data any way we want to. We will begin with what we call freestyle data exploration. We call it freestyle, since we are not going to use any sophisticated libraries, but rather just seven basic commands of R. No statistics yet, and no more sophisticated R programs. These will come later. For now, we are just feeling the data with four plots and three R instructions: subset(), table() and tapply() Through this section we will use our usual initial example, of synthetically generated data describing mysterious methods of grading used by an eccentric Professor Moody. We have been using different versions of this data puzzle over the 6 years of teaching data 101. Data is different but narrative is always the same: 2.3 Professor Moody Puzzle We start our data exploration with the following data puzzle. Professor Moody has been teaching statistics 101 class for many years. His teaching evaluations went considerably south with the chief complaint: he DOES NOT seem to assign grades fairly. Students compared their scores among themselves and found quite a bit of discrepancies! But their complaints went nowhere since the Professor promptly disappeared after posting the final grades and scores. A new brave TA, managed to get hold of the carefully maintained grading table (spanning multiple years) of professor Moody by ….messing a bit with Moody’s computer….well, let’s not explain the details because he would get in trouble. What he found out was a remarkably structured account of how professor Moody assigns his grades. Looks like Professor Moody is in fact very alert in class. He is aware of what students do, detecting texting during class and remembering exactly who asked many questions in class. He also keeps the mysterious “participation index” which is a numerical score from 0 to 1. This is probably related to questions asked and answered by students as well as their general attentiveness in class. Remarkable but a little creepy, isn’t it? What is the best advice the new TA can give future students how to get a good grade in Professor Moody’s class? What factors influence the grade besides the score? Back your recommendation up with plots and evidence from the attached data. The Moody data set is defined here by the following attributes: Table 2.2: Snippet of Moody Dataset score grade texting questions participation 26.89 F never never 0.41 71.57 B always rarely 0.00 90.11 A always never 0.27 31.52 D sometimes rarely 0.68 95.94 A always rarely 0.09 45.72 D always rarely 0.19 90.82 A always always 0.25 75.52 B sometimes never 0.28 52.31 C never never 0.67 39.57 D always always 0.40 Moody[score, grade, participation, questions, texting] Score and grade are self explanatory. Participation is supposedly measuring students’ participation in class. We do not know whether higher participation would necessarily be positive, since Professor Moody’s mood changes from year to year and he may be annoyed by students who are too active and bother him too much. Who knows? We have to find out. Attribute “questions” has several values “always”, “frequently”, “sometimes” and “never”. So does the attribute “texting”. In our data set there are students who are always texting and who never ask any questions. Oh, yes, and some students’ participation index is almost zero. Guess what grade they are getting? F, you probably guessed. Well, but what about their score? It should matter at least to some degree! Grading rules, which Professor Moody applies each year, are different. The objective of our freestyle data exploration is to find some leads/hypotheses which would help us direct students what they should do to get a good grade in his class. This is a good illustration of what data exploration is and can achieve. It is just an example, but one can of course easily see that things we discuss here apply to any data set. Data exploration can be viewed as an indefinite loop: REPEAT{ Plot,one or many plots. Transform Data. } UNTIL GRATIFICATION Put yourself in the position of a student in Moody’s class. What does s/he want to know? What should I do in order to pass his class aside from getting the best score possible? Ask many questions? Do not text? Come to class as often as you can? Presumably improving participation index? What is the approach to discover the secret rules of getting good grades in Professor Moody’s class? First you need “kick the tires”, make some plots, feel the data and perhaps rule out the obvious. In case of Professor Moody data it may mean the following: - Test if straightforward mapping of scores into grades work in Professor Moody’s class. Admittedly it is a long shot. We expect more from professor Moody than just merely following the scoring intervals with A above, say 85, B between 70 and 85 etc! First, we need to establish that it is not the case. Since it would be embarrassing to miss the obvious and simplest recommendation. Just score as high as you can. Otherwise there is no need to come to class, and in class you can text as much as you want to and ask no questions. Does not matter what else you do. You may never ask any questions, always text in class or simply…never even show up. All it would matter is to score as high as you can! There is just one plot which can quickly establish whether this simple rule works. And it is the boxplot. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5ib3hwbG90KG1vb2R5JHNjb3JlIH4gbW9vZHkkZ3JhZGUsIG1haW4gPSAnRGlzdHJpYnV0aW9uIG9mIFNjb3JlcyBieSBHcmFkZScsIHlsYWIgPSdTY29yZSAob3V0IG9mIDEwMCknLCB4bGFiID0gJ0xldHRlciBHcmFkZScsY29sPWMoJ3JlZCcsJ2JsdWUnLCdncmVlbicsJ2N5YW4nLCd5ZWxsb3cnKSkifQ== As illustrated by the boxplot, there are significant overlaps between successive grades. This disproves that there is a deterministic function between score and grade. At least it is not always a function. If your score falls in certain “gray” areas you may get either one of two grades (A or B, B or C, C or D, D or F). And we do not know what is this additional “decider” in such case when score falls into this gray area. Here is how we can check which factors may impact the grade. One way of doing this analysis is to make barplots for all possible slices of Moody data frame by a given categorical variable For example,we want to know if asking questions “matters” for the grade? This can be validated by comparing barplots of grade distribution for different values of attribute “questions”.You can either do it by applying the mosaic plot which allows for two-dimensional representation of data and allows to create multicolored table for grade x questions to eyeball if values of attribute “questions” matter for values of attribute “grade”. To dig deeper into the relationship of categorical variables “questions” and “texting” with “grade” we will use a sequence of bar plots over subsets of the Moody data frame. Then we will follow with the mosaic plots. The following slices represent subsets of the Moody data frame for each of the values of the attribute “questions” The command \\(\\color{violet}{\\text{table}}\\) (one of the 7 commands) will provide us grade distribution for each of these slices. And the barplot, will visualize this table. Let’s look at the example of the above process for students who always ask question. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhYmxlKG1vb2R5W21vb2R5JHF1ZXN0aW9ucz09J2Fsd2F5cycsXSRncmFkZSksbWFpbiA9ICdGcmVxdWVuY3kgb2Ygc3R1ZGVudHMgYnkgR3JhZGUgd2hvIFwiYWx3YXlzXCIgYXNrIHF1ZXN0aW9ucycsIHlsYWIgPSdGcmVxdWVuY3knLCB4bGFiID0gJyBHcmFkZScsY29sPWMoJ3JlZCcsJ2JsdWUnLCdncmVlbicsJ2N5YW4nLCd5ZWxsb3cnKSlcblxuI05vdGljZSB0aGF0IHlvdSBjYW4gbW9kaWZ5IHRoZXNlIGJhcnBsb3RzIGdyYXBocyBhbmQgcmVwbGFjZSB0aGUgdmFsdWUgb2YgbW9vZHkkcXVlc3Rpb25zIGF0dHJpYnV0ZXMgZnJvbSBcImFsd2F5c1wiIHRvIFwic29tZXRpbWVzXCIgb3IgXCJuZXZlclwiIGFuZCBzZWUgaW1wYWN0IHRoZXNlIG5ldyBzbGljZXMgaGF2ZSBvbiB0aGUgZ3JhZGUgZGlzdHJpYnV0aW9uLiBKdXN0IGNoYW5nZSB0byBjb2RlIGFib3ZlIGFuZCBydW4gaXQuIFlvdSBjYW4gYWxzbyBjaGFuZ2UgdGhlIG1vb2R5JHF1ZXN0aW9ucyBhdHRyaWJ1dGUgYW5kIHJlcGxhY2UgaXQgd2l0aCB0aGUgbW9vZHkkdGV4dGluZyBhdHRyaWJ1dGUgYW5kIGl0cyBkaWZmZXJlbnQgdmFsdWVzLiBUaHVzIHlvdSBjYW4gcnVuIDYgZGlmZmVyZW50IGJhcnBsb3RzIHVzaW5nIHRoZSBjb2RlIGFib3ZlIGFuZCBzZWUgaG93IEdyYWRlIGRpc3RyaWJ1dGlvbiBpcyBhZmZlY3RlZCBmb3IgZWFjaCBvZiB0aGVzZSA2IGNhc2VzLiJ9 We can also run two mosaic plots of GRADE vs “questions” or “texting” respectively - and be able to assess the same - do these attributes matter for the grade? In the following command we can combine attribute grade with anyone of the behavioral attributes eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5tb3NhaWNwbG90KG1vb2R5JGdyYWRlfm1vb2R5JHRleHRpbmcseGxhYiA9ICdHcmFkZScseWxhYiA9ICdUZXh0aW5nIGhhYml0JywgbWFpbiA9IFwiTW9zaWFjIG9mIGdyYWRlIHZzIHRleGluZyBoYWJpdCBpbiBjbGFzc1wiLGNvbD1jKCdyZWQnLCdibHVlJywnZ3JlZW4nLCdjeWFuJywneWVsbG93JyksYm9yZGVyPVwiYmxhY2tcIikifQ== This can be concluded by comparing different columns and rows of the mosaic table. If grade distribution is similar for different values of behavioral attributes, this would indicate that these attributes do not matter in establishing the grade. On the other hand we may “catch professor Moody” and find out that for some value of some attribute, grade distribution is significantly affected. This was the case several years ago when students sitting in the first row got a grade bump up, even if they got similar scores to students sitting in the back row. In that case one of the extra attributes included the row where students were sitting during class. We can see that asking many questions (frequently and always) really matters for the grade, there are more A’s and more B’s for these slices than in general. But this may have nothing to do with Professor Moody rewarding students with the bonus for asking questions. It may be simply the case that such students are more involved and study harder (or are more interested in the topic) and simply get higher scores. We need to dig deeper and see which of the two is the case. Moody’s just gives his personal bonus to students who ask a lot of questions or no such bonus is given – such students simply score higher. We can accomplish this using again one of the seven R commands – the tapply. ## always never rarely ## 51.08277 56.32474 53.69217 will return an average score for each of the values of the attribute moody$questions. If these values are more or less uniform then it will informally (not statistically yet, for this we have to wait for the next sections) show that questions matter in professor moody grading method and are not just correlated with student’s score. Take a look at eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhcHBseShtb29keSRzY29yZSwgbW9vZHkkcXVlc3Rpb25zLCBtZWFuKSwgeGxhYiA9ICdxdWVzdGlvbiBjYXRlZ29yaWVzJyx5bGFiID0gJ1Njb3JlIEF2ZXJhZ2UnLCBtYWluID0gXCJNZWFuIFNjb3JlIHZzIFF1ZXN0aW9ucyBBc2tlZCB1c2luZyB0YXBwbHkoKVwiLGNvbD1jKCdyZWQnLCdibHVlJywnZ3JlZW4nLCdjeWFuJywneWVsbG93JyksYm9yZGVyPVwiYmxhY2tcIikifQ== What is the conclusion? Does asking questions often imply a higher score? Is texting less correlated with higher scores? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhcHBseShtb29keSRzY29yZSwgbW9vZHkkdGV4dGluZywgbWVhbiksIHhsYWIgPSAndGV4dGluZyBjYXRlZ29yaWVzJyx5bGFiID0gJ1Njb3JlIEF2ZXJhZ2UnLCBtYWluID0gXCJNZWFuIFNjb3JlIHZzIFRleHRpbmcgdXNpbmcgdGFwcGx5KClcIixjb2w9YygncmVkJywnYmx1ZScsJ2dyZWVuJywnY3lhbicsJ3llbGxvdycpLGJvcmRlcj1cImJsYWNrXCIpIn0= shows that mean scores are the same across different values of the “texting”attribute. Same is true for the mean scores of “questions” attribute. Neither do these two attributes “texting” and “questions” seem to have an impact on the grade. Therefore it seems that the “behavioral” attributes: questions and texting do not seem to have an impact on the grade. We define intervals of score as clear, if there is only one grade associated with scores from such intervals. The remaining intervals are defined as grey - scores where grade can be either A or B, B or C, C or D and D or F respectively. Then we can examine how participation influences grades in these grey areas of score. Our hypothesis is that higher participation would probably offer better odds for higher grades. We can run the following command for different values of q. Let q be a threshold of participation which we want to test. Maybe if participation is higher than q, higher grade (from the two possible grades in the grey area of score) is given, while if participation is lower than q, it works against a student, who then gets lower grade? Let’s check how the grade distribution changes for different values of q from the lower values of q to higher values of q. We can just change q directly in the code below, and see results immediately. Run the following command for different values of q. We will only show it for the grey interval between A and B. The same process can be repeated for other gray areas between B and C, C and D and D and F. In fact one can modify the code below by just replacing grade A and B with B and C respectively as well as replacing the variables LowestA with Lowest B and Highest B with Highest C respectively. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNTaW1wbGUgUiBjb21tYW5kcyB0byBnZXQgaW50ZXJ2YWwgZm9yIGVhY2ggZ3JhZGUuXG5Mb3dlc3RBPC1taW4obW9vZHlbbW9vZHkkZ3JhZGU9PSdBJywgXSRzY29yZSlcbkhpZ2hlc3RCPC1tYXgobW9vZHlbbW9vZHkkZ3JhZGU9PSdCJywgXSRzY29yZSkgXG4jVGhpcyBnaXZlcyB1cyBpbnRlcnZhbCA8SGlnaGVzdEIsIExvd2VzdEE+XG5wcmludChjKExvd2VzdEEsSGlnaGVzdEIpKVxuXG5xPTAuNSAjIFBsZWFzZSBFZGl0IHRoaXMgXCIgcSBcIiB2YWx1ZSBhbmQgc2VlIHRoZSBjaGFuZ2VzIGFzIG1lbnRpb25lZCBhYm92ZS5cbnRhYmxlKG1vb2R5W21vb2R5JHNjb3JlPkxvd2VzdEEgJiBtb29keSRzY29yZTxIaWdoZXN0QiYgbW9vZHkkcGFydGljaXBhdGlvbiA+IHEsXSRncmFkZSlcblxuI05vdGUgdGhlIHNhbWUgcHJvY2VzcyBjYW4gYmUgcmVwZWF0ZWQgd2l0aCBvdGhlciBhZGphY2VudCBncmFkZXMuIEV4OiA8QixDPiBldGMuIn0= Please verify that for higher values of 0&lt;q&lt;1, the table operation shows higher percentages of better grades. Is there a critical value of q which clearly separates, say A’s from B’s? It seems to be q=0.6 - but it is not a clear cut deterministic. Rather, a higher value of participation threshold, q increases probability (frequency) of getting As. We come to the conclusion that participation matters in the grey area of score, in having higher chances for better grades, if participation is higher. Thus, just in case (since no one can predict if they will end up in borderline score) it is better to earn a high participation index – by (probably) coming to class more often and participating in discussions, and answering professor Moody’s questions. Simply put “come to as many classes as you can” should be an additional recommendation for getting good grades. Clearly, this, along with high scores, helps in getting better grades. But while in class, do not worry about texting or asking questions. These two attributes do not seem to matter. Now we can reveal how data was generated? What was the real rule embedded in the data? Now it is time to reveal how we generated our data. We have indeed defined border areas in score value. In these border areas of score, participation plays a role. Student who’s score falls into the grey area may get one of two grades, A or B, B or C, C or D and D or F, depending on the score.For example, a score of 72 may result in A or B. It is more likely to be A if student’s participation is high (higher the better the odds of getting A). If a student’s participation is low, it is much more likely to result in a lower grade, for a score of 72, it would be B. Therefore we have discovered the rule which guided the generation of the Moody’s data set. We have provided students with actionable intelligence on how to increase chances of getting higher grades, namely through the relationship between participation, score and grade. In the process of slicing, dicing and plotting the data we would also discover other interesting relationships still using just 7 commands. Does higher participation mean higher score, in general? Meaning that coming to class is positively correlated with a higher score? We can run scatter plot eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5wbG90KG1vb2R5JHBhcnRpY2lwYXRpb24sIG1vb2R5JHNjb3JlKSJ9 To our surprise it looks like the higher the participation, the lower the score! The distinct linear patterns in the scatter graph seem to be sloping down with participation. We are tempted to infer that participation is bad for score, that somehow Moody’s lectures have negative impact on the score - hence do not carry pedagogical value. Such a conclusion is typical confusion between correlation and causation. What is true in our simulated data set - that students who had some prior background in the subject matter of Professor Moody just do not show up in class that often. They already know the material. Students who do show up are the ones who are not confident in their knowledge of the subject matter - in general “weak A’s” and below. Then of course lower grade students (D’s and F’s) just simply do not apply themselves that much - are not invested into the class and just show up in class even less. Thus, the explanation probably has more to do with the students attribute about the class than with the pedagogical value of Professor Moody’s lectures. We can change values of parameters q and s and examine in more detail the relationships between scores and participation. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNJbnRlcmVzaW5nIGFuYWx5c2lzIGhhcyB0byBkbyB3aXRoIGRpZ2dpbmcgZGVlcGVyIGludG8gcmVsYXRpb25zaGlwIGJldHdlZW4gcGFydGljcGF0aW9uIGFuZCBzY29yZS4gV2hhdCBhcmUgdGhlIHNjb3JlcyBvZiBzdHVkZW50cyB3aG8gcGFydGljaXBhdGUgbGVzcyB0aGFuIHNvbWUgdmFsdWUgcSA8MS4gV2hhdCBhcmUgdGhlIHBhcnRpY2lwYXRpb24gdmFsdWVzIG9mIHN0dWRlbmV0cyB3aG8gc2NvcmUgbGVzcyB0aGFuIHM/ICAgQnkgY2hhbmdpbmcgdGhlIHZhbHVlcyBvZiBxIGFuZCBzIGluIHRoZSBjb2RlIHlvdSBjYW4gZ2FpbiBtb3JlIGluc2lnaHQgaW50byByZWxhdGlvbnNoaXAgYmV0d2VlbiBwYXJ0aXBhdGlvbiBhbmQgc2NvcmUuXG5cbiMgQ2hhbmdlIHRoZSB2YWx1ZXMgb2YgXCJxXCIgYW5kIFwic1wiIGJlbG93LlxucTwtMC4xXG5zPC03MFxubWVhbihtb29keVttb29keSRwYXJ0aWNpcGF0aW9uIDxxLF0kc2NvcmUpXG5tZWFuKG1vb2R5W21vb2R5JHNjb3JlIDxzLF0kcGFydGljaXBhdGlvbikifQ== Exploring Behaviors of Students in Professor Moody’s class. One may even drop the grade entirely from the picture and simply inquire about behavioral characteristics of Professor Moody’s students. We already know what is the distribution of each type of behavior eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG50YWJsZShtb29keSRxdWVzdGlvbnMpO1xudGFibGUobW9vZHkkdGV4dGluZykifQ== But lets ask for associations between behaviors Do students who ask a lot of questions also spend little time texting? Do students with higher participation generally text less? These questions have nothing to do with students’ performance. But they all can be answered using simple R.7 commands. Same data may serve different purposes. We started with predicting what behaviors help getting higher grades in professor Moody’s class. But we can imagine a different study – which is addressing student behavior in professor Moody’s class. Yet another study could address the impact of behavioral attributes on students’ scores (not grades). All these analyses can be done using freestyle exploration and R.7. What we discover is not yet guaranteed to be statistically valid. For this we need statistical evaluation, The p-values, the z-tests etc. Later we will also find statistical functions which can greatly help in data exploration. Free style exploration role is to generate leads otherwise known as conjectures or hypotheses. Our professor Moody’s data puzzle has been traditionally the first data puzzle we ask students to solve in data 101 class. Here are some examples of conclusions reached on different instances of professor Moody’s data set in the past semesters. Notice that attributes of Moody’s data set in past may have been different (like “sleeping in class”) Here is the set of recommendations from a former student who cracked that year’s professor Moody’s puzzle (or did she?) “Judging by plots and means calculated earlier, there are several factors, besides score, that affect students’ grades: • Sleeping in class increases grade • Texting in class decreases grades a little • Being active(participating) in class all the time significantly increases the grade, BUT: • Being active(participating) in class just occasionally decreases the grade even more, than not participating at all. • Being active only occasionally significantly decreases the grades. • Texting does not significantly affect grades . So for students in order to succeed in professor Moody’s class, my advice will be(besides getting high score): • VERY IMPORTANT: Participate all the time., or do not participate at all!!! • Sleep in class(especially if you do not participate anyway) • While texting might bring down your grade a little bit, the difference is very small” "],["stateval.html", "Chapter 3 p-value: Simple Statistical Evaluation 3.1 Permutation Test 3.2 Multiple Hypothesis - Bonferroni Correction.", " Chapter 3 p-value: Simple Statistical Evaluation Randomness is the biggest enemy of your findings. In order to convince your audience that you have found something worthwhile in the data you need to address the question “how do you know that your result is simply not sheer luck, that it is not random?” This is where the concept of p-value enters the picture. What is the p-value? First, there must be a hypothesis. In fact, at least a pair of them - one which is called the NULL hypothesis and another (the one which you hope you can defend), the ALTERNATE hypothesis. Informally, NULL hypothesis is what a SKEPTIC would argue. Skeptic, who says, “Data does not support your findings” in fact, your findings are random. Some call the NULL hypothesis - the boring one. Nothing really happens in your data. You, the data scientist, hope you can reject the NULL hypothesis! Show that it is unlikely to see the observed data by random chance UNDER NULL HYPOTHESIS. Here are some examples of NULL and ALTERNATIVE hypotheses: Examples of NULL hypotheses Traffic in Holland Tunnel is same as in Lincoln Tunnel French Wines are equally priced as Californian Wines Crime has not changed as result of Gun laws Unemployment has not changed as result of tax reductions Democrat candidate is tied with GOP candidate Examples of ALTERNATE hypotheses Traffic in Holland Tunnel is higher as in Lincoln Tunnel French Wines are less expensive than Californian Wines Crime changed as result of Gun laws Unemployment has decreased as result of tax reductions Democrat candidate is leading GOP candidate Notice that the alternate hypothesis is not simply the negation of the null hypothesis. It is mutually exclusive with the NULL hypothesis though. Some of the alternate hypotheses above are two tied, others are one-tied. For example Crime changed as result of Gun laws is an example of two tied alternate hypotheses. No statement is made about how the crime has moved, has it increased, or has it decreased? The alternate hypothesis simply states that Crime has changed. Compare it with alternate hypothesis that Unemployment has decreased as a result of tax reductions. This is a one-tied alternate hypothesis, which is stronger than the Two-tied hypothesis, stating that unemployment has decreased. Thus, it states that unemployment was affected by tax reductions and that it has decreased. Let us now focus on one of these examples. NULL HYPOTHESIS (“The Skeptic”) There is no traffic difference between Lincoln and Holland tunnels ALTERNATE HYPOTHESIS (Data Scientist) Lincoln is busier than Holland The p-value is defined as the probability of obtaining a result equal to or “more extreme” than what was actually observed, when the null hypothesis is true. Thus, p-value is the CONDITIONAL probability. Conditional upon null hypothesis being true In other words: showing that obtained result could have been generated by RANDOM CHANCE under assumption of null hypothesis with probability exceeding the critical value alpha. It was just a “fluke” - which can be attributed to randomness and cannot be used to support the alternate hypothesis. In the case of our example, There is no traffic difference between Lincoln and Holland tunnels, even though the data seems to indicate that Lincoln is busier than Holland. But evidence for the latter is not stored enough. There is, say, p chance (say 0.10) that observed data was obtained as a result of random fluke and tunnels are equally busy. Generally, if p is larger than 0.05, the observed data is not strong enough to reject the NULL. Now we discuss the permutation test which helps us to understand the role of randomness in producing seemingly meaningful evidence. 3.1 Permutation Test Permutation test allows us to observe randomness directly, with naked eye, without the lenses of statistical tests such as z-tests etc. We shuffle data randomly like a deck of cards. There may be many such shuffles - 10,000, 100,000 etc. The goal is always to see how often we can obtain the observed difference of means (since we are testing either one sided or two sided hypothesis), by purely random shuffles of our data. These permutations (shuffles) destroy all relationships which may pre-exist in our data. We are hoping to show that our observed difference of means can be obtained very rarely in completely random fashion. Then we “experimentally” show that our result is unlikely to randomly occur under null hypothesis. Then we can reject the null hypothesis. The less often our results appear in the histogram of permutation test results, the better the news for our alternative hypothesis. What is surprising to many newcomers, is that the permutation test will give different p-values (not dramatically different, but still different) in each run of the permutation test. This is the case because the permutation test is random itself. It is not like for example a z-test which will give the same result when run again for the same hypothesis and same data set. Also the p-value computed by the permutation test will be, in general, different from the p-value computed by z-test. Not very different but different. Again, it is the case because the permutation test provides only an approximation of p-value. Great advantage of the permutation test is that it is universal and robust. One can test different relationships between two variables rather than just difference of means. For example we can use a permutation test to validate whether traffic in Lincoln tunnel is more than twice the traffic in Holland tunnel or even provide different weights for different days of the week. 3.1.1 Permutation Test One Step One step Permutation test is the most direct way to see randomness close by. One step permutation function shows one single data shuffle. By shuffling the data one destroys associations which exist between values of the data frame. This makes the data frame random. You can execute the one step permutation multiple times. This will show how the data frame varies and how it affects the observed difference of means. Apply one step permutation function first, multiple times before you move to the proper Permutation test function. One of the parameters of the Permutation test function specifies the number of “shuffles” that will be performed. This could be a very large number, 10,000 or even 100,000. The purpose of making so many random permutations is to test how often observed differences of means can arise in just random data. The more often this takes place, the more likely your observation is just random. To reject the null hypothesis you need to show that the observed difference of means will come very infrequently in the permutation test. Less than 5% of the time, to be exact. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKSIsInNhbXBsZSI6InN1bW1hcnkodHJhZmZpYylcbkQ8LSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdIb2xsYW5kJywzXSkgLSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywzXSlcbm51bGxfdHVubmVsIDwtIHJlcChcIkhvbGxhbmRcIiwyODAxKSAjIENyZWF0ZSAyODAxIGNvcGllcyBvZiBIb2xsYW5kIFxubnVsbF90dW5uZWxbc2FtcGxlKDI4MDEsMTQwMCldIDwtIFwiTGluY29sblwiICMgUmVwbGFjZSBSQU5ET01MWSAxNDAwIGNvcGllcyB3aXRoIExpbmNvbG5cbm51bGwgPC0gZGF0YS5mcmFtZShudWxsX3R1bm5lbCx0cmFmZmljWywzXSlcbm5hbWVzKG51bGwpIDwtIGMoXCJUVU5ORUxcIixcIlZPTFVNRV9QRVJfTUlOVVRFXCIpXG5zdW1tYXJ5KG51bGwpXG5ob2xsYW5kX251bGwgPC0gbnVsbFtudWxsJFRVTk5FTCA9PSBcIkhvbGxhbmRcIiwyXVxubGluY29sbl9udWxsIDwtIG51bGxbbnVsbCRUVU5ORUwgPT0gXCJMaW5jb2xuXCIsMl1cbm1lYW4oaG9sbGFuZF9udWxsKVxubWVhbihsaW5jb2xuX251bGwpXG5EX251bGwgPC0gbWVhbihsaW5jb2xuX251bGwpIC0gbWVhbihob2xsYW5kX251bGwpXG5jYXQoXCJUaGUgbWVhbiBkaWZmZXJlbmNlIG9mIHBlcm11dGF0aW9uIG9uZSBzdGVwIGRhdGE6IFwiLCBEX251bGwsXCJcXG5cIikjIENhbGN1bGF0ZSB0aGUgZGlmZmVyZW5jZSBiZXR3ZWVuIHRoZSBtZWFuIG9mIHRoZSByYW5kb20gZGF0YS5cbmNhdChcIlRoZSBtZWFuIGRpZmZlcmVuY2Ugb2Ygb3JpZ2luYWwgZGF0YTogXCIsIEQpICMgRGlmZmVyZW5jZSBvZiBtZWFuIHZhbHVlIG9mIG9yaWdpbmFsIGRhdGEuIn0= 3.1.2 Permutation Function The Multi- step permutation function is used to run multiple iterations of the one-step permutation studied above, to get a complete relational understanding between the components involved in any hypothesis. Run the example of permutation test on the Traffic.csv dataset eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKVxuUGVybXV0YXRpb24gPC0gZnVuY3Rpb24oZGYxLGMxLGMyLG4sdzEsdzIpe1xuICBkZiA8LSBhcy5kYXRhLmZyYW1lKGRmMSlcbiAgRF9udWxsPC1jKClcbiAgVjE8LWRmWyxjMV1cbiAgVjI8LWRmWyxjMl1cbiAgc3ViLnZhbHVlMSA8LSBkZltkZlssIGMxXSA9PSB3MSwgYzJdXG4gIHN1Yi52YWx1ZTIgPC0gZGZbZGZbLCBjMV0gPT0gdzIsIGMyXVxuICBEIDwtICBhYnMobWVhbihzdWIudmFsdWUyLCBuYS5ybT1UUlVFKSAtIG1lYW4oc3ViLnZhbHVlMSwgbmEucm09VFJVRSkpXG4gIG09bGVuZ3RoKFYxKVxuICBsPWxlbmd0aChWMVtWMT09dzJdKVxuICBmb3IoamogaW4gMTpuKXtcbiAgICBudWxsIDwtIHJlcCh3MSxsZW5ndGgoVjEpKVxuICAgIG51bGxbc2FtcGxlKG0sbCldIDwtIHcyXG4gICAgbmYgPC0gZGF0YS5mcmFtZShLZXk9bnVsbCwgVmFsdWU9VjIpXG4gICAgbmFtZXMobmYpIDwtIGMoXCJLZXlcIixcIlZhbHVlXCIpXG4gICAgdzFfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzEsMl1cbiAgICB3Ml9udWxsIDwtIG5mW25mJEtleSA9PSB3MiwyXVxuICAgIERfbnVsbCA8LSBjKERfbnVsbCxtZWFuKHcyX251bGwsIG5hLnJtPVRSVUUpIC0gbWVhbih3MV9udWxsLCBuYS5ybT1UUlVFKSlcbiAgfVxuICBteWhpc3Q8LWhpc3QoRF9udWxsLCBwcm9iPVRSVUUpXG4gIG11bHRpcGxpZXIgPC0gbXloaXN0JGNvdW50cyAvIG15aGlzdCRkZW5zaXR5XG4gIG15ZGVuc2l0eSA8LSBkZW5zaXR5KERfbnVsbCwgYWRqdXN0PTIpXG4gIG15ZGVuc2l0eSR5IDwtIG15ZGVuc2l0eSR5ICogbXVsdGlwbGllclsxXVxuICBwbG90KG15aGlzdClcbiAgbGluZXMobXlkZW5zaXR5LCBjb2w9J2JsdWUnKVxuICBhYmxpbmUodj1ELCBjb2w9J3JlZCcpXG4gIE08LW1lYW4oRF9udWxsPkQpXG4gIHJldHVybihNKVxufSIsInNhbXBsZSI6IlBlcm11dGF0aW9uKHRyYWZmaWMsIFwiVFVOTkVMXCIsIFwiVk9MVU1FX1BFUl9NSU5VVEVcIiwxMDAwLFwiSG9sbGFuZFwiLCBcIkxpbmNvbG5cIikifQ== Note: You can find the permutation function code here: Permutation() NOTE: The red line in the output plots of the permutation test function is not the p-value, but it is just the difference of the value of means of the two categories under test. 3.1.3 Exercise - How p-value is affected by difference of means and standard deviations Here, you can generate your own data by changing parameters of the rnorm() function. See how changing the mean and standard deviation (sd) in rnorm distributions affects the p-value! Again you can do it directly in the code and observe the results immediately. It is very revealing. See how changing the mean and sd in rnorm distributions affects the p-value! Think of Val1, and Val2 as traffic volumes in Holland and Lincoln tunnels respectively. The larger the difference between the means of rnorm() function the smaller the p-value - since it is less and less likely that observed difference of means would come frequently, due to random shuffles of permutation function. In other words, it is more likely that the observed difference is real and it reflects the real difference in traffic volume between the two tunnels. Now keep the same means and change the standard deviations (sd). See how changing the standard deviations in rnorm() will affect the p-value and try to explain the effect that standard deviations have on the p-value. In general, the higher the standard deviation, the more widely the data is centered around the mean. Thus, for the same two means, we can see that larger values of standard deviations lead to higher p-values. If standard deviation is higher, with the same mean, the chance of randomly obtaining the observed result is higher leading to higher p-value. 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 3.2 Multiple Hypothesis - Bonferroni Correction. Multiple hypothesis testing refers to simultaneously testing more than one alternate hypothesis against the same NULL hypothesis. This often occurs in the process of so-called p-value chasing. In p-value chasing, a number of alternative hypotheses are tested until one with acceptable p-value is found. The process of p-value chasing is inherently flawed. Since it is prone to simply randomly finding an alternate hypothesis with p-value below the critical value alpha. Thus, we need to make our critical value requirements much stricker. Depending how many hypotheses are tested, we may require a much smaller p-value to reject the NULL. Say, we have tested 100 alternate hypotheses. The probability of getting at least one significant result with \\(\\alpha\\) = 0.05 can be computed as follows: \\[P(\\text{at least one significant result}) = 1- (1-0.05)^{100} ≈ 0.99\\] This means that if we continue to consider 0.05 as the critical value for p-value, then the probability of getting at least one significant result will be about 99%, which leads to false rejection of the NULL. The more hypotheses we test, we randomly will find one which will reject the NULL. Does it mean that testing multiple alternate hypotheses is forbidden? No, it cannot be. We do it all the time. We just have to change our p-value requirements. It has to be much harder to reject the NULL, when testing multiple alternate hypotheses. One such method for adjusting \\(\\alpha\\) is BONFERRONI CORRECTION! The Bonferroni correction sets the significance cut-off at \\(\\alpha / N\\) where N is the number of possible hypotheses. For example, in the example above, with 100 tests and \\(\\alpha = 0.05\\), you’d only reject a null hypothesis if the p-value is less than \\(\\alpha/N = 0.05/100 = 0.0005\\) Thus, the value of \\(\\alpha\\) after Bonferroni correction would be \\(0.0005\\). Again, let’s calculate the probability of observing at least one significant result when using the correction just described: \\[P(\\text{at least one significant result}) = 1 − P(\\text{no significant results}) \\\\ = 1 − (1 − 0.0005)^{100} ≈ 0.048\\] This gives us a 4.8% probability of getting at least one significant result randomly under NULL hypothesis. It is less than 5% and we can reject the NULL, after Bonferroni correction. As we can see this value of probability using Bonferroni correction is much better than the 99% which we saw before when we did not use correction for performing multiple hypothesis testing. The challenge in calculating the Bonferroni correction is correct estimation of the number of multiple hypotheses tested. How do we know what this number really is? For example, if we want to find two items in a supermarket which sell together more often than expected, we may find such a pair after testing all possible pairs of items. With a total number of items in thousands, this would lead to millions of possible pairs and millions of hypotheses tested. Bonferroni correction would then exceed million, and possibly even tens of millions. This would lead to miniscule critical value of 10^-7 or less for the p-value to meet. One may consider it too strict. In fact, the data scientist performing these tests may argue that s/he found the correlated pair of items after only 10 tests, not several millions of tests. Should one trust the data scientist? Or should the worst case number of necessary tests be considered? Both solutions are problematic. The former one relies on trust, why should we trust the data scientist? The latter one seems to conservative and stringent, making it virtually impossible to reject the NULL. This applies in particular to multidimensional data with a large number of dimensions. The larger number of dimensions, the larger (and exponentially so) the potential number of multiple hypotheses. Thus, even though there is more potential for exciting associations, the threshold for p-value is also more and more unlikely to be met. Thus rejection of NULL is exponentially harder. Dimensions (variables) may be correlated, and often are, but Bonferoni protects us assuming all hypotheses are independent. Too conservative and too pessimistic. These are concerns about the Bonferroni coefficient. For sure, however, the Bonferoni coefficient protects us against false rejection of NULL. It may prevent us from correct rejection of NULL, though. 3.2.1 Examples for Multiple hypothesis testing. Let’s consider the Happiness dataset as an example. Table 3.1: Snippet of Happiness Dataset IDN AGE COUNTRY GENDER IMMIGRANT INCOME HAPPINESS 2308 79560 53 Burkina Faso Female 0 93829 7.58 5029 22581 42 Mali Male 0 36832 3.44 810 74126 37 Macedonia Female 0 88653 7.42 3619 45007 61 Croatia Female 0 60472 5.65 5160 45408 21 Gabon Male 0 59253 4.24 2064 35969 55 Ecuador Female 0 51799 4.58 3633 93630 48 Germany Female 0 110105 9.55 2499 32904 53 Liberia Female 0 46973 3.55 1158 69366 49 Congo-Kinshasa Female 0 83952 6.53 6023 63936 30 Austria Female 0 81228 6.34 There are 156 unique countries in the dataset. This can be checked using the unique() function – unique(indiv_happiness$country) Since there are 156 distinct countries, we have \\({{n}\\choose{2}} = {156\\choose2}=(156 * 155)/2 = 12090\\) different hypotheses. Let’s call this value N. Using this N, the P-value cutoff after Bonferroni correction will be, \\(α = 0.05 / 12090 ≈ 4.13 *10^{-6}\\) 3.2.1.1 Example 1 Let’s calculate the P-value for the following hypotheses from the dataset. Our hypothesis: People from Canada are happier than people from Iceland. Null hypothesis: There is no difference in happiness levels of people from Canada and people from Iceland. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgZGF0YXNldFxuaGFwcGluZXNzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L0hBUFBJTkVTUzIwMTcuY3N2XCIsIHN0cmluZ3NBc0ZhY3RvcnMgPSBUKSAjd2ViIGxvYWRcblxuIyBUd28gc3Vic2V0cyBvZiBDYW5hZGEgYW5kIEljZWxhbmQgXG5oYXBwaW5lc3MuY2FuYWRhIDwtIHN1YnNldChoYXBwaW5lc3MkSEFQUElORVNTLCBoYXBwaW5lc3MkQ09VTlRSWSA9PVwiQ2FuYWRhXCIpXG5oYXBwaW5lc3MuaWNlbGFuZCA8LSBzdWJzZXQoaGFwcGluZXNzJEhBUFBJTkVTUywgaGFwcGluZXNzJENPVU5UUlkgPT0gXCJJY2VsYW5kXCIpXG5cbiMgTWVhbiBvZiBzdWJzZXRzLlxubWVhbi5jYW5hZGEgPC0gbWVhbihoYXBwaW5lc3MuY2FuYWRhKVxubWVhbi5pY2VsYW5kIDwtIG1lYW4oaGFwcGluZXNzLmljZWxhbmQpXG5cbm1lYW4uY2FuYWRhXG5tZWFuLmljZWxhbmRcblxuIyBMZW5ndGggb2Ygc3Vic2V0c1xubGVuLmNhbmFkYSA8LSBsZW5ndGgoaGFwcGluZXNzLmNhbmFkYSlcbmxlbi5pY2VsYW5kIDwtIGxlbmd0aChoYXBwaW5lc3MuaWNlbGFuZClcblxuIyBTdGFuZGFyZCBEZXZpYXRpb24gb2YgU3Vic2V0cy5cbnNkLmNhbmFkYSA8LSBzZChoYXBwaW5lc3MuY2FuYWRhKVxuc2QuaWNlbGFuZCA8LSBzZChoYXBwaW5lc3MuaWNlbGFuZClcblxuIyBDYWxjdWxhdGluZyBaLXNjb3JlIFxuemV0YSA8LSAobWVhbi5jYW5hZGEgLSBtZWFuLmljZWxhbmQpLyBzcXJ0KChzZC5jYW5hZGFeMikvbGVuLmNhbmFkYSArIChzZC5pY2VsYW5kXjIpL2xlbi5pY2VsYW5kKVxuemV0YVxuXG4jIENhbGN1bGF0ZSBwLXZhbHVlIGZyb20gWi1zY29yZVxucF92YWx1ZSA8LSBwbm9ybSgtemV0YSlcbnBfdmFsdWUifQ== In this case, after applying Bonferroni Correction we get the value of \\(α = 0.05/12090 ≈ 4.14 * 10^{-06}\\) Here, we get the p-value of 0.25 which is much higher than the value of our α. Based on this we fail reject our null hypothesis. 3.2.1.2 Example 2 Let’s consider the following hypotheses from the dataset. Our hypothesis: People from Italy are happier than people from Afghanistan. Null hypothesis: There is no difference in happiness levels of people from Italy and people from Afghanistan. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgZGF0YXNldFxuaGFwcGluZXNzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L0hBUFBJTkVTUzIwMTcuY3N2XCIsIHN0cmluZ3NBc0ZhY3RvcnMgPSBUKSAjd2ViIGxvYWRcblxuIyBUd28gc3Vic2V0cyBvZiBJdGFseSBhbmQgQWZnaGFuaXN0YW4gXG5oYXBwaW5lc3MuaXRhbHkgPC0gc3Vic2V0KGhhcHBpbmVzcyRIQVBQSU5FU1MsIGhhcHBpbmVzcyRDT1VOVFJZID09XCJJdGFseVwiKVxuaGFwcGluZXNzLmFmZ2hhbmlzdGFuIDwtIHN1YnNldChoYXBwaW5lc3MkSEFQUElORVNTLCBoYXBwaW5lc3MkQ09VTlRSWSA9PSBcIkFmZ2hhbmlzdGFuXCIpXG5cbiMgTWVhbiBvZiBzdWJzZXRzLlxubWVhbi5pdGFseSA8LSBtZWFuKGhhcHBpbmVzcy5pdGFseSlcbm1lYW4uYWZnaGFuaXN0YW4gPC0gbWVhbihoYXBwaW5lc3MuYWZnaGFuaXN0YW4pXG5cbm1lYW4uaXRhbHlcbm1lYW4uYWZnaGFuaXN0YW5cblxuIyBMZW5ndGggb2Ygc3Vic2V0c1xubGVuLml0YWx5IDwtIGxlbmd0aChoYXBwaW5lc3MuaXRhbHkpXG5sZW4uYWZnaGFuaXN0YW4gPC0gbGVuZ3RoKGhhcHBpbmVzcy5hZmdoYW5pc3RhbilcblxuIyBTdGFuZGFyZCBEZXZpYXRpb24gb2YgU3Vic2V0cy5cbnNkLml0YWx5IDwtIHNkKGhhcHBpbmVzcy5pdGFseSlcbnNkLmFmZ2hhbmlzdGFuIDwtIHNkKGhhcHBpbmVzcy5hZmdoYW5pc3RhbilcblxuIyBDYWxjdWxhdGluZyBaLXNjb3JlIFxuemV0YSA8LSAobWVhbi5pdGFseSAtIG1lYW4uYWZnaGFuaXN0YW4pLyBzcXJ0KChzZC5pdGFseV4yKS9sZW4uaXRhbHkgKyAoc2QuYWZnaGFuaXN0YW5eMikvbGVuLmFmZ2hhbmlzdGFuKVxuemV0YVxuXG4jIENhbGN1bGF0ZSBwLXZhbHVlIGZyb20gWi1zY29yZVxucF92YWx1ZSA8LSBwbm9ybSgtemV0YSlcbnBfdmFsdWUifQ== In this case, after applying Bonferroni Correction we get the value of \\(α = 0.05/12090 ≈ 4.14 * 10^{-06}\\) Here, we get the p-value of 0.00364 which is lower than the value of default p-value cutoff \\(α = 0.05\\), but this obtained p-value is higher than our Bonferroni correction cutoff. So, based on the results, we fail to reject our null hypothesis even though the obtained p-value is less than 0.05. EOC "],["classification.html", "Chapter 4 Data Modeling and Prediction techniques for Classification. 4.1 Decision Tree. 4.2 Use of Rpart 4.3 Visualize the Decision tree 4.4 Rpart Control 4.5 Prediction using rpart. 4.6 Split the data yourself. 4.7 Cross Validation", " Chapter 4 Data Modeling and Prediction techniques for Classification. 4.1 Decision Tree. Decision trees are one of the most powerful and popular tools for classification and prediction. The reason decision trees are very popular is that they can generate rules which are easier to understand as compared to other models. They require much less computations for performing modeling and prediction. Both continuous/numerical and categorical variables are handled easily while creating the decision trees. 4.2 Use of Rpart Recursive Partitioning and Regression Tree RPART library is a collection of routines which implement Classification and Regression Tree (CART) which is a type of Decision Tree.The resulting model can be represented as a binary tree. The library associated with this RPART is called rpart. Install this library using install.packages(\"rpart\"). Syntax for building the decision tree using rpart(): rpart( formula , method, data, control,...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... method: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use “anova” for regression, “class” for classification, etc. data: here we provide the dataset on which we want to fit the decision tree on. control: here we provide the control parameters for the decision tree. Explained more in detail in the section further in this chapter. For more info on the rpart function visit rpart documentation Lets look at an example on the Moody 2019 dataset. We will use the rpart() function with the following inputs: prediction -&gt; GRADE predictors -&gt; SCORE, ON_SMARTPHONE, ASKS_QUESTIONS, LEAVES_EARLY, LATE_IN_CLASS data -&gt; moody dataset method -&gt; “class” for classification. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG5ycGFydChHUkFERSB+IFNDT1JFK09OX1NNQVJUUEhPTkUrQVNLU19RVUVTVElPTlMrTEVBVkVTX0VBUkxZK0xBVEVfSU5fQ0xBU1MsIGRhdGEgPSBtb29keVssLWMoMSldLG1ldGhvZCA9IFwiY2xhc3NcIikifQ== We can see that the output of the rpart() function is the decision tree with details of, node -&gt; node number split -&gt; split conditions/tests n -&gt; number of records in either branch i.e. subset yval -&gt; output value i.e. the target predicted value. yprob -&gt; probability of obtaining a particular category as the predicted output. Using the output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section 4.5 But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section. 4.3 Visualize the Decision tree To visualize and understand the rpart() tree output in the easiest way possible, we use a library called rpart.plot. The function rpart.plot() of the rpart.plot library is the function used to visualize decision trees. The rpart.plot library is a front-end wrapper to the library prp which is the most basic library for plotting decision trees. prp allows various aesthetic modifications for visualizing the decision tree. We will look at a few examples of using prp below. But, first lets look at a example to visualize the output decision tree in the previous example on Moody dataset using rpart.plot() NOTE: The online runnable code block does not support rpart.plot and prp library and functions, thus the output of the following code examples are provided directly. # First lets import the rpart library library(rpart) # Import dataset moody &lt;- read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv&quot;) # Use of the rpart() function. tree &lt;- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody,method = &quot;class&quot;) # Now lets import the rpart.plot library to use the rpart.plot() function. library(rpart.plot) # Use of the rpart.plot() function to visualize the decision tree. rpart.plot(tree) Output Plot of rpart.plot() function We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about the grade category. the outcome probability of each grade category. the records percentage out of total records. To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides rpart.plot and Plotting with rpart.plot (PDF) Note that for any beginner using rpart.plot() function is the easiest way. But if you want to learn another way of plotting rpart trees then the following function can be used. So,another form of plotting rpart trees in a very minimalistic way is using the plot rpart i.e. prp() function, which is actually the working function behind rpart.plot(). Lets look at a same example like above but using prp(). # First lets import the rpart library library(rpart) # Import dataset moody &lt;- read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv&quot;) # Use of the rpart() function. tree &lt;- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody[,-c(1)],method = &quot;class&quot;) # Now lets import the rpart.plot library to use the rpart.plot() function. library(rpart.plot) # Use of the prp function to visualize the decision tree. prp(tree) Output Plot of prp() function We can see that the output of the prp() function is a very minimalist tree, without any colors with minimum required information. There are other arguments that can be passed to the prp() function to increase the aesthetic look and the information provided. To learn those extra arguments visit this guide prp() NOTE: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment. 4.4 Rpart Control Now let’s look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function. rpart.control( *minsplit*, *minbucket*, *cp*,...) minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -&gt; the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition. minbucket: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -&gt; the minimum number of observation in the terminal/leaf node of the trees must be 500 or above. cp: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by cp, will not be made in the tree. For more information of the other arguments of the rpart.control() function visit rpart.control Note: The ratio of minsplt to minbucket is 3:1. Thus if only one of the minsplit/minbucket is provided the other value is set using the above ratio. Also if both values are provided, unless the values are not in the above ratio, the rpart.control() the resorts to the default value. Also note, the default value of cp is 0.01. Let look at few examples. Suppose you want to set the control parameter minsplit=200. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIGxpYnJhcnkocnBhcnQpXG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbiB3aXRoIHRoZSBjb250cm9sIHBhcmFtZXRlciBtaW5zcGxpdD0yMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiAuLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKG1pbnNwbGl0ID0gMjAwKSlcblxuIyBDaGVjayB0aGUgY291bnQgb2Ygb2JzZXJ2YXRpb24gYXQgZWFjaCBzcGxpdCB0ZXN0LiBUbyBkbyB0aGlzIHdlIGZpbmQgdGhlIGNvdW50IGF0IGVhY2ggbm9uLWxlYWYvbm9uLXRlcm1pbmFsIG5vZGUuXG50cmVlJGZyYW1lW3RyZWUkZnJhbWUkdmFyIT1cIjxsZWFmPlwiLGMoXCJ2YXJcIixcIm5cIildXG5cbiMgbGlicmFyeShycGFydC5wbG90KVxuIyBycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=200 in rpart.control() function We can see from the output of tree$splits and the tree plot, that at each split the total amount of observations are above 200. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits. Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24gd2l0aCB0aGUgY29udHJvbCBwYXJhbWV0ZXIgbWluc3BsaXQ9MjAwXG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gLiwgZGF0YSA9IG1vb2R5WywtYygxKV0sbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5idWNrZXQgPSAxMDApKVxuXG4jIENoZWNrIHRoZSBjb3VudCBvZiBvYnNlcnZhdGlvbiBpbiBlYWNoIGxlYWYgbm9kZS5cbnRyZWUkZnJhbWVbdHJlZSRmcmFtZSR2YXI9PVwiPGxlYWY+XCIsYyhcInZhclwiLFwiblwiKV1cblxuIyBsaWJyYXJ5KHJwYXJ0LnBsb3QpXG4jIHJwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=100 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. Lets now use the cp parameter and see its effect on the tree. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24gd2l0aCB0aGUgY29udHJvbCBwYXJhbWV0ZXIgY3A9MC4wMDVcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiAuLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKGNwID0gMC4wMDUpKVxuXG4jIENoZWNrIHRoZSBhY2N1cmFjeSBpbmNyZWFzZSBmYWN0b3IgYXQgZWFjaCBzcGxpdC5cbnRyZWUkY3B0YWJsZVxuXG4jIGxpYnJhcnkocnBhcnQucGxvdCkpXG4jIHJwYXJ0LnBsb3QodHJlZSkifQ== Output tree plot of after setting cp=0.005 in rpart.control() function We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005. 4.5 Prediction using rpart. Now that we have seen the process to create a decision tree and also plot it, we will like to use the output tree to predict the required attribute. From the moody example, we are trying to predict the grade of students. Lets look at the predict() function to predict the outcomes. predict(*object*,*data*,*type*,...) object: the generated tree from the rpart function. data: the data on which the prediction is to be performed. type: the type of prediction required. One of “vector”, “prob”, “class” or “matrix”. Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbi5cbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStPTl9TTUFSVFBIT05FK0FTS1NfUVVFU1RJT05TK0xFQVZFU19FQVJMWStMQVRFX0lOX0NMQVNTLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZCkifQ== Our prediction accuracy on the training data set is 93.73% and the error rate is 6.27%.This prediction accuracy calculated on the training dataset is called training accuracy. However, what we are really interested in is testing data accuracy. How can we make sure that our prediction model will achieve good testing data accuracy? It is not enough to see good training data accuracy. We need to do more work. It is called cross validation.Use only part of the training data for training the prediction model and the remaining part for testing. For that we would need to split the training data at least into two parts, Training and Testing and then repeat the training process on the training dataset and the prediction on testing dataset. One way of doing this is to randomly assign data to either training or testing subset. We will look at a small example of splitting the complete dataset into training and testing dataset with a 70-30 ratio. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbi5cbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStPTl9TTUFSVFBIT05FK0FTS1NfUVVFU1RJT05TK0xFQVZFU19FQVJMWStMQVRFX0lOX0NMQVNTLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZClcblxuIyBMZXRzIGNoZWNrIHRoZSBjb3JyZWN0bmVzcyBvZiBlYWNoIHByZWRpY3Rpb24gZm9yIGVhY2ggcmVjb3JkXG5tZWFuKG1vb2R5JEdSQURFPT1wcmVkKSoxMDAifQ== 4.6 Split the data yourself. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFNwbGl0IHJhbmRvbWx5IGludG8gMiBzZXRzIHdpdGggY2VydGFpbiByYXRpby9wcm9iYWJpbGl0eS5cbmlkeCA8LSBzYW1wbGUoIDE6Miwgc2l6ZSA9IG5yb3cobW9vZHkpLCByZXBsYWNlID0gVFJVRSwgcHJvYiA9IGMoLjcsIC4zKSlcbm1vb2R5LnRyYWluIDwtIG1vb2R5W2lkeCA9PSAxLF1cbm1vb2R5LnRlc3QgPC0gbW9vZHlbaWR4ID09IDIsXVxuXG5ucm93KG1vb2R5KVxubnJvdyhtb29keS50cmFpbilcbm5yb3cobW9vZHkudHJhaW4pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnRlc3QpXG5ucm93KG1vb2R5LnRlc3QpL25yb3cobW9vZHkpIn0= As we can see we split the original data with 1580 rows into two dataset, training data with almost 70% of rows of the original, and testing data with almost 30% of the original. Notice that we used a random sampling of the data, and not just sequential, to avoid any unbalanced distribution of attributes. Now, we looked at a method to split the dataset into training and testing data. But there is another type of splitting of the dataset which involves splitting the data into 3 parts namely, training, cross-validation and testing. We will look at the use of cross-validation and the process, in the next section 4.7. Typically, the ratio of train-validation-test is 60-20-20 or 50-25-25. Before that lets look at a simple method to perform a 3 way split with ratio 60-20-20. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFNwbGl0IHJhbmRvbWx5IGludG8gMyBzZXRzIHdpdGggY2VydGFpbiByYXRpby9wcm9iYWJpbGl0eS5cbmlkeCA8LSBzYW1wbGUoIDE6Mywgc2l6ZSA9IG5yb3cobW9vZHkpLCByZXBsYWNlID0gVFJVRSwgcHJvYiA9IGMoMC42LCAwLjIsIDAuMikpXG5tb29keS50cmFpbiA8LSBtb29keVtpZHggPT0gMSxdXG5tb29keS52YWxpZGF0aW9uIDwtIG1vb2R5W2lkeCA9PSAyLF1cbm1vb2R5LnRlc3QgPC0gbW9vZHlbaWR4ID09IDMsXVxuXG5ucm93KG1vb2R5KVxubnJvdyhtb29keS50cmFpbilcbm5yb3cobW9vZHkudHJhaW4pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnZhbGlkYXRpb24pXG5ucm93KG1vb2R5LnZhbGlkYXRpb24pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnRlc3QpXG5ucm93KG1vb2R5LnRlc3QpL25yb3cobW9vZHkpIn0= We can see that the dataset is split into 3 parts, with 60% in training data, 20% in validation data, and 20% in testing data. In general, this process of repetitively using subsets of training data for training and testing is called cross validation. 4.7 Cross Validation The goal of cross-validation is to test the model’s ability to predict new data that was not used in estimating/training it, in order to avoid problems like overfitting and selection bias, and to give an insight on how the model will generalize to an independent dataset(i.e., an unknown dataset). Cross-validation also helps in selecting and fine-tuning the hyper-parameters of the models. In our case of the decision tree, the hyper parameters could be the control parameters that determine the size of the decision tree, which in-turn determines the accuracy of the tree. One round of cross-validation involves partitioning data into complementary subsets, and then performing model training on one subset, and validating the results on the other subset. In most methods, multiple rounds of cross-validation are performed using different partitions in each round, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model’s predictive performance. Another use of cross-validation is when you don’t have the test data, and hence, you don’t have a way to determine the true accuracy of the model. Because we cannot determine accuracy on the test dataset, we partition our training dataset into training and validation (testing). We train our model (rpart or lm) on the train partition and test on the validation partition. The accuracy on the validation data is called cross-validation accuracy, while that on the train data is called training accuracy. Lets not dive too deep into the theory of this cross validation technique, but lets learn about the cross_validate() function, that helps us achieve this. cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*) data: The dataset on which cross validation is to be performed. tree: The decision tree generated using rpart. n_iter: Number of iterations. split_ratio: The splitting ratio of the data into train data and validation data. method: Method of the prediction. “class” for classification. The way the function works is as follows: It randomly partitions your data into training and validation. It then constructs the following two decision trees on training partition: The tree that you pass to the function. The tree is constructed on all attributes as predictors and with no control parameters. -It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees. The first column corresponds to the cross-validation accuracy on the tree that you pass; the second is the cross-validation accuracy on the tree without any control and all attributes. The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting. We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired. The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant. Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration. Let’s look at the cross_validate() function in action in the example below. We will pass the tree with formula as GRADE ~ SCORE+ON_SMARTPHONE+LEAVES_EARLY, and control parameter, with minsplit=100. And for cross_validate() function, we will usen_iter=5, and split_raitio=0.7 NOTE: Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use install.packages(&quot;devtools&quot;) devtools::install_github(&quot;devanshagr/CrossValidation&quot;) CrossValidation::cross_validate() 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 NOTE: If you encounter error while running the cross-validation function that said “new levels encountered” in test, make sure the dataset is imported again with read.csv() attribute stringsAsFactors as TRUE or T. For more information about the inner-working of the cross_validate() function visit cross_validate() We can see in the output the Training accuracy, the table of cross-validation accuracy at each iteration for both the passed tree and the tree on all attribute and also their averages and variances. Few Observation from the selected example above are: For the tree passed with selected attributes and some control parameters, the cross-validation accuracy’s (i.e. accuracy values in the accuracy_subset column) are fairly high for all iterations and have very low variance. They are close to the training accuracy which indicates we are not overfitting. Observe that the accuracy at each iteration of the accuracy_subset and accuracy_all column are relatively, close but not exact, suggesting that there are more attributes or other control parameters that can be included to the passed tree, to further increase the accuracy, thus closing the gap. Thus using cross-validation we were able to figure out with certainty, that the passed tree, is not the best tree that can be created using the training data. Also, we saw whether the generated tree overfits the training data or not. EOC "],["regression.html", "Chapter 5 Data Modelling and Prediction techniques for Classification 5.1 Linear Regression.", " Chapter 5 Data Modelling and Prediction techniques for Classification In chapter 4 we studied modeling and prediction techniques for Classification, where we predicted the class of the output variable, using Decision tree based algorithms. In this chapter we will study techniques for regression based prediction and create models for it. As opposite the the classification where we predict a particular class of the output variable, here we will predict a numerical/continuous value. We will look at two methods of performing this regression based modeling and prediction, first simple linear regression and second regression using a decision tree. 5.1 Linear Regression. Linear regression is a linear approach to modeling the relationship between a numerical response (\\(Y\\)) and one or more independent variables (\\(X_i\\)). Linear models fitted to various different type of data spread. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. Credits: Wikipedia. 5.1.1 Linear regression using lm() function Syntax for building the regression model using the lm() function is as follows: lm(formula, data, ...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the linear regression model is to be trained. For more info on the lm() function visit lm() Lets look at the example on the RealEstate dataset. A snippet of the Realestate Dataset is given below. Table 5.1: Snippet of Real Estate Dataset Price Bedrooms Bathrooms Size 795000 3 3 2371 399000 4 3 2818 545000 4 3 3032 909000 4 4 3540 109900 3 1 1249 324900 3 3 1800 192900 4 2 1603 215000 3 2 1450 999000 4 3 3360 319000 3 2 1323 Now we can build a simple linear regression model to predict the Price attribute based on the various other attributes present in the dataset, as shown above. Since we will be predicting only one attribute values, this model will be called simple linear regression model. For the first example we will predict the price value of house using only size attribute as the predictor. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxucmVhbGVzdGF0ZTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvUmVhbEVzdGF0ZS5jc3ZcIilcblxuIyBzcGxpdGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhyZWFsZXN0YXRlKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC43LCAuMykpXG50cmFpbiA8LSByZWFsZXN0YXRlW2lkeCA9PSAxLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVtpZHggPT0gMixdXG5cbiMgVXNlIHRoZSBsbSgpIGZ1bmN0aW9uIHRvIHByZWRpY3QgdGhlIHByaWNlIGJhc2VkIG9uIHNpemUgb2YgdGhlIGhvdXNlLlxuIyBUaHVzIHRoaXMgaXMgYW4gZXhhbXBsZSBvZiBzaW1wbGUgbGluZWFyIHJlZ3Jlc3Npb24gc2luY2Ugb25seSBvbmUgcHJlZGljdG9yIGFuZCBvbmUgb3V0cHV0IHZhbHVlIGlzIHVzZWQuXG5zaW1wbGUuZml0IDwtIGxtKFByaWNlflNpemUsZGF0YT10cmFpbilcblxuIyBzdW1tYXJ5IG9mIHRoZSBtb2RlbFxuc3VtbWFyeShzaW1wbGUuZml0KVxuXG4jIExpbmVhciByZWxhdGlvbiBiZXR3ZWVuIHRoZSBQcmljZSBhbmQgU2l6ZSBhdHRyaWJ1dGUuXG5wbG90KHRyYWluJFNpemUsdHJhaW4kUHJpY2UpXG5hYmxpbmUoc2ltcGxlLmZpdCAsIGNvbD1cInJlZFwiKVxuXG4jIFByZWRpY3RpbmcgdmFsdWVzIG9uIHRoZSB0ZXN0IGRhdGFzZXQuXG5QcmVkaWN0ZWRQcmljZS5zaW1wbGUgPC0gcHJlZGljdChzaW1wbGUuZml0LHRlc3QpXG4jIFByZWRpY3RlZCBWYWx1ZXNcbmhlYWQoYXMuaW50ZWdlcih1bm5hbWUoUHJlZGljdGVkUHJpY2Uuc2ltcGxlKSkpXG4jIEFjdHVhbCBWYWx1ZXNcbmhlYWQodGVzdCRQcmljZSkifQ== The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The plot of Size vs Price, and the red line represents the fitted line or the linear model line which will be used for prediction. The predicted values are obtained from the predict function using the trained model and the test data. Now we will see an example of multiple linear regression model, where there can be multiple predictors to predict a single output attribute. Let look at an example of predicting the Price of real estate, based on three attributes: Size, Number of Bedrooms and Number of Bathrooms. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxucmVhbGVzdGF0ZTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvUmVhbEVzdGF0ZS5jc3ZcIilcblxuIyBzcGxpdGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhyZWFsZXN0YXRlKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC43LCAuMykpXG50cmFpbiA8LSByZWFsZXN0YXRlW2lkeCA9PSAxLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVtpZHggPT0gMixdXG5cblxuIyBVc2UgdGhlIGxtKCkgZnVuY3Rpb24gdG8gcHJlZGljdCB0aGUgcHJpY2UgYmFzZWQgb24gc2l6ZSwgYmF0aHJvb21zIGFuZCBiZWRyb29tcyBvZiB0aGUgaG91c2UuXG4jIFRodXMgdGhpcyBpcyBhbiBleGFtcGxlIG9mIG11bHRpcGxlIGxpbmVhciByZWdyZXNzaW9uIHNpbmNlIG11bHRpcGxlIHByZWRpY3RvciBhbmQgb25lIG91dHB1dCB2YWx1ZSBpcyB1c2VkLlxubXVsdGlwbGUuZml0IDwtIGxtKFByaWNlflNpemUgKyBCYXRocm9vbXMgKyBCZWRyb29tcyxkYXRhPXRyYWluKVxuXG4jIHN1bW1hcnkgb2YgdGhlIG1vZGVsXG5zdW1tYXJ5KG11bHRpcGxlLmZpdClcblxuIyBQcmVkaWN0aW5nIHZhbHVlcyBvbiB0aGUgdGVzdCBkYXRhc2V0LlxuUHJlZGljdGVkUHJpY2UubXVsdGlwbGUgPC0gcHJlZGljdChtdWx0aXBsZS5maXQsdGVzdClcbiMgUHJlZGljdGVkIFZhbHVlc1xuaGVhZChhcy5pbnRlZ2VyKHVubmFtZShQcmVkaWN0ZWRQcmljZS5tdWx0aXBsZSkpKVxuIyBBY3R1YWwgVmFsdWVzXG5oZWFkKHRlc3QkUHJpY2UpIn0= The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained from the predict function using the trained model and the test data. In comparison to the previous model based on just the Size as predictor, here, when we used 3 predictors, we have more accurate predictions, thus increasing the overall accuracy of the model. 5.1.2 Calculating the Error using mse() The equation to calculate the MSE is as follows: \\[\\begin{equation} MSE=\\frac{1}{n} \\sum_{i=1}^{n}{(Y_i - \\hat{Y_i})^2} \\\\ \\text{where $n$ is the number of data points, $Y_i$ are the observed value}\\\\ \\text{and $\\hat{Y_i}$ are the predicted values} \\end{equation}\\] To implement this, we will use the mse() function present in the Metrics Package, so remember to install the Metrics package and use library(Metrics) in the code for local use. The syntax for mse() function is very simple: mse(actual,predicted) actual: vector of the actual values of the attribute we want to predict. predicted: vector of the predicted values obtained using our model. Now lets look at the MSE of the previous example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxucmVhbGVzdGF0ZTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvUmVhbEVzdGF0ZS5jc3ZcIilcblxuIyBzcGxpdGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhyZWFsZXN0YXRlKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC43LCAuMykpXG50cmFpbiA8LSByZWFsZXN0YXRlW2lkeCA9PSAxLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVtpZHggPT0gMixdXG5cbiMgVXNlIHRoZSBsbSgpIGZ1bmN0aW9uIHRvIHByZWRpY3QgdGhlIHByaWNlIGJhc2VkIG9uIHNpemUgb2YgdGhlIGhvdXNlLlxuc2ltcGxlLmZpdCA8LSBsbShQcmljZX5TaXplLGRhdGE9dHJhaW4pXG4jIFByZWRpY3RpbmcgdmFsdWVzIG9uIHRoZSB0ZXN0IGRhdGFzZXQuXG5QcmVkaWN0ZWRQcmljZS5zaW1wbGUgPC0gcHJlZGljdChzaW1wbGUuZml0LHRlc3QpXG4jIFByZWRpY3RlZCBWYWx1ZXNcbmhlYWQoYXMuaW50ZWdlcih1bm5hbWUoUHJlZGljdGVkUHJpY2Uuc2ltcGxlKSkpXG4jIEFjdHVhbCBWYWx1ZXNcbmhlYWQodGVzdCRQcmljZSlcblxuIyBMZXRzIHVzZSB0aGUgbXNlKCkgZnVuY3Rpb24gdG8gXG5tc2UodGVzdCRQcmljZSxQcmVkaWN0ZWRQcmljZS5zaW1wbGUpIn0= We can see the MSE is too large above 200 billion, and this is huge value could be understandable as we are taking the squared differences of all the records that we predicted. The main intention is to get this huge value to as low as possible possibly near zero, which could be difficult but can be achieved upto a relative error by using a better model and training data. EOC "],["models.html", "Chapter 6 Additional Predition techniques. 6.1 Four Line Method for creating most type of prediction models in R 6.2 Random Forest 6.3 SVM", " Chapter 6 Additional Predition techniques. There are a number of machine learning methods, beyond rpart() and lm() which can be used to build prediction models. As promised, we will show here how to use more sophisticated methods without explaining how these methods work. For example, we use methods such as Support Vector Machines (SVM) and Random Forest (RF). We encourage students to use other methods such as Neural Networks and Linear Discriminant Analysis (LDA). Turns out the same script can be used to apply many advanced ML methods to build prediction models. Here we present a general 4-step script to use these more complex and sophisticated models. 6.1 Four Line Method for creating most type of prediction models in R The first step now will obviously be to install and load the packages that contain a given ML algorithm. To do that on your local machine, use the following code. # Install the library install.packages(&quot;package name&quot;) # Load the library in R library(package_name) Once we have the algorithm library loaded, we then proceed to build the model. pred.model = model_function(formula, data, method*,...) model_function(): the function present in the library to build the model. e.g. rpart() formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the ML model is to be trained on. Remember never used the test data to build the model. method: (OPTIONAL) Used to denote the method of prediction or underlying algorithm. This parameter could be present in some model_function() but not all. Prediction using the predict() function on the training data to assess the models performance/accuracy in next step. pred = predict(pred.model, newdata = train) predict(): the common function for all models used for prediction. pred.model: output of the step 1. newdata: here we assign the data on which the prediction is to be done. Evaluate error in Training phase. We use the mse() function for finding the accuracy of the model. To read more in dept about the mse() function refer to section 5.1.2. mse(actual, pred) actual: vector of the actual values of the attribute we want to predict. pred: vector of the predicted values obtained using our model. Repeat steps 1-4 by changing the ML algorithm. Finally we predict on the testing data using the same predict function as in step 2 but replacing the train data with test data. pred = predict(pred.model, newdata = test) These are the 4 steps to follow while performing any prediction task using ML models in R. We can also add one more step between step 3 and 4, which is step of performing the cross validation process on the newly built models. This can be done either manually, or by using third party libraries. One such library is the rModeling package, which has function crossValidation() which can be used for any type of model_functions(). For more information visit crossValidation() Before we proceed to the next section, please look at the snippet of the earnings.csv dataset, which we will be using for predicting the Earnings attribute based on various other attributes provided in the dataset, using different prediction models. Table 6.1: Snippet of Earnings Dataset GPA Number_Of_Professional_Connections Earnings Major Graduation_Year Height Number_Of_Credits Number_Of_Parking_Tickets 8580 2.91 4 9709.00 Buisness 2011 68.75 121 1 1527 2.94 7 9696.93 STEM 2002 64.53 122 1 7219 3.01 60 11702.19 Professional 1970 68.18 123 1 7623 2.63 7 11747.87 Professional 1982 66.54 120 0 9017 2.09 31 5960.28 Other 1998 66.26 121 0 9982 2.18 27 5726.68 Other 1988 66.72 128 1 5635 2.99 59 13298.95 Vocational 1998 67.49 122 0 141 2.45 24 9746.44 STEM 1969 67.06 122 0 4736 2.17 14 13214.68 Vocational 2003 67.33 121 1 773 2.32 43 9779.64 STEM 1981 68.44 120 1 Now that we have seen the general structure of the model and took a glance at the dataset we will be using, let’s show how we can use Random Forest (RF) and SVM from the R library. 6.2 Random Forest Here is how we use the 4-step script to use Random Forest: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJhbmRvbUZvcmVzdClcbmxpYnJhcnkoTW9kZWxNZXRyaWNzKVxuXG4jIExvYWQgdGhlIGRhdGFzZXQuXG5lYXJuaW5nZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvZWFybmluZ3MuY3N2XCIsc3RyaW5nc0FzRmFjdG9ycyA9IFQpXG5cbiMgc3BsaXR0aW5nIHRoZSBkYXRhc2V0IGludG8gdHJhaW5pbmcgYW5kIHRlc3RpbmcuXG5pZHggPC0gc2FtcGxlKCAxOjIsIHNpemUgPSBucm93KGVhcm5pbmdkYXRhKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC44LCAuMikpXG50cmFpbiA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMSxdXG50ZXN0IDwtIGVhcm5pbmdkYXRhW2lkeCA9PSAyLF1cblxuIyAxLiBCdWlsZCBwcmVkaWN0aW9uIG1vZGVsIHVzaW5nIHJhbmRvbUZvcmVzdCgpIGZ1bmN0aW9uLlxucHJlZC5tb2RlbCA8LSByYW5kb21Gb3Jlc3QoRWFybmluZ3Mgfi4sIGRhdGEgPSB0cmFpbilcblxuIyBMZXRzIHNlZSB0aGUgc3VtbWFyeSBvZiB0aGUgcmFuZG9tRm9yZXN0IG1vZGVsLlxucHJlZC5tb2RlbFxuXG4jIDIuIFByZWRpY3QgdXNpbmcgdGhlIG5ld2x5IGJ1aWx0IG1vZGVsIG9uIHRoZSB0cmFpbmluZyBkYXRhc2V0LlxucHJlZC50cmFpbiA8LSBwcmVkaWN0KHByZWQubW9kZWwsbmV3ZGF0YSA9IHRyYWluKVxuXG4jIDMuIEV2YWx1YXRlIGVycm9yIG9uIHRyYWluaW5nIHVzaW5nIHRoZSBtc2UoKSBmdW5jdGlvbi5cbm1zZSh0cmFpbiRFYXJuaW5ncyxwcmVkLnRyYWluKVxuXG4jIDQuIFByZWRpY3Qgb24gdGhlIHRlc3RpbmcgZGF0YS5cbnByZWQudGVzdCA8LSBwcmVkaWN0KHByZWQubW9kZWwsbmV3ZGF0YSA9IHRlc3QpXG5cbiMgQWRkaXRpb25hbGx5IHNpbmNlIGhlcmUgd2UgaGF2ZSB0aGUgYWN0dWFsL3JlYWwgcHJlZGljdGlvbiB2YWx1ZXMgd2UgY2FuIGFsc28gY2hlY2sgdGhlIGFjY3VyYWN5IG9mIG91ciBwcmVkaWN0aW9uIG9uIHRlc3RpbmcgZGF0YS5cbm1zZSh0ZXN0JEVhcm5pbmdzLHByZWQudGVzdCkifQ== After we run RF, we see the summary of the output randomForest model. We are only interested here in the mean squared error of the predicted values using the training sub-dataset and mean squared error of the predicted values using the testing sub-dataset. 6.3 SVM SVM is another ML method to predict numerical variables. Again, we won’t describe how it works, but rather how to use it, again referring to the 4-step script described earlier. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGUxMDcxKVxubGlicmFyeShNb2RlbE1ldHJpY3MpXG5cbiMgTG9hZCB0aGUgZGF0YXNldC5cbmVhcm5pbmdkYXRhPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9lYXJuaW5ncy5jc3ZcIixzdHJpbmdzQXNGYWN0b3JzID0gVClcblxuXG4jIHNwbGl0dGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhlYXJuaW5nZGF0YSksIHJlcGxhY2UgPSBUUlVFLCBwcm9iID0gYyguOCwgLjIpKVxudHJhaW4gPC0gZWFybmluZ2RhdGFbaWR4ID09IDEsXVxudGVzdCA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMixdXG5cbiMgMS4gQnVpbGQgcHJlZGljdGlvbiBtb2RlbCB1c2luZyBzdm0oKSBmdW5jdGlvbi5cbnByZWQubW9kZWwgPC0gc3ZtKEVhcm5pbmdzIH4uLCBkYXRhID0gdHJhaW4pXG5cbiMgTGV0cyBzZWUgdGhlIHN1bW1hcnkgb2YgdGhlIHN2bSBtb2RlbC5cbnByZWQubW9kZWxcblxuIyAyLiBQcmVkaWN0IHVzaW5nIHRoZSBuZXdseSBidWlsdCBtb2RlbCBvbiB0aGUgdHJhaW5pbmcgZGF0YXNldC5cbnByZWQudHJhaW4gPC0gcHJlZGljdChwcmVkLm1vZGVsLG5ld2RhdGEgPSB0cmFpbilcblxuIyAzLiBFdmFsdWF0ZSBlcnJvciBvbiB0cmFpbmluZyB1c2luZyB0aGUgbXNlKCkgZnVuY3Rpb24uXG5tc2UodHJhaW4kRWFybmluZ3MscHJlZC50cmFpbilcblxuIyA0LiBQcmVkaWN0IG9uIHRoZSB0ZXN0aW5nIGRhdGEuXG5wcmVkLnRlc3QgPC0gcHJlZGljdChwcmVkLm1vZGVsLG5ld2RhdGEgPSB0ZXN0KVxuXG4jIEFkZGl0aW9uYWxseSBzaW5jZSBoZXJlIHdlIGhhdmUgdGhlIGFjdHVhbC9yZWFsIHByZWRpY3Rpb24gdmFsdWVzIHdlIGNhbiBhbHNvIGNoZWNrIHRoZSBhY2N1cmFjeSBvZiBvdXIgcHJlZGljdGlvbiBvbiB0ZXN0aW5nIGRhdGEuXG5tc2UodGVzdCRFYXJuaW5ncyxwcmVkLnRlc3QpIn0= We will use the svm() function from the e1071 package. For more information about this function and its attributes visit svm() Again, from the summary of the output svm model we are interested only in the MSE of the predicted values using the training sub-dataset and the MSE of the predicted values using the testing sub-dataset. There are tens of packages such as Naive Bayes, LDA, or Neural Networks in the R library which we can use following the same simple 4-step script. In order to learn how these methods actually work you need to take more advanced classes on Machine learning and Data Science/ But you already know how to use them. You drive the car! EOC "],["predblogs.html", "Chapter 7 Prediction Challenges 7.1 Prediction Challenge 1. 7.2 Prediction Challenge 2. 7.3 Prediction Challenge 3. 7.4 Prediction Challenge 4.", " Chapter 7 Prediction Challenges Each prediction challenge is based on the training data generated synthetically with some embedded patterns. Students can either build their own prediction models from scratch (coding their own prediction models without using R libraries, “free style”) or utilize R libraries for a multitude of machine learning methods. These methods range from decision trees (rpart, recursive partitioning) through linear regression (lm), svm and even neural networks. Students test their prediction models on the testing data. We use Kaggle to automatically calculate the prediction errors and rank student solutions by prediction accuracy. Depending on the type of independent, target variable we use either prediction accuracy or MSE. As data creators we know the method of data generation, therefore we can construct the perfect prediction model, Accuracy of the perfect prediction model provides a soft upper bound for prediction accuracy of all possible prediction models, created without knowing how data was generated. Thus, there is no general, absolute, notion of a “good” prediction model. It all depends on the data. Sometimes prediction accuracy of 60% is excellent. For other data sets, accuracy of 95% may not be good enough. If the accuracy of a prediction model is near the accuracy of a perfect prediction model, such a model is definitely good. 7.1 Prediction Challenge 1. For this prediction challenge we used our favorite dataset, the Professor Moody dataset, and predicted the Grade category of all students. The Grade category had only 2 factors: Pass OR Fail. Let’s look at a snippet of the moody dataset used for training in this challenge. Table 7.1: Snippet of Moody Dataset(TRAINING) for Prediction Challenge 1 Studentid Attendance Major Questions Score Seniority Texting Grade 29998 40 Stat Rarely 65 Freshman Always Pass 29999 30 Cs Always 46 Senior Rarely Fail 30000 90 Communication Rarely 60 Senior Always Pass 30001 5 Polsci Always 46 Senior Rarely Pass 30002 67 Cs Rarely 36 Senior Always Fail 30003 20 Stat Rarely 50 Senior Rarely Pass 30004 78 Stat Rarely 0 Senior Always Pass 30005 27 Polsci Rarely 45 Junior Always Fail 30006 81 Polsci Rarely 6 Sophomore Always Fail 30007 50 Communication Rarely 97 Senior Always Pass 7.1.1 How the data was generated for Challenge 1 Professor Moody’s data set has been synthetically generated using a random generator which follows probabilistic rules implementing “secret patterns” which we embedded in the data. These patterns are presented below in the form of a decision tree. For example, a rule that Statistics majors with scores over 60, pass the class - reflects the generated data in which a high percentage (90% to be exact) of students majoring in Statistics indeed pass Moody’s class.. Stats students with scores below 60 fail the class in 80% of the cases. Similar rules were applied to all other majors. Based on the way data was generated, the prediction model can be built. We call it the perfect prediction model, since it is constructed with knowledge how data was generated. This model follows the following tree: Major Stat Score &gt; 60 Pass Score &lt;= 60 Fail Comm Score &gt;40 Pass Score &lt;=40 Texting = Rarely Fail Texting = Always PAss Polsci Score &gt;50 Pass Score &lt;=50 Questions = rare Fail Questions = always Pass Cs Score &gt;70 Pass Score &lt;=70 Seniority= Freshman Score &gt;50 Pass Score &lt;=50 Attendance &gt;=60 Score &gt; 40 Pass Score &lt;=40 Fail Attendance &lt;60 Fail Seniority= Sophomore Score &gt;50 Pass Score &lt;=50 Fail Seniority= Junior Fail Seniority = Senior Fail The code below implements perfect prediction model using the above prediction model rules eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YVxuZGF0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMXRlc3Qtc3R1ZGVudHMuY3N2XCIsc3RyaW5nc0FzRmFjdG9ycyA9IFQpXG5cbiMgQ3JlYXRlIGFuIGFsbCBGYWlsIENhdGVnb3J5L1ByZWRpY3RlZCBWYWx1ZSBWZWN0b3IgYW5kIGFwcGVuZCBpdCB0byB0aGUgdGVzdGluZyBkYXRhc2V0LlxuZGF0JEdyYWRlPC1yZXAoJ0ZhaWwnLCBucm93KGRhdCkpXG5cbiMgVGhlIHZhcmlvdXMgY29uZGl0aW9ucyB1c2VkIHRvIHByZWRpY3QgZ3JhZGUgYXR0cmlidXRlLlxuXG4jIEZvciBNYWpvciA9IFN0YXRcbmRhdFtkYXQkTWFqb3I9PSdTdGF0JyAmIGRhdCRTY29yZT42MCxdJEdyYWRlIDwtJ1Bhc3MnXG5cbiMgRm9yIE1ham9yID0gQ29tbVxuZGF0W2RhdCRNYWpvcj09J0NvbW11bmljYXRpb24nICYgZGF0JFNjb3JlPjQwLF0kR3JhZGUgPC0nUGFzcydcbmRhdFtkYXQkTWFqb3I9PSdDb21tdW5pY2F0aW9uJyAmIGRhdCRTY29yZTw9NDAgJiBkYXQkVGV4dGluZz09J0Fsd2F5cycsXSRHcmFkZSA8LSdQYXNzJ1xuXG4jIEZvciBNYWpvciA9IFBvbHNjaVxuZGF0W2RhdCRNYWpvcj09J1BvbHNjaScgJiBkYXQkU2NvcmU+NTAsXSRHcmFkZSA8LSdQYXNzJ1xuZGF0W2RhdCRNYWpvcj09J1BvbHNjaScgJiBkYXQkU2NvcmU8PTUwICYgZGF0JFF1ZXN0aW9ucz09J0Fsd2F5cycsXSRHcmFkZSA8LSdQYXNzJ1xuXG4jIEZvciBNYWpvciA9IENzXG5kYXRbZGF0JE1ham9yPT0nQ3MnICYgZGF0JFNjb3JlPjcwLF0kR3JhZGUgPC0nUGFzcydcbmRhdFtkYXQkTWFqb3I9PSdDcycgJiBkYXQkU2NvcmU8PTcwICYgZGF0JFNlbmlvcml0eT09J0ZyZXNobWFuJyAmIGRhdCRTY29yZT41MCxdJEdyYWRlIDwtJ1Bhc3MnXG5kYXRbZGF0JE1ham9yPT0nQ3MnICYgZGF0JFNjb3JlPD03MCAmIGRhdCRTZW5pb3JpdHk9PSdGcmVzaG1hbicgJiBkYXQkQXR0ZW5kYW5jZSA+PTYwICYgZGF0JFNjb3JlPjQwLF0kR3JhZGUgPC0nUGFzcydcbmRhdFtkYXQkTWFqb3I9PSdDcycgJiBkYXQkU2NvcmU8PTcwICYgZGF0JFNlbmlvcml0eT09J1NvcGhvbW9yZScgJiBkYXQkU2NvcmU+NTAsXSRHcmFkZSA8LSdQYXNzJ1xuXG5cbiMgQ29tcGFyZSBpdCB3aXRoIHRoZSBpZGVhbCBwcmVkaWN0aW9ucyBmb3IgY2hlY2tpbmcgYWNjdXJhY3kgb2Ygb3VyIHByZWRpY3Rpb25zLlxuYW5zd2VyczwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTTIwMjF0ZXN0X2Fuc3dlci5jc3ZcIixzdHJpbmdzQXNGYWN0b3JzID0gVClcbm1lYW4oZGF0JEdyYWRlPT1hbnN3ZXJzJEdyYWRlKSAjIEFjY3VyYWN5In0= Accuracy of the perfect prediction model defined above was around 83% on the testing data set. In the next section we review some top student submissions. Surprisingly all top 10 student solutions achieve accuracy beating the ideal 83% accuracy by a solid few percentage points. In other words, these students do better than the prediction model which is fully aware of the rules used in data generation! How is this possible? There may be several reasons for this. First,the synthetically generated data satisfies some spurious, random patterns, in addition to the patterns built into the data generation process. The smaller the generated data set, the more likely these spurious patterns are. Perfect prediction model will not incorporate these spurious patterns. Prediction models built by students on the basis of the training data set may take advantage of these spurious patterns. Summarizing, the actual data set has some additional noise, and some random patterns which are generated as unintended side effects of our generation procedure. The larger our generated data sets are, the less prominent these spurious patterns are. Typically our data sets have a few thousand tuples. This is not large enough to combat the law of small numbers - some extreme patterns appearing randomly. Another reason could be getting away with overfitting - since even though we have used testing data which is different from training data, we only tested the student prediction models once, against one specific test data set. The students’ models may have overfit the training data and testing data as well (since only one testing data set was used). If we tested against multiple testing data sets, this overfitting effect may have vanished. 7.1.2 Top Submissions for Challenge 1. Jeremy Prasad Jeremy’s PPT Jeremy performed exceptionally well in this prediction challenge. His approach was an iterative learning process, where at each step after performing analysis he tried to decrease the error more and more. He started with a very basic model, of using just the score attribute with a hard threshold for pass or fail grade based on the score value. After this, to increase accuracy, he analysed the data more found which attributes effect the prediction of the data, and which are not really useful After finding these highly effective attributes, he wrote concrete set of attributs that can be used to assign the grade. Most of them were dependent on 2-3 attributes like Major-Senioriy-Score, Major-Score, or Major-Questions-Score,etc. Jeremy’s model has achieved accuracy of nearly 87% - beating the perfect prediction model by 4 percentage points. The reasons for these unexpected results were discussed earlier in this section. Rohit Manjunath Rohit’s PPT Rohit performed well in this prediction challenge, and has a different approach than that of Jeremy’s. In Rohit’s approach, instead of finding the minimum global threshold of pass or fail based on score, he found the threshold for the maximum score, above which every student passed the class. He then analysed the data based on the Majors first and then found interval threshold for each Majors scores. For some Majors, to increase accuracy, he further explored other attributes in detail to find which effects the final grade. Rohit obtained accuracy of almost 85%, beating the accuracy of perfect model by 2%. 7.2 Prediction Challenge 2. Both the training and testing data sets for this prediction challenge were the same as those in the prediction challenge 1. Students were simply allowed to use the rpart library in R to build their prediction models. This provided the opportunity to assess the value of pre-packaged libraries such as rpart as opposed to manual coding the prediction models. With rpart() doing most work of prediction in this task, the students were also asked to provide validation for their models prediction power/accuracy. This involved use of cross-validation techinques, which for the ease of this course level was provided in a custom function, see 4.7. In general, rpart() offered great help to the vast majority of students. In fact around 70% of all students achieved identical accuracy on the testing data set, which was a few points shy of the ideal 83%. This was the case because they used the same prediction model - a simple default rpart model. Interestingly rpart() accuracy did not beat the few top prediction model solutions for challenge 1. These were coded manually without using any R libraries. In fact nobody has beat the top accuracy achieved for the prediction challenge 1 by Jeremy Prasad! However he spent a lot of time building his prediction model from the ground. And got away with some overfitting for sure. To perform this challenge yourself please visit the kaggle site of this prediction challenge. Link to Kaggle Site: Prediction Challenge 2 7.2.1 How the data was generated for Challenge 2 As we saw that the prediction task and the datasets in challenge 2 are similar to that of challenge 1. Thus the data analysis of the challenge 1 would applicable in this case too. 7.2.2 Top Submissions for Challenge 2 Kevin Larkin Kevin’s PPT This was the top submission in terms of accuracy score on Kaggle. Kevin used the rpart() function, for modeling, with all the attributes of the training dataset except Studentid. To increase the accuracy of his model, he used the rpart.control() function parameters, especially the cp parameter of the function, which increased the splitting accuracy. Kevin achieved an accuracy score of over 86% on the test dataset for this challenge. Michael Ryvin Michael’s PPT This was the second best submission as per accuracy score on Kaggle. Michael used the rpart() function, along with some control parameters for creating the decision tree. Michael achieved an accuracy score of over 86% on the test dataset. Shuohao Ping Shuohao’s PPT This was the third best submission as per accuracy score on Kaggle. Shuohao used multiple iterations to create his final model. In each iteration, Shuohao tried to vary the control parameters and its values to find the best fit model after cross-validation. Shuohao, achieved an accuracy score of over 86% on the test dataset. 7.3 Prediction Challenge 3. In prediction challenge 3, the task was to predict Earnings as a numerical variable, using any ML algorithm. Earnings variable is part of the Earnings dataset which is a synthetic data set relating earnings of a person to different attributes such as GPA in college, Major, as well as number of personal connections and several other attributes. Lets look at a snippet of the Earnings dataset used for training the models below. Table 7.2: Snippet of Earnings Dataset(TRAINING) for Prediction Challenge 3 GPA Number_Of_Professional_Connections Earnings Major Graduation_Year Height Number_Of_Credits Number_Of_Parking_Tickets 2.50 1 9756.15 STEM 2001 64.22 124 1 2.98 1 9709.03 STEM 2001 69.55 120 0 2.98 23 9711.37 STEM 1996 68.98 120 1 3.35 5 9656.15 STEM 2008 69.23 124 1 2.47 37 9751.92 STEM 1981 70.45 123 0 2.75 2 9728.30 STEM 2000 65.26 121 0 1.66 17 9847.59 STEM 2001 65.91 121 0 2.59 10 9743.36 STEM 1990 66.35 123 0 1.89 7 9793.38 STEM 1975 70.42 121 1 1.89 22 9810.38 STEM 1997 65.18 122 0 7.3.1 How the data was generated for Challenge 3 The data was generated differently for different values of Education attribute - which described several classes of majors such as STEM, Humanities, Professional etc. The relationship between earnings and other attributes was generated by different formulas for different values of Education attribute as follows: Stem earn = -100 * gpa +10000 Humanities earn = 100* gpa + 10000 Vocational earn = 100 * gpa + 13000 Professional earn = -100gpa +12000 other earn = connection ^2 +5000 business earn = gpa * 100 * parity +10000 where parity = 1 if graduation year = even 0 if graduation year = odd As we can see, these formulas are mostly linear, while the formula for “other” education attributes is quadratic. Also, for “Business” education attribute subjects, the formula is dependent on an additional attribute. Notice that the rule for business majors was quite tricky - making the earning formula dependent on the student’s graduation year being even or odd! For more detailed data analysis please view the document attached here. Pred 3 Analysis How the data was modeled in R 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 We can see that the perfect model, based on knowledge of the way data was generated, achieved MSE of around 3300. Certainly prediction models with MSE close to 3300 would be considered very good. Again, at first quite surprisingly, there were quite a number of submissions with MSE of prediction models being far less than 3300. Explanation of how the perfect model could be so soundly beaten is “getting away” with overfitting models for the case of just single testing data set. 7.3.2 Top Submissions for Challenge 3 Seok Yim Seok’s PPT This was the top submission based on MSE score, with a final score less than 100. Seok’s analyzed data using plots first and quickly discovered that subsetting data based on Education is the way to go For each subset (based on a different value of Education attribute) he subsequently built a different model In comparison to the perfect model accuracy, this MSE was over one order of magnitude smaller, just like Nick Whelan’s solution below. Nick Whelan Nick’s PPT This was another top submission based on MSE score, with final score less than 100. The approach to solving the task was different compared to Seok’s implementation, but was equally good, with nearly the same prediction power/accuracy. Nick tried to use the randomForest algorithm on the whole dataset as the initial model, but the MSE turned out to be near 25,000. Then he did some free-style analysis and found the linear relationship between various subsets of dataset with the earnings value. To implement this he used the fundamentals of linear regression very well while creating a learning model, and also used a quadratic model where needed. This resulted in a very accurate model with low MSE score. Bennett Garcia Bennett’s PPT Bennett had a final MSE score of below 100 and was one of the top submissions for this challenge. A significantly different learning model was used by Bennett to achieve this low MSE. He first analyzed the data, and found attributes on which the dataset can be subsetted on. Then, he here used Neural Networks as models for prediction on those subsets. This Neural Network approach was very well implemented. 7.4 Prediction Challenge 4. The challenge 4 involved “hidden” derived attributes. We have hoped that students who combined prior data inspection (plotting) with machine learning packages from R library will be able to achieve very high accuracy, certainly over 90%. Unfortunately, none of the student solutions came even close to the perfect model accuracy, which was 92%. Pretty much all student solutions centered around the upper 60s percentile. Even the top 3 solutions barely exceeded 68%! Let us first describe the challenge and the way data was generated. Mysterious box was found on the beach. Despite spending probably years in the water, it still works! But what does it do? It has four inputs (electric) &amp; a switch. Setting these inputs and different switch positions emits various weird and scary sounds as output in response to the electric signals. It sizzles, gurgles, hisses, ominously tics like a bomb,etc…..but nothing happens - just sounds. So no harm will happen to surroundings. Predict the output sound of the box based on one of the four inputs INPUTs, INPUT1,…INPUT4 (numerical values) and SWITCH which has five discrete positions. Let’s look at a snippet of the Box on the Beach dataset used for training the models below. The training describes which sounds have been noted in the laboratory in nearly 20,000 experiments combining different input signals and switch positions. Table 7.3: Snippet of Black Box Dataset(TRAINING) for Prediction Challenge 4 ID INPUT1 INPUT2 INPUT3 INPUT4 SWITCH SOUND 86623 30 31 72 29 Low Gargle 57936 87 76 31 79 Low Tick 54301 16 33 87 41 Low Tick 2678 64 77 91 59 Minimum Beep 65827 33 72 53 66 High Beep 22420 5 50 26 50 High Gargle 2285 82 72 60 73 High Tick 62571 44 85 100 8 Minimum Kaboom 49229 92 31 100 64 Low Gargle 63532 28 51 77 4 Low Gargle 7.4.1 How the data was generated for Challenge 4 The value of Sound attribute (the output sound of the box) was dependent on the four input attributes: INPUT1,…INPUT4 and one of the five switch positions: Low, Minimum, Medium, Maximum and High. Data was generated in two phases. In the first phase a derived attribute called OUTPUT was defined. In the second phase a simple decision tree using just OUTPUT was created which defined the output sound of the box. The derived attribute OUTPUT was created as different linear combinations of INPUT1…INPUT4 attributes depending on the value of SWITCH. First we ordered the values of SWITCH attribute as follows The ordering of Switch position is given as: Low = 1 Minimum = 2 Medium = 3 Maximum = 4 High = 5 if Switch == 1 i.e. &quot;Low&quot; then OUTPUT = Input 1+ 5 * Input 2 - 2 * Input3 + sample(2:5,1) if Switch == 2 i.e. &quot;Minimum&quot; then OUTPUT = 3* Input 2 - 2 * Input 4 + sample(2:3,1) else i.e. Position other than &quot;Low&quot; and &quot;Minimum&quot; then OUTPUT = Input1 ^2 -1.5 * Input 3 + sample(5:10,1) Then SOUND totally depends on OUTPUT attribute, but is distributed probabilistically over all possible sound. For example, the SOUND when OUTPUT&gt;150 is distributed as 0, 0, 10, 0, 10, 60, 20. This number list corresponds to Gargle, Tick, Beep, Kaboom, Rumble, Sizzle, Hiss. And thus we can see that &quot;Sizzle&quot; sound has the max probability of 60%, and is this the most likely sound when the OUTPUT value is above 150. OUTPUT &gt; 150 -&gt; Max Probability of finding &quot;Sizzle&quot; 100 &lt; OUTPUT &lt; 150 -&gt; Max Probability of finding &quot;Rumble&quot; 70 &lt; OUTPUT &lt; 100 -&gt; Max Probability of finding &quot;Kaboom&quot; 50 &lt; OUTPUT &lt; 70 -&gt; Max Probability of finding &quot;Hiss&quot; 20 &lt; OUTPUT &lt; 50 -&gt; Max Probability of finding &quot;Tick&quot; OUTPUT &lt; 20 -&gt; Max Probability of finding &quot;Gargle&quot; The OUTPUT variable is created by three different linear combinations of INPUT1,…INPUT4 variables depending on different values of SWITCH attributes. Notice that the same formula is used for “Medium”, “Maximum” and “High” positions of the SWITCH. The only two SWITCH positions which have different linear combinations of INPUT1, INPUT2, INPUT3 and INPUT4 defining OUTPUT are “Low” and “Minimum”. How the data was generated for Challenge 4 in R # Load The Data dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22_answer.csv&quot;,stringsAsFactors = T) dat$OUTPUT &lt;- rep(0,nrow(dat)) dat$OUTPUT &lt;- ((dat$INPUT1^2) - (1.5*dat$INPUT3) + sample(5:10,1)) dat[dat$SWITCH == &quot;Low&quot;,]$OUTPUT &lt;- (dat[dat$SWITCH == &quot;Low&quot;,]$INPUT1 + (5*dat[dat$SWITCH == &quot;Low&quot;,]$INPUT2) - (2*dat[dat$SWITCH == &quot;Low&quot;,]$INPUT3) + sample(2:5,1)) dat[dat$SWITCH == &quot;Minimum&quot;,]$OUTPUT &lt;- ((3*dat[dat$SWITCH == &quot;Minimum&quot;,]$INPUT2) - (2*dat[dat$SWITCH == &quot;Minimum&quot;,]$INPUT4) + sample(2:3,1)) dat$predSound &lt;- rep(&#39;Empty&#39;,nrow(dat)) dat[dat$OUTPUT&gt;150,]$predSound&lt;-&#39;Sizzle&#39; dat[dat$OUTPUT&gt;=100 &amp; dat$OUTPUT&lt;150,]$predSound&lt;-&#39;Rumble&#39; dat[dat$OUTPUT&gt;=70 &amp; dat$OUTPUT&lt;100,]$predSound&lt;-&#39;Kaboom&#39; dat[dat$OUTPUT&gt;=50 &amp; dat$OUTPUT&lt;70,]$predSound&lt;-&#39;Hiss&#39; dat[dat$OUTPUT&gt;=20 &amp; dat$OUTPUT&lt;50,]$predSound&lt;-&#39;Tick&#39; dat[dat$OUTPUT&lt;20,]$predSound&lt;-&#39;Gargle&#39; mean(dat$SOUND==dat$predSound) The accuracy of the perfect model created using the above rules, was 92%. To our disappointment this challenge was not a success. And this is by no means the students fault. It was our fault - since the challenge was really not fair. None of our teaching assistants was able to better the student solutions and therefore each solution had accuracy almost 25% below the accuracy of the perfect model. In fact almost everyone’s model made the range of 65% to 68%. Why such a huge discrepancy? Especially that for the first two challenges, students actually got away with some overfitting and even managed to beat the perfect model by a few percentage points. We suspect that adding noise to the linear formulas defining OUTPUT for different SWITCH positions may have created problems for simple rpart() application based just on five independent variables INPUT1…INPUT4, SWITCH. One certainly could not expect from someone who has no hints about the manner the derived attribute OUTPUT was defined, to be able to find the definition (not to mention even guess the existence of) of OUTPUT. Certainly, if OUTPUT’s definition was known, a simple rpart prediction model using just OUTPUT would achieve accuracy exceeding 90%. 7.4.2 Top Submissions for Challenge 4 Nicole Coria Nicole’s PPT This was the top submission based on accuracy score, with a final score more than 68.7% The approach to solving this challenge was iterative and trail and error based. First, since the task is to predict categorical data, she decided to use rpart(directly). Then, over iteration, by varying the control parameters of rpart, she tried to find the model with the highest accuracy. Use of cross-validation also helped in finding the best fit model. Atharva Patil Atharva’s PPT This was another top submission based on accuracy score, with final score above 68% The approach to solving the task was very well implemented, using external resources too. Atharva tried to analyze the data first. To do this, he used Prof. Imielinski’s online platform called Boundless Analytics. This online platform has ability to analyze the data automatically, and create plots which only matter or provide more information about the data. It eliminates the need to perform the data analysis manually. Then, he proceeded by building the model using the rpart() function and control parameters. Andrew Scovell Andrew’s PPT Bennett had a final accuracy score of above 68% and was one of the top submissions for this challenge. He did a very extensive data analysis using all the attributes of the dataset. He also tried analyzing using mean, sums, standard deviation, etc of the numerical inputs. Using the control parameters of the rpart() function he tried to find the best fitting model, and used cross-validation to avoid overfitting. To perform any of the above challenges yourself, visit the appropriate links. Prediction Challenge 1 https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d Prediction Challenge 2 https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b Prediction Challenge 3 https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd Prediction Challenge 4 https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe "],["appendix.html", "Chapter 8 Appendix 8.1 c() &amp; data.frame() &amp; class() 8.2 summary(), mean(),length(), max(),min(), sd(),nrow(), ncol(), dim() 8.3 Cut 8.4 What would R say? 8.5 Create Column 8.6 Factor Function: factor() 8.7 Coercing Values in data frames 8.8 Merging Two Relational Data Frames. 8.9 Slicing and Dicing. 8.10 Group By 8.11 Handling Date and Time in dataframes.", " Chapter 8 Appendix In this chapter we are going to recap at some basic and useful functions we have used in R. The examples we use here will be helpful in revising few of the functions we studied and would give an baseline of function we would need in the future while programming in R. List of commands: c() cut() summary(), mean(),length(), max(),min(), sd(),nrow(), ncol() class() And also all the plot commands present in section 2.1 8.1 c() &amp; data.frame() &amp; class() c() The c() function is used for combining arguments. The default behavior of the c() method is to combine its arguments to form a vector. All arguments are coerced (forcibly converted) to a common type which is the type of the returned value. For example,the non-character values are coerced to character type if one of the elements is a character. the hierarchy followed is NULL &lt; raw &lt; logical &lt; integer &lt; double &lt; complex &lt; character &lt; list &lt; expression. dataframe() A data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. Following are the characteristics of a data frame. The column names should be non-empty. The row names should be unique. The data stored in a data frame can be of numeric, factor or character type. Each column should contain same number of data items. class() The class() function has multiple uses, but for here, it is used to check the type of object. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgMyB2ZWN0b3JzIHdpdGggdGl0bGUsIGF1dGhvciBhbmQgeWVhci5cbnRpdGxlIDwtIGMoJ0RhdGEgU21hcnQnLCdPcmllbnRhbGlzbScsJ0ZhbHNlIEltcHJlc3Npb25zJywnTWFraW5nIFNvZnR3YXJlJylcbmF1dGhvciA8LSBjKCdGb3JlbWFuLCBKb2huJywnU2FpZCwgRWR3YXJkJywnQXJjaGVyLCBKZWZmZXJ5JywnT3JhbSwgQW5keScpXG55ZWFyIDwtIGMoMjAxMCwyMDExLDIwMTIsMTk5OClcblxuI0xldHMgbG9vayBhdCBob3cgdGhlIGNyZWF0ZWQgdmVjdG9ycyBsb29rLlxudGl0bGVcbmF1dGhvclxueWVhclxuXG4jIEFsc28gbGV0cyBsb29rIGF0IHRoZWlyIHR5cGVzIHVzaW5nIHRoZSBjbGFzcyBmdW5jdGlvbi5cbmNsYXNzKHRpdGxlKVxuY2xhc3MoYXV0aG9yKVxuY2xhc3MoeWVhcilcblxuXG4jIE5vdyBsZXRzIGNyZWF0ZSBhIGRhdGFmcmFtZSB1c2luZyB0aGUgYWJvdmUgY29sdW1uIHZlY3RvcnMuXG5cbmRmIDwtIGRhdGEuZnJhbWUodGl0bGUsIGF1dGhvciwgeWVhcilcbmRmICMgTGV0cyBsb29rIGF0IGhvdyB0aGUgZGF0YWZyYW1lIGxvb2tzLiJ9 8.2 summary(), mean(),length(), max(),min(), sd(),nrow(), ncol(), dim() The functions in this section are very simple yet are always useful to get more information from data. summary() function computes summary statistics of data. mean() function is used to find the average of the data. sd() fucntion is used to find the standard deviation of the data. length() function is used to get or set the length of data. max() function is used to get the maximum valued element in the data. min() function is used to get the minimum valued element in the data. nrow() function is used to find the number/count of the rows present in data. ncol() function is used to find the number/count of the columns present in data. dim() function is used to find the dimensions of the data. Lets look at example of all these functions. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBMZXRzIGxvb2sgYXQgdGhlIHN1bW1hcnlcbnN1bW1hcnkobW9vZHkpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIG51bWJlciBvZiByb3dzIGluIHRoZSBkYXRhc2V0LlxubnJvdyhtb29keSlcblxuI0xldHMgbG9vayBhdCB0aGUgbnVtYmVyIG9mIGNvbHVtbnMgaW4gdGhlIGRhdGFzZXQuXG5uY29sKG1vb2R5KVxuXG4jTGV0cyBsb29rIGF0IHRoZSBkaW1lbnNpb25zIGkuZS4gYm90aCBudW1iZXJzIG9mIHJvd3MgYW5kIGNvbHVtbnMgb2YgdGhlIGRhdGEgdXNpbmcganVzdCBvbmUgY29tbWFuZFxuZGltKG1vb2R5KVxuXG4jTGV0cyBsb29rIGF0IHRoZSBtZWFuIG9mIHNjb3JlIGNvbHVtbi5cbm1lYW4obW9vZHkkc2NvcmUpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIHN0YW5kYXJkIGRldmlhdGlvbiBvZiBzY29yZSBjb2x1bW5cbnNkKG1vb2R5JHNjb3JlKVxuXG4jTGV0cyBsb29rIGF0IHRoZSBsZW5ndGggb2YgdGhlIGdyYWRlIGNvbHVtbiBcbmxlbmd0aChtb29keSRncmFkZSlcblxuI0xldHMgbG9vayBhdCB0aGUgbWluaW11bSB2YWx1ZSBvZiBzY29yZSBpbiB0aGUgc2NvcmUgY29sdW1uLlxubWluKG1vb2R5JHNjb3JlKVxuXG4jbGV0cyBsb29rIGF0IHRoZSBtYXhpbXVtIHZhbHVlIG9mIHRoZSBzY29yZSBpbiB0aGUgc2NvcmUgY29sdW1uXG5tYXgobW9vZHkkc2NvcmUpIn0= 8.3 Cut cut() function in R Language is used to divide a numeric vector into different ranges eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG4jIFdlIGFjY2VzcyB0aGUgU2NvcmUgY29sdW1uIGZyb20gbW9vZHkgZGF0YXNldC5cbnNjb3JlMCA8LSBjdXQobW9vZHkkU0NPUkUsMTApXG50YWJsZShzY29yZTApICNsZXRzIGNoZWNrIHRoZSBkaXN0cmlidXRpb24gb2YgcGVvcGxlIGluIGVhY2ggcGFydGl0aW9uLlxuXG4jIEN1dCBFeGFtcGxlIHVzaW5nIGJyZWFrcyAtIEN1dHRpbmcgZGF0YSB1c2luZyBkZWZpbmVkIHZlY3Rvci4gXG5zY29yZTEgPC0gY3V0KG1vb2R5JFNDT1JFLGJyZWFrcz1jKDAsNTAsMTAwKSxsYWJlbHM9YyhcIkZcIixcIlBcIikpXG50YWJsZShzY29yZTEpIn0= 8.3.1 QuestionWhat would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuY3V0KG1vb2R5JFNDT1JFLCBicmVha3M9YygwLDI1LDcwLDEwMCksbGFiZWxzPWMoXCJsb3dcIiwgXCJtZWRpdW1cIiwgXCJoaWdoXCIpKVxuI1doYXQgd291bGQgUiBzYXk/XG5cbiMgQS4gNSBpbnRlcnZhbHMgb2YgYXR0cmlidXRlIHNjb3JlXG4jIEIuIDMgaW50ZXJ2YWxzICgwLDI1KSAoMjUsNzApICg3NSwxMDApXG4jIEMuIDMgY2F0ZWdvcmljYWwgdmFsdWVzIFwibG93XCIsIFwibWVkaXVtXCIgYW5kIFwiaGlnaFwiIGZvciBkaWZmZXJlbnQgc2NvcmUgaW50ZXJ2YWxzXG4jIEQuIDMgc2VwYXJhdGUgZGF0YXNldHMgd2l0aCBzaW1pbGFyIHNjb3JlIHZhbHVlcyJ9 8.3.2 QuestionWhat would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxub3V0cHV0PC1jdXQobW9vZHkkU0NPUkUsIDUpXG5zdW1tYXJ5KG91dHB1dClcbiNXaGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEuIDUgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBzY29yZSBvZiB1bmVxdWFsIGNvdW50IG9mIGVsZW1lbnRzXG4jIEIuIDUgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBzY29yZSBvZiBlcXVhbCBjb3VudCBvZiBlbGVtZW50c1xuIyBDLiA1IGNhdGVnb3JpY2FsIHZhbHVlcyBmb3IgZGlmZmVyZW50IHNjb3JlIGludGVydmFsc1xuIyBELiA1IHNlcGFyYXRlIGRhdGFzZXQgd2l0aCBzaW1pbGFyIHNjb3JlIHZhbHVlcyJ9 8.3.3 QuestionWhat would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxub3V0cHV0PC1jdXQobW9vZHkkQVNLU19RVUVTVElPTlMsIDIpXG5zdW1tYXJ5KG91dHB1dClcbiNXaGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEuIDIgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBhc2tfcXVlc3Rpb25zIG9mIHVuZXF1YWwgY291bnQgb2YgZWxlbWVudHMgaW4gZWFjaCBpbnRlcnZhbFxuIyBCLiAyIGludGVydmFscyBvZiBhdHRyaWJ1dGUgYXNrX3F1ZXN0aW9ucyBvZiBlcXVhbCBjb3VudCBvZiBlbGVtZW50cyBpbiBlYWNoIGludGVydmFsXG4jIEMuIDIgY2F0ZWdvcmljYWwgdmFsdWVzIGZvciBkaWZmZXJlbnQgYXNrX3F1ZXN0aW9ucyBpbnRlcnZhbHNcbiMgRC4gRXJyb3IuIn0= 8.3.4 A complex example eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuXG5tb29keSRjb25kaXRpb25hbCA8LTBcbm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb248MC41MCwgXSRjb25kaXRpb25hbCA8LSBtb29keVttb29keSRwYXJ0aWNpcGF0aW9uPDAuNTAsIF0kc2NvcmUgLTEwKm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb248MC41MCwgXSRwYXJ0aWNpcGF0aW9uXG5tb29keVttb29keSRwYXJ0aWNpcGF0aW9uPj0wLjUwLCBdJGNvbmRpdGlvbmFsIDwtIG1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb24+PTAuNTAsIF0kc2NvcmUgKzEwKm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb24+PTAuNTAsIF0kcGFydGljaXBhdGlvblxuXG5zdW1tYXJ5KG1vb2R5JGNvbmRpdGlvbmFsKVxuXG5ib3hwbG90KG1vb2R5JGNvbmRpdGlvbmFsLGNvbCA9IGMoXCJyZWRcIiksbWFpbj1cIkNvbXBsZXggRXhhbXBsZVwiKSJ9 8.4 What would R say? In this section we will look at few examples based on the question “What do you think would R say?” All the questions are based on what we have studied in the sections above. INSTRUCTIONS: Do not run the following examples directly, first ask yourself and note down, what do you think would R say? Only then run them. This is the only way to learn simple commands - and have them memorized so you can write code without having to check every single command. 8.4.1 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ3ZWF0aGVyID1kYXRhLmZyYW1lKERheT1jKCd3ZWVrZGF5JywgJ3dlZWtlbmQnKSwgQ29uZGl0aW9ucyA9Yygnc3VubnknLCdyYWlueScsJ2Nsb3VkeScsICdzbm93JywgJ3N0b3JtJywnaWNlJykpXG5kaW0od2VhdGhlcilcbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpIDYgNlxuIyBCKSAyIDZcbiMgQykgNiAyXG4jIEQpIEVycm9yIn0= 8.4.2 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ3ZWF0aGVyID1kYXRhLmZyYW1lKERheT1jKCd3ZWVrZGF5JywgJ3dlZWtlbmQnKSwgQ29uZGl0aW9ucyA9Yygnc3VubnknLCdyYWlueScsJ2Nsb3VkeScsICdzbm93JywgJ3N0b3JtJywnaWNlJykpXG53ZWF0aGVyJHRlbXBlcmF0dXJlID1jKDgwLCA3MCwgNjUsIDQwLCAzMCwyNSlcbndlYXRoZXJbd2VhdGhlciR0ZW1wZXJhdHVyZSA+IDQwLF1cbmRpbSh3ZWF0aGVyKVxuI3doYXQgd291bGQgUiBzYXk/XG5cbiMgQSkgNiAzXG4jIEIpIHN1YnNldCBvZiB0aGUgZGF0YWZyYW1lIHdpdGggdGVtcGVyYXR1cmUgPiA0MC5cbiMgQykgQm90aCBBIGFuZCBCXG4jIEQpIEVycm9yIn0= 8.4.3 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KTtcbkdSQURFPWMoJ0MnLCAnRicsICdBJylcbk9OX1NNQVJUUEhPTkU9YygnYWx3YXlzJywgJ25ldmVyJywgJ3NvbWV0aW1lcycpXG5GSU5BTEVYQU09YygxMiw1LDIwKVxuTT1kYXRhLmZyYW1lKFNDT1JFLCBHUkFERSwgT05fU01BUlRQSE9ORSwgRklOQUxFWEFNKVxuc3Vic2V0KE0sIEdSQURFPT0nRicpXG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSBTdWJzZXQgb2YgZGF0YWZyYW1lIGJhc2VkIG9uIEdyYWRlIGVxdWFsIHRvIEZcbiMgQikgU3Vic2V0IG9mIHRoZSBkYXRhZnJhbWUgYmFzZWQgb24gR3JhZGUgbm90IGVxdWFsIHRvIEZcbiMgQykgdGhlIGNvbXBsZXRlIGRhdGFmcmFtZVxuIyBEKSBFcnJvciJ9 8.4.4 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KTtcbkdSQURFPWMoJ0MnLCAnRicsICdBJylcbk9OX1NNQVJUUEhPTkU9YygnYWx3YXlzJywgJ25ldmVyJywgJ3NvbWV0aW1lcycpXG5GSU5BTEVYQU09YygxMiw1LDIwKVxuTT1kYXRhLmZyYW1lKFNDT1JFLCBHUkFERSwgT05fU01BUlRQSE9ORSwgRklOQUxFWEFNKVxuTVtGSU5BTEVYQU0gPiA1LF1cbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpIFN1YnNldCBvZiBkYXRhZnJhbWUgd2l0aCBmaW5hbGV4YW0gdmFsdWVzIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA2XG4jIEIpIFN1YnNldCBvZiBkYXRhZnJhbWUgd2l0aCBmaW5hbGV4YW0gdmFsdWVzIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA1XG4jIEMpIFN1YnNldCBvZiBkYXRhZnJhbWUgd2l0aCBmaW5hbGV4YW0gdmFsdWVzIGxlc3MgdGhhbiA1LiJ9 8.4.5 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KVxuR1JBREU9YygnQycsICdGJywgJ0EnKVxuT05fU01BUlRQSE9ORT1jKCdhbHdheXMnLCAnbmV2ZXInLCAnc29tZXRpbWVzJylcbkZJTkFMRVhBTT1jKDEyLDUsMjApXG5NPWRhdGEuZnJhbWUoU0NPUkUsIEdSQURFLCBPTl9TTUFSVFBIT05FLCBGSU5BTEVYQU0pXG5NJFFVRVNUSU9OUz0nbm9uZSdcbk1bLDVdXG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSBPdXRwdXQgdGhlIGNvbnRlbnQgb2YgYWxsIHRoZSBjb2x1bW5zXG4jIEIpIE91dHB1dCB3b3JkIFwibm9uZVwiIGZvciAzIHRpbWVzIFxuIyBDKSBPdXRwdXQgd29yZCBcIm5vbmVcIiBmb3IgNSB0aW1lc1xuIyBEKSBFcnJvciJ9 8.4.6 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KVxuR1JBREU9YygnQycsICdGJywgJ0EnKVxuT05fU01BUlRQSE9ORT1jKCdhbHdheXMnLCAnbmV2ZXInLCAnc29tZXRpbWVzJylcbkZJTkFMRVhBTT1jKDEyLDUsMjApXG5NPWRhdGEuZnJhbWUoU0NPUkUsIEdSQURFLCBPTl9TTUFSVFBIT05FLCBGSU5BTEVYQU0pXG50YWJsZShNJFNDT1JFPjE1LCBNJEdSQURFKVxuI3doYXQgd291bGQgUiBzYXk/XG5cbiMgQSkgT3V0cHV0IHRoZSB0YWJsZSBvZiBjb3VudCBvZiBTY29yZSBncmVhdGVyIHRoYW4gMTUgdnMgR3JhZGVcbiMgQikgT3V0cHV0IHRoZSB0YWJsZSBvZiBjb3VudCBvZiBzY29yZSBncmVhdGVyIHRoYW4gMTUgb25seVxuIyBDKSBPdXRwdXQgdGhlIHRhYmxlIG9mIGNvdW50IG9mIGdyYWRlcyBvbmx5XG4jIEQpIE91dHB1dCB0aGUgdGFibGUgb2YgZ3JhZGUgZGlzdHJpYnV0aW9uIHZzIGFsbCBzY29yZS4ifQ== 8.4.7 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ1PC1jKDE6MTApXG53IDwtYygxLC0xLDMpXG51W3c+MF1cbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpICAxICAzICA0ICA2ICA3ICA5IDEwIFxuIyBCKSAgMSAgMiAgMyAgNCAgNSAgNiAgNyAgOCAgOSAgMTBcbiMgQykgIDEgIDMgIDEgIDMgIDEgIDMgIDEgIDMgIDEgIDMgXG4jIEQpIEVycm9yIn0= 8.4.8 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2IDwtIGMoLTIsMCwyLC01KVxudlt2PjBdXG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSAyXG4jIEIpIDAgMlxuIyBDKSBGQUxTRSBGQUxTRSAgVFJVRSBGQUxTRVxuIyBEKSBFcnJvciJ9 8.4.9 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJjKFwiYVwiLDEsVClcbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpIE5hTiAgMSAgTmFOXG4jIEIpIFwiYVwiICAxICBUXG4jIEMpIFwiYVwiICBcIjFcIiAgXCJUUlVFXCJcbiMgRCkgXCJhXCIgIFwiMVwiICBcIlRcIiJ9 8.4.10 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ4PC0xOjRcbnk8LTI6OVxueCt5XG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSAzICA1ICA3ICA5ICA3ICA5IDExIDEzXG4jIEIpIDEgIDIgIDMgIDQgIDIgIDMgIDQgIDUgIDYgIDcgIDggIDlcbiMgQykgMyAgNSAgNyAgOVxuIyBEKSBFcnJvciJ9 8.4.11 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2MTwtIGMoMSwyLDMsNClcbnYyPC0gYygxLDIsMyw0KVxudjM8LSBjKDEsMiwzLDQpXG5cbmRmPC1kYXRhLmZyYW1lKHYxLHYyLHYzKVxuIzFcbmRmW2RmPjJdICBcblxuIyBBLiB2YWx1ZXMgb2YgdjEgdmFyaWFibGUgd2hpY2ggYXJlIGxhcmdlciB0aGFuIDIgXG5cbiMgQi4gdmFsdWVzIG9mIHYxLCB2MiBhbmQgdjMgd2hpY2ggYXJlIGxhcmdlciB0aGFuIDIuIFxuXG4jIEMuIGVycm9yICJ9 8.4.12 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2MTwtIGMoMSwyLDMsNClcbnYyPC0gYygxLDIsMyw0KVxudjM8LSBjKDEsMiwzLDQpXG5cbmRmPC1kYXRhLmZyYW1lKHYxLHYyLHYzKVxuXG5kZiR2MSA+IDIgXG5cbiMgQS4gZXJyb3IgXG5cbiMgQi4gdmFsdWVzIG9mIHYxIHZhcmlhYmxlIHdoaWNoIGFyZSBsYXJnZXIgdGhhbiAyLlxuXG4jIEMuIFRSVUUgd2hlcmUgdGhlIHZhbHVlIGlzIGdyZWF0ZXIgdGhhbiAyIGFuZCBGYWxzZSB3aGVyZSB0aGUgdmFsdWUgaXMgbGVzcyB0aGFuIDIuICJ9 8.4.13 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2MTwtIGMoMSwyLDMsNClcbnYyPC0gYygxLDIsMyw0KVxudjM8LSBjKDEsMiwzLDQpXG5cbmRmPC1kYXRhLmZyYW1lKHYxLHYyLHYzKVxuXG52MT4yXG5cbiMgQS4gVFJVRSB3aGVyZSB0aGUgdmFsdWUgaXMgZ3JlYXRlciB0aGFuIDIgYW5kIEZhbHNlIHdoZXJlIHRoZSB2YWx1ZSBpcyBsZXNzIHRoYW4gMi5cblxuIyBCLiB2YWx1ZXMgb2YgdjEgdmFyaWFibGUgd2hpY2ggYXJlIGxhcmdlciB0aGFuIDJcblxuIyBDLiBlcnJvciJ9 Now we have to introduce the core data structure of R – the data frame and show we can expand it with extra attributes. Defining new attributes can very often be critical in data exploration and help to find patterns and relationships which otherwise would not be visible. For example, may be participation matters but only to Pass/Fail grades? In other words students who Pass (A or B or C) always have participation above a certain threshold? Perhaps students who always text never pass the class? And students who always ask questions never fail? Such rules can only be discovered if we define a new Pass/Fail attribute, additional to grade attribute. Similarly intervals of participation or score may discover important relationships which would not emerge with just numerical values of such attributes. May be High scores correlate with High participation? To establish it one would have first to define categorical attributes with named intervals of their numerical counterparts. 8.5 Create Column Lets put a column I have created using score. Suppose I am given a new column \" pf \" with same number of rows as that of the dataframe with the categories (“P” , “F”). eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiMgcGYgY29sdW1uIGhhcyAyIGNhdGVnb3J5IGFuZCBkaXZpZGVzIG9uIHRoZSBiYXNpcyBvZiBzY29yZS5cbnBmIDwtIGN1dChtb29keSRzY29yZSxicmVha3M9YygwLDUwLDEwMCksbGFiZWxzPWMoXCJGXCIsXCJQXCIpKVxuIyBsZW5ndGgocGYpICMgTnVtYmVyIG9mIHJvd3MgaW4gbmV3IGNvbHVtbi5cbiMgbnJvdyhtb29keSkgIyBOdW1iZXIgb2YgUm93cyBpbiBkYXRhZnJhbWVcblxuIyBUbyBhZGQgdGhpcyBuZXcgY29sdW1uIHBmIGluIGRhdGFmcmFtZSBtb29keS5cbm5hbWVzKG1vb2R5KSAjIEluaXRpYWxseSBkYXRhZnJhbWUgaGFzIDUgY29sdW1uc1xubW9vZHkkcGFzc2ZhaWwgPC0gcGYgI1B1dCBzeW50YXggZGF0YUZyYW1lTmFtZSRjb2x1bW5IZWFkZXJOYW1lIDwtIG5ld0NvbHVtblxubmFtZXMobW9vZHkpICMgTm93IGRhdGFmcmFtZSBoYXMgNiBjb2x1bW5zIn0= eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNXaGF0IGhhcHBlbnMgd2hlbiB5b3UgaGF2ZSBjb2x1bW4gc2l6ZSBtaXNtYXRjaC5cbmJhZGNvbCA8LSBjKDE6MTApXG5sZW5ndGgoYmFkY29sKVxuXG5tb29keSRiYWRjb2wgPC0gYmFkY29sICNUaHJvd3MgQ29tcGF0aWJpbGl0eSBlcnJvci4gIn0= 8.6 Factor Function: factor() Factors are the data objects which are used to categorize the data and store it as levels. They can store both strings and numbers. They are useful in the columns which have a limited number of unique values. Like “Male,”Female\" and True, False etc. Factor data objects are useful in data analysis for statistical modeling. The factor function is used to encode a vector as a factor. Lets look at first example, checking if a data object is of factor type using the function is.factor(x) eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIENyZWF0ZSBhIHZlY3RvciBhcyBpbnB1dC5cbmdlbmRlciA8LSBjKFwibWFsZVwiLFwibWFsZVwiLFwiZmVtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwibWFsZVwiKVxuXG5nZW5kZXJcblxuI0NoZWNrIGlmIGRhdGEgb2JqZWN0IGlzIGZhY3Rvci5cbmlzLmZhY3RvcihnZW5kZXIpIn0= Now lets convert the above vector to a factor data object. To do this we will use the function factor(x). eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIENyZWF0ZSBhIHZlY3RvciBhcyBpbnB1dC5cbmdlbmRlciA8LSBjKFwibWFsZVwiLFwibWFsZVwiLFwiZmVtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwibWFsZVwiKVxuXG4jIEFwcGx5IHRoZSBmYWN0b3IgZnVuY3Rpb24uXG5mYWN0b3JfZ2VuZGVyIDwtIGZhY3RvcihnZW5kZXIpXG5cbmZhY3Rvcl9nZW5kZXJcbmlzLmZhY3RvcihmYWN0b3JfZ2VuZGVyKSJ9 Notice that for the factor data objects, the attribute Levels is also created. This is an extremely important feature of the factor data object. Lets look at how the factor data object looks when included in a dataframe. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIENyZWF0ZSB0aGUgdmVjdG9ycyBmb3IgZGF0YSBmcmFtZS5cbmhlaWdodCA8LSBjKDEzMiwxNTEsMTYyLDEzOSwxNjYsMTQ3LDEyMilcbndlaWdodCA8LSBjKDQ4LDQ5LDY2LDUzLDY3LDUyLDQwKVxuZ2VuZGVyX25vdF9mYWN0b3IgPC0gYyhcIm1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwiZmVtYWxlXCIsXCJtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIilcblxuIyBDT252ZXJ0IHRoZSBnZW5kZXJfbm90X2ZhY3RvciB2ZWN0b3IgdG8gYSBmYWN0b3IgZGF0YSBvYmplY3QuXG5nZW5kZXIgPC0gZmFjdG9yKGdlbmRlcl9ub3RfZmFjdG9yKVxuXG4jIENyZWF0ZSB0aGUgZGF0YSBmcmFtZS5cbmlucHV0X2RhdGEgPC0gZGF0YS5mcmFtZShoZWlnaHQsd2VpZ2h0LGdlbmRlcilcbnByaW50KGlucHV0X2RhdGEpXG5cbiMgVGVzdCBpZiB0aGUgZ2VuZGVyIGNvbHVtbiBpcyBhIGZhY3Rvci5cbnByaW50KGlzLmZhY3RvcihpbnB1dF9kYXRhJGdlbmRlcikpXG5cbiMgUHJpbnQgdGhlIGdlbmRlciBjb2x1bW4gc28gc2VlIHRoZSBsZXZlbHMuXG5wcmludChpbnB1dF9kYXRhJGdlbmRlcikifQ== Note: Sometimes depending on your version of R and packages, you might find that while inserting categorical vector into the data frame using the data.frame() function, without converting the categorical vector to factor, it automatically gets converted into a factor column. But to avoid confusion, it is a better technique to convert the categorical vector into factor using factor() function and then insert it in the data frame. Lets look at an example where the use of factor data object turns out to be useful. We have a categorical vector that we want to coerce as numeric for use in some model/application. Lets look at what happens when we just have a categorical vector, and we try to coerce it to numeric vector. We see that the outcome of the as.numeric() function on a normal categorical vector is coercion to “NA” of all elements. But when we convert the same categorical vector to factor, then after coercion to numeric type, we get a numeric vector of elements corresponding the the index of the labels of the factor data object. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJnZW5kZXJfbm90X2ZhY3RvciA8LSBjKFwibWFsZVwiLFwibWFsZVwiLFwiZmVtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwibWFsZVwiKVxuZ2VuZGVyX25vdF9mYWN0b3JcblxuIyBDb2VyY2UgaW50byBudW1lcmljIHZlY3RvciB3aXRob3V0IGNvbnZlcnRpbmcgdG8gZmFjdG9yXG5hcy5udW1lcmljKGdlbmRlcl9ub3RfZmFjdG9yKVxuXG5cbiMgQ09udmVydCB0aGUgZ2VuZGVyX25vdF9mYWN0b3IgdmVjdG9yIHRvIGEgZmFjdG9yIGRhdGEgb2JqZWN0LlxuZ2VuZGVyIDwtIGZhY3RvcihnZW5kZXJfbm90X2ZhY3RvcilcbmdlbmRlclxuXG4jIENvZXJjZSBpbnRvIG51bWVyaWMgdmVjdG9yIGFmdGVyIGNvbnZlcnRpbmcgdG8gZmFjdG9yIGRhdGEgb2JqZWN0LlxuYXMubnVtZXJpYyhnZW5kZXIpIn0= Lets look at another example where factor is useful. We want to see the distribution of price of each quality for the wine dataset. Upon plotting, it gives us a scatter plot, which makes it hard for us to see the distribution. Thus we convert the quality vector which is numeric initially, to factor and the plot it again. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ3aW5lIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0LzU0MVdJTkUuY3N2XCIpXG5wbG90KHdpbmUkUVVBTElUWSx3aW5lJFBSSUNFKVxuI3dlIHdhbnQgdG8gc2VlIHRoZSBkaXN0cmlidXRpb24gb2YgcHJpY2Ugb2YgZWFjaCBxdWFsaXR5LCBidXQgaXQgZ2l2ZXMgdXMgYSBzY2F0dGVyIHBsb3QsIHdoaWNoIG1ha2VzIGl0IGhhcmQgZm9yIHVzIHRvIHNlZSB0aGUgZGlzdHJpYnV0aW9uLlxuaXMuZmFjdG9yKHdpbmUkUVVBTElUWSlcbiN0aGUgcmVzdWx0IGlzIGZhbHNlLCB3aGljaCBtZWFucyBxdWFsaXR5IGlzIGEgbnVtZXJpYyB2YWx1ZSByYXRoZXIgdGhhbiBhIGZhY3RvclxuXG5mYWN0b3JfcXVhbGl0eSA8LSBmYWN0b3Iod2luZSRRVUFMSVRZKVxuI2NvbnZlcnQgcXVhbGl0eSB2YWx1ZXMgaW50byBmYWN0b3JzXG5wbG90KGZhY3Rvcl9xdWFsaXR5LHdpbmUkUFJJQ0UpXG4jbm93IHdlIGNhbiBnZW5lcmF0ZSB0aGUgYm94IHBsb3QgYW5kIHNlZSB0aGUgZGlzdHJpYnV0aW9uIGNsZWFybHkuIn0= 8.7 Coercing Values in data frames Before coercing data into data frames, lets look at small examples. Lets look at a coerced vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBsb29rIGF0IGEgY29lcmNlZCB2ZWN0b3IuXG5cbiN2ZWN0b3IgY29udGFpbmluZyA0IGVsZW1lbnRzXG5teVZlY3Q8LWMoXCJSb2JlcnRcIiwgXCJFdGhhblwiLCA2LCA0KVxubXlWZWN0XG5cbiNZb3Ugd2lsbCBub3RpY2UgdGhhdCB0aGUgbGFzdCB0d28gZWxlbWVudHMgLCB3aGljaCBhcmUgYW4gaW50ZWdlcnMsIGFyZSBjb2VyY2VkIGludG8gYSBjaGFyYWN0ZXIgdHlwZS5cblxuI2NsYXNzKCkgaXMgdXNlZCB0byBjaGVjayB0aGUgdHlwZSBvZiBhbiBvYmplY3RcbmNsYXNzKG15VmVjdCkifQ== We see that when a vector has elements of mixed data types, they gets coerced into a type with precedence over other types. For example in the above case there were character elements and numeric elements types in the vector. But character type has precedence over numeric type and hence the whole vector is coerced into character type. We can check the types of vectors using a specific type of is function: is.character(), is.double(), is.integer(), is.logical(),etc. There are many other types under the is function, for checking if the data object given is a dataframe, factor, etc. Lets look at the examples. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjdmVjdG9yIGNvbnRhaW5pbmcgNCBlbGVtZW50c1xubXlWZWN0PC1jKFwiUm9iZXJ0XCIsIFwiRXRoYW5cIiwgNiwgNClcbm15VmVjdFxuXG4jIENoZWNrIGlmIHZlY3RvciBpcyBvZiBDaGFyYWN0ZXIgdHlwZS5cbmlzLmNoYXJhY3RlcihteVZlY3QpXG5cbiMgQ2hlY2sgaWYgdmVjdG9yIGlzIG9mIG51bWVyaWMgdHlwZS5cbmlzLm51bWVyaWMobXlWZWN0KVxuXG4jIFVzZSBUUlVFIGFuZCBGQUxTRSAob3IgVCBhbmQgRikgdG8gY3JlYXRlIGxvZ2ljYWwgdmVjdG9yc1xubG9nX3ZlYyA8LSBjKFRSVUUsIEZBTFNFLCBULCBGKVxuXG4jIENoZWNrIGlmIHZlY3RvciBpcyBvZiBsb2dpY2FsIHR5cGUuXG5pcy5sb2dpY2FsKGxvZ192ZWMpIn0= We saw how to check the type of the data. But if you want to convert a column into your choice of data type, you can use the specific type of as function: as.character(), as.double(), as.integer(), as.logical(),etc. Again as we saw above about the is function types, there are also many other types of the as function. Lets look at the example of coercing a vector into character type. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjdmVjdG9yIGNvbnRhaW5pbmcgNCBlbGVtZW50c1xubXlWZWN0PC1jKDIsIDMsIDYsIDQsIFRSVUUsIEZBTFNFKVxubXlWZWN0XG5cbiMgRmlyc3QgbGV0cyBsb29rIGF0IHRoZSBjbGFzcyBvZiB0aGUgdmVjdG9yXG5jbGFzcyhteVZlY3QpXG5cbiMgQ29lcmNlIHRoZSB2ZWN0b3IgdG8gQ2hhcmFjdGVyIHR5cGUuIFxuYXMuY2hhcmFjdGVyKG15VmVjdCkgXG5cbiMgWW91IGNhbiBzZWUgdGhhdCB0aGUgZWxlbWVudHMgb2YgdGhlIG51bWVyaWMgdmVjdG9yIGFyZSBjb2VyY2VkIGludG8gY2hhcmFjdGVyIHR5cGUuIn0= Lets look at an example of coercing a mixed type vector into numeric type. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJteXZlYzwtYyhcIlJvYmVydFwiLCBcIjIyXCIsIDQ1KVxubXl2ZWNcblxuIyBGaXJzdCBsZXRzIGxvb2sgYXQgdGhlIGNsYXNzIG9mIHRoZSB2ZWN0b3JcbmNsYXNzKG15dmVjKVxuXG4jIENvZXJjZSB0aGUgY2hhcmFjdGVyIHZlY3RvciB0byBudW1lcmljIHR5cGUuIFxuYXMubnVtZXJpYyhteXZlYykgXG5cbiMgWW91IGNhbiBzZWUgdGhhdCB0aGUgZWxlbWVudHMgb2YgdGhlIG1peGVkIHZlY3RvciBhcmUgY29lcmNlZCBpbnRvIG51bWVyaWMgdHlwZS5cblxuXG5cbm15dmVjMiA8LSBjKFRSVUUsIEZBTFNFLCBGLCBULCBUKVxubXl2ZWMyXG5cbiMgRmlyc3QgbGV0cyBsb29rIGF0IHRoZSBjbGFzcyBvZiB0aGUgdmVjdG9yXG5jbGFzcyhteXZlYzIpXG5cbiMgQ29lcmNlIHRoZSBsb2dpY2FsIHZlY3RvciB0byBudW1lcmljIHR5cGUuIFxuYXMubnVtZXJpYyhteXZlYzIpIFxuXG4jIFlvdSBjYW4gc2VlIHRoYXQgdGhlIGVsZW1lbnRzIG9mIHRoZSBtaXhlZCB2ZWN0b3IgYXJlIGNvZXJjZWQgaW50byBudW1lcmljIHR5cGUuIn0= We can see in the above example, while converting the character type vector to numeric if we encounter, numbers in character type, they get converted to numeric type. But the characters in character type, are not not converted, and instead we get a warning saying “NAs introduced by coercion”. Also, while converting a logical vector to numeric vector, we see that “TRUE” or “T” is coerced as 1 and “FALSE” or “F” is coerced as 0. Now lets look at how to coerce data column and rewrite it into the dataframe. Suppose in the Moody dataset, you want to change the categorical vector of letter grade to numeric grades between 1 to 5, where A=1, B=2, …, F=5. First, you will convert the grade column vector to factor using the factor() function. Then, convert the grade column with the command as.numeric() to numeric column. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBDb252ZXJ0IHRoZSBjYXRlZ29yaWNhbCBjb2x1bW4gZ3JhZGUgdG8gZmFjdG9yIGRhdGEgY29sdW1uLlxubW9vZHkkZ3JhZGU8LWZhY3Rvcihtb29keSRncmFkZSlcbmhlYWQobW9vZHkkZ3JhZGUpXG5cbiMgTm93IGNvbnZlcnQgdGhlIGxldmVscyB0byBudW1lcmljIHVzaW5nIHRoZSBhcy5udW1lcmljIGZ1bmN0aW9uXG5tb29keSRncmFkZSA8LSBmYWN0b3IoYXMubnVtZXJpYyhtb29keSRncmFkZSkpXG5oZWFkKG1vb2R5JGdyYWRlKSJ9 We can see that the outcome of the above code, gives us a moody dataframe with grade column as a numeric column converted from the previous categorical column. We can also see that the we used the as.numeric() function inside the factor function while converting from categorical to numeric, to maintain the levels information of the grade column. Now, suppose you also want to change the labels of the grade column. Lets change the grades from capital letters to small letters, i.e. A -&gt; a, B -&gt; b, and so on. To do this, we can provide our user defined labels vector to the labels attribute of the factor() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBDb252ZXJ0IHRoZSBjYXRlZ29yaWNhbCBjb2x1bW4gZ3JhZGUgdG8gZmFjdG9yIGRhdGEgY29sdW1uIHdpdGggdXNlciBkZWZpbmVkIGxhYmVscy5cbm1vb2R5JGdyYWRlIDwtIGZhY3Rvcihtb29keSRncmFkZSxsYWJlbHMgPSBjKFwiYVwiLFwiYlwiLFwiY1wiLFwiZFwiLFwiZlwiKSlcbmhlYWQobW9vZHkkZ3JhZGUpIn0= We can see that the capital letter are now transformed to small letters. 8.8 Merging Two Relational Data Frames. Often, we have data from multiple sources/multiple databases, files etc. To perform analysis, we need to merge these dataframes together with one or more common key variables. In R the merge() function allows merging two data frames by common columns or row names. This function allows you to perform different SQL joins, like left join, inner join, right join or full join, among others. We will look at merging datasets in R with this function, along with examples. Consider the following 2 datasets. First is a smaller just 4 record data subset of the Moody dataset. Table 8.1: Small subset of Moody Dataset STUDENTID SCORE GRADE ON_SMARTPHONE ASKS_QUESTIONS FINALEXAM 65446 23.67 D never always 12.874804 79686 8.41 F never never 5.044093 56400 69.76 C never always 23.585730 16792 95.51 A never always 23.476748 Second is another dataset of students with respective GPA and Majors. Table 8.2: Small dataset of students information STUDENTID GPA Major 65446 1.559626 computer science 79686 3.813033 economics 56400 2.840912 political science 10001 2.664000 economics NOTE: We can see from the above snippets of the above the top 3 records in both dataset have same STUDENTID, but the 4th records in both datasets are of different students. The most important element while discussing the examples below, will focus on what happens to the 4th records of both datasets when using the various merge options and attributes. 8.8.1 Inner Join This is the most usual type of join of datasets that you can perform. It consists of merging two dataframes in one that contains common elements of both. In order to merge the two datasets, you just have to pass them to the merge() function without the need of changing other arguments. Inner join merge is the default merge of the merge() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aG91dCBhbnkgYXR0cmlidXRlcy5cbm1lcmdlKG1vb2R5X2RmLHN0dWRlbnRfZGYpIn0= We can see that there are only 3 record in the output. The reason being that, the studentid of the fourth record in both the dataset did not match. And thus the merge function did not know which datasets record to be kept and which not. Also the reason the merge function tried to match and merge the two datasets, is by using the first columns from both the datasets, which in both case was the “STUDENTID” column. We can also do the same process, and get he same outcome, by defining the index column by yourself. Lets look at this in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aCB0aGUgXCIgYnkgXCIgIGF0dHJpYnV0ZS5cbm1lcmdlKG1vb2R5X2RmLHN0dWRlbnRfZGYsYnkgPSBcIlNUVURFTlRJRFwiKSJ9 As we can see, the output remains the same. But we understand that we can define any other common column as the index column based on which the merging can occur. IMPORTANT NOTE: There are also arguments like by.x and by.y which correspond to indexing based on one of the column from the left(first) or right(second) datasets respectively. This could come extremely handy, when the two datasets you want to merge, have different column name for the index column. For example, suppose in the two dataset that we have considered above, the first dataset had students records indexed by the studentid column where the indexing column name is studentid, but in the second dataset the indexing column even though with same student id’s as entries but with the column name of stu-id. Now while merging, you can face error since the merge() function will have trouble finding the two index columns to match since they are named differently in the two datasets. Here you can provide the argument by.x = \"studentid\" , by.y = \"stu-id\" in the function while merging. 8.8.1.1 Another example Suppose you have the happiness index dataset, Table 8.3: Happiness Index Dataset for all countries IDN AGE COUNTRY GENDER IMMIGRANT INCOME HAPPINESS 88364 29 Kyrgyzstan Male 0 103305 8.35 37692 41 Afghanistan Male 0 51682 4.44 57856 20 Azerbaijan Female 0 72381 6.24 49453 62 South Korea Female 0 65658 5.66 93485 63 Jordan Female 1 109581 3.17 97976 36 Congo-Kinshasa Female 0 112432 9.43 where you have the survey data of people of various countries with records of information about AGE, COUNTRY, GENDER, IMMIGRANT, INCOME, and HAPPINESS. You can do analysis on the above dataset per country, per age group,etc. But if you want to do analysis based on per continent, then you will have to create lists of all the countries in each continent, and then subset using the appropriate subset method/s from section below 8.9. Alternate method will be acquiring another dataset, with information of each country and its respective continent, and do merge, which we can then use to subset easily. Lets look at an example of this process below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCB0aGUgSGFwcGluZXNzIGluZGV4IGRhdGFzZXQuXG5oYXBweTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvSEFQUElORVNTMjAxNy5jc3ZcIilcbmhlYWQoaGFwcHkpXG5cbiMgTm93IGxldHMgbG9hZCB0aGUgc2ltcGxlIGRhdGFzZXQgb2YgY291bnRyeSBhbmQgY29udGluZW50cy5cbmNvbnRpbmVudHM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L2NvdW50cnktY29udGluZW50cy5jc3ZcIilcbmhlYWQoY29udGluZW50cylcblxuIyBOb3cgd2UgY2FuIHVzZSB0aGUgbWVyZ2UgZnVuY3Rpb24gdG8gaW5jbHVkZSB0aGUgY29udGluZW50cyBvZiBlYWNoIGNvdW50cnkgaW4gdGhlIGhhcHBpbmVzcyBkYXRhc2V0IGFnYWluc3QgZWFjaCBvdGhlci5cbmhhcHB5LmM8LW1lcmdlKGhhcHB5LGNvbnRpbmVudHMpXG5oZWFkKGhhcHB5LmMpXG5cbmhhcHB5LmNbc2FtcGxlKG5yb3coaGFwcHkuYyksMTApLF0ifQ== We can see from the output of the above example, the new dataframe created in happy.c after applying merge() function on the Happiness index dataset and the country-continents dataset, the CONTINENT column is added from the country-continents dataset into the happyness index dataset. And each country in the happy.c dataframe has now the value of its respective continent in the the CONTINENT column. 8.8.2 Full Join Full Join is also known as the outer join or the full outer join. It merges all the columns of both datasets into one. For those records with non-intersecting index elements, Full join keeps both the records, and fills the missing values with NA , i.e. Not Available(NA) keyword. In order to create this type of full join of the two dataframes in R, we need to set the argument all to TRUE or T. Lets look at this in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aCB0aGUgXCIgYWxsIFwiICBhdHRyaWJ1dGUuXG5tZXJnZShtb29keV9kZixzdHVkZW50X2RmLGFsbCA9IFRSVUUpIn0= We can see that the first record of the output with studentid = 10001 was present in the second dataset only, thus the values corresponding to the columns of the first dataset are set to NA. Similarly, the same occurs with the record with studentid = 16792, which was only present in the first dataset, and thus has NA in the place of columns of second dataset. 8.8.3 Left Join The left join in R involves matching all the rows in the first data frame with the corresponding records on the second dataframe. To create this left join, you just have to set the argument all.x to TRUE or T. Recall while doing the full join, we set the argument all to TRUE or T. Similarly, since we consider x as the first dataset or the left dataset, we will set the argument of all.x where the .x is the key to select the first dataset. We have seen in the snippets above, the student with studentid = 16792 is only present in the first dataset but not the second. So lets see the result of merging using the left join in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aCB0aGUgXCIgYWxsIFwiICBhdHRyaWJ1dGUuXG5tZXJnZShtb29keV9kZixzdHVkZW50X2RmLGFsbC54ID0gVFJVRSkifQ== We can see that the record of student with studentid = 16792 has NA as the entry in the columns merged from the right dataset. Also, the record of student with studentid = 10001 is completely excluded, since it belongs to the second dataset. 8.8.4 Right Join The right join merge involves joining all the rows in the second data frame with the corresponding records on the first dataframe. The right join is opposite to that of left join. In consequence, here, you will need to set the argument all.y to TRUE or T, since we consider the right dataset or the second dataset as y. We have seen in the snippet above, that the student with studenid = 10001 is only present in the second dataset but not the first. So lets see the result of merging using the right join in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cblxuIyBVc2UgdGhlIG1lcmdlIGZ1bmN0aW9uLCB3aXRoIHRoZSBcIiBhbGwgXCIgIGF0dHJpYnV0ZS5cbm1lcmdlKG1vb2R5X2RmLHN0dWRlbnRfZGYsYWxsLnkgPSBUUlVFKSJ9 We can see that the record of student with studentid = 10001 has NA as the entry in the columns merged from the left dataset. Also, the record of student with studentid = 16792 is completely excluded, since it belongs to the first dataset. 8.9 Slicing and Dicing. R was made especially for data analysis and graphics. SQL was made especially for databases. They are allies in this field of data science. The data structure in R that most closely matches a SQL table is a data frame. The terms rows and columns are used in both. There is an R package called sqldf that allows you to use SQL commands to extract data from an R data frame. We will not use this package in the examples but look at a way the operations in SQL translate to basic R commands that we have studied in previous chapter ??. In R we have seen how subsetting of rows and columns happen using the subset function in earlier chapters 2.2.1. Please review this section before proceeding ahead. 8.9.1 Subsetting on Columns ( DICING ) So lets start with dicing the dataframe. In other words, lets look at subsetting operations on columns. Columns in SQL are also called “fields”. In R it is commonly called “variables”. In SQL the subset of columns is determined by SELECT statement. We can do these type of SQL operation in R using the normal subsetting method, either using the subset() function or using the square brackets [ ]. NOTE: In most of the examples below, to avoid printing of the complete dataset after any operations, we have used the head() function to truncate the output to only top 6 rows. However you can always remove the function or change the limit or output records to your choice by passing additional attribute n = user_defined_limit to the head() function. Just to recap subsetting on columns, 8.9.1.1 Subset single column. Remember: You can either use the column names or the column location index, to dice the dataframe. Suppose we want to subset the moody dataset only the grade column. Lets look at this example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBMZXRzIHN1YnNldCB0aGUgZ3JhZGUgY29sdW1uIGZvcm0gdGhlIG1vb2R5IGRhdGFzZXQgYW5kIGxvb2sgYXQgaXRzIGZpcnN0IGZldyBlbGVtZW50cy4uXG5oZWFkKG1vb2R5WywnZ3JhZGUnLGRyb3A9Rl0pXG5cbiMgV2l0aG91dCBgZHJvcD1GYCBpbiB0aGUgYXR0cmlidXRlLCB5b3Ugd2lsbCBnZXQgb25seSB0aGUgdmFsdWVzIG9mIHRoZSBjb2x1bW4uXG5oZWFkKG1vb2R5WywnZ3JhZGUnXSkifQ== We can see that only one column is selected form the dataframe. The drop = F attribute is provided to keep the dataframe structure. You can also see the effect of not using the drop = F attribute in the above example. Note: In some cases, where you want to use the subsetted column with other function, e.g. mean(subsetted_column) you must not use the drop=F attribute, otherwise it will result in error. 8.9.1.2 Subset multiple column. Suppose you want to subset multiple columns by name, you can create a vector or the column names you want to subset and then include it wile subsetting. Lets look at the example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNyZWF0ZSBhIHZlY3RvciBvZiBjb2x1bW4gbmFtZSB0aGF0IHdlIHdhbnQgdG8gc3Vic2V0LlxuY29sdW1uTmFtZXM8LSBjKFwiZ3JhZGVcIixcInNjb3JlXCIpXG5cbiMgSW5jbHVkaW5nIHRoZSBhYm92ZSB2ZWN0b3Igd2lsZSBzdWJzZXR0aW5nLlxuaGVhZChtb29keVssY29sdW1uTmFtZXNdKSJ9 We can see that only the two column of “grade” and “score” are kept in the subset. Similarly, we can include the multiple column names and get subset. 8.9.1.3 Subset on all columns You can get all the columns in the subset, by keeping the space after the comma blank. This gives the complete set of columns. Suppose you did some slicing on the dataframe and want to keep all the columns in the output, you can just keep the space after the comma blank while subsetting. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNhbiBzZWUgdGhhdCB3ZSBzbGljZWQgdGhlIGRhdGFmcmFtZSB0byBvbmx5IGtlZXAgdGhlIHJlY29yZHMgb2Ygc3R1ZGVudHMgd2l0aCBncmFkZSBcIkFcIi4gXG5oZWFkKG1vb2R5W21vb2R5JGdyYWRlPT1cIkFcIiwgXSlcblxuXG4jIFdlICB3aWxsIGxvb2sgYXQgc2xpY2luZyBpbiB0aGUgc3Vic2VxdWVudCBzZWN0aW9ucy4ifQ== 8.9.2 Subsetting on Rows ( SLICING ) Now that we have seen dicing Or subsetting on columns, which is similar to the select statement of SQL, we will now look at slicing on the dataframe. Or in other words subsetting on rows. There are many statements of SQL that does subsetting on rows, i.e. SELECT, WHERE, AND, OR, IN, LIKE, LIMIT, and many more. We will look at few of them, by implementing them using the basic R functions. 8.9.2.1 Subsetting based on single condition. We will look at a subsetting condition based on value. For subsetting based on value, you can use the relational operators e.g. &gt; , &lt; , &gt;= , &lt;= , == , etc between the attribute name and the value. Lets look at this in the following example. Suppose you want to keep all the observations of where score of students are greater than 80. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNhbiBzZWUgdGhhdCB3ZSBzbGljZWQgdGhlIGRhdGFmcmFtZSB0byBvbmx5IGtlZXAgdGhlIHJlY29yZHMgb2Ygc3R1ZGVudHMgd2l0aCBzY29yZSBncmVhdGVyIHRoYW4gODAuIFxuaGVhZChtb29keVttb29keSRzY29yZT44MCwgXSkifQ== We can see from the above result, the subset has only records of students having score greater than 80. This example is similar to using where statement in SQL. 8.9.2.2 Subsetting based on multiple conditions. Similar to the above example, suppose you want to subset based on multiple conditions. To do this, we will use the logical operators e.g. AND (\" &amp; \") , OR (\" | \") , NOT (\" ! \") between the various conditions. Lets look at an example for this type of subsetting. Suppose you want to slice the records of the moody dataset, based on two conditions: - Students with grade equal to \" A \" - AND - Students with score greater than 90. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNhbiBzZWUgdGhhdCB3ZSBzbGljZWQgdGhlIGRhdGFmcmFtZSB0byBvbmx5IGtlZXAgdGhlIHJlY29yZHMgb2Ygc3R1ZGVudHMgd2l0aCBzY29yZSBncmVhdGVyIHRoYW4gOTAgQU5EIHdpdGggZ3JhZGUgZXF1YWwgdG8gXCJBXCIgLiBcbmhlYWQobW9vZHlbbW9vZHkkc2NvcmU+OTAgJiBtb29keSRncmFkZSA9PSBjKFwiQVwiKSwgXSkifQ== We can see that the records of students with score greater than 90 and grade equal to A are kept, rest all records are removed. This example is similar to using the AND, OR, NOT clause in SQL. 8.9.2.3 Subset based on multiple values. We will look at subsetting the dataframe based on one condition with multiple values. Suppose you want to subset the moody dataset, based on the students grade, but you want to keep students records with grade equal to both “B” and “C” Well you can use multiple conditions as seen above with an AND clause between the two conditions with different values on the same variable/columns, but there is a simple and useful way to do this with just one conditional statement. We will make use of a vector of all the values that we want to use, and then assign this vector to the condition statement. Lets look at this in the following example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBsZXRzIGNyZWF0ZSBhIHZlY3RvciBvZiB2YWx1ZXMgcmVxdWlyZWQgaW4gdGhlIGNvbmRpdGlvbmFsIHN0YXRlbWVudFxuY29uZFZhbHVlczwtIGMoXCJCXCIsXCJDXCIpXG5cbiMgaGVyZSB3ZSBjYW4gc2VlIHRoYXQgd2Ugc2xpY2VkIHRoZSBkYXRhZnJhbWUgdG8gb25seSBrZWVwIHRoZSByZWNvcmRzIG9mIHN0dWRlbnRzIHdpdGggZ3JhZGUgZXF1YWwgdG8gXCJCXCIgb3IgXCJDXCIgLiBcbmhlYWQobW9vZHlbbW9vZHkkZ3JhZGUgPT0gY29uZFZhbHVlcywgXSlcbnVuaXF1ZShtb29keVttb29keSRncmFkZSA9PSBjb25kVmFsdWVzLCBdJGdyYWRlKVxuXG4jIHdlIGNhbiBhbHNvIGRpcmVjdGx5IHdyaXRlIHRoZSB2ZWN0b3Igd2l0aG91dCBhc3NpZ25pbmcgYSB2YXJpYWJsZS5cbmhlYWQobW9vZHlbbW9vZHkkZ3JhZGUgPT0gYyhcIkJcIixcIkNcIiksXSlcbnVuaXF1ZShtb29keVttb29keSRncmFkZSA9PSBjKFwiQlwiLFwiQ1wiKSxdJGdyYWRlKSJ9 We can see that the output has only records of students with grades B or C. And both the methods, result in same output. This example is similar to the IN operator of the SQL 8.9.2.4 Subset based on a partial/complete text/character. We will look at subsetting the dataset based on a specific pattern of text/characters. This type of subsetting proves useful in text columns where each record has one or more than one sentence, and you want to search for a particular keyword or pattern. Most simple example would be of a survey dateset, where each record in the dataset consists of text paragraph, answering the questions asked in the survey, and you want to figure out the count of particular keywords in each response. Lets look at an example based on the Happiness dataset. We would like to find the subset of countries with the letters \" and \" in their name. eg. Iceland, Uganda, Poland, etc. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJoYXBweTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvSEFQUElORVNTMjAxNy5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBTdWJzZXQgdXNpbmcgdGhlIGdyZXAgZnVuY3Rpb24gdG8gZmluZCB0aGUgcGF0dGVybiBcImFuZFwiIGluIHRoZSBuYW1lcyBvZiB0aGUgY291bnRyaWVzXG5oZWFkKGhhcHB5W2dyZXAoXCJhbmRcIixoYXBweSRDT1VOVFJZLGlnbm9yZS5jYXNlID0gVCksXSlcbnVuaXF1ZShoYXBweVtncmVwKFwiYW5kXCIsaGFwcHkkQ09VTlRSWSxpZ25vcmUuY2FzZSA9IFQpLF0kQ09VTlRSWSkifQ== We can see the output has subset of the happy dataset with records of only those countries with the pattern “and” in its name. To do this we have used the grep() function, which is a really important function for finding patterns in text and data. We don’t need to study this grep() function in detail, but one can find very good resources explaining it online. This example is similar to the LIKE operator in SQL. 8.10 Group By Now that we have done Slicing and Dicing, we will like to apply some functions and gain measured information form the subsets. Although there is no straightforward, direct/ one step function to perform the function as that of the GROUP BY from SQL, but we can get the required functionality, by combining various functions step by step from the R.7 commands list 2.2 and the things we learned in this section ?? and the revision section ??. This section will involve use of the table(), tapply() function to apply the functions like mean, count, sum, etc on the subsets categorical or numerical columns. More importantly, we will look at a very useful example below, which will tie together all that we have learned until now. Suppose you want to get the statistics/numbers of average scores per grade and frequency of students per grade, and then use this table afterwards. So the SQL query will look something like SELECT grade, avg(score) as averagescore, count(*) as student_number FROM moody GROUP BY grade. To implement this above query functionality in R we would fisrt need to use the tapply function to find get the average score per grade and frequency per grade, and then combine it. Lets look at this process in the code below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuXG4jIENyZWF0ZSBhIHRhYmxlIG9mIGZyZXF1ZW5jeSBvZiBzdHVkZW50cyBwZXIgZ3JhZGUuXG5ncmFkZS5jb3VudCA8LSB0YXBwbHkobW9vZHkkZ3JhZGUsbW9vZHkkZ3JhZGUsbGVuZ3RoKVxuXG4jIENyZWF0ZSBhIHRhYmxlIG9mIGF2ZXJhZ2Ugb2Ygc3R1ZGVudHMgc2NvcmUgcGVyIGdyYWRlLlxuZ3JhZGUubWVhbiA8LSB0YXBwbHkobW9vZHkkc2NvcmUsbW9vZHkkZ3JhZGUsbWVhbilcblxuIyBXZSBub3cgY29tYmluZSB0aGUgdHdvIHRhYmxlcyB0b2dldGhlciB1c2luZyBjYmluZCBhbmQgc3RvcmUgaXQgYXMgZGF0YS5mcmFtZSBmb3Igc2ltcGxlIHBvc3QtcHJvY2Vzc2luZy5cbm91dDwtYXMuZGF0YS5mcmFtZShjYmluZChncmFkZS5jb3VudCxncmFkZS5tZWFuKSlcbm91dCJ9 We can see the combined table of both the average scores and frequency per grade. Now suppose we want to go one step ahead and want to order the out table from the example above, based on decreasing value of frequency of students per grade. To do this, we introduce the order() function. Order() Function The order() function returns a permutation of the order of the elements of a vector. You can decide by passing the argument to order the elements in ascending or descending order. An important thing to note, is that for our use for the example we discussed above, we will use the order function as a subset parameter. Lets look at this in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIilcbmdyYWRlLmNvdW50IDwtIHRhcHBseShtb29keSRncmFkZSxtb29keSRncmFkZSxsZW5ndGgpXG5ncmFkZS5tZWFuIDwtIHRhcHBseShtb29keSRzY29yZSxtb29keSRncmFkZSxtZWFuKVxub3V0PC1hcy5kYXRhLmZyYW1lKGNiaW5kKGdyYWRlLmNvdW50LGdyYWRlLm1lYW4pKVxuXG5vdXRcblxuIyBOb3cgbGV0cyBvcmRlciB0aGUgb3V0IGRhdGEsIGJhc2VkIG9uIHRoZSBncmFkZS5jb3VudCBjb2x1bW4sIGluIGFzY2VuZGluZyBvcmRlci5cbm91dFtvcmRlcihvdXRbLCdncmFkZS5jb3VudCddKSxdXG5cbiMgSWYgeW91IHdhbnQgdGhlIG91dHB1dCBpbiBkZXNjZW5kaW5nIG9yZGVyIGp1c3QgcGFzcyAnVFJVRScgb3IgJ1QnIHRoZSAgZGVjcmVhc2luZyBhcmd1bWVudCBvZiB0aGUgb3JkZXIgZnVuY3Rpb24uXG5vdXRbb3JkZXIob3V0WywnZ3JhZGUuY291bnQnXSxkZWNyZWFzaW5nID0gVCksXSJ9 We saw how we can use implement ordering in R. This is similar to using the ORDER BY statement of SQL. Another thing we can do is subsetting on the output. Suppose you want to keep only those grade records in the out data with frequency of students greater than 150 for particular grade. To do this we will use the technique studied in the slicing section 8.9.2. Lets look at the working of the above example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIilcbmdyYWRlLmNvdW50IDwtIHRhcHBseShtb29keSRncmFkZSxtb29keSRncmFkZSxsZW5ndGgpXG5ncmFkZS5tZWFuIDwtIHRhcHBseShtb29keSRzY29yZSxtb29keSRncmFkZSxtZWFuKVxub3V0PC1hcy5kYXRhLmZyYW1lKGNiaW5kKGdyYWRlLmNvdW50LGdyYWRlLm1lYW4pKVxuXG5vdXRcblxuIyBUbyBrZWVwIHRoZSByZWNvcmRzIHdoZXJlIGZyZXF1ZW5jeSBvZiBzdHVkZW50cyBpbiBwYXJ0aWN1bGFyIGdyYWRlIGlzIGdyZWF0ZXIgdGhhbiAxNTAuXG5vdXRbb3V0JGdyYWRlLmNvdW50PjE1MCxdIn0= We see that the B Grade had only 108 students in the record, it is removed from the out dataframe. This is similar to using the HAVING clause of SQL. 8.11 Handling Date and Time in dataframes. one of the most common issue that a novice or even an experienced R user can face is of handling date and time information available into the dataset, and importing it to use as a variable that is appropriate ans usable during analysis. Also getting R to agree that your data contains the dates and times can be tricky sometimes. We will see an example where the usual R data import fails to read date and time as actually date and time. To simplify this issue, we use a package called lubridate, which makes it easier to work with dates and times and converts them into POSIXct format. POSIXct is a class of data recognized by R as being a date or date and time. Lubridate’s functions handle wide variety of formats and separators, which simplifies the parsing process. Lets look how easy it is to use it and convert date and time input to be used in analysis. First we will look at converting date in character format to POSIXct. First we will convert \"20200317\"which is in year-month-date format. To convert this we will use the ymd() function of the lubridate package. Second we will convert \"03-17-2020\"which is in month-date-year format. To convert this we will use the mdy() function of the lubridate package. Third we will convert \"17/03/2020\"which is in date-month-year format. To convert this we will use the dmy() function of the lubridate package. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGx1YnJpZGF0ZSkgIyBpbmNsdWRlIHRoZSBsdWJyaWRhdGUgbGlicmFyeS5cblxuIyBGaXJzdCB3ZSB1c2UgdGhlIHltZCgpIGZ1bmN0aW9uLlxueW1kKFwiMjAyMDAzMTdcIilcblxuIyBTZWNvbmQgd2UgdXNlIHRoZSBtZHkoKSBmdW5jdGlvbi5cbm1keShcIjAzLTE3LTIwMjBcIilcblxuIyBUaGlyZCB3ZSB1c2UgdGhlIGRteSgpIGZ1bmN0aW9uLlxuZG15KFwiMTcvMDMvMjAyMFwiKSJ9 We can see the output of all the 3 function is the same, this means that the functions used have successfully converted all the input character type dates into the standardized POSIXct data type. Now lets look at converting time in character format to POSIXct. First we will convert \"18:20\" which is in hour-minutes format. To convert this we ill use the hm() function of the lubridate package. Second we will convert \"18:20:30\" which is in hour-minute-second format. To convert this we ill use the hms() function of the lubridate package. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGx1YnJpZGF0ZSkgIyBpbmNsdWRlIHRoZSBsdWJyaWRhdGUgbGlicmFyeS5cblxuIyBGaXJzdCB3ZSB3aWxsIHVzZSB0aGUgaG0oKSBmdW5jdGlvbi5cbmhtKFwiMTg6MjBcIilcblxuIyBTZWNvbmQgd2Ugd2lsbCB1c2UgdGhlIGhtcygpIGZ1bmN0aW9uLlxuaG1zKFwiMTg6MjA6MzBcIikifQ== We can see that the output of the 2 functions above are in POSIXct format and has the information of hours minutes and seconds annotated properly. There are various other functions in the lubridate package like for various use case, but we will not cover them since they are not useful here. To learn more about it you can visit the official lubridate package vignette linked here: lubridate Now coming back to the main example of avoiding issues/errors while importing date and time attributes present in dataset. For this we will look at the AirQualityUCI dataset. And here is the snippet of the dataset below. Table 8.4: Air Quality Dataset of amount of elements and pollutants in air. Date Time CO Tin.Oxide Non.Metanic.HydroCarbons Benzene 3/10/2004 18:00:00 2.6 1360 150 11.9 3/10/2004 19:00:00 2.0 1292 112 9.4 3/10/2004 20:00:00 2.2 1402 88 9.0 3/10/2004 21:00:00 2.2 1376 80 9.2 3/10/2004 22:00:00 1.6 1272 51 6.5 3/10/2004 23:00:00 1.2 1197 38 4.7 You will see from the dataset that the date and time columns are imported correctly. But in fact, and as we will see in the code below, the date column is of type character and the time is also of type character. Now to convert these columns into POSIXct supported date time columns we will use the lubridate functions. And then we will count the number of records in the dataset per year using the year() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhcTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvQWlyUXVhbGl0eVVDSS5jc3ZcIikgI3dlYiBsb2FkXG5hcTwtYXFbLDE6Nl0gIyBSZWR1Y2luZyB0aGUgbnVtYmVyIG9mIGNvbHVtbnMuXG5cbmhlYWQoYXEpXG5cbiMgTGV0cyBsb29rIGF0IHRoZSB0eXBlIG9mIHRoZSBEYXRlIGNvbHVtbiBhZnRlciBpbXBvcnRpbmcgdGhlIGRhdGFzZXQuXG5jbGFzcyhhcSREYXRlKSAjIERhdGUgQ29sdW1uXG5cbiMgTm93IGxldHMgdXNlIHRoZSBtZHkoKSBmdW5jdGlvbiB3aGljaCBjb252ZXJ0cyB0aGUgbW9udGgtZGF5LXllYXIgZm9ybWF0IHRvIFBPU0lYY3QgZm9ybWF0LlxuYXEkRGF0ZTwtbWR5KGFxJERhdGUpXG5oZWFkKGFxKVxuXG4jIE5vdyBsZXRzIGNoZWNrIHRoZSB0eXBlIG9mIHRoZSBEYXRlIGNvbHVtbiBhZ2Fpbi5cbmNsYXNzKGFxJERhdGUpXG5cblxuIyBMZXRzIGNyZWF0ZSBhIGZyZXF1ZW5jeSB0YWJsZSBmb3IgdGhlIGZyZXF1ZW5jeSBvZiByZWNvcmRzIHBlciB5ZWFyIHVzaW5nIHRoZSB0YWJsZSgpIGFuZCB5ZWFyKCkgZnVuY3Rpb25cbnRhYmxlKHllYXIoYXEkRGF0ZSkpIn0= We can see that the original type of the date column was \"character\" but then after using the lubridate’s function, we converted it to a suitable POSIXct format of \"Date\". Then we were easily able to subset the dataset based on the year, and get the frequency count of the records per year, as seen from the table for the years 2004 and 2005. Similarly, we can also convert the time column and probably use it later in analysis process. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhcTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvQWlyUXVhbGl0eVVDSS5jc3ZcIikgI3dlYiBsb2FkXG5hcTwtYXFbLDE6Nl0gIyBSZWR1Y2luZyB0aGUgbnVtYmVyIG9mIGNvbHVtbnMuXG5cbmhlYWQoYXEpXG5cbiMgTGV0cyBsb29rIGF0IHRoZSB0eXBlIG9mIHRoZSBUaW1lIGNvbHVtbiBhZnRlciBpbXBvcnRpbmcgdGhlIGRhdGFzZXQuXG5jbGFzcyhhcSRUaW1lKSAjIFRpbWUgQ29sdW1uXG5cbiMgU2ltaWxhcmx5IGZvciB0aGUgdGltZSBjb2x1bW4gbGV0cyB1c2UgdGhlIGhtcygpIGZ1bmN0aW9uLlxuYXEkVGltZTwtaG1zKGFxJFRpbWUpXG5cbiMgTm93IGxldHMgY2hlY2sgdGhlIHR5cGUgb2YgdGhlIFRpbWUgY29sdW1uIGFnYWluLlxuY2xhc3MoYXEkVGltZSkifQ== We can see that the original type of the time column was \"character\" but then after using the lubridate’s function, we converted it to a suitable POSIXct format of \"Period\" which is used to represent time information. EOC "]]
