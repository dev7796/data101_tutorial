[["index.html", "DATA 101 Shortest Textbook How To Accomplish More With Less ", " DATA 101 Shortest Textbook How To Accomplish More With Less Tomasz Imielinski 2021-08-10 This is a textbook based on the DATA 101 course thought by Prof. Tomasz Imielinski at Computer Science Department at Rutgers University, New Brunswick. Data 101 is an introductory course for beginners from any field of study, interested in the field of Data Science. This book’s pages are created using Rmarkdown from Rstudio (similar to jupyter notebook) and the book is compiled using “bookdown” package. The most important aspect of this interactive book are the interactive code chunks for running code, which are powered by a minimal version of Datacamp’s learning interface called “Datacamp Light”. For more info visit Datacamp Light Note1: This book is undergoing constant updates following along with the course thought in Spring 2021. Topics under progress are marked with a \" * \". Note2: This book uses Datacamp Light for supporting runnable code chunks. In case the code chunks do not connect to run-time session, please copy the code and run in RStudio. Also, please report if facing this issue to the instructor via email. "],["intro.html", "Chapter 1 Introduction 1.1 Setting Up R", " Chapter 1 Introduction The objective of this textbook is to provide you with the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in a statistically convincing manner, perhaps in a colorful and visually appealing way. Questions which you will have to anticipate and you will have to answer are - How do you know that your findings are not random? - And fundamental of all questions: - So what? Even the most impressing looking results may come up randomly. And you will be asked this question along with the question “what was your p-value and how did you compute it” And even if you convince your audience that your results are not random, you will have to be ready to explain why your audience should care about the results you reported. In other words, is there any actionable value in your results? Or they are just simply interesting, good to know, but no one really needs to care much about them otherwise? Hopefully it is the former not the latter. In the following sections we will address these questions and go through the process of data exploration, validation, and presentation. We will start with making plots, follow with free style data exploration – which allows us to form the leads, that is hypotheses. Then we will follow with simple statistical tests which will allow us to validate these hypothesis and defend our findings against randomness claims. - We will learn how to calculate p-values and how to use them to defend our findings. We will use as few R commands as possible and reach our goal in the shortest possible path. In fact we will demonstrate how using just 7 R commands we can perform quite sophisticated data exploration. In the appendix, we show many more useful commands of R which eventually you would have to use. However, our goal in this short textbook, is to present the shortest path to data analysis which will let you import the data, plot it, make some analysis yourself and use R-libraries. In this textbook and in this class we do not teach how to clean the data (data wrangling) and how to deal with a wide variety of data types. We also do not address complex data transformations such as multi-frame operations like merge (we show them in appendix). We also do not explain how different machine learning methods work, we only show you how to use them. It is similar to teaching one how to drive a car without knowing how a car engine works. 1.1 Setting Up R Important Instructions Installation of R is required before installing RStudio “R” is a programming language, and, “RStudio” is an Integrated Development Environment (IDE) which provides you a platform to code in R. How to download and install R &amp; RStudio? Downloading and installing R. For Windows Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cran.r-project.org/bin/windows/base/ Click on the link at top left where it says “Download R 4.0.3 for windows” or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. For MAC Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cloud.r-project.org/bin/macosx/ Under “Latest release”, click on “R-4.0.3.pkg” or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. Downloading and installing RStudio. For Windows Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named “All Installers”. Click on the download link beside “Windows 10/8/7” to download the windows version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. For MAC Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named “All Installers”. Click on the link beside “macOS 10.13+” to start your download the MAC version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. How to upload a data set? To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon. Let us look at the example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFJlYWQgaW4gdGhlIGRhdGFcbmRmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMGIuY3N2XCIpXG5cbiMgUHJpbnQgb3V0IGBkZmBcbmhlYWQoZGYpIn0= "],["dataexp.html", "Chapter 2 Data Exploration 2.1 Plots 2.2 Free Style data exploration with just seven R commands \" R.7 \"", " Chapter 2 Data Exploration 2.1 Plots Table 2.1: Snippet of Moody Dataset score grade texting questions participation 26.89 F never never 0.41 71.57 B always rarely 0.00 90.11 A always never 0.27 31.52 D sometimes rarely 0.68 95.94 A always rarely 0.09 45.72 D always rarely 0.19 90.82 A always always 0.25 75.52 B sometimes never 0.28 52.31 C never never 0.67 39.57 D always always 0.40 2.1.1 Scatter Plot Scatter Plot are used to plot points on the Cartesian plane (X-Y Plane) Hence it is used when both the labels are numerical values. Lets look at example of scatter plot using Moody. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExldCdzIGxvb2sgYXQgYSAyIGF0dHJpYnV0ZSBzY2F0dGVyIHBsb3QuXG4jIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5wbG90KG1vb2R5JHBhcnRpY2lwYXRpb24sbW9vZHkkc2NvcmUseWxhYj1cInNjb3JlXCIseGxhYj1cInBhcnRpY2lwYXRpb25cIixtYWluPVwiIFBhcnRpY2lwYXRpb24gdnMgU2NvcmVcIixjb2w9XCJyZWRcIikifQ== 2.1.2 Bar Plot A bar plot is used to plot rectangular bars proportional to the values present in a numerical vector. This rectangle height is proportional to the value of the variable in the vector. Barplots are also used to graphically represent the distribution of a categorical variable, after converting the categorical vector into a table(i.e. frequency distribution table) In a bar plot, you can also give different colors to each bar. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cbiNsZXRzIG1ha2UgYSB0YWJsZSBmb3IgdGhlIGdyYWRlcyBvZiBzdHVkZW50cyBhbmQgY291bnRzIG9mIHN0dWRlbnRzIGZvciBlYWNoIEdyYWRlLiBcblxudDwtdGFibGUobW9vZHkkZ3JhZGUpXG5cbiNvbmNlIHdlIGhhdmUgdGhlIHRhYmxlIGxldHMgY3JlYXRlIGEgYmFycGxvdCBmb3IgaXQuXG5cbmJhcnBsb3QodCx4bGFiPVwiR3JhZGVcIix5bGFiPVwiTnVtYmVyIG9mIFN0dWRlbnRzXCIsY29sPWNvbG9ycyxcbm1haW49XCJCYXJwbG90IGZvciBzdHVkZW50IGdyYWRlIGRpc3RyaWJ1dGlvblwiLGJvcmRlcj1cImJsYWNrXCIpIn0= 2.1.3 Box Plot A boxplot shows the distribution of data in a dataset. A boxplot shows the following things: Minimum Maximum Median First quartile Third quartile Outliers You can create a single boxplot using just a vector or a multiple boxplot using a formula. When you write a formula, you should use the Tilde (~) operator. This column name on the left side of this operator goes on the y axis and the column name on the right side of this operator goes on the x axis. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cblxuI1N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCB0aGUgZGlzdHJpYnV0aW9uIG9mIHN0dWRlbnRzIHNjb3JlIHBlciBHcmFkZS4gV2UgdXNlIGJveCBwbG90IGZvciBnZXR0aW5nIHRoYXQuIFxuYm94cGxvdChzY29yZX5ncmFkZSxkYXRhPW1vb2R5LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJTY29yZVwiLCBtYWluPVwiQm94cGxvdCBvZiBncmFkZSB2cyBzY29yZVwiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIilcblxuIyB0aGUgY2lyY2xlcyByZXByZXNlbnQgb3V0bGllcnMuIn0= 2.1.4 Mosaic Plot Mosaic plot is a graphical method for visualizing data from two or more qualitative variables. The length of the rectangles in the mosaic plot represents the frequency of that particular value. The width and length of the mosaic plot can be used to interpret the frequencies of the elements. For example, if you want to plot the number of individuals per letter grade using a smartphone, you want to look at a mosaic plot. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cbiNzdXBwb3NlIHlvdSB3YW50IHRvIGZpbmQgbnVtYmVycyBvZiBzdHVkZW50cyB3aXRoIGEgcGFydGljdWxhciBncmFkZSBiYXNlZCBvbiB0aGVpciB0ZXh0aW5nIGhhYml0cy4gVXNlIE1vc2lhYy1wbG90LlxuXG5tb3NhaWNwbG90KG1vb2R5JGdyYWRlfm1vb2R5JHRleHRpbmcseGxhYiA9ICdHcmFkZScseWxhYiA9ICdUZXh0aW5nIGhhYml0JywgbWFpbiA9IFwiTW9zaWFjIG9mIGdyYWRlIHZzIHRleGluZyBoYWJpdCBpbiBjbGFzc1wiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIikifQ== 2.2 Free Style data exploration with just seven R commands \" R.7 \" We have to know 7 commands in R which we call R.7. 4 commands to plot (barplot, boxplot, plot and histogram) These we have already discussed. One command to slice and dice data: subset() or df[condition,]$variable Two commands to aggregate data: table() and tapply() One data structure: Data frame And with these seven commands of R.7 we will be able to do quite a bit of data exploration. Manipulate data any way we want to. We will begin with what we call freestyle data exploration. We call it freestyle, since we are not going to use any sophisticated libraries, but rather just seven basic commands of R. No statistics yet, and no more sophisticated R programs. These will come later. For now, we are just feeling the data with four plots and three R instructions: subset(), table() and tapply() Through this section we will use our usual initial example, of synthetically generated data describing mysterious methods of grading used by an eccentric Professor Moody. We have been using different versions of this data puzzle over the 6 years of teaching data 101. Data is different but narrative is always the same: 2.2.0.1 Professor Moody Puzzle We start our data exploration with the following data puzzle. Professor Moody has been teaching statistics 101 class for many years. His teaching evaluations went considerably south with the chief complaint: he DOES NOT seem to assign grades fairly. Students compared their scores among themselves and found quite a bit of discrepancies! But their complaints went nowhere since the Professor promptly disappeared after posting the final grades and scores. A new brave TA, managed to get hold of the carefully maintained grading table (spanning multiple years) of professor Moody by ….messing a bit with Moody’s computer….well, let’s not explain the details because he would get in trouble. What he found out was a remarkably structured account of how professor Moody assigns his grades. Looks like Professor Moody is in fact very alert in class. He is aware of what students do, detecting texting during class and remembering exactly who asked many questions in class. He also keeps the mysterious “participation index” which is a numerical score from 0 to 1. This is probably related to questions asked and answered by students as well as their general attentiveness in class. Remarkable but a little creepy, isn’t it? What is the best advice the new TA can give future students how to get a good grade in Professor Moody’s class? What factors influence the grade besides the score? Back your recommendation up with plots and evidence from the attached data. The Moody data set is defined here by the following attributes: Table 2.2: Snippet of Moody Dataset score grade texting questions participation 26.89 F never never 0.41 71.57 B always rarely 0.00 90.11 A always never 0.27 31.52 D sometimes rarely 0.68 95.94 A always rarely 0.09 45.72 D always rarely 0.19 90.82 A always always 0.25 75.52 B sometimes never 0.28 52.31 C never never 0.67 39.57 D always always 0.40 Moody[score, grade, participation, questions, texting] Score and grade are self explanatory. Participation is supposedly measuring students’ participation in class. We do not know whether higher participation would necessarily be positive, since Professor Moody’s mood changes from year to year and he may be annoyed by students who are too active and bother him too much. Who knows? We have to find out. Attribute “questions” has several values “always”, “frequently”, “sometimes” and “never”. So does the attribute “texting”. In our data set there are students who are always texting and who never ask any questions. Oh, yes, and some students’ participation index is almost zero. Guess what grade they are getting? F, you probably guessed. Well, but what about their score? It should matter at least to some degree! Grading rules, which Professor Moody applies each year, are different. The objective of our freestyle data exploration is to find some leads/hypotheses which would help us direct students what they should do to get a good grade in his class. This is a good illustration of what data exploration is and can achieve. It is just an example, but one can of course easily see that things we discuss here apply to any data set. Data exploration can be viewed as an indefinite loop: REPEAT{ Plot,one or many plots. Transform Data. } UNTIL GRATIFICATION Put yourself in the position of a student in Moody’s class. What does s/he want to know? What should I do in order to pass his class aside from getting the best score possible? Ask many questions? Do not text? Come to class as often as you can? Presumably improving participation index? What is the approach to discover the secret rules of getting good grades in Professor Moody’s class? First you need “kick the tires”, make some plots, feel the data and perhaps rule out the obvious. In case of Professor Moody data it may mean the following: - Test if straightforward mapping of scores into grades work in Professor Moody’s class. Admittedly it is a long shot. We expect more from professor Moody than just merely following the scoring intervals with A above, say 85, B between 70 and 85 etc! First, we need to establish that it is not the case. Since it would be embarrassing to miss the obvious and simplest recommendation. Just score as high as you can. Otherwise there is no need to come to class, and in class you can text as much as you want to and ask no questions. Does not matter what else you do. You may never ask any questions, always text in class or simply…never even show up. All it would matter is to score as high as you can! There is just one plot which can quickly establish whether this simple rule works. And it is the boxplot. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5ib3hwbG90KG1vb2R5JHNjb3JlIH4gbW9vZHkkZ3JhZGUsIG1haW4gPSAnRGlzdHJpYnV0aW9uIG9mIFNjb3JlcyBieSBHcmFkZScsIHlsYWIgPSdTY29yZSAob3V0IG9mIDEwMCknLCB4bGFiID0gJ0xldHRlciBHcmFkZScsY29sPWMoJ3JlZCcsJ2JsdWUnLCdncmVlbicsJ2N5YW4nLCd5ZWxsb3cnKSkifQ== As illustrated by the boxplot, there are significant overlaps between successive grades. This disproves that there is a deterministic function between score and grade. At least it is not always a function. If your score falls in certain “gray” areas you may get either one of two grades (A or B, B or C, C or D, D or F). And we do not know what is this additional “decider” in such case when score falls into this gray area. Here is how we can check which factors may impact the grade. One way of doing this analysis is to make barplots for all possible slices of Moody data frame by a given categorical variable For example,we want to know if asking questions “matters” for the grade? This can be validated by comparing barplots of grade distribution for different values of attribute “questions”.You can either do it by applying the mosaic plot which allows for two-dimensional representation of data and allows to create multicolored table for grade x questions to eyeball if values of attribute “questions” matter for values of attribute “grade”. To dig deeper into the relationship of categorical variables “questions” and “texting” with “grade” we will use a sequence of bar plots over subsets of the Moody data frame. Then we will follow with the mosaic plots. The following slices represent subsets of the Moody data frame for each of the values of the attribute “questions” The command \\(\\color{violet}{\\text{table}}\\) (one of the 7 commands) will provide us grade distribution for each of these slices. And the barplot, will visualize this table. Let’s look at the example of the above process for students who always ask question. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhYmxlKG1vb2R5W21vb2R5JHF1ZXN0aW9ucz09J2Fsd2F5cycsXSRncmFkZSksbWFpbiA9ICdGcmVxdWVuY3kgb2Ygc3R1ZGVudHMgYnkgR3JhZGUgd2hvIFwiYWx3YXlzXCIgYXNrIHF1ZXN0aW9ucycsIHlsYWIgPSdGcmVxdWVuY3knLCB4bGFiID0gJyBHcmFkZScsY29sPWMoJ3JlZCcsJ2JsdWUnLCdncmVlbicsJ2N5YW4nLCd5ZWxsb3cnKSlcblxuI05vdGljZSB0aGF0IHlvdSBjYW4gbW9kaWZ5IHRoZXNlIGJhcnBsb3RzIGdyYXBocyBhbmQgcmVwbGFjZSB0aGUgdmFsdWUgb2YgbW9vZHkkcXVlc3Rpb25zIGF0dHJpYnV0ZXMgZnJvbSBcImFsd2F5c1wiIHRvIFwic29tZXRpbWVzXCIgb3IgXCJuZXZlclwiIGFuZCBzZWUgaW1wYWN0IHRoZXNlIG5ldyBzbGljZXMgaGF2ZSBvbiB0aGUgZ3JhZGUgZGlzdHJpYnV0aW9uLiBKdXN0IGNoYW5nZSB0byBjb2RlIGFib3ZlIGFuZCBydW4gaXQuIFlvdSBjYW4gYWxzbyBjaGFuZ2UgdGhlIG1vb2R5JHF1ZXN0aW9ucyBhdHRyaWJ1dGUgYW5kIHJlcGxhY2UgaXQgd2l0aCB0aGUgbW9vZHkkdGV4dGluZyBhdHRyaWJ1dGUgYW5kIGl0cyBkaWZmZXJlbnQgdmFsdWVzLiBUaHVzIHlvdSBjYW4gcnVuIDYgZGlmZmVyZW50IGJhcnBsb3RzIHVzaW5nIHRoZSBjb2RlIGFib3ZlIGFuZCBzZWUgaG93IEdyYWRlIGRpc3RyaWJ1dGlvbiBpcyBhZmZlY3RlZCBmb3IgZWFjaCBvZiB0aGVzZSA2IGNhc2VzLiJ9 We can also run two mosaic plots of GRADE vs “questions” or “texting” respectively - and be able to assess the same - do these attributes matter for the grade? In the following command we can combine attribute grade with anyone of the behavioral attributes eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5tb3NhaWNwbG90KG1vb2R5JGdyYWRlfm1vb2R5JHRleHRpbmcseGxhYiA9ICdHcmFkZScseWxhYiA9ICdUZXh0aW5nIGhhYml0JywgbWFpbiA9IFwiTW9zaWFjIG9mIGdyYWRlIHZzIHRleGluZyBoYWJpdCBpbiBjbGFzc1wiLGNvbD1jKCdyZWQnLCdibHVlJywnZ3JlZW4nLCdjeWFuJywneWVsbG93JyksYm9yZGVyPVwiYmxhY2tcIikifQ== This can be concluded by comparing different columns and rows of the mosaic table. If grade distribution is similar for different values of behavioral attributes, this would indicate that these attributes do not matter in establishing the grade. On the other hand we may “catch professor Moody” and find out that for some value of some attribute, grade distribution is significantly affected. This was the case several years ago when students sitting in the first row got a grade bump up, even if they got similar scores to students sitting in the back row. In that case one of the extra attributes included the row where students were sitting during class. We can see that asking many questions (frequently and always) really matters for the grade, there are more A’s and more B’s for these slices than in general. But this may have nothing to do with Professor Moody rewarding students with the bonus for asking questions. It may be simply the case that such students are more involved and study harder (or are more interested in the topic) and simply get higher scores. We need to dig deeper and see which of the two is the case. Moody’s just gives his personal bonus to students who ask a lot of questions or no such bonus is given – such students simply score higher. We can accomplish this using again one of the seven R commands – the tapply. ## always never rarely ## 51.08277 56.32474 53.69217 will return an average score for each of the values of the attribute moody$questions. If these values are more or less uniform then it will informally (not statistically yet, for this we have to wait for the next sections) show that questions matter in professor moody grading method and are not just correlated with student’s score. Take a look at eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhcHBseShtb29keSRzY29yZSwgbW9vZHkkcXVlc3Rpb25zLCBtZWFuKSwgeGxhYiA9ICdxdWVzdGlvbiBjYXRlZ29yaWVzJyx5bGFiID0gJ1Njb3JlIEF2ZXJhZ2UnLCBtYWluID0gXCJNZWFuIFNjb3JlIHZzIFF1ZXN0aW9ucyBBc2tlZCB1c2luZyB0YXBwbHkoKVwiLGNvbD1jKCdyZWQnLCdibHVlJywnZ3JlZW4nLCdjeWFuJywneWVsbG93JyksYm9yZGVyPVwiYmxhY2tcIikifQ== What is the conclusion? Does asking questions often imply a higher score? Is texting less correlated with higher scores? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhcHBseShtb29keSRzY29yZSwgbW9vZHkkdGV4dGluZywgbWVhbiksIHhsYWIgPSAndGV4dGluZyBjYXRlZ29yaWVzJyx5bGFiID0gJ1Njb3JlIEF2ZXJhZ2UnLCBtYWluID0gXCJNZWFuIFNjb3JlIHZzIFRleHRpbmcgdXNpbmcgdGFwcGx5KClcIixjb2w9YygncmVkJywnYmx1ZScsJ2dyZWVuJywnY3lhbicsJ3llbGxvdycpLGJvcmRlcj1cImJsYWNrXCIpIn0= shows that mean scores are the same across different values of the “texting”attribute. Same is true for the mean scores of “questions” attribute. Neither do these two attributes “texting” and “questions” seem to have an impact on the grade. Therefore it seems that the “behavioral” attributes: questions and texting do not seem to have an impact on the grade. We define intervals of score as clear, if there is only one grade associated with scores from such intervals. The remaining intervals are defined as grey - scores where grade can be either A or B, B or C, C or D and D or F respectively. Then we can examine how participation influences grades in these grey areas of score. Our hypothesis is that higher participation would probably offer better odds for higher grades. We can run the following command for different values of q. Let q be a threshold of participation which we want to test. Maybe if participation is higher than q, higher grade (from the two possible grades in the grey area of score) is given, while if participation is lower than q, it works against a student, who then gets lower grade? Let’s check how the grade distribution changes for different values of q from the lower values of q to higher values of q. We can just change q directly in the code below, and see results immediately. Run the following command for different values of q. We will only show it for the grey interval between A and B. The same process can be repeated for other gray areas between B and C, C and D and D and F. In fact one can modify the code below by just replacing grade A and B with B and C respectively as well as replacing the variables LowestA with Lowest B and Highest B with Highest C respectively. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNTaW1wbGUgUiBjb21tYW5kcyB0byBnZXQgaW50ZXJ2YWwgZm9yIGVhY2ggZ3JhZGUuXG5Mb3dlc3RBPC1taW4obW9vZHlbbW9vZHkkZ3JhZGU9PSdBJywgXSRzY29yZSlcbkhpZ2hlc3RCPC1tYXgobW9vZHlbbW9vZHkkZ3JhZGU9PSdCJywgXSRzY29yZSkgXG4jVGhpcyBnaXZlcyB1cyBpbnRlcnZhbCA8SGlnaGVzdEIsIExvd2VzdEE+XG5wcmludChjKExvd2VzdEEsSGlnaGVzdEIpKVxuXG5xPTAuNSAjIFBsZWFzZSBFZGl0IHRoaXMgXCIgcSBcIiB2YWx1ZSBhbmQgc2VlIHRoZSBjaGFuZ2VzIGFzIG1lbnRpb25lZCBhYm92ZS5cbnRhYmxlKG1vb2R5W21vb2R5JHNjb3JlPkxvd2VzdEEgJiBtb29keSRzY29yZTxIaWdoZXN0QiYgbW9vZHkkcGFydGljaXBhdGlvbiA+IHEsXSRncmFkZSlcblxuI05vdGUgdGhlIHNhbWUgcHJvY2VzcyBjYW4gYmUgcmVwZWF0ZWQgd2l0aCBvdGhlciBhZGphY2VudCBncmFkZXMuIEV4OiA8QixDPiBldGMuIn0= Please verify that for higher values of 0&lt;q&lt;1, the table operation shows higher percentages of better grades. Is there a critical value of q which clearly separates, say A’s from B’s? It seems to be q=0.6 - but it is not a clear cut deterministic. Rather, a higher value of participation threshold, q increases probability (frequency) of getting As. We come to the conclusion that participation matters in the grey area of score, in having higher chances for better grades, if participation is higher. Thus, just in case (since no one can predict if they will end up in borderline score) it is better to earn a high participation index – by (probably) coming to class more often and participating in discussions, and answering professor Moody’s questions. Simply put “come to as many classes as you can” should be an additional recommendation for getting good grades. Clearly, this, along with high scores, helps in getting better grades. But while in class, do not worry about texting or asking questions. These two attributes do not seem to matter. Now we can reveal how data was generated? What was the real rule embedded in the data? Now it is time to reveal how we generated our data. We have indeed defined border areas in score value. In these border areas of score, participation plays a role. Student who’s score falls into the grey area may get one of two grades, A or B, B or C, C or D and D or F, depending on the score.For example, a score of 72 may result in A or B. It is more likely to be A if student’s participation is high (higher the better the odds of getting A). If a student’s participation is low, it is much more likely to result in a lower grade, for a score of 72, it would be B. Therefore we have discovered the rule which guided the generation of the Moody’s data set. We have provided students with actionable intelligence on how to increase chances of getting higher grades, namely through the relationship between participation, score and grade. In the process of slicing, dicing and plotting the data we would also discover other interesting relationships still using just 7 commands. Does higher participation mean higher score, in general? Meaning that coming to class is positively correlated with a higher score? We can run scatter plot eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5wbG90KG1vb2R5JHBhcnRpY2lwYXRpb24sIG1vb2R5JHNjb3JlKSJ9 To our surprise it looks like the higher the participation, the lower the score! The distinct linear patterns in the scatter graph seem to be sloping down with participation. We are tempted to infer that participation is bad for score, that somehow Moody’s lectures have negative impact on the score - hence do not carry pedagogical value. Such a conclusion is typical confusion between correlation and causation. What is true in our simulated data set - that students who had some prior background in the subject matter of Professor Moody just do not show up in class that often. They already know the material. Students who do show up are the ones who are not confident in their knowledge of the subject matter - in general “weak A’s” and below. Then of course lower grade students (D’s and F’s) just simply do not apply themselves that much - are not invested into the class and just show up in class even less. Thus, the explanation probably has more to do with the students attribute about the class than with the pedagogical value of Professor Moody’s lectures. We can change values of parameters q and s and examine in more detail the relationships between scores and participation. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNJbnRlcmVzaW5nIGFuYWx5c2lzIGhhcyB0byBkbyB3aXRoIGRpZ2dpbmcgZGVlcGVyIGludG8gcmVsYXRpb25zaGlwIGJldHdlZW4gcGFydGljcGF0aW9uIGFuZCBzY29yZS4gV2hhdCBhcmUgdGhlIHNjb3JlcyBvZiBzdHVkZW50cyB3aG8gcGFydGljaXBhdGUgbGVzcyB0aGFuIHNvbWUgdmFsdWUgcSA8MS4gV2hhdCBhcmUgdGhlIHBhcnRpY2lwYXRpb24gdmFsdWVzIG9mIHN0dWRlbmV0cyB3aG8gc2NvcmUgbGVzcyB0aGFuIHM/ICAgQnkgY2hhbmdpbmcgdGhlIHZhbHVlcyBvZiBxIGFuZCBzIGluIHRoZSBjb2RlIHlvdSBjYW4gZ2FpbiBtb3JlIGluc2lnaHQgaW50byByZWxhdGlvbnNoaXAgYmV0d2VlbiBwYXJ0aXBhdGlvbiBhbmQgc2NvcmUuXG5cbiMgQ2hhbmdlIHRoZSB2YWx1ZXMgb2YgXCJxXCIgYW5kIFwic1wiIGJlbG93LlxucTwtMC4xXG5zPC03MFxubWVhbihtb29keVttb29keSRwYXJ0aWNpcGF0aW9uIDxxLF0kc2NvcmUpXG5tZWFuKG1vb2R5W21vb2R5JHNjb3JlIDxzLF0kcGFydGljaXBhdGlvbikifQ== Exploring Behaviors of Students in Professor Moody’s class. One may even drop the grade entirely from the picture and simply inquire about behavioral characteristics of Professor Moody’s students. We already know what is the distribution of each type of behavior eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG50YWJsZShtb29keSRxdWVzdGlvbnMpO1xudGFibGUobW9vZHkkdGV4dGluZykifQ== But lets ask for associations between behaviors Do students who ask a lot of questions also spend little time texting? Do students with higher participation generally text less? These questions have nothing to do with students’ performance. But they all can be answered using simple R.7 commands. Same data may serve different purposes. We started with predicting what behaviors help getting higher grades in professor Moody’s class. But we can imagine a different study – which is addressing student behavior in professor Moody’s class. Yet another study could address the impact of behavioral attributes on students’ scores (not grades). All these analyses can be done using freestyle exploration and R.7. What we discover is not yet guaranteed to be statistically valid. For this we need statistical evaluation, The p-values, the z-tests etc. Later we will also find statistical functions which can greatly help in data exploration. Free style exploration role is to generate leads otherwise known as conjectures or hypotheses. Our professor Moody’s data puzzle has been traditionally the first data puzzle we ask students to solve in data 101 class. Here are some examples of conclusions reached on different instances of professor Moody’s data set in the past semesters. Notice that attributes of Moody’s data set in past may have been different (like “sleeping in class”) Here is the set of recommendations from a former student who cracked that year’s professor Moody’s puzzle (or did she?) “Judging by plots and means calculated earlier, there are several factors, besides score, that affect students’ grades: • Sleeping in class increases grade • Texting in class decreases grades a little • Being active(participating) in class all the time significantly increases the grade, BUT: • Being active(participating) in class just occasionally decreases the grade even more, than not participating at all. • Being active only occasionally significantly decreases the grades. • Texting does not significantly affect grades . So for students in order to succeed in professor Moody’s class, my advice will be(besides getting high score): • VERY IMPORTANT: Participate all the time., or do not participate at all!!! • Sleep in class(especially if you do not participate anyway) • While texting might bring down your grade a little bit, the difference is very small” "],["stateval.html", "Chapter 3 Simple Statistical Evaluation 3.1 Z-test 3.2 Permutation Test 3.3 Multiple Hypothesis - Bonferroni Correction.", " Chapter 3 Simple Statistical Evaluation The biggest enemy of your findings is randomness. In order to convince your audience that you have found something you need to address the question “how do you know your result is simply sheer luck, it is random?” This is where you need statistical tests for use in hypothesis testing. 3.0.0.1 Two Important Formula’s: Mean \\[\\begin{equation} \\bar{X}=\\frac{\\sum{X}}{N} \\ \\text{where, X is set of numbers and N is size of set.} \\end{equation}\\] Standard Deviation \\[\\begin{equation} \\sigma = \\sqrt{\\frac{\\sum{(X - \\mu)^2}}{N}}\\\\ \\text{where, X is set of numbers, $\\mu$ is average of set of numbers, }\\\\ \\text{ N is size of the set, $\\sigma$ is standard deviation} \\end{equation}\\] 3.1 Z-test A z-test is any statistical test used in hypothesis testing with an underlying normal distribution. In other words, when the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution, z-test can be used. Outcome of the z-test is the z-score which is a numerical measure to test the mean of a distribution. z-score is measured in terms of standard deviation from mean. 3.1.1 Steps for hypothesis testing using Z-test. Running a Z-test requires 5 steps: State the null hypothesis and the alternate hypothesis Select a null hypothesis and an alternate hypothesis which will be tested using the z-test. Choose an Alpha \\(\\alpha\\) level. Usually this is selected to be small, such that the area under the normal distribution curve is accumulated most in the range between the alpha level. Thus mostly in statistical testing, \\(\\alpha = 0.05\\) is selected. Calculate the z-test statistic. The z-test statistic is calculated using the z-score formula. \\[\\begin{equation} z = \\frac{x-\\mu}{\\sigma}\\text{ where, $z$ = z-score, $x$ = raw score, $\\mu$ = mean and $\\sigma$ = standard deviation } \\end{equation}\\] Calculate the p-value using the z-score Once we have the z-score we want to calculate the p-value from it. To do this, there are 2 ways, First use the z-table available online at z-table.com Second, use the pnorm() function in R to find the p-value. Compare the p-value with \\(\\alpha\\) After getting the p-value from step 4, compare it with the \\(\\alpha\\) level we selected in step 2. This decides if we can reject the null hypothesis or not. If the p-value obtained is lower than \\(\\alpha\\), then we can reject the null hypothesis. If the p-value is more than \\(\\alpha\\), we fail to reject the null hypothesis due to lack of significant evidence. Some important relation between one-sided and two sided test while using hypothesis testing is as follows: First, estimate the expected value \\(\\mu\\) of T(statistic) under the null hypothesis, and obtain an estimate \\(\\sigma\\) of the standard deviation of T. Second, determine the properties of T : one tailed or two tailed. For Null hypothesis H0: \\(\\mu \\geq \\mu_0\\) vs alternative hypothesis H1: \\(\\mu &lt; \\mu_0\\) , it is upper/right-tailed (one tailed). For Null hypothesis H0:\\(\\mu \\leq \\mu_0\\) vs alternative hypothesis H1: \\(\\mu &gt; \\mu_0\\) , it is lower/left-tailed (one tailed). For Null hypothesis H0: \\(\\mu = \\mu_0\\) vs alternative hypothesis H1: \\(\\mu \\neq \\mu_0\\) , it is two-tailed. Once you calculate the pnorm() in step 4, depending on the properties of two as described above, use pnorm(-Z) for right tailed tests, use 2*pnorm(-Z) for two tailed test, and use pnorm(Z) for left tailed tests. Note: (Here Z = z-score). Also the method mentioned above works similar to that studied in class/recitations, but is simple to understand, and does not require subtracting the pnorm() output from 1. 3.1.2 Z-test Example 1 (Right Sided) Now lets look at an example to use this z-test for hypothesis testing. We will study the example to statistically find the relation of the traffic volume per minute between two tunnels, namely Holland and Lincoln . Table 3.1: Snippet of Traffic Dataset TUNNEL DAY VOLUME_PER_MINUTE 2280 Lincoln weekday 63 2250 Lincoln weekday 73 1504 Lincoln weekday 93 1826 Lincoln weekday 50 832 Holland weekday 37 913 Holland weekday 38 1408 Lincoln weekday 52 1412 Lincoln weekday 67 2705 Lincoln weekend 62 1961 Lincoln weekday 77 Thus stating out Null Hypothesis and Alternate Hypothesis. Null Hypothesis H0: Traffic in Lincoln is same as Traffic in Holland tunnel. Alternate Hypothesis H1: Traffic in Lincoln is higher than traffic in Holland tunnel. Once we have stated our hypothesis, lets see the z-test in practice. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YXNldFxuVFJBRkZJQzwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9UUkFGRklDLmNzdicpXG5zdW1tYXJ5KFRSQUZGSUMpICNnaXZlcyB1cyB0aGUgZGF0YSBzdGF0aXN0aWNzXG5cbiNkYXRhIGNsZWFuIGFuZCBzdWJzZXRcbmxpbmNvbG4uZGF0YSA8LSBzdWJzZXQoVFJBRkZJQywgVFJBRkZJQyRUVU5ORUwgPT0gXCJMaW5jb2xuXCIpXG5ob2xsYW5kLmRhdGEgPC0gc3Vic2V0KFRSQUZGSUMsIFRSQUZGSUMkVFVOTkVMID09IFwiSG9sbGFuZFwiKVxuXG4jIHRyYWZmaWMgYXQgbGluY29sblxuIyBUaGlzIHZhcmlhYmxlIGlzIGEgY29sdW1uIG9mIDE0MDEgcm93cy5cbmxpbmNvbG4udHJhZmZpYyA8LSBsaW5jb2xuLmRhdGEkVk9MVU1FX1BFUl9NSU5VVEVcblxuIyB0cmFmZmljIGF0IGhvbGxhbmRcbiMgVGhpcyB2YXJpYWJsZSBpcyBhIGNvbHVtbiBvZiAxNDAxIHJvd3MuXG5ob2xsYW5kLnRyYWZmaWMgPC0gaG9sbGFuZC5kYXRhJFZPTFVNRV9QRVJfTUlOVVRFXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuIyBUaGUgZmluYWwgdmFsdWUgaXMgdGhlIHN0YW5kYXJkIGRldmlhdGlvbiwgaW4gVm9sdW1lIHBlciBtaW51dGUuXG5zZC5saW5jb2xuIDwtIHNkKGxpbmNvbG4udHJhZmZpYylcbnNkLmhvbGxhbmQgPC0gc2QoaG9sbGFuZC50cmFmZmljKVxuXG4jIG1lYW5zIG9mIHR3byBzYW1wbGVzXG5tZWFuLmxpbmNvbG4gPC0gbWVhbihsaW5jb2xuLnRyYWZmaWMpXG5tZWFuLmhvbGxhbmQgPC0gbWVhbihob2xsYW5kLnRyYWZmaWMpXG5cbiMgbGVuZ3RoIG9mIGxpbmNvbG4gYW5kIGhvbGxhbmRcbmxlbl9saW5jb2xuIDwtIGxlbmd0aChsaW5jb2xuLnRyYWZmaWMpXG5sZW5faG9sbGFuZCA8LSBsZW5ndGgoaG9sbGFuZC50cmFmZmljKVxuXG4jIHN0YW5kYXJkIGRldmlhdGlvbiBvZiB0cmFmZmljXG5zZC5saW4uaG9sIDwtIHNxcnQoc2QubGluY29sbl4yL2xlbl9saW5jb2xuICsgc2QuaG9sbGFuZF4yL2xlbl9ob2xsYW5kKVxuXG4jIHogc2NvcmVcbnpldGEgPC0gKG1lYW4ubGluY29sbiAtIG1lYW4uaG9sbGFuZCkvc2QubGluLmhvbFxuemV0YVxuXG4jIGdldCBwXG5wID0gcG5vcm0oLXpldGEpXG5wXG5cbiMgcGxvdCB0aGUgemV0YSB2YWx1ZSBvbiB0aGUgbm9ybWFsIGRpc3RyaWJ1dGlvbiBjdXJ2ZS5cbnBsb3QoeD1zZXEoZnJvbSA9IC0yNSwgdG89IDI1LCBieT0wLjEpLHk9ZG5vcm0oc2VxKGZyb20gPSAtMjUsIHRvPSAyNSwgIGJ5PTAuMSksbWVhbj0wKSx0eXBlPSdsJyx4bGFiID0gJ21lYW4gZGlmZmVyZW5jZScsICB5bGFiPSdwb3NzaWJpbGl0eScpXG5hYmxpbmUodj16ZXRhLCBjb2w9J3JlZCcpIn0= We can see that form the P-Value obtained is near to 0, which is less than 0.05. Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that traffic in Lincoln is higher than traffic Holland. 3.1.3 Z-test Example 2 (Left Sided) Now lets look at another example to use this z-test for hypothesis testing. We will study the example to statistically find the relation between capital gains of people with two Zodiac Signs , namely Aquarius and Libra. Table 3.2: Snippet of Zodiac Dataset AGE STATUS EDUCATION YEARS PROFESSION CAPITALGAINS CAPITALLOSS NATIVE ZODIAK 16576 40 Self-emp-not-inc HS-grad 9 Farming-fishing 0 0 United-States Taurus 16173 40 Private 1st-4th 2 Other-service 0 0 El-Salvador Leo 7590 30 Private Some-college 10 Craft-repair 0 2051 United-States Gemini 15774 39 State-gov Bachelors 13 Exec-managerial 0 0 United-States Taurus 6895 29 Private HS-grad 9 Exec-managerial 0 0 United-States Libra 19038 44 Self-emp-inc Some-college 10 Sales 5178 0 United-States Aquarius 4808 27 Private Assoc-voc 11 Sales 0 0 United-States Aquarius 3402 25 Private Assoc-voc 11 Prof-specialty 298228 0 United-States Gemini 579 21 Private HS-grad 9 Adm-clerical 223960 0 United-States Scorpio 26362 57 Federal-gov Bachelors 13 Prof-specialty 0 0 United-States Aquarius Now stating out Null Hypothesis and Alternate Hypothesis. Null Hypothesis H0: Capital Gains of people with Aquarius is same as people with Libra zodiac sign. Alternate Hypothesis H1: Capital Gains of people with Aquarius is lower than as people with Libra zodiac sign. Once we have stated our hypothesis, lets see the z-test in practice. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YXNldFxuWm9kaWFjRGF0YTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9ab2RpYWNDaGFsbGVuZ2UuY3N2JylcbnN1bW1hcnkoWm9kaWFjRGF0YSkgI2dpdmVzIHVzIHRoZSBkYXRhIHN0YXRpc3RpY3NcblxuI2RhdGEgY2xlYW4gYW5kIHN1YnNldFxuQXF1YXJpdXMuZGF0YSA8LSBzdWJzZXQoWm9kaWFjRGF0YSwgWm9kaWFjRGF0YSRaT0RJQUsgPT0gXCJBcXVhcml1c1wiKVxuTGlicmEuZGF0YSA8LSBzdWJzZXQoWm9kaWFjRGF0YSwgWm9kaWFjRGF0YSRaT0RJQUsgPT0gXCJMaWJyYVwiKVxuXG4jIFpvZGlhYyBBcXVhcml1c1xuQXF1YXJpdXMuWm9kaWFjIDwtIEFxdWFyaXVzLmRhdGEkQ0FQSVRBTEdBSU5TXG5cbiMgWm9kaWFjICBMaWJyYVxuTGlicmEuWm9kaWFjIDwtIExpYnJhLmRhdGEkQ0FQSVRBTEdBSU5TXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuc2QuQXF1YXJpdXMgPC0gc2QoQXF1YXJpdXMuWm9kaWFjKVxuc2QuTGlicmEgPC0gc2QoTGlicmEuWm9kaWFjKVxuXG4jIG1lYW5zIG9mIHR3byBzYW1wbGVzXG5tZWFuLkFxdWFyaXVzIDwtIG1lYW4oQXF1YXJpdXMuWm9kaWFjKVxubWVhbi5MaWJyYSA8LSBtZWFuKExpYnJhLlpvZGlhYylcblxuIyBsZW5ndGggb2YgQXF1YXJpdXMgYW5kIExpYnJhXG5sZW5fQXF1YXJpdXMgPC0gbGVuZ3RoKEFxdWFyaXVzLlpvZGlhYylcbmxlbl9MaWJyYSA8LSBsZW5ndGgoTGlicmEuWm9kaWFjKVxuXG4jIHN0YW5kYXJkIGRldmlhdGlvblxuc2QuYXF1LmxpYiA8LSBzcXJ0KHNkLkFxdWFyaXVzXjIvbGVuX0FxdWFyaXVzICsgc2QuTGlicmFeMi9sZW5fTGlicmEpXG5cbiMgeiBzY29yZVxuemV0YSA8LSAobWVhbi5BcXVhcml1cyAtIG1lYW4uTGlicmEpL3NkLmFxdS5saWJcbnpldGFcblxuIyBnZXQgcFxucCA9IHBub3JtKHpldGEpXG5wXG5cbiMgcGxvdCB0aGUgemV0YSB2YWx1ZSBvbiB0aGUgbm9ybWFsIGRpc3RyaWJ1dGlvbiBjdXJ2ZS5cbnBsb3QoeD1zZXEoZnJvbSA9IC0yNSwgdG89IDI1LCBieT0wLjEpLHk9ZG5vcm0oc2VxKGZyb20gPSAtMjUsIHRvPSAyNSwgIGJ5PTAuMSksbWVhbj0wKSx0eXBlPSdsJyx4bGFiID0gJ21lYW4gZGlmZmVyZW5jZScsICB5bGFiPSdwb3NzaWJpbGl0eScpXG5hYmxpbmUodj16ZXRhLCBjb2w9J3JlZCcpIn0= We can see that form the P-Value obtained is less than 0.05. Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that Capital Gains of people with Aquarius is lower than as people with Libra zodiac sign. 3.1.4 Z-test Example 3 (Two Tailed) We will study the example to statistically find the relation between capital gains of people with two Countries, namely US and Columbia. Now stating out Null Hypothesis and Alternate Hypothesis. Null Hypothesis H0: Capital Gains of people of United States is same as people of Colombia. Alternate Hypothesis H1: Capital Gains of people of United States is not equal to that of the people of Colombia. Once we have stated our hypothesis, lets see the z-test in practice. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YXNldFxuWm9kaWFjRGF0YTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9ab2RpYWNDaGFsbGVuZ2UuY3N2JylcbnN1bW1hcnkoWm9kaWFjRGF0YSkgI2dpdmVzIHVzIHRoZSBkYXRhIHN0YXRpc3RpY3NcblxuI2RhdGEgY2xlYW4gYW5kIHN1YnNldFxuVVMuZGF0YSA8LSBzdWJzZXQoWm9kaWFjRGF0YSwgWm9kaWFjRGF0YSROQVRJVkUgPT0gXCJVbml0ZWQtU3RhdGVzXCIpXG5Db2x1bWJpYS5kYXRhIDwtIHN1YnNldChab2RpYWNEYXRhLCBab2RpYWNEYXRhJE5BVElWRSA9PSBcIkNvbHVtYmlhXCIpXG5cbiMgQ291bnRyeSBVU1xuVVMuY291bnRyeSA8LSBVUy5kYXRhJENBUElUQUxHQUlOU1xuXG4jIENvdW50cnkgIENvbHVtYmlhXG5Db2x1bWJpYS5jb3VudHJ5IDwtIENvbHVtYmlhLmRhdGEkQ0FQSVRBTEdBSU5TXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuc2QuVVMgPC0gc2QoVVMuY291bnRyeSlcbnNkLkNvbHVtYmlhIDwtIHNkKENvbHVtYmlhLmNvdW50cnkpXG5cbiMgbWVhbnMgb2YgdHdvIHNhbXBsZXNcbm1lYW4uVVMgPC0gbWVhbihVUy5jb3VudHJ5KVxubWVhbi5Db2x1bWJpYSA8LSBtZWFuKENvbHVtYmlhLmNvdW50cnkpXG5cbiMgbGVuZ3RoIG9mIFVTIGFuZCBDb2x1bWJpYVxubGVuX1VTIDwtIGxlbmd0aChVUy5jb3VudHJ5KVxubGVuX0NvbHVtYmlhIDwtIGxlbmd0aChDb2x1bWJpYS5jb3VudHJ5KVxuXG4jIHN0YW5kYXJkIGRldmlhdGlvblxuc2QudXMuY29sIDwtIHNxcnQoc2QuVVNeMi9sZW5fVVMgKyBzZC5Db2x1bWJpYV4yL2xlbl9Db2x1bWJpYSlcblxuIyB6IHNjb3JlXG56ZXRhIDwtIChtZWFuLlVTIC0gbWVhbi5Db2x1bWJpYSkvc2QudXMuY29sXG56ZXRhXG5cbiMgZ2V0IHBcbnAgPSAyKnBub3JtKC16ZXRhKVxucFxuXG4jIHBsb3QgdGhlIHpldGEgdmFsdWUgb24gdGhlIG5vcm1hbCBkaXN0cmlidXRpb24gY3VydmUuXG5wbG90KHg9c2VxKGZyb20gPSAtMjUsIHRvPSAyNSwgYnk9MC4xKSx5PWRub3JtKHNlcShmcm9tID0gLTI1LCB0bz0gMjUsICBieT0wLjEpLG1lYW49MCksdHlwZT0nbCcseGxhYiA9ICdtZWFuIGRpZmZlcmVuY2UnLCAgeWxhYj0ncG9zc2liaWxpdHknKVxuYWJsaW5lKHY9emV0YSwgY29sPSdyZWQnKSJ9 We can see that form the P-Value obtained is less than 0.05. Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that Capital Gains of people of United States is not equal to that of the people of Colombia. 3.2 Permutation Test Permutation test allows us to observe randomness directly, with naked eye, without the lenses of statistical tests such as z-tests etc. We shuffle data randomly like a deck of cards. There may be many such shuffles - 10,000, 100,000 etc. The goal is always to see how often we can obtained the observed difference of means (since we are testing either one sided or two sided hypothesis), by purely random shuffles of our data. These permutations (shuffles) destroy all relationships which may pre-exist in our data. We are hoping to show that our observed difference of means can be obtained very rarely in completely random fashion. Then we “experimentally” show that our result is unlikely to randomly occur under null hypothesis. Then we can reject the null hypothesis. The less often our result appear in the histogram of permutation test results, the better the news for our alternative hypothesis. What is surprising to many newcomers, is that permutation test will give different p-values (not dramatically different, but still different) in each run of permutation test. This is the case because permutation test in random itself. It is not like z-test which will give the same result when run again for the same hypothesis and same data set. Also p-value computed by permutation test will be, in general, different than p-value computed by z-test. Not very different but different. Again, it is the case because permutation test provides only approximation of p-value. Great advantage of permutation test is that it is universal and robust. One can test different relationships between two variables than just difference of means. For example we can use permutation test to validate whether traffic in Lincoln tunnel is more than twice the traffic in Holland tunnel or even provide different weights for different days of the week. 3.2.1 Permutation Test One Step Permutation test in one step is the most direct way to see randomness close by. One step permutation function shows one single data shuffle. By shuffling the data one destroys associations which exist between values of the data frame. This make data frame random. You can execute the one step permutation multiple times. This will show how data frame varies and how does it affect the observed difference of means. Apply one step permutation function first, multiple times before you move to the proper Permutation test function. One of the parameters of the Permutation test function specifies the number of “shuffles” which will be preformed. This could be a very large number, 10,000 or even 100,000. The purpose of making so many random permutations is to test how often observed difference of means can arise in just random data. The more often this takes place, the more likely you observation is just random. To reject the null hypothesis you need to show that the observed difference of means will come very infrequently in permutation test. Less than 5% of the time, to be exact. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKSIsInNhbXBsZSI6InN1bW1hcnkodHJhZmZpYylcbkQ8LSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdIb2xsYW5kJywzXSkgLSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywzXSlcbm51bGxfdHVubmVsIDwtIHJlcChcIkhvbGxhbmRcIiwyODAxKSAjIENyZWF0ZSAyODAxIGNvcGllcyBvZiBIb2xsYW5kIFxubnVsbF90dW5uZWxbc2FtcGxlKDI4MDEsMTQwMCldIDwtIFwiTGluY29sblwiICMgUmVwbGFjZSBSQU5ET01MWSAxNDAwIGNvcGllcyB3aXRoIExpbmNvbG5cbm51bGwgPC0gZGF0YS5mcmFtZShudWxsX3R1bm5lbCx0cmFmZmljWywzXSlcbm5hbWVzKG51bGwpIDwtIGMoXCJUVU5ORUxcIixcIlZPTFVNRV9QRVJfTUlOVVRFXCIpXG5zdW1tYXJ5KG51bGwpXG5ob2xsYW5kX251bGwgPC0gbnVsbFtudWxsJFRVTk5FTCA9PSBcIkhvbGxhbmRcIiwyXVxubGluY29sbl9udWxsIDwtIG51bGxbbnVsbCRUVU5ORUwgPT0gXCJMaW5jb2xuXCIsMl1cbm1lYW4oaG9sbGFuZF9udWxsKVxubWVhbihsaW5jb2xuX251bGwpXG5EX251bGwgPC0gbWVhbihsaW5jb2xuX251bGwpIC0gbWVhbihob2xsYW5kX251bGwpXG5jYXQoXCJUaGUgbWVhbiBkaWZmZXJlbmNlIG9mIHBlcm11dGF0aW9uIG9uZSBzdGVwIGRhdGE6IFwiLCBEX251bGwsXCJcXG5cIikjIENhbGN1bGF0ZSB0aGUgZGlmZmVyZW5jZSBiZXR3ZWVuIHRoZSBtZWFuIG9mIHRoZSByYW5kb20gZGF0YS5cbmNhdChcIlRoZSBtZWFuIGRpZmZlcmVuY2Ugb2Ygb3JpZ2luYWwgZGF0YTogXCIsIEQpICMgRGlmZmVyZW5jZSBvZiBtZWFuIHZhbHVlIG9mIG9yaWdpbmFsIGRhdGEuIn0= 3.2.2 Permutation Function The permutation function is used to run multiple iteration of the one-step permutation studied above, to get a complete relational understanding between the components involved in any hypothesis. Here you can run the example of running the permutation test on the Traffic.csv dataset, on volume of traffic in Holland and Lincoln Tunnel. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKVxuUGVybXV0YXRpb24gPC0gZnVuY3Rpb24oZGYxLGMxLGMyLG4sdzEsdzIpe1xuICBkZiA8LSBhcy5kYXRhLmZyYW1lKGRmMSlcbiAgRF9udWxsPC1jKClcbiAgVjE8LWRmWyxjMV1cbiAgVjI8LWRmWyxjMl1cbiAgc3ViLnZhbHVlMSA8LSBkZltkZlssIGMxXSA9PSB3MSwgYzJdXG4gIHN1Yi52YWx1ZTIgPC0gZGZbZGZbLCBjMV0gPT0gdzIsIGMyXVxuICBEIDwtICBhYnMobWVhbihzdWIudmFsdWUyLCBuYS5ybT1UUlVFKSAtIG1lYW4oc3ViLnZhbHVlMSwgbmEucm09VFJVRSkpXG4gIG09bGVuZ3RoKFYxKVxuICBsPWxlbmd0aChWMVtWMT09dzJdKVxuICBmb3IoamogaW4gMTpuKXtcbiAgICBudWxsIDwtIHJlcCh3MSxsZW5ndGgoVjEpKVxuICAgIG51bGxbc2FtcGxlKG0sbCldIDwtIHcyXG4gICAgbmYgPC0gZGF0YS5mcmFtZShLZXk9bnVsbCwgVmFsdWU9VjIpXG4gICAgbmFtZXMobmYpIDwtIGMoXCJLZXlcIixcIlZhbHVlXCIpXG4gICAgdzFfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzEsMl1cbiAgICB3Ml9udWxsIDwtIG5mW25mJEtleSA9PSB3MiwyXVxuICAgIERfbnVsbCA8LSBjKERfbnVsbCxtZWFuKHcyX251bGwsIG5hLnJtPVRSVUUpIC0gbWVhbih3MV9udWxsLCBuYS5ybT1UUlVFKSlcbiAgfVxuICBteWhpc3Q8LWhpc3QoRF9udWxsLCBwcm9iPVRSVUUpXG4gIG11bHRpcGxpZXIgPC0gbXloaXN0JGNvdW50cyAvIG15aGlzdCRkZW5zaXR5XG4gIG15ZGVuc2l0eSA8LSBkZW5zaXR5KERfbnVsbCwgYWRqdXN0PTIpXG4gIG15ZGVuc2l0eSR5IDwtIG15ZGVuc2l0eSR5ICogbXVsdGlwbGllclsxXVxuICBwbG90KG15aGlzdClcbiAgbGluZXMobXlkZW5zaXR5LCBjb2w9J2JsdWUnKVxuICBhYmxpbmUodj1ELCBjb2w9J3JlZCcpXG4gIE08LW1lYW4oRF9udWxsPkQpXG4gIHJldHVybihNKVxufSIsInNhbXBsZSI6IlBlcm11dGF0aW9uKHRyYWZmaWMsIFwiVFVOTkVMXCIsIFwiVk9MVU1FX1BFUl9NSU5VVEVcIiwxMDAwLFwiSG9sbGFuZFwiLCBcIkxpbmNvbG5cIikifQ== Note: You can find the permutation function code here: Permutation() NOTE: The red line in the output plots of the permutation test function is not the p-value, but it is just the difference of the value of means of the two categories under test. 3.2.3 Exercise - How p-value is affected by difference of means and standard deviations Here, you can generate your own data by changing parameters of the rnorm() function. See how changing the mean and sd in rnorm distributions affects the p-value! Again you can do it directly in the code and observe the results immediately. It is very revealing.. Think of Val1, and Val2 as traffic volumes in Holland and Lincoln tunnels respectively. The larger the difference between the means of rnorm() function the smaller the p-value - since it is less and less likely that observed difference of means would come frequently, due to random shuffles of permutation function. Now keep the same means and change the variances. See how changing the variances in rnorm() will affect the p-value and try to explain the effect that standard deviations have on the p-value. In general, the higher the standard deviation, the more widely data is centered around the mean. Thus even for the same two means, and two different value of deviations, we can see larger value of deviation to lead to higher p-value. Since we are less certain of the role of the “mean” if standard deviation is higher. Therefore, the chance of randomly obtaining the observed result, is higher. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJOLmggPC0gMTAgI051bWJlciBvZiB0dXBsZXMgZm9yIEhvbGxhbmQgVHVubmVsXG5OLmwgPC0gMTAgI051bWJlciBvZiB0dXBsZXMgZm9yIExpbmNvbG4gVHVubmVsXG5cbkNhdDE8LXJlcChcIkdyb3VwQVwiLE4uaCkgICMgZm9yIGV4YW1wbGUgR3JvdXBBIGNhbiBiZSBIb2xsYW5kIFR1bm5lbFxuQ2F0MjwtcmVwKFwiR3JvdXBCXCIsTi5sKSAgIyBmb3IgZXhhbXBsZSBHcm91cCBCIHdpbGwgYmUgTGluY29sbiBUdW5uZWxcblxuQ2F0MVxuQ2F0MlxuXG4jVGhlIHJlcCBjb21tYW5kIHdpbGwgcmVwZWF0LCB0aGUgdmFyaWFibGVzIHdpbGwgYmUgb2YgdHlwZSBjaGFyYWN0ZXIgYW5kIHdpbGwgY29udGFpbiAxMCB2YWx1ZXMgZWFjaC5cblxuQ2F0PC1jKENhdDEsQ2F0MikgIyBBIHZhcmlhYmxlIHdpdGggZmlyc3QgMTAgdmFsdWVzIEdyb3VwQSBhbmQgbmV4dCAxMCB2YWx1ZXMgR3JvdXBCXG5DYXRcblxuI1RyeSBjaGFuZ2luZyBtZWFuIGFuZCBzZCB2YWx1ZXMuIFdoZW4geW91IHJ1biB0aGlzIHlvdSB3aWxsIHNlZSB0aGF0IHRoZSBkaWZmZXJlbmNlIGlzIHNvbWV0aW1lcyBuZWdhdGl2ZSAjb3Igc29tZXRpbWVzIHBvc2l0aXZlLlxuXG5WYWwxPC1ybm9ybShOLmgsbWVhbj0yNSwgc2Q9MTApICNzYXksIHRyYWZmaWMgdm9sdW1lIGluIEhvbGxhbmQgVCBhcyBub3JtYWwgZGlzdHJpYnV0aW9uIHdpdGggbWVhbiBhbmQgc2RcblZhbDI8LXJub3JtKE4ubCxtZWFuPTMwLCBzZD0xMCkgI3NheSwgdHJhZmZpYyB2b2x1bWUgaW4gTGluY29sbiBUIGFzIG5vcm1hbCBkaXN0cmlidXRpb24gd2l0aCBtZWFuIGFuZCBzZFxuXG5WYWw8LWMoVmFsMSxWYWwyKSAjQSB2YXJpYWJsZSB3aXRoIDIwIHJvd3MsIHdpdGggZmlyc3QgMTAgcm93cyBjb250YWluaW5nIDEwIHJhbmRvbSBub3JtYWwgdmFsdWVzIG9mIFZhbDEgI2FuZCB0aGUgbmV4dCAxMCB2YWx1ZXMgb2YgVmFsMlxuXG5WYWxcblxuZDwtZGF0YS5mcmFtZShDYXQsVmFsKVxuXG5PYnNlcnZlZF9EaWZmZXJlbmNlPC1tZWFuKGRbZCRDYXQ9PSdHcm91cEEnLDJdKS1tZWFuKGRbZCRDYXQ9PSdHcm91cEInLDJdKVxuXG4jVGhpcyB3aWxsIGNhbGN1bGF0ZSB0aGUgbWVhbiBvZiB0aGUgc2Vjb25kIGNvbHVtbiAoaGF2aW5nIDEwIHJhbmRvbSB2YWx1ZXMgZm9yIGVhY2ggZ3JvdXApLCBhbmQgdGhlIG1lYW4gb2YgZ3JvdXBCIHZhbHVlcyBpcyBzdWJ0cmFjdGVkIGZyb20gdGhlIG1lYW4gb2YgZ3JvdXBBIHZhbHVlcywgd2hpY2ggd2lsbCBnaXZlIHlvdSB0aGUgdmFsdWUgb2YgdGhlIGRpZmZlcmVuY2Ugb2YgdGhlIG1lYW4uXG5PYnNlcnZlZF9EaWZmZXJlbmNlXG5cblxuUGVybXV0YXRpb24oZCwgXCJDYXRcIiwgXCJWYWxcIiwxMDAwMCwgXCJHcm91cEFcIiwgXCJHcm91cEJcIilcblxuI1RoZSBQZXJtdXRhdGlvbiBmdW5jdGlvbiByZXR1cm5zIHRoZSBhYnNvbHV0ZSB2YWx1ZSBvZiB0aGUgZGlmZmVyZW5jZS4gU28gdGhlIHJlZCBsaW5lIGlzIHRoZSBhYnNvbHV0ZSB2YWx1ZSBvZiB0aGUgb2JzZXJ2ZWQgZGlmZmVyZW5jZS4gWW91IHdpbGwgc2VlIGEgaGlzdG9ncmFtIGhhdmluZyBhIG5vcm1hbCBkaXN0cmlidXRpb24gd2l0aCBhIHJlZCBzaG93aW5nIHRoZSBvYnNlcnZlZCBkaWZmZXJlbmNlLiJ9 3.3 Multiple Hypothesis - Bonferroni Correction. While dealing with the dataset with several number of dimensions, it is possible to get a lot of amazing and interesting insights and conclusions from it. But, unfortunately, sometimes a lot of the data included in case of such large dataset, might be junk. We can make multiple assumptions from such data. But, while doing so, we may consider some useless data/patterns that might hamper our results and lead to the pitfall of believing in hypotheses, that are not actually true. This is common when performing multiple hypothesis testing. Multiple hypothesis testing refers to any instance that involves the simultaneous testing of more than one hypothesis. Let’s consider the example of Traffic dataset. We have given two tunnels ”Holland” and “Lincoln”, but what if we were given all the tunnels in the US? We can make a lot of hypotheses in that case. And for each set of hypothesis, would you still consider the value of α as 0.05 as the cut-off for P-value? It may seem to be a good idea to just go and check the p-value for any set of hypotheses with the cut-off value of \\(\\alpha\\) as 0.05. But this might not give you the correct answer always. If you have 100 different hypotheses to consider in the data, then the probability of getting at least one significant result with \\(\\alpha = 0.05\\) will be, \\[P(\\text{at least one significant result}) = 1- (1-0.05)^{100} ≈ 0.99\\] This means that if we consider 0.05 as our cut-off value, then the probability of getting at least one significant result will be about 99%, which leads to overfitting of data and it clearly doesn’t give us proper idea about our hypothesis. Methods for dealing with multiple testing frequently call for adjusting \\(\\alpha\\) in some way, so that the probability of observing at least one significant result due to chance remains below your desired significance level. One such method for adjusting \\(\\alpha\\) is BONFERRONI CORRECTION! The Bonferroni correction sets the significance cut-off at \\(\\alpha / N\\) where N is the number of possible hypotheses. For example, in the example above, with 100 tests and \\(\\alpha = 0.05\\), you’d only reject a null hypothesis if the p-value is less than \\(\\alpha/N = 0.05/100 = 0.0005\\) Thus, the value of \\(\\alpha\\) after Bonferroni correction would be \\(0.0005\\). Again, let’s calculate the probability of observing at least one significant result when using the correction just described: \\[P(\\text{at least one significant result}) = 1 − P(\\text{no significant results}) \\\\ = 1 − (1 − 0.0005)^{100} ≈ 0.048\\] This gives us 4.8% probability of getting at least one significant result. As we can see this value of probability using Bonferroni correction is much better than the 99% which we saw before when we did not use correction for performing multiple hypothesis testing. But there are some downfall of using Bonferroni correction too. (Although for the scope of this course Bonferroni Correction works fine.) The Bonferroni correction tends to be a bit too conservative. Also, we benefit here from assuming that all tests are independent of each other. In practical applications, that is often not the case. Depending on the correlation structure of the tests, the Bonferroni correction could beextremely conservative, leading to a high rate of false negatives. 3.3.1 Examples for Multiple hypothesis testing. Let’s consider the Happiness dataset as an example. Table 3.3: Snippet of Happiness Dataset IDN AGE COUNTRY GENDER IMMIGRANT INCOME HAPPINESS 3801 87213 53 Slovakia Male 0 103052 8.59 2399 57569 70 Afghanistan Male 0 71590 5.74 4024 32397 30 Dominican Republic Female 0 47249 3.32 1195 73282 38 Uruguay Female 0 89630 7.43 2737 87824 58 Uganda Male 0 102176 8.64 2877 32645 29 Slovakia Male 0 48506 4.32 3655 68460 23 Gabon Female 0 82447 6.27 4675 78143 70 Malta Male 1 94744 6.24 574 76917 60 Iraq Male 0 91681 7.65 4105 54246 48 Sierra Leone Male 0 68541 5.52 There are 156 unique countries in the dataset. This can be checked using the unique() function – unique(indiv_happiness$country) Since there are 156 distinct countries, we have \\({{n}\\choose{2}} = {156\\choose2}=(156 * 155)/2 = 12090\\) different hypotheses. Let’s call this value N. Using this N, the P-value cutoff after Bonferroni correction will be, \\(α = 0.05 / 12090 ≈ 4.13 *10^{-6}\\) 3.3.1.1 Example 1 Let’s calculate the P-value for the following hypotheses from the dataset. Our hypothesis: People from Canada are happier than people from Iceland. Null hypothesis: There is no difference in happiness levels of people from Canada and people from Iceland. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgZGF0YXNldFxuaGFwcGluZXNzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L0hBUFBJTkVTUzIwMTcuY3N2XCIsIHN0cmluZ3NBc0ZhY3RvcnMgPSBUKSAjd2ViIGxvYWRcblxuIyBUd28gc3Vic2V0cyBvZiBDYW5hZGEgYW5kIEljZWxhbmQgXG5oYXBwaW5lc3MuY2FuYWRhIDwtIHN1YnNldChoYXBwaW5lc3MkSEFQUElORVNTLCBoYXBwaW5lc3MkQ09VTlRSWSA9PVwiQ2FuYWRhXCIpXG5oYXBwaW5lc3MuaWNlbGFuZCA8LSBzdWJzZXQoaGFwcGluZXNzJEhBUFBJTkVTUywgaGFwcGluZXNzJENPVU5UUlkgPT0gXCJJY2VsYW5kXCIpXG5cbiMgTWVhbiBvZiBzdWJzZXRzLlxubWVhbi5jYW5hZGEgPC0gbWVhbihoYXBwaW5lc3MuY2FuYWRhKVxubWVhbi5pY2VsYW5kIDwtIG1lYW4oaGFwcGluZXNzLmljZWxhbmQpXG5cbm1lYW4uY2FuYWRhXG5tZWFuLmljZWxhbmRcblxuIyBMZW5ndGggb2Ygc3Vic2V0c1xubGVuLmNhbmFkYSA8LSBsZW5ndGgoaGFwcGluZXNzLmNhbmFkYSlcbmxlbi5pY2VsYW5kIDwtIGxlbmd0aChoYXBwaW5lc3MuaWNlbGFuZClcblxuIyBTdGFuZGFyZCBEZXZpYXRpb24gb2YgU3Vic2V0cy5cbnNkLmNhbmFkYSA8LSBzZChoYXBwaW5lc3MuY2FuYWRhKVxuc2QuaWNlbGFuZCA8LSBzZChoYXBwaW5lc3MuaWNlbGFuZClcblxuIyBDYWxjdWxhdGluZyBaLXNjb3JlIFxuemV0YSA8LSAobWVhbi5jYW5hZGEgLSBtZWFuLmljZWxhbmQpLyBzcXJ0KChzZC5jYW5hZGFeMikvbGVuLmNhbmFkYSArIChzZC5pY2VsYW5kXjIpL2xlbi5pY2VsYW5kKVxuemV0YVxuXG4jIENhbGN1bGF0ZSBwLXZhbHVlIGZyb20gWi1zY29yZVxucF92YWx1ZSA8LSBwbm9ybSgtemV0YSlcbnBfdmFsdWUifQ== In this case, after applying Bonferroni Correction we get the value of \\(α = 0.05/12090 ≈ 4.14 * 10^{-06}\\) Here, we get the p-value of 0.25 which is much higher than the value of our α. Based on this we fail reject our null hypothesis. 3.3.1.2 Example 2 Let’s consider the following hypotheses from the dataset. Our hypothesis: People from Italy are happier than people from Afghanistan. Null hypothesis: There is no difference in happiness levels of people from Italy and people from Afghanistan. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgZGF0YXNldFxuaGFwcGluZXNzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L0hBUFBJTkVTUzIwMTcuY3N2XCIsIHN0cmluZ3NBc0ZhY3RvcnMgPSBUKSAjd2ViIGxvYWRcblxuIyBUd28gc3Vic2V0cyBvZiBJdGFseSBhbmQgQWZnaGFuaXN0YW4gXG5oYXBwaW5lc3MuaXRhbHkgPC0gc3Vic2V0KGhhcHBpbmVzcyRIQVBQSU5FU1MsIGhhcHBpbmVzcyRDT1VOVFJZID09XCJJdGFseVwiKVxuaGFwcGluZXNzLmFmZ2hhbmlzdGFuIDwtIHN1YnNldChoYXBwaW5lc3MkSEFQUElORVNTLCBoYXBwaW5lc3MkQ09VTlRSWSA9PSBcIkFmZ2hhbmlzdGFuXCIpXG5cbiMgTWVhbiBvZiBzdWJzZXRzLlxubWVhbi5pdGFseSA8LSBtZWFuKGhhcHBpbmVzcy5pdGFseSlcbm1lYW4uYWZnaGFuaXN0YW4gPC0gbWVhbihoYXBwaW5lc3MuYWZnaGFuaXN0YW4pXG5cbm1lYW4uaXRhbHlcbm1lYW4uYWZnaGFuaXN0YW5cblxuIyBMZW5ndGggb2Ygc3Vic2V0c1xubGVuLml0YWx5IDwtIGxlbmd0aChoYXBwaW5lc3MuaXRhbHkpXG5sZW4uYWZnaGFuaXN0YW4gPC0gbGVuZ3RoKGhhcHBpbmVzcy5hZmdoYW5pc3RhbilcblxuIyBTdGFuZGFyZCBEZXZpYXRpb24gb2YgU3Vic2V0cy5cbnNkLml0YWx5IDwtIHNkKGhhcHBpbmVzcy5pdGFseSlcbnNkLmFmZ2hhbmlzdGFuIDwtIHNkKGhhcHBpbmVzcy5hZmdoYW5pc3RhbilcblxuIyBDYWxjdWxhdGluZyBaLXNjb3JlIFxuemV0YSA8LSAobWVhbi5pdGFseSAtIG1lYW4uYWZnaGFuaXN0YW4pLyBzcXJ0KChzZC5pdGFseV4yKS9sZW4uaXRhbHkgKyAoc2QuYWZnaGFuaXN0YW5eMikvbGVuLmFmZ2hhbmlzdGFuKVxuemV0YVxuXG4jIENhbGN1bGF0ZSBwLXZhbHVlIGZyb20gWi1zY29yZVxucF92YWx1ZSA8LSBwbm9ybSgtemV0YSlcbnBfdmFsdWUifQ== In this case, after applying Bonferroni Correction we get the value of \\(α = 0.05/12090 ≈ 4.14 * 10^{-06}\\) Here, we get the p-value of 0.00364 which is lower than the value of default p-value cutoff \\(α = 0.05\\), but this obtained p-value is higher than our Bonferroni correction cutoff. So, based on the results, we fail to reject our null hypothesis even though the obtained p-value is less than 0.05. EOC "],["classification.html", "Chapter 4 Data Modeling and Prediction techniques for Classification. 4.1 Decision Tree. 4.2 Use of Rpart 4.3 Visualize the Decision tree 4.4 Rpart Control 4.5 Prediction using rpart. 4.6 Split the data yourself. 4.7 Cross Validation", " Chapter 4 Data Modeling and Prediction techniques for Classification. 4.1 Decision Tree. Decision tree is one of the most powerful and popular tool for classification and prediction. It is a supervised learning predictive model that uses a set of binary rules to calculate a target value. It is a flow chart like tree structure, where each internal node has a test on a particular attribute, each branch denotes the outcome of the test, and each leaf node holds a class label/ numeric value. The reason decision tree are very popular are: - It is able to generate rules easier to understand as compared to other models.. - It require much less computations for performing modeling and prediction. - Both continuous/numerical and categorical variables are handled easily while creating the decision trees. There are a few drawbacks too while using decision trees in certain case of inputs and tasks. But for the scope of discussion of this course we won’t go into much details of it. Also, we won’t go into the depth of the internals of the formation/creation of the decision tree and its underlying algorithm, but we will look at decision tree as a way to create prediction model for both classification and regression. 4.2 Use of Rpart Recursive Partitioning and Regression Tree RPART library is a collection of routines which implement Classification and Regression Tree (CART) which is a type of Decision Tree.The resulting model can be represented as a binary tree. The library associated with this RPART is called rpart. Install this library using install.packages(\"rpart\"). Syntax for building the decision tree using rpart(): rpart( formula , method, data, control,...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... method: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use “anova” for regression, “class” for classification, etc. data: here we provide the dataset on which we want to fit the decision tree on. control: here we provide the control parametes for the decision tree. Explained more in detail in section further in this chapter. For more info on the rpart function visit rpart documentation Lets look at an example on the Moody 2019 dataset. We will use the rpart() function with the following inputs: prediction -&gt; GRADE predictors -&gt; SCORE, ON_SMARTPHONE, ASKS_QUESTIONS, LEAVES_EARLY, LATE_IN_CLASS data -&gt; moody dataset method -&gt; “class” for classification. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG5ycGFydChHUkFERSB+IFNDT1JFK09OX1NNQVJUUEhPTkUrQVNLU19RVUVTVElPTlMrTEVBVkVTX0VBUkxZK0xBVEVfSU5fQ0xBU1MsIGRhdGEgPSBtb29keVssLWMoMSldLG1ldGhvZCA9IFwiY2xhc3NcIikifQ== We can see that the output of the rpart() function is the decision tree with details of, node -&gt; node number split -&gt; split conditions/tests n -&gt; number of records in either branch i.e. subset yval -&gt; output value i.e. the target predicted value. yprob -&gt; probability of obtaining a particular category as the predicted output. Using output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section 4.5 But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section. 4.3 Visualize the Decision tree To visualize and understand the rpart() tree output in the easiest way possible, we use a library called rpart.plot. The function rpart.plot() of the rpart.plot library is the function used to visualize decision trees. The rpart.plot library is a front-end wrapper to the library prp which is the most basic library for plotting decision trees. prp allows various aesthetic modifications for visualizing the decision tree. We will look at a few examples of using prp below. But, first lets look at a example to visualize the output decision tree in the previous example on Moody dataset using rpart.plot() NOTE: The online runnable code block does not support rpart.plot and prp library and functions, thus the output of the following code examples are provided directly. # First lets import the rpart library library(rpart) # Import dataset moody &lt;- read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv&quot;) # Use of the rpart() function. tree &lt;- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody,method = &quot;class&quot;) # Now lets import the rpart.plot library to use the rpart.plot() function. library(rpart.plot) # Use of the rpart.plot() function to visualize the decision tree. rpart.plot(tree) Output Plot of rpart.plot() function We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about the grade category. the outcome probability of each grade category. the records percentage out of total records. To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides rpart.plot and Plotting with rpart.plot (PDF) Note that for any beginner using rpart.plot() function is the easiest way. But if you want to learn another way of plotting rpart trees then the following function can be used. So,another form of plotting rpart trees in a very minimalistic way is using the plot rpart i.e. prp() function, which is actually the working function behind rpart.plot(). Lets look at a same example like above but using prp(). # First lets import the rpart library library(rpart) # Import dataset moody &lt;- read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv&quot;) # Use of the rpart() function. tree &lt;- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody[,-c(1)],method = &quot;class&quot;) # Now lets import the rpart.plot library to use the rpart.plot() function. library(rpart.plot) # Use of the prp function to visualize the decision tree. prp(tree) Output Plot of prp() function We can see that the output of the prp() function is a very minimalist tree, without any colors with minimum required information. There are other arguments that can be passed to the prp() function to increase the aesthetic look and the information provided. To learn those extra arguments visit this guide prp() NOTE: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment. 4.4 Rpart Control We will now look at the control argument used in rpart() function, which is one of the important argument. The control argument of rpart() function is used to manually decide the control parameters of the decision tree. The advantages of using control method: - It restricts the height of the decision tree. - It avoids overfitting on the training dataset. - It can be used to eliminate attributes that affect less significantly on the splitting constraints. - Helps to terminate the creation process of tree earlier, thus reducing required computational time. The disadvantages of using control method: - It creates risk of generating trees with lesser accuracy compared to uncontrolled tree. - It could hamper the splitting condition selection in negative way. - Could result in underfitting, if control parameters not chosen carefully. As we can see, that controlling the decision tree provides us with lot of advantage in certain condition, we also risk in reducing the accuracy of the prediction from using the tree. Thus these control methods must be applied only in certain case, where the uncontrolled method takes large amount of time to create the tree, and overfits the train data. When the datasets have more significantly higher count of columns but less records of data, using control methods could be suitable. Now lets look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function. rpart.control( *minsplit*, *minbucket*, *cp*,...) minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -&gt; the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition. minbucket: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -&gt; the minimum number of observation in the terminal/leaf node of the trees must be 500 or above. cp: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by cp, will not be made in the tree. For more information of the other arguments of the rpart.control() function visit rpart.control Note: The ratio of minsplt to minbucket is 3:1. Thus if only one of the minsplit/minbucket is provided the other value is set using the above ratio. Also if both values are provided, unless the values are not in the above ratio, the rpart.control() the resorts to the default value. Also note, the default value of cp is 0.01. Let look at few examples. Suppose you want to set the control parameter minsplit=200. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIGxpYnJhcnkocnBhcnQpXG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbiB3aXRoIHRoZSBjb250cm9sIHBhcmFtZXRlciBtaW5zcGxpdD0yMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiAuLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKG1pbnNwbGl0ID0gMjAwKSlcblxuIyBDaGVjayB0aGUgY291bnQgb2Ygb2JzZXJ2YXRpb24gYXQgZWFjaCBzcGxpdCB0ZXN0LiBUbyBkbyB0aGlzIHdlIGZpbmQgdGhlIGNvdW50IGF0IGVhY2ggbm9uLWxlYWYvbm9uLXRlcm1pbmFsIG5vZGUuXG50cmVlJGZyYW1lW3RyZWUkZnJhbWUkdmFyIT1cIjxsZWFmPlwiLGMoXCJ2YXJcIixcIm5cIildXG5cbiMgbGlicmFyeShycGFydC5wbG90KVxuIyBycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=200 in rpart.control() function We can see from the output of tree$splits and the tree plot, that at each split the total amount of observations are above 200. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits. Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24gd2l0aCB0aGUgY29udHJvbCBwYXJhbWV0ZXIgbWluc3BsaXQ9MjAwXG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gLiwgZGF0YSA9IG1vb2R5WywtYygxKV0sbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5idWNrZXQgPSAxMDApKVxuXG4jIENoZWNrIHRoZSBjb3VudCBvZiBvYnNlcnZhdGlvbiBpbiBlYWNoIGxlYWYgbm9kZS5cbnRyZWUkZnJhbWVbdHJlZSRmcmFtZSR2YXI9PVwiPGxlYWY+XCIsYyhcInZhclwiLFwiblwiKV1cblxuIyBsaWJyYXJ5KHJwYXJ0LnBsb3QpXG4jIHJwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=100 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. Lets now use the cp parameter and see its effect on the tree. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24gd2l0aCB0aGUgY29udHJvbCBwYXJhbWV0ZXIgY3A9MC4wMDVcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiAuLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKGNwID0gMC4wMDUpKVxuXG4jIENoZWNrIHRoZSBhY2N1cmFjeSBpbmNyZWFzZSBmYWN0b3IgYXQgZWFjaCBzcGxpdC5cbnRyZWUkY3B0YWJsZVxuXG4jIGxpYnJhcnkocnBhcnQucGxvdCkpXG4jIHJwYXJ0LnBsb3QodHJlZSkifQ== Output tree plot of after setting cp=0.005 in rpart.control() function We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005. Now we saw the most important control parameters of the rpart.control function. Remember there are other parameters too, which you can study if you wish, but studying just these 3 discussed above are sufficient for the scope of this course. Also, note we have not check the effects of the control parameters on the prediction accuracy of the decision tree. Using the control parameters you could either increase the accuracy, but also risk, decreasing the accuracy. So choosing the controls parameter very carefully is very important to push the accuracy in the right direction. 4.5 Prediction using rpart. Now that we saw process to create a decision tree and also plotting it, we will like to use the output tree to predict the required attribute. From the moody example, we are trying to predict the grade of students. Lets look at the predict() function to predict the outcomes. predict(*object*,*data*,*type*,...) object: the generated tree from the rpart function. data: the data on which the prediction is to be performed. type: the type of prediction required. One of “vector”, “prob”, “class” or “matrix”. Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbi5cbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStPTl9TTUFSVFBIT05FK0FTS1NfUVVFU1RJT05TK0xFQVZFU19FQVJMWStMQVRFX0lOX0NMQVNTLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZCkifQ== We see that the output of the predict function is a vector of grades corresponding to each record of the Moody dataframe. Each index has a grade among “A”, “B”, “C”, “D”, “F”. Although we see the output, how do we compare the accuracy and correctness of the outputs. Lets look at one of the basic test we can do is perform a record by record comparison of the grade already in the dataset and the predicted grade, in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbi5cbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStPTl9TTUFSVFBIT05FK0FTS1NfUVVFU1RJT05TK0xFQVZFU19FQVJMWStMQVRFX0lOX0NMQVNTLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZClcblxuIyBMZXRzIGNoZWNrIHRoZSBjb3JyZWN0bmVzcyBvZiBlYWNoIHByZWRpY3Rpb24gZm9yIGVhY2ggcmVjb3JkXG5tZWFuKG1vb2R5JEdSQURFPT1wcmVkKSoxMDAifQ== Using just a row by row comparison we can see that the outcomes of the predicted grades and the original grades from the Moody dataset, are matching 93.73%. Thus our prediction accuracy is 93.73% and the error rate is 6.27%, which is very good. This prediction accuracy calculated on the training dataset is called training accuracy. Notice that we just compared the predicted grades with the already present grades from the Moody dataset, the same dataset on which the tree was built. In such scenario, one can say that you are predicting the grades on data already considered while training. So in some sense, you just output the known grades, and did not do any useful prediction. But to really see the use of our tree, we must predict on a data which has never been used in the training of the tree, OR, where the Prof. Moody has not assigned the grades. In the latter case it is difficult to prove the accuracy, without actually checking with Prof. Moody, if he/she would have assigned the same grade as the grade predicted by our model. But we have a solution for the former case, where we can predict grades on a subset of data, which we have not used while training. For that we would need to split the provided data into 2 parts, Training and Testing and then repeat the training process on the training dataset and the prediction on testing dataset. 4.6 Split the data yourself. We introduced the first and basic model for data analysis, Decision tree, in the section above. But we found that we need to perform a basic routine before dwelling deep into the creation of decision tree. The routine is to split the dataset in multiple parts, to check the accuracy of our tree’s prediction. This routine is the first step you perform after acquiring a cleaned dataset. The most useful splitting of dataset is done in 2 parts, Training and Testing. While splitting, the split ratio between training and testing should be decided properly. Mostly, training data is kept bigger,and testing is done on a relatively smaller subset. But the ratio should not be too biased, where there are only few observations in test data compared to training data. Usually, the math behind splitting is that, even after split, the smaller subset, i.e. the test subset should represent the distribution of the complete dataset. This means the test data should at-least have few record of each possible combination of attribute’s categories if categorical data, or, if numerical data then the numerical distribution is same as of the complete dataset. Thus, typically the train-test ratio is 80-20 or 70-30 or in some case even 60-40. Also, while selecting the records to assign to either training/testing data, they should be randomly picked from the original data, so as to avoid unbalanced distribution. We will look at a small example of splitting the complete dataset into training and testing dataset with a 70-30 ratio. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFNwbGl0IHJhbmRvbWx5IGludG8gMiBzZXRzIHdpdGggY2VydGFpbiByYXRpby9wcm9iYWJpbGl0eS5cbmlkeCA8LSBzYW1wbGUoIDE6Miwgc2l6ZSA9IG5yb3cobW9vZHkpLCByZXBsYWNlID0gVFJVRSwgcHJvYiA9IGMoLjcsIC4zKSlcbm1vb2R5LnRyYWluIDwtIG1vb2R5W2lkeCA9PSAxLF1cbm1vb2R5LnRlc3QgPC0gbW9vZHlbaWR4ID09IDIsXVxuXG5ucm93KG1vb2R5KVxubnJvdyhtb29keS50cmFpbilcbm5yb3cobW9vZHkudHJhaW4pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnRlc3QpXG5ucm93KG1vb2R5LnRlc3QpL25yb3cobW9vZHkpIn0= As we can see we split the original data with 1580 rows into two dataset, training data with almost 70% of rows of the original, and testing data with almost 30% of the original. Notice that we used a random sampling of the data, and not just sequential, to avoid any unbalanced distribution of attributes. Now, we looked at a method to split the dataset into training and testing data. But there is another type of splitting of the dataset which involves splitting the data into 3 parts namely, training, cross-validation and testing. We will look at the use of cross-validation and the process, in the next section 4.7. Typically, the ratio of train-validation-test is 60-20-20 or 50-25-25. Before that lets look at a simple method to perform a 3 way split with ratio 60-20-20. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFNwbGl0IHJhbmRvbWx5IGludG8gMyBzZXRzIHdpdGggY2VydGFpbiByYXRpby9wcm9iYWJpbGl0eS5cbmlkeCA8LSBzYW1wbGUoIDE6Mywgc2l6ZSA9IG5yb3cobW9vZHkpLCByZXBsYWNlID0gVFJVRSwgcHJvYiA9IGMoMC42LCAwLjIsIDAuMikpXG5tb29keS50cmFpbiA8LSBtb29keVtpZHggPT0gMSxdXG5tb29keS52YWxpZGF0aW9uIDwtIG1vb2R5W2lkeCA9PSAyLF1cbm1vb2R5LnRlc3QgPC0gbW9vZHlbaWR4ID09IDMsXVxuXG5ucm93KG1vb2R5KVxubnJvdyhtb29keS50cmFpbilcbm5yb3cobW9vZHkudHJhaW4pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnZhbGlkYXRpb24pXG5ucm93KG1vb2R5LnZhbGlkYXRpb24pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnRlc3QpXG5ucm93KG1vb2R5LnRlc3QpL25yb3cobW9vZHkpIn0= We can see that the dataset is split into 3 parts, with 60% in training data, 20% in validation data, and 20% in testing data. 4.7 Cross Validation Cross validation is a model validation technique for assessing generalization of the results of statistical analysis to an independent dataset. In other words, it is a technique to estimate the accuracy of a predictive model’s performance in practice. The goal of cross-validation is to test the model’s ability to predict new data that was not used in estimating/training it, in order to avoid problems like over-fitting and selection bias, and to give an insight on how the model will generalize to an independent dataset(i.e., an unknown dataset). Cross-validation also helps in selecting and fine-tuning the hyper-parameters of the models. In our case of decision tree, the hyper parameters could be the control parameters that determind the size of the decision tree, which in-turn determines the accuracy of the tree. One round of cross-validation involves partitioning data into complementary subsets, and then performing model training on one subset, and validating the results on the other subset. In most methods, multiple rounds of cross-validation are performed using different partitions in each round, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model’s predictive performance. Another use of cross-validation is when you don’t have the test data, and hence, you don’t have a way to determine the true accuracy of the model. Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. The accuracy on the validation data is called cross-validation accuracy, while that on the train data is called training accuracy. Lets not dive too deep into the theory of this cross validation technique, but lets learn about the cross_validate() function, that helps us achieve this. cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*) data: The dataset on which cross validation is to be performed. tree: The decision tree generated using rpart. n_iter: Number of iterations. split_ratio: The splitting ratio of the data into train data and validation data. method: Method of the prediction. “class” for classification. The way the function works is as follows: It randomly partitions your data into training and validation. It then constructs the following two decision trees on training partition: The tree that you pass to the function. The tree constructed on all attributes as predictors and with no control parameters. -It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees. The first column corresponds to the cross-validation accuracy on the tree that you pass; the second is the cross-validation accuracy on the tree without any control and all attributes. The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting. We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired. The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant. Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration. Lets look at the cross_validate() function in action in the example below. We will pass the tree with formula as GRADE ~ SCORE+ON_SMARTPHONE+LEAVES_EARLY, and control parameter, with minsplit=100. And for cross_validate() function, we will usen_iter=5, and split_raitio=0.7 NOTE: Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use install.packages(&quot;devtools&quot;) devtools::install_github(&quot;devanshagr/CrossValidation&quot;) CrossValidation::cross_validate() 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 NOTE: If you encounter error while running the cross-validation function that said “new levels encountered” in test, make sure the dataset is imported again with read.csv() attribute stringsAsFactors as TRUE or T. For more information about the inner-working of the cross_validate() function visit cross_validate() We can see in the output the Training accuracy, the table of cross-validation accuracy at each iteration for both the passed tree and the tree on all attribute and also their averages and variances. Few Observation from the selected example above are: For the tree passed with selected attributes and some control parameters, the cross-validation accuracy’s (i.e. accuracy values in the accuracy_subset column) are fairly high for all iterations and have very low variance. They are close to the training accuracy which indicates we are not overfitting. Observe that the accuracy at each iteration of the accuracy_subset and accuracy_all column are relatively, close but not exact, suggesting that there are more attributes or other control parameters that can be included to the passed tree, to further increase the accuracy, thus closing the gap. Thus using cross-validation we were able to figure out with certainty, that the passed tree, is not the best tree that can be created using the training data. Also, we saw whether the generated tree overfits the training data or not. EOC "],["regression.html", "Chapter 5 Data Modelling and Prediction techniques for Regression. 5.1 Linear Regression. 5.2 Regression using RPART", " Chapter 5 Data Modelling and Prediction techniques for Regression. In chapter 4 we studied modeling and prediction techniques for Classification, where we predicted the class of the output variable, using Decision tree based algorithms. In this chapter we will study techniques for regression based prediction and create models for it. As opposite the the classification where we predict a particular class of the output variable, here we will predict a numerical/continuous value. We will look at two methods of performing this regression based modeling and prediction, first simple linear regression and second regression using decision tree. 5.1 Linear Regression. Linear regression is a linear approach to modeling the relationship between a scalar response (\\(Y\\)) and one or more explanatory variables(\\(X_i\\), where i is the number of explanatory variables). Scalar response means the predicted output. These variables are also known as dependent variables i.e. they are derived by applying some law/rule or function onto some other variable/s Usually in linear regression, models are used to predict only one scalar variable. But there are two subtype if these models: First when there is only one explanatory variable and one output variable. This type of linear regression model known as simple linear regression. Second, when there are multiple predictors, i.e. explanatory/dependent variables for the output variable. This type of linear regression model known as multiple linear regression. But in the case of prediction of multiple correlated output variables, we call this type of prediction using linear regression model as multivariate linear regression. Explanatory variables are the predictors on which the output predictions are based on. These variables are also known as independent variables and are independently sufficient to be used as predictors in regression models. Linear models fitted to various different type of data spread. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. Credits: Wikipedia. Since the relationship between the explanatory variables and the output variable is modeled linearly, these models are called as linear models. To do this, we need to find a linear regression equation for the set of input predictors and the output variable. But without going into the mathematics of finding this linear regression equation, we will use a tool/function provided in R to model and predict the output variable. 5.1.1 Linear regression using lm() function Syntax for building the regression model using the lm() function is as follows: lm(formula, data, ...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the linear regression model is to be trained. For more info on the lm() function visit lm() Lets look at the example on the RealEstate dataset. A snippet of the Realestate Dataset is given below. Table 5.1: Snippet of Real Estate Dataset Price Bedrooms Bathrooms Size 795000 3 3 2371 399000 4 3 2818 545000 4 3 3032 909000 4 4 3540 109900 3 1 1249 324900 3 3 1800 192900 4 2 1603 215000 3 2 1450 999000 4 3 3360 319000 3 2 1323 Now we can build a simple linear regression model to predict the Price attribute based on the various other attributes present in the dataset, as shown above. Since we will be predicting only one attribute values, this model will be called simple linear regression model. For the first example we will predict the price value of house using only size attribute as the predictor. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxucmVhbGVzdGF0ZTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvUmVhbEVzdGF0ZS5jc3ZcIilcblxuIyBzcGxpdGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhyZWFsZXN0YXRlKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC43LCAuMykpXG50cmFpbiA8LSByZWFsZXN0YXRlW2lkeCA9PSAxLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVtpZHggPT0gMixdXG5cbiMgVXNlIHRoZSBsbSgpIGZ1bmN0aW9uIHRvIHByZWRpY3QgdGhlIHByaWNlIGJhc2VkIG9uIHNpemUgb2YgdGhlIGhvdXNlLlxuIyBUaHVzIHRoaXMgaXMgYW4gZXhhbXBsZSBvZiBzaW1wbGUgbGluZWFyIHJlZ3Jlc3Npb24gc2luY2Ugb25seSBvbmUgcHJlZGljdG9yIGFuZCBvbmUgb3V0cHV0IHZhbHVlIGlzIHVzZWQuXG5zaW1wbGUuZml0IDwtIGxtKFByaWNlflNpemUsZGF0YT10cmFpbilcblxuIyBzdW1tYXJ5IG9mIHRoZSBtb2RlbFxuc3VtbWFyeShzaW1wbGUuZml0KVxuXG4jIExpbmVhciByZWxhdGlvbiBiZXR3ZWVuIHRoZSBQcmljZSBhbmQgU2l6ZSBhdHRyaWJ1dGUuXG5wbG90KHRyYWluJFNpemUsdHJhaW4kUHJpY2UpXG5hYmxpbmUoc2ltcGxlLmZpdCAsIGNvbD1cInJlZFwiKVxuXG4jIFByZWRpY3RpbmcgdmFsdWVzIG9uIHRoZSB0ZXN0IGRhdGFzZXQuXG5QcmVkaWN0ZWRQcmljZS5zaW1wbGUgPC0gcHJlZGljdChzaW1wbGUuZml0LHRlc3QpXG4jIFByZWRpY3RlZCBWYWx1ZXNcbmhlYWQoYXMuaW50ZWdlcih1bm5hbWUoUHJlZGljdGVkUHJpY2Uuc2ltcGxlKSkpXG4jIEFjdHVhbCBWYWx1ZXNcbmhlYWQodGVzdCRQcmljZSkifQ== We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The plot of Size vs Price, and the red line represents the fitted line or the linear model line which will be used for prediction. The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the actual values, the predicted values are some times close,some time far and few are very far. We saw above an example of simple linear regression model, where only one predictor was used for predicting a single output attribute. Now we will see an example of multiple linear regression model, where there can be multiple predictors to predict a single output attribute. (Note: Please do not confuse this with the multivariate linear regression.) Let look at an example of predicting the Price of the real estate, based on 3 attributes Size, Number of Bedrooms and Number of Bathrooms. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxucmVhbGVzdGF0ZTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvUmVhbEVzdGF0ZS5jc3ZcIilcblxuIyBzcGxpdGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhyZWFsZXN0YXRlKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC43LCAuMykpXG50cmFpbiA8LSByZWFsZXN0YXRlW2lkeCA9PSAxLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVtpZHggPT0gMixdXG5cblxuIyBVc2UgdGhlIGxtKCkgZnVuY3Rpb24gdG8gcHJlZGljdCB0aGUgcHJpY2UgYmFzZWQgb24gc2l6ZSwgYmF0aHJvb21zIGFuZCBiZWRyb29tcyBvZiB0aGUgaG91c2UuXG4jIFRodXMgdGhpcyBpcyBhbiBleGFtcGxlIG9mIG11bHRpcGxlIGxpbmVhciByZWdyZXNzaW9uIHNpbmNlIG11bHRpcGxlIHByZWRpY3RvciBhbmQgb25lIG91dHB1dCB2YWx1ZSBpcyB1c2VkLlxubXVsdGlwbGUuZml0IDwtIGxtKFByaWNlflNpemUgKyBCYXRocm9vbXMgKyBCZWRyb29tcyxkYXRhPXRyYWluKVxuXG4jIHN1bW1hcnkgb2YgdGhlIG1vZGVsXG5zdW1tYXJ5KG11bHRpcGxlLmZpdClcblxuIyBQcmVkaWN0aW5nIHZhbHVlcyBvbiB0aGUgdGVzdCBkYXRhc2V0LlxuUHJlZGljdGVkUHJpY2UubXVsdGlwbGUgPC0gcHJlZGljdChtdWx0aXBsZS5maXQsdGVzdClcbiMgUHJlZGljdGVkIFZhbHVlc1xuaGVhZChhcy5pbnRlZ2VyKHVubmFtZShQcmVkaWN0ZWRQcmljZS5tdWx0aXBsZSkpKVxuIyBBY3R1YWwgVmFsdWVzXG5oZWFkKHRlc3QkUHJpY2UpIn0= We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the previous model based on just the Size as predictor, here, when we used 3 predictors, we have more accurate predictions, thus increasing the overall accuracy of the model. 5.1.2 Calculating the Error using mse() As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test won’t prove useful now in our numerical predictions scenario. Also we don’t want to eyeball everytime we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique. To do this we will use the Mean Squared Error(MSE). The MSE is a measure of the quality of an predictor/estimator It is always non-negative Values closer to zero are better. The equation to calculate the MSE is as follows: \\[\\begin{equation} MSE=\\frac{1}{n} \\sum_{i=1}^{n}{(Y_i - \\hat{Y_i})^2} \\\\ \\text{where $n$ is the number of data points, $Y_i$ are the observed value}\\\\ \\text{and $\\hat{Y_i}$ are the predicted values} \\end{equation}\\] To implement this, we will use the mse() function present in the Metrics Package, so remember to install the Metrics package and use library(Metrics) in the code for local use. The syntax for mse() function is very simple: mse(actual,predicted) actual: vector of the actual values of the attribute we want to predict. predicted: vector of the predicted values obtained using our model. Now lets look at the MSE of the previous example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxucmVhbGVzdGF0ZTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvUmVhbEVzdGF0ZS5jc3ZcIilcblxuIyBzcGxpdGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhyZWFsZXN0YXRlKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC43LCAuMykpXG50cmFpbiA8LSByZWFsZXN0YXRlW2lkeCA9PSAxLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVtpZHggPT0gMixdXG5cbiMgVXNlIHRoZSBsbSgpIGZ1bmN0aW9uIHRvIHByZWRpY3QgdGhlIHByaWNlIGJhc2VkIG9uIHNpemUgb2YgdGhlIGhvdXNlLlxuc2ltcGxlLmZpdCA8LSBsbShQcmljZX5TaXplLGRhdGE9dHJhaW4pXG4jIFByZWRpY3RpbmcgdmFsdWVzIG9uIHRoZSB0ZXN0IGRhdGFzZXQuXG5QcmVkaWN0ZWRQcmljZS5zaW1wbGUgPC0gcHJlZGljdChzaW1wbGUuZml0LHRlc3QpXG4jIFByZWRpY3RlZCBWYWx1ZXNcbmhlYWQoYXMuaW50ZWdlcih1bm5hbWUoUHJlZGljdGVkUHJpY2Uuc2ltcGxlKSkpXG4jIEFjdHVhbCBWYWx1ZXNcbmhlYWQodGVzdCRQcmljZSlcblxuIyBMZXRzIHVzZSB0aGUgbXNlKCkgZnVuY3Rpb24gdG8gXG5tc2UodGVzdCRQcmljZSxQcmVkaWN0ZWRQcmljZS5zaW1wbGUpIn0= We can see the MSE is too large above 200 billion, and this is huge value could be understandable as we are taking the squared differences of all the records that we predicted. The main intention is to get this huge value to as low as possible possibly near zero, which could be difficult but can be achieved upto a relative error by using a better model and training data. 5.2 Regression using RPART Since we have already used the rpart library for performing decision tree algorithms also referred as CART(classification and regression tree) algorithms, we will now look at this type algorithm for regression based prediction. Remember we have discussed the usage of Rpart in the section 4.2 in great detail. Thus for using Rpart for regression based prediction we will need to provide the rpart() functions, method attribute, with the keyword “anova”. For more details on the use of Rpart for prediction please refer to section 4.2. Lets look at an example of regression based prediction using Rpart for the Price attribute of the Real estate Dataset with Size, Number of Bedrooms and Number of Bathrooms as predictors. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubGlicmFyeShNb2RlbE1ldHJpY3MpXG5cbiMgTG9hZCB0aGUgZGF0YXNldC5cbnJlYWxlc3RhdGU8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1JlYWxFc3RhdGUuY3N2XCIpXG5cbiMgc3BsaXRpbmcgdGhlIGRhdGFzZXQgaW50byB0cmFpbmluZyBhbmQgdGVzdGluZy5cbmlkeCA8LSBzYW1wbGUoIDE6Miwgc2l6ZSA9IG5yb3cocmVhbGVzdGF0ZSksIHJlcGxhY2UgPSBUUlVFLCBwcm9iID0gYyguNywgLjMpKVxudHJhaW4gPC0gcmVhbGVzdGF0ZVtpZHggPT0gMSxdXG50ZXN0IDwtIHJlYWxlc3RhdGVbaWR4ID09IDIsXVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbiB0byBwcmVkaWN0IHRoZSBwcmljZSBiYXNlZCBvbiB0aGUgc2l6ZSwgYmF0aHJvb21zIGFuZCBiZWRyb29tcyBvZiB0aGUgaG91c2UuXG5ycGFydC5maXQgPC0gcnBhcnQoUHJpY2V+U2l6ZStCYXRocm9vbXMrQmVkcm9vbXMsZGF0YT10cmFpbixtZXRob2QgPSBcImFub3ZhXCIpXG5cbiMgUHJlZGljdGluZyB2YWx1ZXMgb24gdGhlIHRlc3QgZGF0YXNldC5cblByZWRpY3RlZFByaWNlLnJwYXJ0IDwtIHByZWRpY3QocnBhcnQuZml0LHRlc3QpXG4jIFByZWRpY3RlZCBWYWx1ZXNcbmhlYWQoYXMuaW50ZWdlcih1bm5hbWUoUHJlZGljdGVkUHJpY2UucnBhcnQpKSlcbiMgQWN0dWFsIFZhbHVlc1xuaGVhZCh0ZXN0JFByaWNlKVxuXG4jIExldHMgdXNlIHRoZSBtc2UoKSBmdW5jdGlvbiB0byBcbm1zZSh0ZXN0JFByaWNlLFByZWRpY3RlZFByaWNlLnJwYXJ0KSJ9 Output tree plot of rpart() model using for regression using “anova” method We can see, The output decision tree of the rpart() function The predicted values obtained using the model created by the rpart() function. The MSE of the model on the testing dataset. An important point to note while using decision trees for regression purpose, is that since the underlying process of modeling is still a decision tree, the output still represent a set of distinct classes, even though the values of the classes are numeric. Thus we can see that the predicted values are repeated even for varying inputs. Hence Decision tree must be used carefully when used for regression based prediction models. EOC "],["models.html", "Chapter 6 Additional Modeling techniques. 6.1 Four Line Method for creating most type of prediction models in R 6.2 Random Forest 6.3 SVM 6.4 Neural Network.", " Chapter 6 Additional Modeling techniques. In this chapter, we will see some additional machine learning models used in practice, for various purpose. After studying both classification models and regression models in the previous 2 chapters 4 &amp; 5 respectively, we will now look into other generic models used for classification and/or regression purpose. Below is the list some of the widely used algorithms with their use case(either classification/regression or both) and training and prediction complexities for using particular learning models. Usage and Complexity of various machine learning algorithms. Credits:thekerneltrip.com As we can see that many of these algorithms can be used for classification and regression all together, as we saw in the case of the Decision tree models using Rpart in section 4.1, and also some model used for only a particular type of prediction e.g. linear regression. We will look at few algorithms from the above list: Random Forest Support Vector Machine (SVM) Neural Networks 6.1 Four Line Method for creating most type of prediction models in R But before we learn about these algorithms, let us see a four line method to build models using any of the above algorithms using R. We can safely assume that the data going to be used to build the model, has been pre-processed and based on requirements split into the required subsets. To see how to split the data refer to section 4.6. The zeroth step now will be obviously to install and load the packages that contain the ML algorithm. To do that on your local machine, use the following code. # Install the library install.packages(&quot;package name&quot;) # Load the library in R library(package_name) Once we have the algorithm library loaded, we then proceed to build the model. pred.model = model_function(formula, data, method*,...) model_function(): the function present in the library to build the model. e.g. rpart() formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the ML model is to be trained on. Remember never used the test data to build the model. method: (OPTIONAL) Used to denote the method of prediction or underlying algorithm. This parameter could be present in some model_function() but not all. Prediction using the predict() function on the training data to assess the models performance/accuracy in next step. pred = predict(pred.model, newdata = train) predict(): the common function for all models used for prediction. pred.model: output of the step 1. newdata: here we assign the data on which the prediction is to be done. Evaluate error in Training phase. We use the mse() function for finding the accuracy of the model. To read more in dept about the mse() function refer to section 5.1.2. mse(actual, pred) actual: vector of the actual values of the attribute we want to predict. pred: vector of the predicted values obtained using our model. Repeat steps 0,1,2 and 3 by changing the ML algorithm or manipulating dataset to perform better when used to train using ML model, so as to achieve as low MSE value as possible. Finally we predict on the testing data using the same predict function as in step 2 but replacing the train data with test data. pred = predict(pred.model, newdata = test) These are the 4 steps to follow while performing any prediction task using ML models in R. We can also add one more step between step 3 and 4, which is step of performing the cross validation process on the newly built models. This can be done either manually, or by using third party libraries. One such library is the rModeling package, which has function crossValidation() which can be used for any type of model_functions(). For more information visit crossValidation() Before we proceed to the next section, please look at the snippet of the earnings.csv dataset, which we will be using for predicting the Earnings attribute based on various other attributes provided in the dataset, using different prediction models. Table 6.1: Snippet of Earnings Dataset GPA Number_Of_Professional_Connections Earnings Major Graduation_Year Height Number_Of_Credits Number_Of_Parking_Tickets 2581 2.28 47 10229.70 Humanities 1996 63.44 124 3 4676 2.21 13 13216.72 Vocational 1981 70.07 124 0 5046 1.04 60 13111.27 Vocational 1988 69.62 121 2 2760 1.98 10 10190.64 Humanities 1973 65.98 121 0 7029 2.08 16 11802.45 Professional 1962 67.95 121 2 667 2.46 7 9735.24 STEM 1990 69.56 126 1 8675 2.02 3 9798.00 Buisness 1977 64.59 121 0 5378 3.26 10 13329.50 Vocational 1987 66.52 123 0 3613 2.29 3 10231.89 Humanities 1984 69.08 120 1 6560 1.91 2 11815.21 Professional 1984 67.34 124 1 Now that we saw the general structure of the model and took a glace at the dataset we will be using, lets look at few of the algorithms as we promised from the list above. 6.2 Random Forest Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the value that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees. In simpler terms, the random forest algorithm creates multiple decision trees based on varying attributes and biases, and then predicts the output for each tree, and aggregates this prediction into one final output by some technique like majority count or average/etc. Visual Representation of a Random Forest learning model. Credits: Random Forest Wikipedia The main idea behind Random Forest arise from a method called ensemble learning method. Ensemble learning is the method of solving a problem by building multiple ML models and combining them. It is primarily used to improve the performance of classification, prediction, and function approximation models. Forests are type of ensemble learning methods, where they act like, pulling together all of decision tree algorithm efforts. Taking the teamwork of many trees thus improving the performance of a single random decision tree. Random decision forests correct for decision trees’ habit of overfitting to their training set. Now lets look at an example of prediction by the random forest model using the randomForest() function present in the randomForest library package. For more information about the randomForest() function and its attributes visit randomforest() Thus following the 4 step method of prediction for predicting a numerical attribute “Earnings” using the randomForest() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJhbmRvbUZvcmVzdClcbmxpYnJhcnkoTW9kZWxNZXRyaWNzKVxuXG4jIExvYWQgdGhlIGRhdGFzZXQuXG5lYXJuaW5nZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvZWFybmluZ3MuY3N2XCIsc3RyaW5nc0FzRmFjdG9ycyA9IFQpXG5cbiMgc3BsaXR0aW5nIHRoZSBkYXRhc2V0IGludG8gdHJhaW5pbmcgYW5kIHRlc3RpbmcuXG5pZHggPC0gc2FtcGxlKCAxOjIsIHNpemUgPSBucm93KGVhcm5pbmdkYXRhKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC44LCAuMikpXG50cmFpbiA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMSxdXG50ZXN0IDwtIGVhcm5pbmdkYXRhW2lkeCA9PSAyLF1cblxuIyAxLiBCdWlsZCBwcmVkaWN0aW9uIG1vZGVsIHVzaW5nIHJhbmRvbUZvcmVzdCgpIGZ1bmN0aW9uLlxucHJlZC5tb2RlbCA8LSByYW5kb21Gb3Jlc3QoRWFybmluZ3Mgfi4sIGRhdGEgPSB0cmFpbilcblxuIyBMZXRzIHNlZSB0aGUgc3VtbWFyeSBvZiB0aGUgcmFuZG9tRm9yZXN0IG1vZGVsLlxucHJlZC5tb2RlbFxuXG4jIDIuIFByZWRpY3QgdXNpbmcgdGhlIG5ld2x5IGJ1aWx0IG1vZGVsIG9uIHRoZSB0cmFpbmluZyBkYXRhc2V0LlxucHJlZC50cmFpbiA8LSBwcmVkaWN0KHByZWQubW9kZWwsbmV3ZGF0YSA9IHRyYWluKVxuXG4jIDMuIEV2YWx1YXRlIGVycm9yIG9uIHRyYWluaW5nIHVzaW5nIHRoZSBtc2UoKSBmdW5jdGlvbi5cbm1zZSh0cmFpbiRFYXJuaW5ncyxwcmVkLnRyYWluKVxuXG4jIDQuIFByZWRpY3Qgb24gdGhlIHRlc3RpbmcgZGF0YS5cbnByZWQudGVzdCA8LSBwcmVkaWN0KHByZWQubW9kZWwsbmV3ZGF0YSA9IHRlc3QpXG5cbiMgQWRkaXRpb25hbGx5IHNpbmNlIGhlcmUgd2UgaGF2ZSB0aGUgYWN0dWFsL3JlYWwgcHJlZGljdGlvbiB2YWx1ZXMgd2UgY2FuIGFsc28gY2hlY2sgdGhlIGFjY3VyYWN5IG9mIG91ciBwcmVkaWN0aW9uIG9uIHRlc3RpbmcgZGF0YS5cbm1zZSh0ZXN0JEVhcm5pbmdzLHByZWQudGVzdCkifQ== We can see, the summary of the output randomForest model with details of: Formula used. Type of random forest Number of trees created in the forest Number of variables used at each split. And some performance parameter. The mean squared error of the predicted values using training sub-dataset. The mean squared error of the predicted values using the testing sub-dataset. Note: Since a random forest is an ensemble learning method, it will usually take a lot more time to train that its counterparts. Thus you can see a significant waiting/execution time while running the above code and acquiring answer. 6.3 SVM Support-Vector Machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification(mostly) and regression(also works in some cases) analysis. The goal of the SVM is to find a hyperplane in an N-dimensional space (where N corresponds with the number of features) that distinctly classifies/regresses the data points. The accuracy of the results directly correlates with the hyperplane that we choose. We should find a plane that has the maximum distance between data points of both classes. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier. Support Vector Machine Linear classification hyperplane(line) example. Credits: Support Vector Machines Wikipedia Note that the dimension of the hyperplane depends on the number of features. If the number of input features is two, then the hyperplane is just a line. If the number of input features is three, then the hyperplane becomes a two-dimensional plane. It becomes difficult to draw on a graph a model when the number of features exceeds three. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Example of the kernal trick for non-linear classifier. Credits: Support Vector Machines Wikipedia Why is this called a support vector machine? Support vectors are data points closest to the hyperplane. They directly influence the position and orientation of the hyperplane and minimizes the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These points thus help to build our SVM model. SVM is great because it gives quite accurate results with minimum computation power. Lets look at the example of Support vector machine algorithm in use for predicting the Earnings attribute of the Earnings dataset. We will use the svm() function from the e1071 package. For more information about this function and its attributes visit svm() Thus following the 4 step model for prediction and using the “svm()” function as the model function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGUxMDcxKVxubGlicmFyeShNb2RlbE1ldHJpY3MpXG5cbiMgTG9hZCB0aGUgZGF0YXNldC5cbmVhcm5pbmdkYXRhPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9lYXJuaW5ncy5jc3ZcIixzdHJpbmdzQXNGYWN0b3JzID0gVClcblxuXG4jIHNwbGl0dGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhlYXJuaW5nZGF0YSksIHJlcGxhY2UgPSBUUlVFLCBwcm9iID0gYyguOCwgLjIpKVxudHJhaW4gPC0gZWFybmluZ2RhdGFbaWR4ID09IDEsXVxudGVzdCA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMixdXG5cbiMgMS4gQnVpbGQgcHJlZGljdGlvbiBtb2RlbCB1c2luZyBzdm0oKSBmdW5jdGlvbi5cbnByZWQubW9kZWwgPC0gc3ZtKEVhcm5pbmdzIH4uLCBkYXRhID0gdHJhaW4pXG5cbiMgTGV0cyBzZWUgdGhlIHN1bW1hcnkgb2YgdGhlIHN2bSBtb2RlbC5cbnByZWQubW9kZWxcblxuIyAyLiBQcmVkaWN0IHVzaW5nIHRoZSBuZXdseSBidWlsdCBtb2RlbCBvbiB0aGUgdHJhaW5pbmcgZGF0YXNldC5cbnByZWQudHJhaW4gPC0gcHJlZGljdChwcmVkLm1vZGVsLG5ld2RhdGEgPSB0cmFpbilcblxuIyAzLiBFdmFsdWF0ZSBlcnJvciBvbiB0cmFpbmluZyB1c2luZyB0aGUgbXNlKCkgZnVuY3Rpb24uXG5tc2UodHJhaW4kRWFybmluZ3MscHJlZC50cmFpbilcblxuIyA0LiBQcmVkaWN0IG9uIHRoZSB0ZXN0aW5nIGRhdGEuXG5wcmVkLnRlc3QgPC0gcHJlZGljdChwcmVkLm1vZGVsLG5ld2RhdGEgPSB0ZXN0KVxuXG4jIEFkZGl0aW9uYWxseSBzaW5jZSBoZXJlIHdlIGhhdmUgdGhlIGFjdHVhbC9yZWFsIHByZWRpY3Rpb24gdmFsdWVzIHdlIGNhbiBhbHNvIGNoZWNrIHRoZSBhY2N1cmFjeSBvZiBvdXIgcHJlZGljdGlvbiBvbiB0ZXN0aW5nIGRhdGEuXG5tc2UodGVzdCRFYXJuaW5ncyxwcmVkLnRlc3QpIn0= We can see, the summary of the output svm model with details of: Formula used. Type of SVM model The SVM Kernel Used And some performance parameter. Also, the Number of Support Vectors. The mean squared error of the predicted values using training sub-dataset. The mean squared error of the predicted values using the testing sub-dataset. 6.4 Neural Network. An Artificial Neural Network (ANN), usually simply called neural network(NN) is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. Thus in other words, a neural network is a sequence of neurons connected by synapses, which reminds of the structure of the human brain. However, the human brain is even more complex, and a NN is just a model that mimics a human brain. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The “signal” at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times. A visual representation of typical neural network with various nodes and edges along with layers. Credits: Artificial Neural Network Wikipedia What is great about neural networks is that they can be used for basically any task from spam filtering to computer vision. However, they are normally applied for machine translation, anomaly detection and risk management, speech recognition and language generation, face recognition, and more. To accommodate such a wide variety of application, neural nets are transformed and models in various different ways. To find multiple types of neural networks please visit Neural Network Zoo Now lets try to implement a neural network learning model for the Earnings prediction problem of Earnings dataset. To do this we will use the 4 step method of prediction and use the nnet() function from the “nnet” package as the model_function. Let look at the nnet() function and its parameters. nnet(formula,data,size,linout,...) formula and data are the same as mentions in Step 1 of section 6.1. size: denotes the number of units in the hidden layer. linout: Assign TRUE is predicting numerical value. Default is FALSE, for predicting categorical value. rang: set the initial random weights on each edge. maxit: maximum number of iterations. decay: weight decay parameter. Can also be understood as learning rate. Note: The nnet() function can only create a single hidden layer neural network model. To create more complex models please use different packages like neuralnet or h2o, deepnet, etc Now lets use the nnet() function for predction. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KG5uZXQpXG5saWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxuZWFybmluZ2RhdGE8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L2Vhcm5pbmdzLmNzdlwiLHN0cmluZ3NBc0ZhY3RvcnMgPSBUKVxuXG4jIHNwbGl0dGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhlYXJuaW5nZGF0YSksIHJlcGxhY2UgPSBUUlVFLCBwcm9iID0gYyguOCwgLjIpKVxudHJhaW4gPC0gZWFybmluZ2RhdGFbaWR4ID09IDEsXVxudGVzdCA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMixdXG5cbiMgMS4gQnVpbGQgcHJlZGljdGlvbiBtb2RlbCB1c2luZyBubmV0KCkgZnVuY3Rpb24uXG5wcmVkLm1vZGVsIDwtIG5uZXQoRWFybmluZ3MvMjAwMDAgfi4sIGRhdGEgPSB0cmFpbiwgc2l6ZSA9IDUwLCBkZWNheT01ZS01LG1heGl0ID0gNTAwLGxpbm91dCA9IFQpXG5cbiMgTGV0cyBzZWUgdGhlIHN1bW1hcnkgb2YgdGhlIG5uZXQgbW9kZWwuXG5wcmVkLm1vZGVsXG5cbiMgMi4gUHJlZGljdCB1c2luZyB0aGUgbmV3bHkgYnVpbHQgbW9kZWwgb24gdGhlIHRyYWluaW5nIGRhdGFzZXQuXG5wcmVkLnRyYWluIDwtIHByZWRpY3QocHJlZC5tb2RlbCxuZXdkYXRhID0gdHJhaW4pKjIwMDAwXG5cbiMgMy4gRXZhbHVhdGUgZXJyb3Igb24gdHJhaW5pbmcgdXNpbmcgdGhlIG1zZSgpIGZ1bmN0aW9uLlxubXNlKHRyYWluJEVhcm5pbmdzLHByZWQudHJhaW4pXG5cbiMgNC4gUHJlZGljdCBvbiB0aGUgdGVzdGluZyBkYXRhLlxucHJlZC50ZXN0IDwtIHByZWRpY3QocHJlZC5tb2RlbCxuZXdkYXRhID0gdGVzdCkqMjAwMDBcblxuIyBBZGRpdGlvbmFsbHkgc2luY2UgaGVyZSB3ZSBoYXZlIHRoZSBhY3R1YWwvcmVhbCBwcmVkaWN0aW9uIHZhbHVlcyB3ZSBjYW4gYWxzbyBjaGVjayB0aGUgYWNjdXJhY3kgb2Ygb3VyIHByZWRpY3Rpb24gb24gdGVzdGluZyBkYXRhLlxubXNlKHRlc3QkRWFybmluZ3MscHJlZC50ZXN0KSJ9 NOTE: If you see a very high value of MSE after running the above code, please re-run it. Usually you will find the MSE to be better than all the models we have studied uptil now for the Earnings prediction problem. We can see from the output, the summary of the output, neural network model, with details of: Number of weights in the complete neural network Initial Value and Final Value of the model weights along with iter value. Structure of the neural network in I-H-O format where the numbers, I is input, H is hidden and O is output nodes. Input node attributes. Note that the Majors column attribute are split into unique number of factors, thus creating new individual attributes. Output node attributes. Network Options. The mean squared error of the predicted values using training sub-dataset. The mean squared error of the predicted values using the testing sub-dataset. After Comparing all these models, we can see that the MSE values for the 3 models are SVM &gt; Random Forest &gt; Neural Network. This suggest one trend that, to get as best result as possible, one must invest most time in choosing the right model,and use the model with cleaned dataset for training. Eventually, since we use R language here, the code for model creation just boils down to few lines of code, 4 steps to be more accurate. But since we might find one model works better than other, we must choose the best fit model. Also, we saw the trend of time required for training of the models studied above was SVM &gt; RandomForest &gt; Neural Net. This also suggest a proportional relationship with the time required for a particular model to train and in turn producing the best possible results. Although one can say that, we can just use Neural Networks all the time, well this statement is true to some extent, but depending on the resources, the complexity of the data, and the complexity of the model itself, one needs to make some trade-offs. One should not try to throw a ball just few meters with a cannon, using mere hands will do the job. In essence, do not try to overuse the neural net model for the sake of adding 2 mumbers, simple addition will suffice. Studying these tradeoffs and more models in depth though is out of this course scope. EOC "],["predblogs.html", "Chapter 7 Prediction Challenges 7.1 General Structure of the Prediction Challenges. 7.2 Prediction Challange 1. 7.3 Prediction Challenge 2. 7.4 Prediction Challenge 3. 7.5 Prediction Challenge 4.", " Chapter 7 Prediction Challenges Until we have studied multiple methods of data analysis in sections 2.2,??, statistical testing in sections 3, &amp; building prediction models for both classification 4 and regression 5 along with advanced ML models 6. Now its time to utilize them in various ways for analysis and prediction of data. To do this, in this course, we have designed few prediction challenges, which test your ability to implement skills learnt in the course until now. First challenge is a basic prediction challenge using only data analysis using the freestyle techniques from section 2.2. Then onwards, prediction challenges used multitude of modeling techniques which were studied in 4 and 5. 7.1 General Structure of the Prediction Challenges. Usually there is a task to be performed in each prediction challenge. Either predicting a numerical of categorical values is the task of each challenge. The way to perform those task are constrained differently for different prediction challenges based on levels of difficulty and ML models to be used. The submission will take place on Kaggle which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submission. The datasets provided for each prediction challenge is as follows: Training Dataset. It is used for training and cross-validation purpose in the prediction challenge. This data has all the training attributes along and the ideal values of the prediction attribute. Models for prediction are to be trained using this dataset only. Testing Dataset. It is used for prediction only. It consists of all the attributes that were used for training, but it does not contain any values of the actual prediction attributes, which is actually the attribute that the prediction challenge predicts. Since its only used for prediction purpose and is not involved in training of the models, it is thus not involved in the cross-validation phase too. Submission Dataset. After prediction using the “testing” dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g. ID or name,etc) and second the predicted attribute column. Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle. To read the datasets use the read.csv() function and for writing the dataset to the file, use the write.csv() function. Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle. To avoid this, you can add the parameter, row.names = F in the write.csv() function. e.g. write.csv(*dataframe*,*fileaddress*,row.names = F). Now lets look at the prediction challenges that took place in this course along with the top submissions by students. 7.2 Prediction Challange 1. In Prediction challange 1, the task was to predict a categorical value using only free-style prediction. For this prediction challenge we used our favorite dataset, the Moody dataset, and predicted the Grade category of all students. The Grade category had only 2 factors: Pass OR Fail. Let look at a snippet of the moody dataset used for training in this challenge. Table 7.1: Snippet of Moody Dataset(TRAINING) for Prediction Challenge 1 Studentid Attendance Major Questions Score Seniority Texting Grade 29998 40 Stat Rarely 65 Freshman Always Pass 29999 30 Cs Always 46 Senior Rarely Fail 30000 90 Communication Rarely 60 Senior Always Pass 30001 5 Polsci Always 46 Senior Rarely Pass 30002 67 Cs Rarely 36 Senior Always Fail 30003 20 Stat Rarely 50 Senior Rarely Pass 30004 78 Stat Rarely 0 Senior Always Pass 30005 27 Polsci Rarely 45 Junior Always Fail 30006 81 Polsci Rarely 6 Sophomore Always Fail 30007 50 Communication Rarely 97 Senior Always Pass We can see that there are multiple attributes like Score, Attendence, Major, etc. that can be used as predictors, and then there is Grade attribute with ideal values for each record of student which will be used while training and then will be predicted on the testing dataset. 7.2.1 How the data was generated for Challenge 1 Professor Moody data set has been synthetically generated using random generator which follows probabilistic rules implementing “secret patterns” which we embedded in the data. These patterns are presented below in the form of decision tree. For example a rule that statistics major with score over 60, pass the class - reflects the generated data in which high percentage (but not 100%) of such students indeed pass Moody’s class. There will always be random exceptions to these rules. But majority of stat students with score above 60 will pass the class The data is based on a tree given by the following conditions: Tree which is embedded in the data (secret pattern for Moody -challenge1/2) Major Stat Score &gt; 60 Pass Score &lt;= 60 Fail Comm Score &gt;40 Pass Score &lt;=40 Texting = Rarely Fail Texting = Always PAss Polsci Score &gt;50 Pass Score &lt;=50 Questions = rare Fail Questions = always Pass Cs Score &gt;70 Pass Score &lt;=70 Seniority= Freshman Score &gt;50 Pass Score &lt;=50 Attendance &gt;=60 Score &gt; 40 Pass Score &lt;=40 Fail Attendance &lt;60 Fail Seniority= Sophomore Score &gt;50 Pass Score &lt;=50 Fail Seniority= Junior Fail Seniority = Senior Fail We can see this in pictorial representation below based on each subset of Majors. For Stats Major: We can see that the rule was very simple, with the final grade decided based on only the Score attribute of the students. Thus finding this pattern would have been easier for students For Communication Major Students: The grade prediction for students from the Communication Major was based not only on the Score attribute but was also based on Texting attribute of the students records. As we can see, finding this pattern would have been not that difficult. For Political Science Major Students: The grade prediction for students from the Political Science Major was based not only on the Score attribute but was also based on Questions attribute of the students records. As we can see, finding this pattern would have been not that difficult. For Computer Science Major Students: The grade prediction for students from the Computer Science Major was the most involved and was based on various students attribute. Attributes like Score, Seniority and Attendance were involved in prediction, and the subsetting conditions were very complex. As we can see, finding this huge pattern would have been very difficult for students. If you want to see a well detailed data analysis of the dataset based on Majors as subset, then please look at Rohit Manjunath’s submission in the Top Submission section for prediction challenge 1. How the data was generated using R You can see a simple way to write the data using the above patterns. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YVxuZGF0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMXRlc3Qtc3R1ZGVudHMuY3N2XCIsc3RyaW5nc0FzRmFjdG9ycyA9IFQpXG5cbiMgQ3JlYXRlIGFuIGFsbCBGYWlsIENhdGVnb3J5L1ByZWRpY3RlZCBWYWx1ZSBWZWN0b3IgYW5kIGFwcGVuZCBpdCB0byB0aGUgdGVzdGluZyBkYXRhc2V0LlxuZGF0JEdyYWRlPC1yZXAoJ0ZhaWwnLCBucm93KGRhdCkpXG5cbiMgVGhlIHZhcmlvdXMgY29uZGl0aW9ucyB1c2VkIHRvIHByZWRpY3QgZ3JhZGUgYXR0cmlidXRlLlxuXG4jIEZvciBNYWpvciA9IFN0YXRcbmRhdFtkYXQkTWFqb3I9PSdTdGF0JyAmIGRhdCRTY29yZT42MCxdJEdyYWRlIDwtJ1Bhc3MnXG5cbiMgRm9yIE1ham9yID0gQ29tbVxuZGF0W2RhdCRNYWpvcj09J0NvbW11bmljYXRpb24nICYgZGF0JFNjb3JlPjQwLF0kR3JhZGUgPC0nUGFzcydcbmRhdFtkYXQkTWFqb3I9PSdDb21tdW5pY2F0aW9uJyAmIGRhdCRTY29yZTw9NDAgJiBkYXQkVGV4dGluZz09J0Fsd2F5cycsXSRHcmFkZSA8LSdQYXNzJ1xuXG4jIEZvciBNYWpvciA9IFBvbHNjaVxuZGF0W2RhdCRNYWpvcj09J1BvbHNjaScgJiBkYXQkU2NvcmU+NTAsXSRHcmFkZSA8LSdQYXNzJ1xuZGF0W2RhdCRNYWpvcj09J1BvbHNjaScgJiBkYXQkU2NvcmU8PTUwICYgZGF0JFF1ZXN0aW9ucz09J0Fsd2F5cycsXSRHcmFkZSA8LSdQYXNzJ1xuXG4jIEZvciBNYWpvciA9IENzXG5kYXRbZGF0JE1ham9yPT0nQ3MnICYgZGF0JFNjb3JlPjcwLF0kR3JhZGUgPC0nUGFzcydcbmRhdFtkYXQkTWFqb3I9PSdDcycgJiBkYXQkU2NvcmU8PTcwICYgZGF0JFNlbmlvcml0eT09J0ZyZXNobWFuJyAmIGRhdCRTY29yZT41MCxdJEdyYWRlIDwtJ1Bhc3MnXG5kYXRbZGF0JE1ham9yPT0nQ3MnICYgZGF0JFNjb3JlPD03MCAmIGRhdCRTZW5pb3JpdHk9PSdGcmVzaG1hbicgJiBkYXQkQXR0ZW5kYW5jZSA+PTYwICYgZGF0JFNjb3JlPjQwLF0kR3JhZGUgPC0nUGFzcydcbmRhdFtkYXQkTWFqb3I9PSdDcycgJiBkYXQkU2NvcmU8PTcwICYgZGF0JFNlbmlvcml0eT09J1NvcGhvbW9yZScgJiBkYXQkU2NvcmU+NTAsXSRHcmFkZSA8LSdQYXNzJ1xuXG5cbiMgQ29tcGFyZSBpdCB3aXRoIHRoZSBpZGVhbCBwcmVkaWN0aW9ucyBmb3IgY2hlY2tpbmcgYWNjdXJhY3kgb2Ygb3VyIHByZWRpY3Rpb25zLlxuYW5zd2VyczwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTTIwMjF0ZXN0X2Fuc3dlci5jc3ZcIixzdHJpbmdzQXNGYWN0b3JzID0gVClcbm1lYW4oZGF0JEdyYWRlPT1hbnN3ZXJzJEdyYWRlKSAjIEFjY3VyYWN5In0= Thus we can see, using the ideal patterns the students would have expected to score around 83% accuracy. 7.2.2 Top Submissions for Challenge 1. Students with accuracy over 60% were considered passed for this prediction challenge. Jeremy Prasad Jeremy’s PPT Jeremy performed exceptionally well in this prediction challenge. His approach was a iterative learning process, where at each step after performing analysis he tried to decrease the error more and more. He started with a very basic model, of using just the score attribute with a hard threshold for pass or fail grade based on the score value. After this, to increase accuracy, he analysed the data more found which attributes effect the prediction of the data, and which are not really useful After finding these highly effective attributes, he wrote concrete set of attributs that can be used to assign the grade. Most of them were dependent on 2-3 attributes like Major-Senioriy-Score, Major-Score, or Major-Questions-Score,etc. This gave him a much better accuracy value for prediction. Rohit Manjunath Rohit’s PPT Rohit performed well in this prediction challenge, and has a different approach than that of Jeremy’s. In Rohit’s approach, instead of finding the minimum global threshold of pass or fail based on score, he found the threshold for the maximum score, above which every student passed the class. He then analysed the data based on the Majors first and then found interval threshold for each Majors scores. For some Majors, to increase accuracy, he further explored other attributes in detail to find which effects the final grade. Rohit obtained accuracy of almost 85%. 7.3 Prediction Challenge 2. In Prediction Challenge 2, we introduced the use of Decision Tree algorithm for prediction model building, to complete the same task as we saw in the Prediction Challenge 1. This was intended to see the first learning model in action, and also to see the ease in which the process of prediction can be completed using such prediction model against the trivial data analysis techniques. The datasets for this prediction challenge were the same as those in the prediction challenge 1. Since, the task in the prediction challenge was to predict a categorical value(Grade value) the learning algorithm allowed to be used in this task was the Decision Tree algorithm based on the CART model. Read more about how to use decision tree’s in section 4.1 . To implement this algorithm, students were allowed to use the RPART package 4.2 With rpart() doing most work of prediction in this task, the students were also asked to provide validation for their models prediction power/accuracy. This involved use of cross-validation techinques, which for the ease of this course level was provided in a custom function, see 4.7. 7.3.1 How the data was generated to Challenge 2 As we saw that the prediction task and the datasets in challenge 2 are similar to that of the challenge 1. Thus the data analysis of the challenge 1 would applicable in this case too. 7.3.2 Top Submissions for Challenge 2 Since rpart() is a very powerful function to find patterns with higher accuracy, the passing criteria for this challenge was above 80% accuracy score. Kevin Larkin Kevin’s PPT This was the top submission in terms of accuracy score on Kaggle. Kevin used the rpart() function, for modeling, with all the attributes of the training dataset except Studentid. To increase the accuracy of his model, he used the rpart.control() function parameters, especially the cp parameter of the function, which increased the splitting accuracy. Kevin acheived an accuracy score of over 86% on the test dataset for this challenge. Michael Ryvin Michael’s PPT This was the second best submission as per accuracy score on Kaggle. Michael used the rpart() function, along with some control parameters for creating the decision tree. Michael achieved an accuracy score of over 86% on the test dataset. Shuohao Ping Shuohao’s PPT This was the third best submission as per accuracy score on Kaggle. Shuohao used multiple iterations to create his final model. In each iteration, Shuohao tried to vary the control parameters and its values to find the best fit model after cross-validation. Shuohao, acheived an accuracy score of over 86% on the test dataset. 7.4 Prediction Challenge 3. After studying prediction of categorical data in the previous 2 prediction challenges, in prediction challenge 3, the task was to predict Earnings a numerical variable, using any ML algorithm. Earnings variable is part of the Earnings dataset which has details about a persons connections, GPA, Major,etc, and using these attributes, the students had to predict the numerical value of earnings of each person in the dataset. Students were recommended to first find some correlation between data by using free-style analysis, and then proceed to using ML models. This was included so as to show the effect of human intervention/input on the selection and performance of ML model, and also to avoid the trap of blindly applying the most costly ML model which might perform well, but is a overkill to perform task which could be completed using other less costly models. ( Cost here refers to the computation resources and time involved in training the models. ) To read more about prediction of a numerical variable in R, see section 5 and 6 Lets look at a snippet of the Earnings dataset used for training the models below. Table 7.2: Snippet of Earnings Dataset(TRAINING) for Prediction Challenge 3 GPA Number_Of_Professional_Connections Earnings Major Graduation_Year Height Number_Of_Credits Number_Of_Parking_Tickets 2.50 1 9756.15 STEM 2001 64.22 124 1 2.98 1 9709.03 STEM 2001 69.55 120 0 2.98 23 9711.37 STEM 1996 68.98 120 1 3.35 5 9656.15 STEM 2008 69.23 124 1 2.47 37 9751.92 STEM 1981 70.45 123 0 2.75 2 9728.30 STEM 2000 65.26 121 0 1.66 17 9847.59 STEM 2001 65.91 121 0 2.59 10 9743.36 STEM 1990 66.35 123 0 1.89 7 9793.38 STEM 1975 70.42 121 1 1.89 22 9810.38 STEM 1997 65.18 122 0 We can see that there are multiple attributes like GPA,Major,Graduation_Year,Height,etc. that can be used as predictors, and then there is Earnings attribute with ideal values for each record of student which will be used while training and then will be predicted on the testing dataset. 7.4.1 How the data was generated for Challenge 3 In this challenge, the Earnings variable was calculated using the attributes like GPA, Connections and Graduation Year in some cases. The main attribute on which the data is subsetted is the Education attribute. Then further, there is a predetermined polynomial formula based on the various attributes. The main idea behind these formulas, is to include a linear/ quadratic relation between the predictors and the Earnings attribute which will be predicted. These mathematical relation can be modeled using the most simplest linear regression algorithm to the most complex neural nets. The ideal formulas are listed below: Stem earn = -100 * gpa +10000 Humanities earn = 100* gpa + 10000 Vocational earn = 100 * gpa + 13000 Professional earn = -100gpa +12000 other earn = connection ^2 +5000 business earn = gpa * 100 * parity +10000 where parity = 1 if graduation year = even 0 if graduation year = odd As we can see, these formulas are mostly linear, while the formula for “other” education attribute is quadratic. Also, for “Business” education attribute subjects, the formula is dependent on an additional attribute. For more detailed data analysis please view the document attached here. Pred 3 Analysis How the data was modeled in R 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 We can see that after using the ideal formulas, we get an MSE of around 3300. Thus students who took part in this prediction challenge and scored around this MSE score would be top submissions. 7.4.2 Top Submissions for Challenge 3 For this prediction challenge, the MSE score below 30000 was considered a Passing score. Seok Yim Seok’s PPT This was the top submission based on MSE score, with a final score less than 100. The approach to solving this challenge was really well implemented. First, he looked at the dataset on whole, tried to find some interesting patterns. Then, after finding the patterns, he did not predict on the complete dataset using one big model, but subseted the data based on one attribute, and then modeled the ML model on these small subsets. This not only reduced the MSE to such low levels, thus increasing accuracy, but also led to faster model learning time, and prediction time. Nick Whelan Nick’s PPT This was another top submission based on MSE score, with final score less than 100. The approach to solving the task was different compared to Seok’s implementation, but was equally good, with nearly the same prediction power/accuracy. Nick tried to use the randomForest algorithm on the whole dataset as the initial model, but the MSE turned out to be near 25,000. Then he did some free-style analysis and found the linear relationship between various subsets of dataset with the earnings value. To implement this he used the fundamentals of linear regression very well while creating a learning model, and also used a quadratic model where needed. This resulted in a very accurate model with low MSE score. Bennett Garcia Bennett’s PPT Bennett had a final MSE score of below 100 and was one of the top submissions for this challenge. A significantly different learning model was used by Bennett to achieve this low MSE. He first analyzed the data, and found attributes on which the dataset can be subsetted on. Then, he here used Neural Networks as models for prediction on those subsets. This Neural Network approach was very well implemented. 7.5 Prediction Challenge 4. Challenge 4 was a relatively newer challenge, and was built to test and combine all that has been learnt from the previous challenges. In this challenge, there was a scenario as described below: Mysterious box was found on the beach. Despite spending probably years in the water, it still works! But what does it do? It has four inputs (electric) &amp; a switch. Setting these inputs and different switch positions emits various weird and scary sounds as output in response to the electric signals. It sizzles, gurgles, hisses, ominously tics like a bomb,etc…..but nothing happens - just sounds. So no harm will happen to surroundings. As we can see from the scenario, the task now in this challenge, is to predict the sounds that the Mysterios Box will make upon providing various set of inputs and different switch positions. Henceforth, we will refer to this mysterious box as Black Box. Also, since there are only finite number of sounds the box can make, the output sounds attribute is a categorical value, which will be predicted in this task. Students were recommended to first find some correlation between data by using free-style analysis, and then proceed to using any ML models. To read more about prediction in R, see sections 4,5 and 6 Lets look at a snippet of the Mysterious Box/ Black Box dataset used for training the models below. The training describes which sounds has been noted in the laboratory in nearly 20,000 experiments combining different input signals and switch positions. Table 7.3: Snippet of Black Box Dataset(TRAINING) for Prediction Challenge 4 ID INPUT1 INPUT2 INPUT3 INPUT4 SWITCH SOUND 86623 30 31 72 29 Low Gargle 57936 87 76 31 79 Low Tick 54301 16 33 87 41 Low Tick 2678 64 77 91 59 Minimum Beep 65827 33 72 53 66 High Beep 22420 5 50 26 50 High Gargle 2285 82 72 60 73 High Tick 62571 44 85 100 8 Minimum Kaboom 49229 92 31 100 64 Low Gargle 63532 28 51 77 4 Low Gargle We can see that there are multiple attributes like INPUT1,2,3,4 and Switch that can be used as predictors, and then there is Sound attribute with ideal values for each record of the experiment record which will be used while training and then will be predicted on the testing dataset. 7.5.1 How the data was generated for Challenge 4 This challenge was the most involved of the 4 challenges in this blog. There was no direct and straight forward answer to this challenge, but it required more data analysis, as compared to the other challenge. Although the relation of the Sound attribute was dependent on the 4 input and the switch position, figuring the relation between various inputs and the correct switch position was a non-trivial task. The solution to this challenge involved creating a new numeric variable(Say “OUTPUT”) which will be dependent on the 4 input values, and also the various switch positions. The relation between the inputs and the OUTPUT variable is given below: The ordering of Switch position is given as: Low = 1 Minimum = 2 Medium = 3 Maximum = 4 High = 5 if Switch == 1 i.e. &quot;Low&quot; then OUTPUT = Input 1+ 5 * Input 2 - 2 * Input3 + sample(2:5,1) if Switch == 2 i.e. &quot;Minimum&quot; then OUTPUT = 3* Input 2 - 2 * Input 4 + sample(2:3,1) else i.e. Position other than &quot;Low&quot; and &quot;Minimum&quot; then OUTPUT = Input1 ^2 -1.5 * Input 3 + sample(5:10,1) Then SOUND totally depends on OUTPUT attribute, but is distributed probabilistically over all possible sound. For example, the SOUND when OUTPUT&gt;150 is distributed as 0, 0, 10, 0, 10, 60, 20. This number list corresponds to Gargle, Tick, Beep, Kaboom, Rumble, Sizzle, Hiss. And thus we can see that &quot;Sizzle&quot; sound has the max probability of 60%, and is this the most likely sound when the OUTPUT value is above 150. OUTPUT &gt; 150 -&gt; Max Probability of finding &quot;Sizzle&quot; 100 &lt; OUTPUT &lt; 150 -&gt; Max Probability of finding &quot;Rumble&quot; 70 &lt; OUTPUT &lt; 100 -&gt; Max Probability of finding &quot;Kaboom&quot; 50 &lt; OUTPUT &lt; 70 -&gt; Max Probability of finding &quot;Hiss&quot; 20 &lt; OUTPUT &lt; 50 -&gt; Max Probability of finding &quot;Tick&quot; OUTPUT &lt; 20 -&gt; Max Probability of finding &quot;Gargle&quot; As we can see, that the 4 Input attributes are used to calculate the OUTPUT values based on a polynomial formula, and the particular formula is choosen by the Switch attribute. After finding the OUTPUT values, then the decision tree like structure can be implemented to assign Sound attribute to corresponding OUTPUT value based on the chart above. How the data was generated for Challenge 4 in R # Load The Data dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22_answer.csv&quot;,stringsAsFactors = T) dat$OUTPUT &lt;- rep(0,nrow(dat)) dat$OUTPUT &lt;- ((dat$INPUT1^2) - (1.5*dat$INPUT3) + sample(5:10,1)) dat[dat$SWITCH == &quot;Low&quot;,]$OUTPUT &lt;- (dat[dat$SWITCH == &quot;Low&quot;,]$INPUT1 + (5*dat[dat$SWITCH == &quot;Low&quot;,]$INPUT2) - (2*dat[dat$SWITCH == &quot;Low&quot;,]$INPUT3) + sample(2:5,1)) dat[dat$SWITCH == &quot;Minimum&quot;,]$OUTPUT &lt;- ((3*dat[dat$SWITCH == &quot;Minimum&quot;,]$INPUT2) - (2*dat[dat$SWITCH == &quot;Minimum&quot;,]$INPUT4) + sample(2:3,1)) dat$predSound &lt;- rep(&#39;Empty&#39;,nrow(dat)) dat[dat$OUTPUT&gt;150,]$predSound&lt;-&#39;Sizzle&#39; dat[dat$OUTPUT&gt;=100 &amp; dat$OUTPUT&lt;150,]$predSound&lt;-&#39;Rumble&#39; dat[dat$OUTPUT&gt;=70 &amp; dat$OUTPUT&lt;100,]$predSound&lt;-&#39;Kaboom&#39; dat[dat$OUTPUT&gt;=50 &amp; dat$OUTPUT&lt;70,]$predSound&lt;-&#39;Hiss&#39; dat[dat$OUTPUT&gt;=20 &amp; dat$OUTPUT&lt;50,]$predSound&lt;-&#39;Tick&#39; dat[dat$OUTPUT&lt;20,]$predSound&lt;-&#39;Gargle&#39; mean(dat$SOUND==dat$predSound) 7.5.2 Top Submissions for Challenge 4 Since this challenge involved stochastically generated data, the prediction accuracy required for passing this challenge was above 60%. Nicole Coria Nicole’s PPT This was the top submission based on accuracy score, with a final score more than 68.7% The approach to solving this challenge was iterative and trail and error based. First, since the task is to predict categorical data, she decided to use rpart(directly). Then, over iteration, by varying the control parameters of rpart, she tried to find the model with the highest accuracy. Use of cross-validation also helped in finding the best fit model. Atharva Patil Atharva’s PPT This was another top submission based on accuracy score, with final score above 68% The approach to solving the task was very well implemented, using external resources too. Atharva tried to analyze the data first. To do this, he used Prof. Imielinski’s online platform called Boundless Analytics. This online platform has ability to analyze the data automatically, and create plots which only matter or provide more information about the data. It eliminates the need to perform the data analysis manually. Then, he proceeded by building the model using the rpart() function and control parameters. Andrew Scovell Andrew’s PPT Bennett had a final accuracy score of above 68% and was one of the top submissions for this challenge. He did a very extensive data analysis using all the attributes of the dataset. He also tried analyzing using mean, sums, standard deviation, etc of the numerical inputs. Using the control parameters of the rpart() function he tried to find the best fitting model, and used cross-validation to avoid overfitting. To perform any of the above challenges yourself, visit the appropriate links. Prediction Challenge 1 https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d Prediction Challenge 2 https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b Prediction Challenge 3 https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd Prediction Challenge 4 https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe "],["appendix.html", "Chapter 8 Appendix 8.1 Z-test 8.2 Permutation Test 8.3 Multiple Hypothesis - Bonferroni Correction.", " Chapter 8 Appendix The biggest enemy of your findings is randomness. In order to convince your audience that you have found something you need to address the question “how do you know your result is simply sheer luck, it is random?” This is where you need statistical tests for use in hypothesis testing. 8.0.0.1 Two Important Formula’s: Mean \\[\\begin{equation} \\bar{X}=\\frac{\\sum{X}}{N} \\ \\text{where, X is set of numbers and N is size of set.} \\end{equation}\\] Standard Deviation \\[\\begin{equation} \\sigma = \\sqrt{\\frac{\\sum{(X - \\mu)^2}}{N}}\\\\ \\text{where, X is set of numbers, $\\mu$ is average of set of numbers, }\\\\ \\text{ N is size of the set, $\\sigma$ is standard deviation} \\end{equation}\\] 8.1 Z-test A z-test is any statistical test used in hypothesis testing with an underlying normal distribution. In other words, when the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution, z-test can be used. Outcome of the z-test is the z-score which is a numerical measure to test the mean of a distribution. z-score is measured in terms of standard deviation from mean. 8.1.1 Steps for hypothesis testing using Z-test. Running a Z-test requires 5 steps: State the null hypothesis and the alternate hypothesis Select a null hypothesis and an alternate hypothesis which will be tested using the z-test. Choose an Alpha \\(\\alpha\\) level. Usually this is selected to be small, such that the area under the normal distribution curve is accumulated most in the range between the alpha level. Thus mostly in statistical testing, \\(\\alpha = 0.05\\) is selected. Calculate the z-test statistic. The z-test statistic is calculated using the z-score formula. \\[\\begin{equation} z = \\frac{x-\\mu}{\\sigma}\\text{ where, $z$ = z-score, $x$ = raw score, $\\mu$ = mean and $\\sigma$ = standard deviation } \\end{equation}\\] Calculate the p-value using the z-score Once we have the z-score we want to calculate the p-value from it. To do this, there are 2 ways, First use the z-table available online at z-table.com Second, use the pnorm() function in R to find the p-value. Compare the p-value with \\(\\alpha\\) After getting the p-value from step 4, compare it with the \\(\\alpha\\) level we selected in step 2. This decides if we can reject the null hypothesis or not. If the p-value obtained is lower than \\(\\alpha\\), then we can reject the null hypothesis. If the p-value is more than \\(\\alpha\\), we fail to reject the null hypothesis due to lack of significant evidence. Some important relation between one-sided and two sided test while using hypothesis testing is as follows: First, estimate the expected value \\(\\mu\\) of T(statistic) under the null hypothesis, and obtain an estimate \\(\\sigma\\) of the standard deviation of T. Second, determine the properties of T : one tailed or two tailed. For Null hypothesis H0: \\(\\mu \\geq \\mu_0\\) vs alternative hypothesis H1: \\(\\mu &lt; \\mu_0\\) , it is upper/right-tailed (one tailed). For Null hypothesis H0:\\(\\mu \\leq \\mu_0\\) vs alternative hypothesis H1: \\(\\mu &gt; \\mu_0\\) , it is lower/left-tailed (one tailed). For Null hypothesis H0: \\(\\mu = \\mu_0\\) vs alternative hypothesis H1: \\(\\mu \\neq \\mu_0\\) , it is two-tailed. Once you calculate the pnorm() in step 4, depending on the properties of two as described above, use pnorm(-Z) for right tailed tests, use 2*pnorm(-Z) for two tailed test, and use pnorm(Z) for left tailed tests. Note: (Here Z = z-score). Also the method mentioned above works similar to that studied in class/recitations, but is simple to understand, and does not require subtracting the pnorm() output from 1. 8.1.2 Z-test Example 1 (Right Sided) Now lets look at an example to use this z-test for hypothesis testing. We will study the example to statistically find the relation of the traffic volume per minute between two tunnels, namely Holland and Lincoln . Table 8.1: Snippet of Traffic Dataset TUNNEL DAY VOLUME_PER_MINUTE 834 Holland weekday 59 1394 Holland weekend 71 1329 Holland weekend 45 1653 Lincoln weekday 74 1668 Lincoln weekday 75 398 Holland weekday 47 423 Holland weekday 44 1348 Holland weekend 61 2393 Lincoln weekday 75 661 Holland weekday 55 Thus stating out Null Hypothesis and Alternate Hypothesis. Null Hypothesis H0: Traffic in Lincoln is same as Traffic in Holland tunnel. Alternate Hypothesis H1: Traffic in Lincoln is higher than traffic in Holland tunnel. Once we have stated our hypothesis, lets see the z-test in practice. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YXNldFxuVFJBRkZJQzwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9UUkFGRklDLmNzdicpXG5zdW1tYXJ5KFRSQUZGSUMpICNnaXZlcyB1cyB0aGUgZGF0YSBzdGF0aXN0aWNzXG5cbiNkYXRhIGNsZWFuIGFuZCBzdWJzZXRcbmxpbmNvbG4uZGF0YSA8LSBzdWJzZXQoVFJBRkZJQywgVFJBRkZJQyRUVU5ORUwgPT0gXCJMaW5jb2xuXCIpXG5ob2xsYW5kLmRhdGEgPC0gc3Vic2V0KFRSQUZGSUMsIFRSQUZGSUMkVFVOTkVMID09IFwiSG9sbGFuZFwiKVxuXG4jIHRyYWZmaWMgYXQgbGluY29sblxuIyBUaGlzIHZhcmlhYmxlIGlzIGEgY29sdW1uIG9mIDE0MDEgcm93cy5cbmxpbmNvbG4udHJhZmZpYyA8LSBsaW5jb2xuLmRhdGEkVk9MVU1FX1BFUl9NSU5VVEVcblxuIyB0cmFmZmljIGF0IGhvbGxhbmRcbiMgVGhpcyB2YXJpYWJsZSBpcyBhIGNvbHVtbiBvZiAxNDAxIHJvd3MuXG5ob2xsYW5kLnRyYWZmaWMgPC0gaG9sbGFuZC5kYXRhJFZPTFVNRV9QRVJfTUlOVVRFXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuIyBUaGUgZmluYWwgdmFsdWUgaXMgdGhlIHN0YW5kYXJkIGRldmlhdGlvbiwgaW4gVm9sdW1lIHBlciBtaW51dGUuXG5zZC5saW5jb2xuIDwtIHNkKGxpbmNvbG4udHJhZmZpYylcbnNkLmhvbGxhbmQgPC0gc2QoaG9sbGFuZC50cmFmZmljKVxuXG4jIG1lYW5zIG9mIHR3byBzYW1wbGVzXG5tZWFuLmxpbmNvbG4gPC0gbWVhbihsaW5jb2xuLnRyYWZmaWMpXG5tZWFuLmhvbGxhbmQgPC0gbWVhbihob2xsYW5kLnRyYWZmaWMpXG5cbiMgbGVuZ3RoIG9mIGxpbmNvbG4gYW5kIGhvbGxhbmRcbmxlbl9saW5jb2xuIDwtIGxlbmd0aChsaW5jb2xuLnRyYWZmaWMpXG5sZW5faG9sbGFuZCA8LSBsZW5ndGgoaG9sbGFuZC50cmFmZmljKVxuXG4jIHN0YW5kYXJkIGRldmlhdGlvbiBvZiB0cmFmZmljXG5zZC5saW4uaG9sIDwtIHNxcnQoc2QubGluY29sbl4yL2xlbl9saW5jb2xuICsgc2QuaG9sbGFuZF4yL2xlbl9ob2xsYW5kKVxuXG4jIHogc2NvcmVcbnpldGEgPC0gKG1lYW4ubGluY29sbiAtIG1lYW4uaG9sbGFuZCkvc2QubGluLmhvbFxuemV0YVxuXG4jIGdldCBwXG5wID0gcG5vcm0oLXpldGEpXG5wXG5cbiMgcGxvdCB0aGUgemV0YSB2YWx1ZSBvbiB0aGUgbm9ybWFsIGRpc3RyaWJ1dGlvbiBjdXJ2ZS5cbnBsb3QoeD1zZXEoZnJvbSA9IC0yNSwgdG89IDI1LCBieT0wLjEpLHk9ZG5vcm0oc2VxKGZyb20gPSAtMjUsIHRvPSAyNSwgIGJ5PTAuMSksbWVhbj0wKSx0eXBlPSdsJyx4bGFiID0gJ21lYW4gZGlmZmVyZW5jZScsICB5bGFiPSdwb3NzaWJpbGl0eScpXG5hYmxpbmUodj16ZXRhLCBjb2w9J3JlZCcpIn0= We can see that form the P-Value obtained is near to 0, which is less than 0.05. Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that traffic in Lincoln is higher than traffic Holland. 8.1.3 Z-test Example 2 (Left Sided) Now lets look at another example to use this z-test for hypothesis testing. We will study the example to statistically find the relation between capital gains of people with two Zodiac Signs , namely Aquarius and Libra. Table 8.2: Snippet of Zodiac Dataset AGE STATUS EDUCATION YEARS PROFESSION CAPITALGAINS CAPITALLOSS NATIVE ZODIAK 27988 61 Self-emp-inc HS-grad 9 Exec-managerial 2829 0 United-States Scorpio 18550 43 Private Bachelors 13 Exec-managerial 0 13905 United-States Leo 16742 41 Private HS-grad 9 Sales 0 0 United-States Cancer 17779 42 Private HS-grad 9 Machine-op-inspct 0 0 United-States Leo 246 21 Private HS-grad 9 Other-service 0 0 United-States Gemini 1873 23 Private Some-college 10 Sales 0 0 United-States Leo 9754 32 Private Some-college 10 Tech-support 0 0 United-States Taurus 10574 33 Self-emp-not-inc HS-grad 9 Craft-repair 0 0 United-States Taurus 4307 26 Private HS-grad 9 Exec-managerial 0 0 United-States Leo 12378 35 Private HS-grad 9 Craft-repair 0 0 United-States Taurus Now stating out Null Hypothesis and Alternate Hypothesis. Null Hypothesis H0: Capital Gains of people with Aquarius is same as people with Libra zodiac sign. Alternate Hypothesis H1: Capital Gains of people with Aquarius is lower than as people with Libra zodiac sign. Once we have stated our hypothesis, lets see the z-test in practice. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YXNldFxuWm9kaWFjRGF0YTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9ab2RpYWNDaGFsbGVuZ2UuY3N2JylcbnN1bW1hcnkoWm9kaWFjRGF0YSkgI2dpdmVzIHVzIHRoZSBkYXRhIHN0YXRpc3RpY3NcblxuI2RhdGEgY2xlYW4gYW5kIHN1YnNldFxuQXF1YXJpdXMuZGF0YSA8LSBzdWJzZXQoWm9kaWFjRGF0YSwgWm9kaWFjRGF0YSRaT0RJQUsgPT0gXCJBcXVhcml1c1wiKVxuTGlicmEuZGF0YSA8LSBzdWJzZXQoWm9kaWFjRGF0YSwgWm9kaWFjRGF0YSRaT0RJQUsgPT0gXCJMaWJyYVwiKVxuXG4jIFpvZGlhYyBBcXVhcml1c1xuQXF1YXJpdXMuWm9kaWFjIDwtIEFxdWFyaXVzLmRhdGEkQ0FQSVRBTEdBSU5TXG5cbiMgWm9kaWFjICBMaWJyYVxuTGlicmEuWm9kaWFjIDwtIExpYnJhLmRhdGEkQ0FQSVRBTEdBSU5TXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuc2QuQXF1YXJpdXMgPC0gc2QoQXF1YXJpdXMuWm9kaWFjKVxuc2QuTGlicmEgPC0gc2QoTGlicmEuWm9kaWFjKVxuXG4jIG1lYW5zIG9mIHR3byBzYW1wbGVzXG5tZWFuLkFxdWFyaXVzIDwtIG1lYW4oQXF1YXJpdXMuWm9kaWFjKVxubWVhbi5MaWJyYSA8LSBtZWFuKExpYnJhLlpvZGlhYylcblxuIyBsZW5ndGggb2YgQXF1YXJpdXMgYW5kIExpYnJhXG5sZW5fQXF1YXJpdXMgPC0gbGVuZ3RoKEFxdWFyaXVzLlpvZGlhYylcbmxlbl9MaWJyYSA8LSBsZW5ndGgoTGlicmEuWm9kaWFjKVxuXG4jIHN0YW5kYXJkIGRldmlhdGlvblxuc2QuYXF1LmxpYiA8LSBzcXJ0KHNkLkFxdWFyaXVzXjIvbGVuX0FxdWFyaXVzICsgc2QuTGlicmFeMi9sZW5fTGlicmEpXG5cbiMgeiBzY29yZVxuemV0YSA8LSAobWVhbi5BcXVhcml1cyAtIG1lYW4uTGlicmEpL3NkLmFxdS5saWJcbnpldGFcblxuIyBnZXQgcFxucCA9IHBub3JtKHpldGEpXG5wXG5cbiMgcGxvdCB0aGUgemV0YSB2YWx1ZSBvbiB0aGUgbm9ybWFsIGRpc3RyaWJ1dGlvbiBjdXJ2ZS5cbnBsb3QoeD1zZXEoZnJvbSA9IC0yNSwgdG89IDI1LCBieT0wLjEpLHk9ZG5vcm0oc2VxKGZyb20gPSAtMjUsIHRvPSAyNSwgIGJ5PTAuMSksbWVhbj0wKSx0eXBlPSdsJyx4bGFiID0gJ21lYW4gZGlmZmVyZW5jZScsICB5bGFiPSdwb3NzaWJpbGl0eScpXG5hYmxpbmUodj16ZXRhLCBjb2w9J3JlZCcpIn0= We can see that form the P-Value obtained is less than 0.05. Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that Capital Gains of people with Aquarius is lower than as people with Libra zodiac sign. 8.1.4 Z-test Example 3 (Two Tailed) We will study the example to statistically find the relation between capital gains of people with two Countries, namely US and Columbia. Now stating out Null Hypothesis and Alternate Hypothesis. Null Hypothesis H0: Capital Gains of people of United States is same as people of Colombia. Alternate Hypothesis H1: Capital Gains of people of United States is not equal to that of the people of Colombia. Once we have stated our hypothesis, lets see the z-test in practice. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YXNldFxuWm9kaWFjRGF0YTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9ab2RpYWNDaGFsbGVuZ2UuY3N2JylcbnN1bW1hcnkoWm9kaWFjRGF0YSkgI2dpdmVzIHVzIHRoZSBkYXRhIHN0YXRpc3RpY3NcblxuI2RhdGEgY2xlYW4gYW5kIHN1YnNldFxuVVMuZGF0YSA8LSBzdWJzZXQoWm9kaWFjRGF0YSwgWm9kaWFjRGF0YSROQVRJVkUgPT0gXCJVbml0ZWQtU3RhdGVzXCIpXG5Db2x1bWJpYS5kYXRhIDwtIHN1YnNldChab2RpYWNEYXRhLCBab2RpYWNEYXRhJE5BVElWRSA9PSBcIkNvbHVtYmlhXCIpXG5cbiMgQ291bnRyeSBVU1xuVVMuY291bnRyeSA8LSBVUy5kYXRhJENBUElUQUxHQUlOU1xuXG4jIENvdW50cnkgIENvbHVtYmlhXG5Db2x1bWJpYS5jb3VudHJ5IDwtIENvbHVtYmlhLmRhdGEkQ0FQSVRBTEdBSU5TXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuc2QuVVMgPC0gc2QoVVMuY291bnRyeSlcbnNkLkNvbHVtYmlhIDwtIHNkKENvbHVtYmlhLmNvdW50cnkpXG5cbiMgbWVhbnMgb2YgdHdvIHNhbXBsZXNcbm1lYW4uVVMgPC0gbWVhbihVUy5jb3VudHJ5KVxubWVhbi5Db2x1bWJpYSA8LSBtZWFuKENvbHVtYmlhLmNvdW50cnkpXG5cbiMgbGVuZ3RoIG9mIFVTIGFuZCBDb2x1bWJpYVxubGVuX1VTIDwtIGxlbmd0aChVUy5jb3VudHJ5KVxubGVuX0NvbHVtYmlhIDwtIGxlbmd0aChDb2x1bWJpYS5jb3VudHJ5KVxuXG4jIHN0YW5kYXJkIGRldmlhdGlvblxuc2QudXMuY29sIDwtIHNxcnQoc2QuVVNeMi9sZW5fVVMgKyBzZC5Db2x1bWJpYV4yL2xlbl9Db2x1bWJpYSlcblxuIyB6IHNjb3JlXG56ZXRhIDwtIChtZWFuLlVTIC0gbWVhbi5Db2x1bWJpYSkvc2QudXMuY29sXG56ZXRhXG5cbiMgZ2V0IHBcbnAgPSAyKnBub3JtKC16ZXRhKVxucFxuXG4jIHBsb3QgdGhlIHpldGEgdmFsdWUgb24gdGhlIG5vcm1hbCBkaXN0cmlidXRpb24gY3VydmUuXG5wbG90KHg9c2VxKGZyb20gPSAtMjUsIHRvPSAyNSwgYnk9MC4xKSx5PWRub3JtKHNlcShmcm9tID0gLTI1LCB0bz0gMjUsICBieT0wLjEpLG1lYW49MCksdHlwZT0nbCcseGxhYiA9ICdtZWFuIGRpZmZlcmVuY2UnLCAgeWxhYj0ncG9zc2liaWxpdHknKVxuYWJsaW5lKHY9emV0YSwgY29sPSdyZWQnKSJ9 We can see that form the P-Value obtained is less than 0.05. Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that Capital Gains of people of United States is not equal to that of the people of Colombia. 8.2 Permutation Test Permutation test allows us to observe randomness directly, with naked eye, without the lenses of statistical tests such as z-tests etc. We shuffle data randomly like a deck of cards. There may be many such shuffles - 10,000, 100,000 etc. The goal is always to see how often we can obtained the observed difference of means (since we are testing either one sided or two sided hypothesis), by purely random shuffles of our data. These permutations (shuffles) destroy all relationships which may pre-exist in our data. We are hoping to show that our observed difference of means can be obtained very rarely in completely random fashion. Then we “experimentally” show that our result is unlikely to randomly occur under null hypothesis. Then we can reject the null hypothesis. The less often our result appear in the histogram of permutation test results, the better the news for our alternative hypothesis. What is surprising to many newcomers, is that permutation test will give different p-values (not dramatically different, but still different) in each run of permutation test. This is the case because permutation test in random itself. It is not like z-test which will give the same result when run again for the same hypothesis and same data set. Also p-value computed by permutation test will be, in general, different than p-value computed by z-test. Not very different but different. Again, it is the case because permutation test provides only approximation of p-value. Great advantage of permutation test is that it is universal and robust. One can test different relationships between two variables than just difference of means. For example we can use permutation test to validate whether traffic in Lincoln tunnel is more than twice the traffic in Holland tunnel or even provide different weights for different days of the week. 8.2.1 Permutation Test One Step Permutation test in one step is the most direct way to see randomness close by. One step permutation function shows one single data shuffle. By shuffling the data one destroys associations which exist between values of the data frame. This make data frame random. You can execute the one step permutation multiple times. This will show how data frame varies and how does it affect the observed difference of means. Apply one step permutation function first, multiple times before you move to the proper Permutation test function. One of the parameters of the Permutation test function specifies the number of “shuffles” which will be preformed. This could be a very large number, 10,000 or even 100,000. The purpose of making so many random permutations is to test how often observed difference of means can arise in just random data. The more often this takes place, the more likely you observation is just random. To reject the null hypothesis you need to show that the observed difference of means will come very infrequently in permutation test. Less than 5% of the time, to be exact. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKSIsInNhbXBsZSI6InN1bW1hcnkodHJhZmZpYylcbkQ8LSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdIb2xsYW5kJywzXSkgLSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywzXSlcbm51bGxfdHVubmVsIDwtIHJlcChcIkhvbGxhbmRcIiwyODAxKSAjIENyZWF0ZSAyODAxIGNvcGllcyBvZiBIb2xsYW5kIFxubnVsbF90dW5uZWxbc2FtcGxlKDI4MDEsMTQwMCldIDwtIFwiTGluY29sblwiICMgUmVwbGFjZSBSQU5ET01MWSAxNDAwIGNvcGllcyB3aXRoIExpbmNvbG5cbm51bGwgPC0gZGF0YS5mcmFtZShudWxsX3R1bm5lbCx0cmFmZmljWywzXSlcbm5hbWVzKG51bGwpIDwtIGMoXCJUVU5ORUxcIixcIlZPTFVNRV9QRVJfTUlOVVRFXCIpXG5zdW1tYXJ5KG51bGwpXG5ob2xsYW5kX251bGwgPC0gbnVsbFtudWxsJFRVTk5FTCA9PSBcIkhvbGxhbmRcIiwyXVxubGluY29sbl9udWxsIDwtIG51bGxbbnVsbCRUVU5ORUwgPT0gXCJMaW5jb2xuXCIsMl1cbm1lYW4oaG9sbGFuZF9udWxsKVxubWVhbihsaW5jb2xuX251bGwpXG5EX251bGwgPC0gbWVhbihsaW5jb2xuX251bGwpIC0gbWVhbihob2xsYW5kX251bGwpXG5jYXQoXCJUaGUgbWVhbiBkaWZmZXJlbmNlIG9mIHBlcm11dGF0aW9uIG9uZSBzdGVwIGRhdGE6IFwiLCBEX251bGwsXCJcXG5cIikjIENhbGN1bGF0ZSB0aGUgZGlmZmVyZW5jZSBiZXR3ZWVuIHRoZSBtZWFuIG9mIHRoZSByYW5kb20gZGF0YS5cbmNhdChcIlRoZSBtZWFuIGRpZmZlcmVuY2Ugb2Ygb3JpZ2luYWwgZGF0YTogXCIsIEQpICMgRGlmZmVyZW5jZSBvZiBtZWFuIHZhbHVlIG9mIG9yaWdpbmFsIGRhdGEuIn0= 8.2.2 Permutation Function The permutation function is used to run multiple iteration of the one-step permutation studied above, to get a complete relational understanding between the components involved in any hypothesis. Here you can run the example of running the permutation test on the Traffic.csv dataset, on volume of traffic in Holland and Lincoln Tunnel. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKVxuUGVybXV0YXRpb24gPC0gZnVuY3Rpb24oZGYxLGMxLGMyLG4sdzEsdzIpe1xuICBkZiA8LSBhcy5kYXRhLmZyYW1lKGRmMSlcbiAgRF9udWxsPC1jKClcbiAgVjE8LWRmWyxjMV1cbiAgVjI8LWRmWyxjMl1cbiAgc3ViLnZhbHVlMSA8LSBkZltkZlssIGMxXSA9PSB3MSwgYzJdXG4gIHN1Yi52YWx1ZTIgPC0gZGZbZGZbLCBjMV0gPT0gdzIsIGMyXVxuICBEIDwtICBhYnMobWVhbihzdWIudmFsdWUyLCBuYS5ybT1UUlVFKSAtIG1lYW4oc3ViLnZhbHVlMSwgbmEucm09VFJVRSkpXG4gIG09bGVuZ3RoKFYxKVxuICBsPWxlbmd0aChWMVtWMT09dzJdKVxuICBmb3IoamogaW4gMTpuKXtcbiAgICBudWxsIDwtIHJlcCh3MSxsZW5ndGgoVjEpKVxuICAgIG51bGxbc2FtcGxlKG0sbCldIDwtIHcyXG4gICAgbmYgPC0gZGF0YS5mcmFtZShLZXk9bnVsbCwgVmFsdWU9VjIpXG4gICAgbmFtZXMobmYpIDwtIGMoXCJLZXlcIixcIlZhbHVlXCIpXG4gICAgdzFfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzEsMl1cbiAgICB3Ml9udWxsIDwtIG5mW25mJEtleSA9PSB3MiwyXVxuICAgIERfbnVsbCA8LSBjKERfbnVsbCxtZWFuKHcyX251bGwsIG5hLnJtPVRSVUUpIC0gbWVhbih3MV9udWxsLCBuYS5ybT1UUlVFKSlcbiAgfVxuICBteWhpc3Q8LWhpc3QoRF9udWxsLCBwcm9iPVRSVUUpXG4gIG11bHRpcGxpZXIgPC0gbXloaXN0JGNvdW50cyAvIG15aGlzdCRkZW5zaXR5XG4gIG15ZGVuc2l0eSA8LSBkZW5zaXR5KERfbnVsbCwgYWRqdXN0PTIpXG4gIG15ZGVuc2l0eSR5IDwtIG15ZGVuc2l0eSR5ICogbXVsdGlwbGllclsxXVxuICBwbG90KG15aGlzdClcbiAgbGluZXMobXlkZW5zaXR5LCBjb2w9J2JsdWUnKVxuICBhYmxpbmUodj1ELCBjb2w9J3JlZCcpXG4gIE08LW1lYW4oRF9udWxsPkQpXG4gIHJldHVybihNKVxufSIsInNhbXBsZSI6IlBlcm11dGF0aW9uKHRyYWZmaWMsIFwiVFVOTkVMXCIsIFwiVk9MVU1FX1BFUl9NSU5VVEVcIiwxMDAwLFwiSG9sbGFuZFwiLCBcIkxpbmNvbG5cIikifQ== Note: You can find the permutation function code here: Permutation() NOTE: The red line in the output plots of the permutation test function is not the p-value, but it is just the difference of the value of means of the two categories under test. 8.2.3 Exercise - How p-value is affected by difference of means and standard deviations Here, you can generate your own data by changing parameters of the rnorm() function. See how changing the mean and sd in rnorm distributions affects the p-value! Again you can do it directly in the code and observe the results immediately. It is very revealing.. Think of Val1, and Val2 as traffic volumes in Holland and Lincoln tunnels respectively. The larger the difference between the means of rnorm() function the smaller the p-value - since it is less and less likely that observed difference of means would come frequently, due to random shuffles of permutation function. Now keep the same means and change the variances. See how changing the variances in rnorm() will affect the p-value and try to explain the effect that standard deviations have on the p-value. In general, the higher the standard deviation, the more widely data is centered around the mean. Thus even for the same two means, and two different value of deviations, we can see larger value of deviation to lead to higher p-value. Since we are less certain of the role of the “mean” if standard deviation is higher. Therefore, the chance of randomly obtaining the observed result, is higher. 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 8.3 Multiple Hypothesis - Bonferroni Correction. While dealing with the dataset with several number of dimensions, it is possible to get a lot of amazing and interesting insights and conclusions from it. But, unfortunately, sometimes a lot of the data included in case of such large dataset, might be junk. We can make multiple assumptions from such data. But, while doing so, we may consider some useless data/patterns that might hamper our results and lead to the pitfall of believing in hypotheses, that are not actually true. This is common when performing multiple hypothesis testing. Multiple hypothesis testing refers to any instance that involves the simultaneous testing of more than one hypothesis. Let’s consider the example of Traffic dataset. We have given two tunnels ”Holland” and “Lincoln”, but what if we were given all the tunnels in the US? We can make a lot of hypotheses in that case. And for each set of hypothesis, would you still consider the value of α as 0.05 as the cut-off for P-value? It may seem to be a good idea to just go and check the p-value for any set of hypotheses with the cut-off value of \\(\\alpha\\) as 0.05. But this might not give you the correct answer always. If you have 100 different hypotheses to consider in the data, then the probability of getting at least one significant result with \\(\\alpha = 0.05\\) will be, \\[P(\\text{at least one significant result}) = 1- (1-0.05)^{100} ≈ 0.99\\] This means that if we consider 0.05 as our cut-off value, then the probability of getting at least one significant result will be about 99%, which leads to overfitting of data and it clearly doesn’t give us proper idea about our hypothesis. Methods for dealing with multiple testing frequently call for adjusting \\(\\alpha\\) in some way, so that the probability of observing at least one significant result due to chance remains below your desired significance level. One such method for adjusting \\(\\alpha\\) is BONFERRONI CORRECTION! The Bonferroni correction sets the significance cut-off at \\(\\alpha / N\\) where N is the number of possible hypotheses. For example, in the example above, with 100 tests and \\(\\alpha = 0.05\\), you’d only reject a null hypothesis if the p-value is less than \\(\\alpha/N = 0.05/100 = 0.0005\\) Thus, the value of \\(\\alpha\\) after Bonferroni correction would be \\(0.0005\\). Again, let’s calculate the probability of observing at least one significant result when using the correction just described: \\[P(\\text{at least one significant result}) = 1 − P(\\text{no significant results}) \\\\ = 1 − (1 − 0.0005)^{100} ≈ 0.048\\] This gives us 4.8% probability of getting at least one significant result. As we can see this value of probability using Bonferroni correction is much better than the 99% which we saw before when we did not use correction for performing multiple hypothesis testing. But there are some downfall of using Bonferroni correction too. (Although for the scope of this course Bonferroni Correction works fine.) The Bonferroni correction tends to be a bit too conservative. Also, we benefit here from assuming that all tests are independent of each other. In practical applications, that is often not the case. Depending on the correlation structure of the tests, the Bonferroni correction could beextremely conservative, leading to a high rate of false negatives. 8.3.1 Examples for Multiple hypothesis testing. Let’s consider the Happiness dataset as an example. Table 8.3: Snippet of Happiness Dataset IDN AGE COUNTRY GENDER IMMIGRANT INCOME HAPPINESS 4658 45506 46 Mauritania Male 0 60160 5.50 2030 55952 43 Kyrgyzstan Female 0 70893 5.47 4232 41463 64 Tanzania Male 0 55170 4.67 5198 43857 46 Vietnam Male 1 59841 9.37 1621 75249 61 Belarus Male 1 91394 7.83 4573 62002 56 Poland Female 1 77943 8.43 6390 18042 49 Mauritius Female 0 33528 2.50 5226 28619 21 Mali Female 0 42594 3.23 476 88429 56 Montenegro Male 0 103580 8.62 2789 69660 54 Malaysia Male 0 85449 7.59 There are 156 unique countries in the dataset. This can be checked using the unique() function – unique(indiv_happiness$country) Since there are 156 distinct countries, we have \\({{n}\\choose{2}} = {156\\choose2}=(156 * 155)/2 = 12090\\) different hypotheses. Let’s call this value N. Using this N, the P-value cutoff after Bonferroni correction will be, \\(α = 0.05 / 12090 ≈ 4.13 *10^{-6}\\) 8.3.1.1 Example 1 Let’s calculate the P-value for the following hypotheses from the dataset. Our hypothesis: People from Canada are happier than people from Iceland. Null hypothesis: There is no difference in happiness levels of people from Canada and people from Iceland. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgZGF0YXNldFxuaGFwcGluZXNzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L0hBUFBJTkVTUzIwMTcuY3N2XCIsIHN0cmluZ3NBc0ZhY3RvcnMgPSBUKSAjd2ViIGxvYWRcblxuIyBUd28gc3Vic2V0cyBvZiBDYW5hZGEgYW5kIEljZWxhbmQgXG5oYXBwaW5lc3MuY2FuYWRhIDwtIHN1YnNldChoYXBwaW5lc3MkSEFQUElORVNTLCBoYXBwaW5lc3MkQ09VTlRSWSA9PVwiQ2FuYWRhXCIpXG5oYXBwaW5lc3MuaWNlbGFuZCA8LSBzdWJzZXQoaGFwcGluZXNzJEhBUFBJTkVTUywgaGFwcGluZXNzJENPVU5UUlkgPT0gXCJJY2VsYW5kXCIpXG5cbiMgTWVhbiBvZiBzdWJzZXRzLlxubWVhbi5jYW5hZGEgPC0gbWVhbihoYXBwaW5lc3MuY2FuYWRhKVxubWVhbi5pY2VsYW5kIDwtIG1lYW4oaGFwcGluZXNzLmljZWxhbmQpXG5cbm1lYW4uY2FuYWRhXG5tZWFuLmljZWxhbmRcblxuIyBMZW5ndGggb2Ygc3Vic2V0c1xubGVuLmNhbmFkYSA8LSBsZW5ndGgoaGFwcGluZXNzLmNhbmFkYSlcbmxlbi5pY2VsYW5kIDwtIGxlbmd0aChoYXBwaW5lc3MuaWNlbGFuZClcblxuIyBTdGFuZGFyZCBEZXZpYXRpb24gb2YgU3Vic2V0cy5cbnNkLmNhbmFkYSA8LSBzZChoYXBwaW5lc3MuY2FuYWRhKVxuc2QuaWNlbGFuZCA8LSBzZChoYXBwaW5lc3MuaWNlbGFuZClcblxuIyBDYWxjdWxhdGluZyBaLXNjb3JlIFxuemV0YSA8LSAobWVhbi5jYW5hZGEgLSBtZWFuLmljZWxhbmQpLyBzcXJ0KChzZC5jYW5hZGFeMikvbGVuLmNhbmFkYSArIChzZC5pY2VsYW5kXjIpL2xlbi5pY2VsYW5kKVxuemV0YVxuXG4jIENhbGN1bGF0ZSBwLXZhbHVlIGZyb20gWi1zY29yZVxucF92YWx1ZSA8LSBwbm9ybSgtemV0YSlcbnBfdmFsdWUifQ== In this case, after applying Bonferroni Correction we get the value of \\(α = 0.05/12090 ≈ 4.14 * 10^{-06}\\) Here, we get the p-value of 0.25 which is much higher than the value of our α. Based on this we fail reject our null hypothesis. 8.3.1.2 Example 2 Let’s consider the following hypotheses from the dataset. Our hypothesis: People from Italy are happier than people from Afghanistan. Null hypothesis: There is no difference in happiness levels of people from Italy and people from Afghanistan. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgZGF0YXNldFxuaGFwcGluZXNzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L0hBUFBJTkVTUzIwMTcuY3N2XCIsIHN0cmluZ3NBc0ZhY3RvcnMgPSBUKSAjd2ViIGxvYWRcblxuIyBUd28gc3Vic2V0cyBvZiBJdGFseSBhbmQgQWZnaGFuaXN0YW4gXG5oYXBwaW5lc3MuaXRhbHkgPC0gc3Vic2V0KGhhcHBpbmVzcyRIQVBQSU5FU1MsIGhhcHBpbmVzcyRDT1VOVFJZID09XCJJdGFseVwiKVxuaGFwcGluZXNzLmFmZ2hhbmlzdGFuIDwtIHN1YnNldChoYXBwaW5lc3MkSEFQUElORVNTLCBoYXBwaW5lc3MkQ09VTlRSWSA9PSBcIkFmZ2hhbmlzdGFuXCIpXG5cbiMgTWVhbiBvZiBzdWJzZXRzLlxubWVhbi5pdGFseSA8LSBtZWFuKGhhcHBpbmVzcy5pdGFseSlcbm1lYW4uYWZnaGFuaXN0YW4gPC0gbWVhbihoYXBwaW5lc3MuYWZnaGFuaXN0YW4pXG5cbm1lYW4uaXRhbHlcbm1lYW4uYWZnaGFuaXN0YW5cblxuIyBMZW5ndGggb2Ygc3Vic2V0c1xubGVuLml0YWx5IDwtIGxlbmd0aChoYXBwaW5lc3MuaXRhbHkpXG5sZW4uYWZnaGFuaXN0YW4gPC0gbGVuZ3RoKGhhcHBpbmVzcy5hZmdoYW5pc3RhbilcblxuIyBTdGFuZGFyZCBEZXZpYXRpb24gb2YgU3Vic2V0cy5cbnNkLml0YWx5IDwtIHNkKGhhcHBpbmVzcy5pdGFseSlcbnNkLmFmZ2hhbmlzdGFuIDwtIHNkKGhhcHBpbmVzcy5hZmdoYW5pc3RhbilcblxuIyBDYWxjdWxhdGluZyBaLXNjb3JlIFxuemV0YSA8LSAobWVhbi5pdGFseSAtIG1lYW4uYWZnaGFuaXN0YW4pLyBzcXJ0KChzZC5pdGFseV4yKS9sZW4uaXRhbHkgKyAoc2QuYWZnaGFuaXN0YW5eMikvbGVuLmFmZ2hhbmlzdGFuKVxuemV0YVxuXG4jIENhbGN1bGF0ZSBwLXZhbHVlIGZyb20gWi1zY29yZVxucF92YWx1ZSA8LSBwbm9ybSgtemV0YSlcbnBfdmFsdWUifQ== In this case, after applying Bonferroni Correction we get the value of \\(α = 0.05/12090 ≈ 4.14 * 10^{-06}\\) Here, we get the p-value of 0.00364 which is lower than the value of default p-value cutoff \\(α = 0.05\\), but this obtained p-value is higher than our Bonferroni correction cutoff. So, based on the results, we fail to reject our null hypothesis even though the obtained p-value is less than 0.05. EOC "]]
