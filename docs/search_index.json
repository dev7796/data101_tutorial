[["intro.html", "Section: 1 Introduction", " Section: 1 Introduction The objective of this textbook is to provide you with the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in a statistically convincing manner, perhaps in a colorful and visually appealing way. Questions which you will have to anticipate and you will have to answer are - How do you know that your findings are not random? - And fundamental of all questions: - So what? Even the most impressing looking results may come up randomly. And you will be asked this question along with the question ‚Äúwhat was your p-value and how did you compute it‚Äù And even if you convince your audience that your results are not random, you will have to be ready to explain why your audience should care about the results you reported. In other words, is there any actionable value in your results? Or they are just simply interesting, good to know, but no one really needs to care much about them otherwise? Hopefully it is the former not the latter. In the following sections we will address these questions and go through the process of data exploration, validation, and presentation. We will start with making plots, follow with free style data exploration ‚Äì which allows us to form the leads, that is hypotheses. Then we will follow with simple statistical tests which will allow us to validate these hypothesis and defend our findings against randomness claims. - We will learn how to calculate p-values and how to use them to defend our findings. We will use as few R commands as possible and reach our goal in the shortest possible path. In fact we will demonstrate how using just 7 R commands we can perform quite sophisticated data exploration. In the appendix, we show many more useful commands of R which eventually you would have to use. However, our goal in this short textbook, is to present the shortest path to data analysis which will let you import the data, plot it, make some analysis yourself and use R-libraries. In this textbook and in this class we do not teach how to clean the data (data wrangling) and how to deal with a wide variety of data types. We also do not address complex data transformations such as multi-frame operations like merge (we show them in appendix). We also do not explain how different machine learning methods work, we only show you how to use them. It is similar to teaching one how to drive a car without knowing how a car engine works. "],["b2022.html", "Section: 2 üîñ Best Works of 2022 2.1 DataBlog 2.2 Prediction Challenge 1 2.3 Prediction Challenge 2 2.4 Prediction Challenge 3", " Section: 2 üîñ Best Works of 2022 2.1 DataBlog Ella Walmsley 2.2 Prediction Challenge 1 Upsham Naik Jeevanandan Ramasamy 2.3 Prediction Challenge 2 Jeevanandan Ramasamy 2.4 Prediction Challenge 3 Eva Zhang "],["DLL.html", "Section: 3 Data League Leaderboard", " Section: 3 Data League Leaderboard Table 3.1: Leaderboard 2022 Rank Participant.Name 1 Jeevanandan Ramasamy 2 George Basta 3 Joyce Huang 4 Jiaxu Hu 5 Dhiren Patel 6 Chicheng Shao 7 Cheyenne Pourkay 8 Christopher Nguyen 9 Aaron Mok 10 Ethan Matta Honourable Mentions: Upsham Naik, Joshua B. Sze, Kirtan Patel, Maria Xu, Devam Patel, Eva Zhang, Toshanraju Vysyaraju, Maanas Pimplikar, Jared Chiou, Nitya Narayanan, Shrish Vellore, Yousra Belgaid, Mitali Shroff, Michael Jucan, Jackie Hong, Arvin Sung, Eric Xuan, Eva Allred, Leah Ranavat, Nami Jain, Gautam Agarwal, Aditya Patil "],["data-puzzles-secrets.html", "Section: 4 üîñ Data puzzles secrets 4.1 Moody Data Puzzle 4.2 Movies Data Hunt 4.3 Minimarket Data Hunt 4.4 Predicting grades in Professor Moody‚Äôs class", " Section: 4 üîñ Data puzzles secrets Lecture slides: Data Exploration 4.1 Moody Data Puzzle Table 4.1: Snippet of Moody Dataset SCORE GRADE DOZES_OFF TEXTING_IN_CLASS PARTICIPATION 21.33 F never never 0.29 71.57 C always rarely 0.11 90.11 A always never 0.26 31.52 D sometimes rarely 0.03 95.94 A always rarely 0.21 Moody Data Puzzle is our first example of a data puzzle. By data puzzles we mean synthetically generated data sets which have some embedded patterns. Your goal is to find the embedded pattern(s). You may also find patterns similar (implied) by patterns embedded in the data puzzle. This is fine too. The goal of data puzzles is to excite you about exploratory data analysis. In many ways it is like a game. Puzzle description: Professor Moody has been teaching statistics 101 class for many years. His teaching evaluations went considerably south with the chief complaint: he DOES NOT seem to assign grades fairly. Students compared their scores among themselves and found quite a bit of discrepancies! But their complaints went nowhere since Professor promptly disappeared after posting the final grades and scores. A new brave TA, managed to get hold of the carefully maintained grading table (spanning multiple years) of professor Moody by ‚Ä¶.messing a bit with Moody‚Äôs computer‚Ä¶.well, let‚Äôs not explain the details because he would get in trouble. What he found out was a remarkably structured account of how professor Moody assigns his grades. Looks like Professor Moody is in fact very alert in class. He is aware of what students do, detecting texting during class and remembering exactly who was dozed off in class. He also keeps the mysterious ‚Äúparticipation index‚Äù which is a numerical score from 0 to 1. This is probably related to questions asked and answered by students as well as their general attentiveness in class. Remarkable but a little creepy, isn‚Äôt it? What is the best advice the new TA, can give future students how to get a good grade in Professor Moody‚Äôs class? What factors influence the grade besides the score? Back your recommendation up with plots and evidence from the attached data. What are examples of patterns we are looking for here? Here are some: ‚ÄúStudents who text a lot‚Äù have lower chance to get an A in the class‚Äù ‚ÄúStudents whose participation is lower than 0.25 fail the class more often‚Äù ‚ÄúDozing off does not matter if your score is more than 90, you still get an A‚Äù ‚ÄúIf you score is less than 30, you fail the class regardless of what your other attributes are‚Äù 4.1.1 Secrets Revealed- Patterns in Professor Moody‚Äôs data? My Patterns Many student solutions falsely attribute higher grades to higher values of participation attribute.. This is a classic example of a hidden variable described in the reference attached below. The truth is that participation attribute value impacts the score attribute value. Generally, the higher the participation in Moody‚Äôs class, the higher the score. But it is the score attribute which has a direct impact on the grade. Thus, it is the score which is the real ‚Äúhidden variable‚Äù impacting the final grade. Thus, the score already reflects participation. Professor Moody seemed to look only at texting and dozing off attributes in grade determination (see the power points above with the explanation) Compare with examples of hidden variables in the following reference about correlation and causation. https://www.stewartmath.com/precalc_7e_dp/precalc_7e_dp6.html 4.1.2 Best Student‚Äôs Submissions 2022 Lauretta Martin Sanjaya Budhathoki Sandhya Senthilkumar 4.2 Movies Data Hunt Table 4.2: Snippet of Movies Dataset country content imdb_score Gross Budget genre 11360 USA PG 6.04 Medium Low History 6449 France R 6.27 Low Medium Comedy 7430 USA PG-13 8.09 High Medium Action 4453 USA PG-13 8.02 Medium Medium Comedy 11357 USA G 7.08 Low Low History Puzzle description: contains imdb scores of 12,800+ movies along with several attributes including budget, gross genre, content rating etc. What are the most promising alternative hypotheses about imdb scores to test? Name your three top candidates along with the evidence which backs them up: either in the form of R instruction(s) or plot. 4.2.1 Secrets Revealed- Patterns in Movies data? Secrets Revealed 4.2.2 Best Student‚Äôs Submissions 2022 Joshua Sze Andrew Fasano 4.3 Minimarket Data Hunt Table 4.3: Snippet of Minimarket Dataset BREAD BUTTER COOKIES COFFEE TEA 4421 0 0 1 0 1 8050 1 0 1 0 1 5846 0 1 0 1 1 2996 1 1 0 0 1 3353 1 0 0 1 0 Puzzle description: Each row of Minimarket.csv contains one customer transaction which is represented as binary vector (treat this as NUM values). 1 means that customer bought an item, 0 - means that customer did not but that item. For example if customer bought Bread but did not buy Butter you will see 1 in the Bread column and 0 in the Butter column. Summary Here‚Äôs what you‚Äôd do: 1. Come up with a null hypothesis: ‚ÄúBread does not impact the sales of butter‚Äù 2. Come up with an alternative hypothesis: ‚ÄúBread impacts the sale of butter‚Äù 3. Compute the mean value of the Butter column for all the rows where Bread value = 0. Let‚Äôs say this is mean1. 4. Compute the mean value of the Butter column for all the rows where Bread value = 1. Let‚Äôs say this is mean2. 4.3.1 What were the secret associations between items in the minimarket? Secrets Revealed 4.4 Predicting grades in Professor Moody‚Äôs class 4.4.1 How did I cook the Professor Moody Prediction challenge data? Secrets Revealed "],["setting-up-r.html", "Section: 5 Setting Up R 5.1 Create New Project 5.2 How to upload a data set? 5.3 Saving your work 5.4 General R References 5.5 Textbook Concepts 5.6 R functions used in this class 5.7 Data sets", " Section: 5 Setting Up R Important Instructions Installation of R is required before installing RStudio ‚ÄúR‚Äù is a programming language, and, ‚ÄúRStudio‚Äù is an Integrated Development Environment (IDE) which provides you a platform to code in R. How to download and install R &amp; RStudio? Downloading and installing R. For Windows Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cran.r-project.org/bin/windows/base/ Click on the link at top left where it says ‚ÄúDownload R 4.0.3 for windows‚Äù or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. For MAC Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cloud.r-project.org/bin/macosx/ Under ‚ÄúLatest release‚Äù, click on ‚ÄúR-4.0.3.pkg‚Äù or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. Downloading and installing RStudio. For Windows Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named ‚ÄúAll Installers‚Äù. Click on the download link beside ‚ÄúWindows 10/8/7‚Äù to download the windows version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. For MAC Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named ‚ÄúAll Installers‚Äù. Click on the link beside ‚ÄúmacOS 10.13+‚Äù to start your download the MAC version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. 5.1 Create New Project After installing R studio successfully the first step is to create a project R studio. Step 1: Go to File -&gt; New Project New Project Step 2: Select New Directory New Directory Step 3: Select New Project New Project Step 4: Give your preferred directory name like ‚ÄúData101_Assignmnets‚Äù Directory Name Step 5: Click on Create Project and finally the R studio should look like Rstudio 5.2 How to upload a data set? To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon. There are two options while accessing the dataset from your local machine: To avoid giving long directory paths for accessing the dataset, one should use the command getwd() to get the current working directory and store the dataset in the same directory. Getwd To access the dataset stored in the same directory one can use the following: read.csv(‚ÄúMOODY_DATA.csv‚Äù). Store the moody dataset in the same directory One can also store the dataset at a different location and can access it using the following command: (Suppose the dataset is stored inside the folder Data101_Tutorials on the desktop) - For Windows Users. - Example: read.csv(&quot;C:/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv&quot;) - For MAC Users. - Example: read.csv(&quot;/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv&quot;) Note: The directory path given here is the current working directory hosted on Github where the dataset has been stored. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFJlYWQgaW4gdGhlIGRhdGFcbmRmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIilcblxuIyBQcmludCBvdXQgYGRmYFxuaGVhZChkZikifQ== 5.3 Saving your work To save your work go to File -&gt; Save. It will ask you to give a name for your .R file and then click on Save. Save After making modifications to your saved file, you will need to save the file again. If the name of the file on the top is in Red Color indicates that the file have unsaved changes. Unsaved File Go to File -&gt; Save to save your .R file again. After saving the file the color of the file name i.e.¬†HW1.R will again change back to black. Saved File Note: You can create multiple files inside the same project such as for your each homework assignments 5.4 General R References https://www.w3schools.com/r/ https://cran.r-project.org/doc/contrib/Short-refcard.pdf https://www.amazon.com/Statistics-Engineers-Scientists-William-Navidi/dp/0073376337/ref=pd_lpo_3?pd_rd_i=0073376337&amp;psc=1 https://data101.cs.rutgers.edu/laboratory/ 5.5 Textbook Concepts Hypothesis testing: 8, 9 Difference of means hypothesis testing: 9 Null Hypothesis: 8 Alternative Hypothesis: 8 z-value: 8 critical value: 8 significance level: 8 p-value: 8 Bonferroni correction: 11 Chi square test: 10 Independence: 10 Multiple Hypothesis testing: 11 False Discovery Proportion: 11 Contingency Matrix: 10 Bayesian Reasoning: 14 Prior odds: 14 Posterior odds: 14 Likelihood ratio: 14 False positive: 14 True positive: 14 Crossvalidation: 17.4 Decision trees: 17 Linear regression: 18 Recursive partitioning: 18 MSE: 18 Prediction accuracy: 18 Training: 18 Testing: 18 5.6 R functions used in this class Elementary instructions: c() 6.1, mean() 7.1.1, nrow() 7.2.1, rep(), sd() 7.1.5, cut() 7.4.2 Plots: plot() 6.4, barplot() 6.5, boxplot() 6.6 mosaicplot() 6.7 Data Transformations: subset() 7.2, tapply() 7.3, table() 6.3, aggregate() Library functions: chisq.test() 10, pnorm() 8.2, Permutation() 9, rpart() 17, predict() 17.5, lm() 18.1, crossvalidation() 17.4 Parameters of rpart: minsplit 17.3, minbucket 17.3, cp 17.3 5.7 Data sets 5.7.1 Moody Download: moody2022_new.csv Table 5.1: Snippet of Moody Dataset SCORE GRADE DOZES_OFF TEXTING_IN_CLASS PARTICIPATION 510 23.20 F sometimes always 0.39 737 5.54 F sometimes always 0.80 67 65.65 B sometimes never 0.71 420 95.56 A never always 0.88 201 21.20 F always rarely 0.19 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5zdW1tYXJ5KG1vb2R5KSJ9 5.7.2 Movies Download: Movies2022F-4.csv Table 5.2: Snippet of Movies Dataset country content imdb_score Gross Budget genre 1103 USA R 7.52 High High Family 5836 USA PG-13 7.26 Medium Low History 9480 USA PG-13 7.10 Medium Medium History 10961 France R 8.25 Low Low History 3774 USA PG 6.01 Medium Medium Comedy eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuc3VtbWFyeShtb3ZpZXMpIn0= 5.7.3 Traffic Download: Traffic2022.csv Table 5.3: Snippet of Traffic Dataset TUNNEL DAY VOLUME_PER_MINUTE 2658 Lincoln weekend 86.0 1058 Holland weekend 67.5 2003 Lincoln weekday 65.0 236 Holland weekday 78.5 1782 Lincoln weekday 62.0 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0cmFmZmljPC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvVHJhZmZpYzIwMjIuY3N2Jylcblxuc3VtbWFyeSh0cmFmZmljKSJ9 5.7.4 Hindex Download: Hindex.csv Table 5.4: Snippet of Hindex Dataset IDN COUNTRY HAPPINESS 3428 20051 Syria 2.53 1740 50897 Chad 5.53 2032 83200 Ireland 5.47 5152 42770 Botswana 4.72 3037 27800 Kenya 2.44 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJoaW5kZXggPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L0hpbmRleC5jc3ZcIilcblxuc3VtbWFyeShoaW5kZXgpIn0= 5.7.5 Prediction 1 Dataset Download: M2022train.csv Table 5.5: Snippet of Moody Predicition 1 dataset Major Score Seniority Grade 94 Economics 24 Junior F 135 Statistics 84 Senior B 117 Psychology 71 Freshman A 343 Statistics 29 Senior F 517 Psychology 47 Freshman B eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMnRyYWluLmNzdlwiKVxuXG5zdW1tYXJ5KG1vb2R5KSJ9 5.7.6 Midterm, Project and Final Exam distribution in Prof.¬†Moody class Download: MoodyNUM.csv Assumptions: Midterm, Project and Final Exam are all out of 100 Table 5.6: Midterm, Project and Final Exam distribution in Prof.¬†Moody class Midterm Project FinalExam ClassScore 823 1 6 66 23.00000 605 98 2 53 36.50000 938 12 2 64 24.48016 834 27 17 23 20.80000 40 11 77 81 64.94886 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9Nb29keU5VTS5jc3ZcIilcblxuc3VtbWFyeShtb29keSkifQ== 5.7.7 Minimarket Download: HomeworkMarket2022.csv Table 5.7: Minimarket dataset Beer Day Location SoftDrinks Sweets Wine Snacks 16263 Ale Weekday Princeton Sprite Milky Way None None 14926 None Weekend Metuchen Cola Milky Way White Popcorn 10933 Lager Weekday Edison Sprite Milky Way Red Crackers 5408 Lager Weekday Princeton Sprite Twix Red None 17530 None Weekend Princeton None Twix None None eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtaW5pbWFya2V0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L0hvbWV3b3JrTWFya2V0MjAyMi5jc3ZcIilcblxuc3VtbWFyeShtaW5pbWFya2V0KSJ9 "],["plots.html", "Section: 6 üîñ Plots 6.1 Vector 6.2 Data Frames 6.3 Table 6.4 Scatter Plot 6.5 Bar Plot 6.6 Box Plot 6.7 Mosaic Plot 6.8 Additional References", " Section: 6 üîñ Plots Lecture slides: Plots 6.1 Vector A vector is simply a list of items that are of the same type. 6.1.1 Snippet 1 Lets look at example of creating a vector: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgMyB2ZWN0b3JzIHdpdGggdGl0bGUsIGF1dGhvciBhbmQgeWVhci5cbmNvbG9yIDwtIGMoJ1JlZCcsJ0JsdWUnLCdZZWxsb3cnLCdHcmVlbicpXG5cbiNMZXRzIGxvb2sgYXQgaG93IHRoZSBjcmVhdGVkIHZlY3RvcnMgbG9vay5cbmNvbG9yIn0= 6.1.2 Snippet 2 Create a vector with numerical values in a sequence, use the : operator: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgYSB2ZWN0b3JzIHdpdGggbnVtZXJpY2FsIHNlcXVlbmNlLlxueWVhciA8LSAyMDE4OjIwMjJcblxuI0xldHMgbG9vayBhdCBob3cgdGhlIGNyZWF0ZWQgdmVjdG9ycyBsb29rLlxueWVhciJ9 6.2 Data Frames Data Frames are data displayed in a format as a table. 6.2.1 Snippet 1 Populating the dataframe: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQgaW50byB0aGUgbW9vZHkgdmFyaWFibGVcbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMGIuY3N2XCIpXG5cbiMgTm93IGxldHMgdmlldyB0aGUgZGF0YWZyYW1lIG1vb2R5IHdpdGgganVzdCA1LTYgdHVwbGVzXG5oZWFkKG1vb2R5KSJ9 6.2.2 Snippet 2 Get the summary of the dataframe: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQgaW50byB0aGUgbW9vZHkgdmFyaWFibGVcbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMGIuY3N2XCIpXG5cbiMgVXNlIHRoZSBzdW1tYXJ5KCkgZnVuY3Rpb24gdG8gc3VtbWFyaXplIHRoZSBkYXRhIGZyb20gYSBEYXRhIEZyYW1lOlxuc3VtbWFyeShtb29keSkifQ== 6.2.3 Snippet 3 Use the notation data[rows, columns], which selects the subsets of rows and columns: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQgaW50byB0aGUgbW9vZHkgdmFyaWFibGVcbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMGIuY3N2XCIpXG5cbiMgUmV0dXJuIHJvdyAxXG5tb29keVsxLCBdXG5cbiMgUmV0dXJuIGNvbHVtbiA1XG5tb29keVssIDVdXG5cbiMgUm93cyAxOjUgYW5kIGNvbHVtbiAyXG5tb29keVsxOjUsIDJdXG5cbiMgR2l2ZSBtZSByb3dzIDEtMyBhbmQgY29sdW1ucyAyIGFuZCA0IG9mIG1vb2R5XG5tb29keVsxOjMsIGMoMjo0KV0ifQ== 6.3 Table table() displays frequency distribution of its arguments. 6.3.1 Snippet 1 The below examples show how to use this function: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcblxuI2xldHMgbWFrZSBhIHRhYmxlIGZvciB0aGUgZ3JhZGVzIG9mIHN0dWRlbnRzIGFuZCBjb3VudHMgb2Ygc3R1ZGVudHMgZm9yIGVhY2ggR3JhZGUuIFxuZ3JhZGVzIDwtIHRhYmxlKG1vb2R5JGdyYWRlKVxuXG4jbGV0cyBzZWUgdGhlIGFib3ZlIGZyZXF1ZW5jeSBkaXN0cmJ1dGVkIHRhYmxlc1xuZ3JhZGVzIn0= 6.3.2 Code Review 6.3.2.1 What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMGIuY3N2XCIpXG5cbnRhYmxlKG1vb2R5W21vb2R5JHF1ZXN0aW9ucyE9J2Fsd2F5cycsXSRncmFkZSlcbiNXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gZXJyb3JcbiMgQi4gZGlzdHJpYnV0aW9uIG9mIGdyYWRlcyBmb3Igc3R1ZGVudHMgd2hvIGFsd2F5cyBhc2sgcXVlc3Rpb25zXG4jIEMuIGRpc3RyaWJ1dGlvbiBvZiBncmFkZXMgZm9yIHN0dWRlbnRzIHdobyBkbyBub3QgYWx3YXlzIGFzayBxdWVzdGlvbnMgIn0= 6.3.2.2 What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMGIuY3N2XCIpXG5cbnRhYmxlKG1vb2R5W21vb2R5JHF1ZXN0aW9ucz09KCdhbHdheXMnLCduZXZlcicpLF0kZ3JhZGUpXG4jV2hhdCB3aWxsIFIgc2F5P1xuXG4jIEEuIGVycm9yLlxuIyBCLiBkaXN0cmlidXRpb24gb2YgZ3JhZGVzIGZvciBzdHVkZW50cyB3aG8gYWx3YXlzIG9yIG5ldmVyIGFzayBxdWVzdGlvbnMuICBcbiMgQy4gZGlzdHJpYnV0aW9uIG9mIGdyYWRlcyBmb3Igc3R1ZGVudHMgd2hvIGRvIG5vdCBhc2sgcXVlc3Rpb25zIGFsd2F5cyBvciBuZXZlci4gIn0= Table 6.1: Snippet of Moody Dataset score grade texting questions participation 26.89 F never never 0.41 71.57 B always rarely 0.00 90.11 A always never 0.27 31.52 D sometimes rarely 0.68 95.94 A always rarely 0.09 6.4 Scatter Plot Scatter Plot are used to plot two numerical variables. Hence it is used when both the labels are numerical values. Lets look at example of scatter plot using Moody. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExldCdzIGxvb2sgYXQgYSAyIGF0dHJpYnV0ZSBzY2F0dGVyIHBsb3QuXG4jIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbnBsb3QobW9vZHkkcGFydGljaXBhdGlvbixtb29keSRzY29yZSx5bGFiPVwic2NvcmVcIix4bGFiPVwicGFydGljaXBhdGlvblwiLG1haW49XCIgUGFydGljaXBhdGlvbiB2cyBTY29yZVwiLGNvbD1cInJlZFwiKSJ9 6.5 Bar Plot A bar plot are used to plot a categorical variable. This rectangle height is proportional to the value of the variable in the vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbmNvbG9yczwtIGMoJ3JlZCcsJ2JsdWUnLCdjeWFuJywneWVsbG93JywnZ3JlZW4nKSAjIEFzc2lnbmluZyBkaWZmZXJlbnQgY29sb3JzIHRvIGJhcnNcblxuI2xldHMgbWFrZSBhIHRhYmxlIGZvciB0aGUgZ3JhZGVzIG9mIHN0dWRlbnRzIGFuZCBjb3VudHMgb2Ygc3R1ZGVudHMgZm9yIGVhY2ggR3JhZGUuIFxuXG50PC10YWJsZShtb29keSRncmFkZSlcblxuI29uY2Ugd2UgaGF2ZSB0aGUgdGFibGUgbGV0cyBjcmVhdGUgYSBiYXJwbG90IGZvciBpdC5cblxuYmFycGxvdCh0LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJOdW1iZXIgb2YgU3R1ZGVudHNcIixjb2w9Y29sb3JzLCBcbiAgICAgICAgbWFpbj1cIkJhcnBsb3QgZm9yIHN0dWRlbnQgZ3JhZGUgZGlzdHJpYnV0aW9uXCIsYm9yZGVyPVwiYmxhY2tcIikifQ== 6.6 Box Plot A boxplot is used to display a numerical variable. A boxplot shows the distribution of data in a dataset. A boxplot shows the following things: Minimum Maximum Median First quartile Third quartile Outliers eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbmNvbG9yczwtIGMoJ3JlZCcsJ2JsdWUnLCdjeWFuJywneWVsbG93JywnZ3JlZW4nKSAjIEFzc2lnbmluZyBkaWZmZXJlbnQgY29sb3JzIHRvIGJhcnNcblxuI1N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCB0aGUgZGlzdHJpYnV0aW9uIG9mIHN0dWRlbnRzIHNjb3JlIHBlciBHcmFkZS4gV2UgdXNlIGJveCBwbG90IGZvciBnZXR0aW5nIHRoYXQuIFxuYm94cGxvdChzY29yZX5ncmFkZSxkYXRhPW1vb2R5LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJTY29yZVwiLCBtYWluPVwiQm94cGxvdCBvZiBncmFkZSB2cyBzY29yZVwiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIilcblxuIyB0aGUgY2lyY2xlcyByZXByZXNlbnQgb3V0bGllcnMuIn0= 6.7 Mosaic Plot Mosaic plot is used to visualize two categorical variables. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbmNvbG9yczwtIGMoJ3JlZCcsJ2JsdWUnLCdjeWFuJywneWVsbG93JywnZ3JlZW4nKSAjIEFzc2lnbmluZyBkaWZmZXJlbnQgY29sb3JzIHRvIGJhcnNcblxuI3N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCBudW1iZXJzIG9mIHN0dWRlbnRzIHdpdGggYSBwYXJ0aWN1bGFyIGdyYWRlIGJhc2VkIG9uIHRoZWlyIHRleHRpbmcgaGFiaXRzLiBVc2UgTW9zaWFjLXBsb3QuXG5cbm1vc2FpY3Bsb3QobW9vZHkkZ3JhZGV+bW9vZHkkdGV4dGluZyx4bGFiID0gJ0dyYWRlJyx5bGFiID0gJ1RleHRpbmcgaGFiaXQnLCBtYWluID0gXCJNb3NpYWMgb2YgZ3JhZGUgdnMgdGV4aW5nIGhhYml0IGluIGNsYXNzXCIsY29sPWNvbG9ycyxib3JkZXI9XCJibGFja1wiKSJ9 6.8 Additional References https://www.datamentor.io/r-programming/plot-function/ "],["data-transformation.html", "Section: 7 üîñ Data Transformation 7.1 Basic Functions 7.2 Subset 7.3 tapply 7.4 Derived Attribute", " Section: 7 üîñ Data Transformation Lecture slides: Data Transformation 7.1 Basic Functions 7.1.1 mean() mean() function is used to find the average of values in a numerical vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKVxuXG4jTGV0cyBsb29rIGF0IHRoZSBtZWFuIG9mIHNjb3JlIGNvbHVtbi5cbm1lYW4obW9vZHkkc2NvcmUpIn0= 7.1.2 length() length() function is used to get the number of elements in any vector eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKVxuXG4jTGV0cyBsb29rIGF0IHRoZSBsZW5ndGggb2YgdGhlIGdyYWRlIGNvbHVtbiBcbmxlbmd0aChtb29keSRncmFkZSkifQ== 7.1.3 max() max() function is used to get the maximum value in a numerical vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKVxuXG4jbGV0cyBsb29rIGF0IHRoZSBtYXhpbXVtIHZhbHVlIG9mIHRoZSBzY29yZSBpbiB0aGUgc2NvcmUgY29sdW1uXG5tYXgobW9vZHkkc2NvcmUpIn0= 7.1.4 min() min() function is used to get the minimum value in a numerical vector eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKVxuXG4jTGV0cyBsb29rIGF0IHRoZSBtaW5pbXVtIHZhbHVlIG9mIHNjb3JlIGluIHRoZSBzY29yZSBjb2x1bW4uXG5taW4obW9vZHkkc2NvcmUpIn0= 7.1.5 sd() sd() function is used to find the standard deviation of numerical vector eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKVxuXG4jTGV0cyBsb29rIGF0IHRoZSBzdGFuZGFyZCBkZXZpYXRpb24gb2Ygc2NvcmUgY29sdW1uXG5zZChtb29keSRzY29yZSkifQ== 7.2 Subset 7.2.1 Snippet 1- example of subset function eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuI1N1YnNldCBvZiByb3dzXG5tb29keV9uZXZlcl9zbWFydHBob25lPC1zdWJzZXQobW9vZHksT05fU01BUlRQSE9ORT09XCJuZXZlclwiKVxubnJvdyhtb29keSlcbm5yb3cobW9vZHlfbmV2ZXJfc21hcnRwaG9uZSkifQ== 7.2.2 Snippet 2- example of subset function eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuI1N1YnNldCBvZiByb3dzXG5tb29keTE8LXN1YnNldChtb29keSxPTl9TTUFSVFBIT05FPT1cIm5ldmVyXCIpXG4jIFlvdSBjYW4gc2VlIG9ubHkgc3R1ZGVudCBuZXZlciBvbiBzbWFydHBob25lIGFyZSBpbiB0aGUgc3Vic2V0LlxudGFibGUobW9vZHkxJE9OX1NNQVJUUEhPTkUpICJ9 7.2.3 Snippet 3- subset as subframe eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuI0FsdGVybmF0ZSB3YXkgdG8gc3Vic2V0LlxubW9vZHkyPC1tb29keVttb29keSRPTl9TTUFSVFBIT05FPT1cIm5ldmVyXCIsIF1cbiMgWW91IGNhbiBzZWUgYSBzaW1pbGFyIHRhYmxlIGFzIGFib3ZlLlxudGFibGUobW9vZHkyJE9OX1NNQVJUUEhPTkUpICJ9 7.2.4 Snippet 4- subsetting columns eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuY29sbmFtZXMobW9vZHkpXG4jc3Vic2V0IG9mIGNvbHVtbnNcbm1vb2R5Mzwtc3Vic2V0KG1vb2R5LCBzZWxlY3QgPSAtYygxKSlcbm5jb2wobW9vZHkzKVxuIyBZb3UgY2FuIHNlZSB0aGUgbnVtYmVyIG9mIGNvbHVtbnMgaGFzIGJlZW4gcmVkdWNlZCBieSAxLCBkdWUgdG8gc3ViLXNldHRpbmcgd2l0aG91dCBjb2x1bW4gMVxubmNvbChtb29keTMpIn0= 7.2.5 Snippet 5- sub-setting rows and columns eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuI1N1YnNldCBvZiBSb3dzIGFuZCBDb2x1bW5zXG5tb29keTE8LXN1YnNldChtb29keSwgc2VsZWN0ID0gYygyOjQpLCBPTl9TTUFSVFBIT05FID09IFwibmV2ZXJcIilcbmNvbG5hbWVzKG1vb2R5MSlcbiNOb3RpY2UgdGhhdCBvbmx5IDMgY29sdW1ucyBhcmUgcmVtYWluaW5nXG5kaW0obW9vZHkxKSJ9 7.2.6 Code Review 7.2.6.1 What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxubW9vZHlbbW9vZHkkU0NPUkU+PTkwLDNdXG4jIFdoYXQgd2lsbCBSIHNheT9cblxuXG4jIEEuIEdldCBzdWJzZXQgb2YgYWxsIGNvbHVtbnMgd2hpY2ggY29udGFpbnMgc3R1ZGVudHMgd2hvIHNjb3JlZCBtb3JlIHRoYW4gZXF1YWwgdG8gOTBcbiMgQi4gZXJyb3JcbiMgQy4gZ2V0IGFsbCBzY29yZSB2YWx1ZXMgd2hpY2ggYXJlIG1vcmUgdGhhbiBlcXVhbCB0byA5MFxuIyBELiBnZXQgc3Vic2V0IG9mIG9ubHkgdGhlIGdyYWRlcyBvZiBzdHVkZW50cyB3aXRoIHNjb3JlIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA5MCJ9 7.2.6.2 What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG5tb29keVttb29keSRTQ09SRT49ODAuMCAmIG1vb2R5JEdSQURFID09J0InLF0gXG4jIFdoYXQgd2lsbCBSIHNheT9cblxuIyBBLiBzdWJzZXQgb2YgbW9vZHkgZGF0YSBmcmFtZSB3aG8gZ290IEIgZ3JhZGUuXG4jIEIuIGVycm9yLlxuIyBDLiBzdWJzZXQgb2YgbW9vZHkgZGF0YSBmcmFtZSB3aXRoIHNjb3JlIGdyZWF0ZXIgdGhhbiA4MC5cbiMgRC4gc3Vic2V0IG9mIG1vb2R5IGRhdGEgZnJhbWUgd2l0aCBzY29yZSBtb3JlIHRoYW4gODAgYW5kIGdvdCBCIGdyYWRlLiJ9 7.3 tapply tapply() computes a measure (mean, median, min, max, etc..) or a function for each factor variable in a vector. It is a very useful function that lets you create a subset of a vector and then apply some functions to each of the subset. tapply(numerical, categorical, aggregagte function) 7.3.1 Snippet 1- Example of tapply followed by barplot eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG5cbiMgVG8gYXBwbHkgdGFwcGx5KCkgb24gU0NPUkUgZmFjdG9yZWQgb24gT05fU01BUlRQSE9ORVxuXG5tb29keTE8LXRhcHBseShtb29keSRTQ09SRSxtb29keSRPTl9TTUFSVFBIT05FLG1lYW4pXG5tb29keTEgIyBXZSBjYW4gc2VlIGl0IGNhbGN1bGF0ZWQgbWVhbiB2YWx1ZSBvZiB0aGUgc2NvcmUgYnkgc3R1ZGVudHMgd2l0aCByZXNwZWN0IHRvIHRoZWlyIHVzZSBvZiBwaG9uZSBpbiBjbGFzcy5cblxuYmFycGxvdChtb29keTEsY29sID0gXCJjeWFuXCIseGxhYiA9IFwiTGFiZWxzXCIsIHlsYWIgPSBcIm1lYW5fdmFsXCIsbWFpbiA9IFwidGFwcGx5KCkgZXhhbXBsZSAxXCIsbGFzID0gMiwgY2V4Lm5hbWVzID0gMC43NSkjcGxvdCJ9 7.3.2 Code Review 7.3.2.1 What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG50YXBwbHkobW9vZHksIEdSQURFLCBTQ09SRSwgbWluKVxuIyBXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gbWluaW11bSBzY29yZSBmb3IgZWFjaCBncmFkZVxuIyBCLiBtaW5pbXVtIGdyYWRlIGZvciBlYWNoIHNjb3JlXG4jIEMuIG1pbmltdW0gZ3JhZGUgb25seSBcbiMgRC4gRXJyb3IuIn0= 7.4 Derived Attribute R allows creating new data frame attributes (columns) ‚Äúon the fly‚Äù. These are new vectors, which are often defined as functions of existing attributes. Hence, the name - derived attributes. Derived attributes will play an important role in data exploration as well as in building prediction models. Very often, derived attributes allow discovery of important patterns in data. Similarly, derived attributes may be more predictive than original attributes in the imported data sets. The term feature engineering is often used in machine learning to describe creation of derived attributes. 7.4.1 Snippet 1 - Making new categorical attribute. The line 4 initializes the new attribute PF (Pass/Fail) to ‚ÄúPass‚Äù. The line 5 replaces ‚ÄúPass‚Äù by ‚ÄúFail‚Äù for students who received F. This new attribute, PF, will allow exploratory analysis to find ‚ÄúHow to pass Professor Moody‚Äôs class‚Äù. The answer to this question may be different than then answer to ‚ÄúHow to get a good grade in Professor Moody‚Äôs class‚Äù. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIEN1dCBFeGFtcGxlIHVzaW5nIGJyZWFrcyAtIEN1dHRpbmcgZGF0YSB1c2luZyBkZWZpbmVkIHZlY3Rvci4gXG5tb29keSRQRjwtJ1Bhc3MnXG5tb29keVttb29keSRHUkFERT09J0YnLF0kUEY8LSdGYWlsJ1xuXG4jIGxldHMgc2VlIG91ciBhZGRlZCBjb2x1bW4gUEZcbm1vb2R5In0= 7.4.2 Cut cut() function divides the range of x into intervals. Provides ability to label intervals as well. It plays important role in defining derived attributes from attributes which are numerical. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIEN1dCBFeGFtcGxlIHVzaW5nIGJyZWFrcyAtIEN1dHRpbmcgZGF0YSB1c2luZyBkZWZpbmVkIHZlY3Rvci4gXG5zY29yZTEgPC0gY3V0KG1vb2R5JFNDT1JFLGJyZWFrcz1jKDAsNTAsMTAwKSxsYWJlbHM9YyhcIkZcIixcIlBcIikpXG50YWJsZShzY29yZTEpIn0= 7.4.3 Code Review 7.4.3.1 What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG5jdXQobW9vZHkkU0NPUkUsIGJyZWFrcz1jKDAsMjUsNzAsMTAwKSxsYWJlbHM9YyhcImxvd1wiLCBcIm1lZGl1bVwiLCBcImhpZ2hcIikpXG4jV2hhdCB3b3VsZCBSIHNheT9cblxuIyBBLiA1IGludGVydmFscyBvZiBhdHRyaWJ1dGUgc2NvcmVcbiMgQi4gMyBpbnRlcnZhbHMgKDAsMjUpICgyNSw3MCkgKDc1LDEwMClcbiMgQy4gMyBjYXRlZ29yaWNhbCB2YWx1ZXMgXCJsb3dcIiwgXCJtZWRpdW1cIiBhbmQgXCJoaWdoXCIgZm9yIGRpZmZlcmVudCBzY29yZSBpbnRlcnZhbHNcbiMgRC4gMyBzZXBhcmF0ZSBkYXRhc2V0cyB3aXRoIHNpbWlsYXIgc2NvcmUgdmFsdWVzIn0= 7.4.3.2 What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG5vdXRwdXQ8LWN1dChtb29keSRTQ09SRSwgNSlcbnN1bW1hcnkob3V0cHV0KVxuI1doYXQgd291bGQgUiBzYXk/XG5cbiMgQS4gNSBpbnRlcnZhbHMgb2YgYXR0cmlidXRlIHNjb3JlIG9mIHVuZXF1YWwgY291bnQgb2YgZWxlbWVudHNcbiMgQi4gNSBpbnRlcnZhbHMgb2YgYXR0cmlidXRlIHNjb3JlIG9mIGVxdWFsIGNvdW50IG9mIGVsZW1lbnRzXG4jIEMuIDUgY2F0ZWdvcmljYWwgdmFsdWVzIGZvciBkaWZmZXJlbnQgc2NvcmUgaW50ZXJ2YWxzXG4jIEQuIDUgc2VwYXJhdGUgZGF0YXNldCB3aXRoIHNpbWlsYXIgc2NvcmUgdmFsdWVzIn0= 7.4.3.3 What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG5vdXRwdXQ8LWN1dChtb29keSRBU0tTX1FVRVNUSU9OUywgMilcbnN1bW1hcnkob3V0cHV0KVxuI1doYXQgd291bGQgUiBzYXk/XG5cbiMgQS4gMiBpbnRlcnZhbHMgb2YgYXR0cmlidXRlIGFza19xdWVzdGlvbnMgb2YgdW5lcXVhbCBjb3VudCBvZiBlbGVtZW50cyBpbiBlYWNoIGludGVydmFsXG4jIEIuIDIgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBhc2tfcXVlc3Rpb25zIG9mIGVxdWFsIGNvdW50IG9mIGVsZW1lbnRzIGluIGVhY2ggaW50ZXJ2YWxcbiMgQy4gMiBjYXRlZ29yaWNhbCB2YWx1ZXMgZm9yIGRpZmZlcmVudCBhc2tfcXVlc3Rpb25zIGludGVydmFsc1xuIyBELiBFcnJvci4ifQ== 7.4.3.4 More complex example of defining derived attributes The next snippet illustrates defining a new numerical attribute, $adjustedScore of a student in the Moody data frame. Score is adjusted by the value of participation attribute in the following way: If participation is larger than 0.5 - a bonus proportional to participation * 10 is added to the score. If participation is smaller than 0.5, a penalty of 1-participation) * 10 is subtracted from the score. In this way, for someone with very small participation, the 10 point penalty will be imposed (10 points subtracted from the score). Conversely, someone with perfect participation (1.0) will receive a 10 point bonus. 7.4.3.4.1 Snippet 1 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKVxuXG5cbm1vb2R5JGNvbmRpdGlvbmFsIDwtMFxubW9vZHlbbW9vZHkkcGFydGljaXBhdGlvbjwwLjUwLCBdJGNvbmRpdGlvbmFsIDwtIG1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb248MC41MCwgXSRzY29yZSAtMTAqKDEtbW9vZHlbbW9vZHkkcGFydGljaXBhdGlvbjwwLjUwLCBdJHBhcnRpY2lwYXRpb24pXG5tb29keVttb29keSRwYXJ0aWNpcGF0aW9uPj0wLjUwLCBdJGNvbmRpdGlvbmFsIDwtIG1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb24+PTAuNTAsIF0kc2NvcmUgKzEwKm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb24+PTAuNTAsIF0kcGFydGljaXBhdGlvblxuXG4jIHByaW50IHRoZSBjb2x1bW4gbmFtZXNcbmNvbG5hbWVzKG1vb2R5KVxuXG4jIGxldHMgbG9vayBhdCB0aGUgY29uZGl0aW9uYWwgYXR0cmlidXRlIFxuaGVhZChtb29keSlcblxuI3N1YnNldCB0aGUgbW9vZHkgZGF0YXNldCByb3dzID0gMSB0byAxMCBhbmQgY29scyA9IDEsNVxubW9vZHlbMToxMCwgYygxLDUpXVxuXG4jc3Vic2V0IHRoZSBtb29keSBkYXRhc2V0IHJvd3MgPSAxIHRvIDEwIGFuZCBjb2xzID0gMSw1LDZcbm1vb2R5WzE6MTAsIGMoMSw1LDYpXVxuXG4jIHByaW50IHN1bW1hcnkgb2YgaW5pZGl2aWR1YWwgY29sdW1uc1xuc3VtbWFyeShtb29keSRzY29yZSlcbnN1bW1hcnkobW9vZHkkY29uZGl0aW9uYWwpXG5cbiMgUGxvdHRpbmcgdGhlIGNvbmRpdGlvbmFsIGF0dHJpYnV0ZSB1c2luZyBib3hwbG90XG5ib3hwbG90KG1vb2R5JGNvbmRpdGlvbmFsLGNvbCA9IGMoXCJyZWRcIiksbWFpbj1cIkNvbXBsZXggRXhhbXBsZVwiKVxuXG4jIFBsb3R0aW5nIHRoZSBzY29yZSBhdHRyaWJ1dGUgdXNpbmcgYm94cGxvdFxuYm94cGxvdChtb29keSRzY29yZSxjb2wgPSBjKFwiYmx1ZVwiKSxtYWluPVwiQ29tcGxleCBFeGFtcGxlXCIpIn0= "],["ztest.html", "Section: 8 üîñ Hypothesis Testing: z-test 8.1 Introduction 8.2 Snippet 1: Shows the code for z-test to test the following hypotheses 8.3 Snippet 2: Make your own data and see how p-value changes 8.4 Additional References", " Section: 8 üîñ Hypothesis Testing: z-test Lecture slides: Hypothesis Testing Central Limit Theorem Great Resource: Khan Academy 8.1 Introduction The following synthetic data describes daily traffic on weekday and weekend days in Lincoln and Holland tunnels. The data frame has three attributes: TUNNEL, DAY and VOLUME_PER_MINUTE. Below we show a small sample of the TRAFFIC data frame Table 8.1: Snippet of Traffic Dataset TUNNEL DAY VOLUME_PER_MINUTE 941 Holland weekday 57.5 526 Holland weekday 61.5 544 Holland weekday 98.5 1380 Holland weekend 44.5 1050 Holland weekend 57.5 1800 Lincoln weekday 72.0 337 Holland weekday 69.5 1158 Holland weekend 41.5 2296 Lincoln weekday 68.0 2001 Lincoln weekday 77.0 The following snippet 6.2 shows the code for hypothesis test of difference of means. Is the mean traffic (VOLUME_PER_MINUTE) in the Holland tunnel bigger than mean traffic (VOLUME_PER_MINUTE) in the Lincoln? 8.2 Snippet 1: Shows the code for z-test to test the following hypotheses Null Hypothesis - Traffic in Holland tunnel is the same as traffic in Lincoln tunnel. Alternative Hypothesis - Traffic in the Holland Tunnel is larger than traffic in the Lincoln tunnel. In the snippet 6.2 we end up calculating the p-value which leads to rejection of Null hypothesis (good news for data scientist, bad for the sceptic). Indeed, p-value is less than the significance level of 5%. This means, that under null hypothesis it is extremely unlikely (less than 5% chance) to see the result which is at least as big as the observed difference of means. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJUUkFGRklDPC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvVHJhZmZpYzIwMjIuY3N2JylcblxuI2RhdGEgY2xlYW4gYW5kIHN1YnNldCwgZWl0aGVyXG5saW5jb2xuLmRhdGEgPC0gc3Vic2V0KFRSQUZGSUMsIFRSQUZGSUMkVFVOTkVMID09IFwiTGluY29sblwiKVxuaG9sbGFuZC5kYXRhIDwtIHN1YnNldChUUkFGRklDLCBUUkFGRklDJFRVTk5FTCA9PSBcIkhvbGxhbmRcIilcblxuI3RyYWZmaWMgYXQgbGluY29sblxubGluY29sbi50cmFmZmljIDwtIGxpbmNvbG4uZGF0YSRWT0xVTUVfUEVSX01JTlVURVxuXG4jdHJhZmZpYyBhdCBob2xsYW5kXG5ob2xsYW5kLnRyYWZmaWMgPC0gaG9sbGFuZC5kYXRhJFZPTFVNRV9QRVJfTUlOVVRFXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuc2QubGluY29sbiA8LSBzZChsaW5jb2xuLnRyYWZmaWMpXG5zZC5ob2xsYW5kIDwtIHNkKGhvbGxhbmQudHJhZmZpYylcbnNkLmxpbmNvbG5cbnNkLmhvbGxhbmRcblxuI2xlbmd0aCBvZiBsaW5jb2xuIGFuZCBob2xsYW5kXG5sZW5fbGluY29sbiA8LSBsZW5ndGgobGluY29sbi50cmFmZmljKVxubGVuX2hvbGxhbmQgPC0gbGVuZ3RoKGhvbGxhbmQudHJhZmZpYylcbmxlbl9saW5jb2xuXG5sZW5faG9sbGFuZFxuXG4jc3RhbmRhcmQgZGV2aWF0aW9uIG9mIGRpZmZlcmVuY2UgdHJhZmZpY1xuc2QubGluLmhvbCA8LSBzcXJ0KHNkLmxpbmNvbG5eMi9sZW5fbGluY29sbiArIHNkLmhvbGxhbmReMi9sZW5faG9sbGFuZClcbnNkLmxpbi5ob2xcblxuI21lYW5zIG9mIHR3byBzYW1wbGVzXG5tZWFuLmxpbmNvbG4gPC0gbWVhbihsaW5jb2xuLnRyYWZmaWMpXG5tZWFuLmhvbGxhbmQgPC0gbWVhbihob2xsYW5kLnRyYWZmaWMpXG5tZWFuLmxpbmNvbG5cbm1lYW4uaG9sbGFuZFxuXG4jeiBzY29yZVxuemV0YSA8LSAobWVhbi5saW5jb2xuIC0gbWVhbi5ob2xsYW5kKS9zZC5saW4uaG9sXG56ZXRhXG5cbiNwbG90IHJlZCBsaW5lXG5wbG90KHg9c2VxKGZyb20gPSAtNSwgdG89IDUsIGJ5PTAuMSkseT1kbm9ybShzZXEoZnJvbSA9IC01LCB0bz0gNSwgIGJ5PTAuMSksbWVhbj0wKSx0eXBlPSdsJyx4bGFiID0gJ21lYW4gZGlmZmVyZW5jZScsICB5bGFiPSdwb3NzaWJpbGl0eScpXG5hYmxpbmUodj16ZXRhLCBjb2w9J3JlZCcpXG5cbiNnZXQgcFxucCA9IDEtcG5vcm0oemV0YSlcbnAifQ== 8.3 Snippet 2: Make your own data and see how p-value changes How p-value is affected by difference of means and standard deviations. We will build two distributions ourselves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. You can do it by changing values of mean and standard deviation in the rnorm() function. Clearly the further apart the mean values are - the lower the p-value. But how do standard deviations affect the p-value? See for yourself. Build the data frame with two attributes: Cat and Val, using rnorm() function eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJWYWwxPC1ybm9ybSgxMCxtZWFuPTI1LCBzZD0xMClcblZhbDI8LXJub3JtKDEwLG1lYW49MzUsIHNkPTEwKVxuQ2F0MTwtcmVwKFwiR3JvdXBBXCIsMTApICBcbkNhdDI8LXJlcChcIkdyb3VwQlwiLDEwKSAgXG5DYXQ8LWMoQ2F0MSxDYXQyKSBcblZhbDwtYyhWYWwxLFZhbDIpXG5cbmQ8LWRhdGEuZnJhbWUoQ2F0LFZhbClcbk9ic2VydmVkX0RpZmZlcmVuY2U8LW1lYW4oZFtkJENhdD09J0dyb3VwQicsMl0pLW1lYW4oZFtkJENhdD09J0dyb3VwQScsMl0pXG5PYnNlcnZlZF9EaWZmZXJlbmNlXG5cbmRBPC1kW2QkQ2F0PT0nR3JvdXBBJyxdXG5kQVxuZEI8LWRbZCRDYXQ9PSdHcm91cEInLF1cbmRCXG5cbm1lYW5BPC1tZWFuKGRBJFZhbClcbm1lYW5CPC1tZWFuKGRCJFZhbClcblxuc2RBPC1zZChkQSRWYWwpXG5zZEI8LXNkKGRCJFZhbClcblxubGVuQTwtbnJvdyhkQSlcbmxlbkFcbmxlbkI8LW5yb3coZEIpXG5sZW5CXG5cbnNkLkEuQiA8LSBzcXJ0KHNkQV4yL2xlbkEgK3NkQl4yL2xlbkIpXG5zZC5BLkJcblxuemV0YSA8LSAobWVhbkIgLSBtZWFuQSkvc2QuQS5CXG56ZXRhXG5wPC0xLXBub3JtKHpldGEpXG5wIn0= 8.4 Additional References https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/p-value/ http://www.z-table.com/ https://www.statisticshowto.com/probability-and-statistics/z-score/ https://sixsigmastudyguide.com/z-scores-z-table-z-transformations/ "],["ptest.html", "Section: 9 üîñ Hypothesis Testing: Permutation Test 9.1 Snippet 1 9.2 Snippet 2 9.3 Snippet 3", " Section: 9 üîñ Hypothesis Testing: Permutation Test Lecture slides: Permutation Test 9.1 Snippet 1 Exercise - How p-value is affected by difference of means and standard deviations We will build our two distributions ourseleves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use permutation test (can be z-test as well) to test difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. Build the data frame with two attributes: Cat and Val, using rnorm() function eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJWYWwxPC1ybm9ybSgxMCxtZWFuPTI1LCBzZD0xMClcblZhbDI8LXJub3JtKDEwLG1lYW49MzAsIHNkPTEwKVxuIFxuQ2F0MTwtcmVwKFwiR3JvdXBBXCIsMTApICAjIGZvciBleGFtcGxlIEdyb3VwQSBjYW4gYmUgSG9sbGFuZCBUdW5uZWxcbkNhdDI8LXJlcChcIkdyb3VwQlwiLDEwKSAgIyBmb3IgZXhhbXBsZSBHcm91cCBCIHdpbGwgYmUgTGluY29sbiBUdW5uZWxcblxuQ2F0MVxuQ2F0MlxuXG4jVGhlIHJlcCBjb21tYW5kIHdpbGwgcmVwZWF0LCB0aGUgdmFyaWFibGVzIHdpbGwgYmUgb2YgdHlwZSBjaGFyYWN0ZXIgYW5kIHdpbGwgY29udGFpbiAxMCB2YWx1ZXMgZWFjaC5cblxuQ2F0PC1jKENhdDEsQ2F0MikgIyBBIHZhcmlhYmxlIHdpdGggZmlyc3QgMTAgdmFsdWVzIEdyb3VwQSBhbmQgbmV4dCAxMCB2YWx1ZXMgR3JvdXBCXG5DYXRcblxuVmFsPC1jKFZhbDEsVmFsMilcblZhbFxuXG5kPC1kYXRhLmZyYW1lKENhdCxWYWwpXG5kXG5cbk9ic2VydmVkX0RpZmZlcmVuY2U8LW1lYW4oZFtkJENhdD09J0dyb3VwQicsMl0pLW1lYW4oZFtkJENhdD09J0dyb3VwQScsMl0pXG5PYnNlcnZlZF9EaWZmZXJlbmNlXG5cbiNUaGlzIHdpbGwgY2FsY3VsYXRlIHRoZSBtZWFuIG9mIHRoZSBzZWNvbmQgY29sdW1uIChoYXZpbmcgMTAgcmFuZG9tIHZhbHVlcyBmb3IgZWFjaCBncm91cCksIGFuZCB0aGUgbWVhbiBvZiBncm91cEIgdmFsdWVzIGlzIHN1YnRyYWN0ZWQgZnJvbSB0aGUgbWVhbiBvZiBncm91cEEgdmFsdWVzLCB3aGljaCB3aWxsIGdpdmUgeW91IHRoZSB2YWx1ZSBvZiB0aGUgZGlmZmVyZW5jZSBvZiB0aGUgbWVhbi5cbiBcbiAjVHJ5IGNoYW5naW5nIG1lYW4gYW5kIHNkIHZhbHVlcy4gV2hlbiB5b3UgcnVuIHRoaXMgeW91IHdpbGwgc2VlIHRoYXQgdGhlIGRpZmZlcmVuY2UgaXMgc29tZXRpbWVzIG5lZ2F0aXZlICNvciBzb21ldGltZXMgcG9zaXRpdmUuIn0= 9.2 Snippet 2 Do this in your R studio, since we cannot install our package in data camp service we are using to run the code snippets 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 9.3 Snippet 3 One permutation at a time eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0cmFmZmljPC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvVHJhZmZpYzIwMjIuY3N2JylcblxucmFuTnVtIDwtIHNhbXBsZSgxOm5yb3codHJhZmZpYyksbnJvdyh0cmFmZmljKSlcbnJhbk51bVsxOjVdXG5cblZPTFVNRV9QRVJfTUlOVVRFPC10cmFmZmljJFZPTFVNRV9QRVJfTUlOVVRFW3Jhbk51bV1cblRVTk5FTDwtdHJhZmZpYyRUVU5ORUxcblxuUGVybXV0ZWRfdHJhZmZpYzwtZGF0YS5mcmFtZShUVU5ORUwsIFZPTFVNRV9QRVJfTUlOVVRFKVxuXG5tZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywgXSRWT0xVTUVfUEVSX01JTlVURSkgLW1lYW4odHJhZmZpY1t0cmFmZmljJFRVTk5FTD09J0hvbGxhbmQnLCBdJFZPTFVNRV9QRVJfTUlOVVRFKVxuXG5tZWFuKFBlcm11dGVkX3RyYWZmaWNbUGVybXV0ZWRfdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywgXSRWT0xVTUVfUEVSX01JTlVURSktbWVhbihQZXJtdXRlZF90cmFmZmljW1Blcm11dGVkX3RyYWZmaWMkVFVOTkVMPT0nSG9sbGFuZCcsIF0kVk9MVU1FX1BFUl9NSU5VVEUpIn0= "],["chitest.html", "Section: 10 üîñ Chi Square Analysis 10.1 Snippet 1 10.2 Snippet 2 10.3 Snippet 3 10.4 Snippet 4", " Section: 10 üîñ Chi Square Analysis Lecture slides: Chi Square Great Resource: Khan Academy 10.1 Snippet 1 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJFeHBlY3RlZCA8LW1hdHJpeChjKDIwMCw0MjAsMTgwLCA0MCwxMjAsNDApLCBucm93PTMsIG5jb2w9Milcbk9ic2VydmVkPC1tYXRyaXgoYygyMDAsNDIwLDE4MCwzNSwxMjAsNDUpLCBucm93PTMsIG5jb2w9MilcbkV4cGVjdGVkXG5PYnNlcnZlZFxuY2hpc3EudGVzdChPYnNlcnZlZCkifQ== 10.2 Snippet 2 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZHBseXIpXG5kPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3ZcIilcbmhlYWQoZClcbm9wdGlvbnMod2Fybj0tMSlcbmNoaV90ZXN0IDwtIGZ1bmN0aW9uKGRhdGEsY29sMSxjb2wyLGl0ZXIpIHtcbiAgXG4gIGRmIDwtIGRhdGEuZnJhbWUoZGF0YSlcbiAgdmFsczwtIHVuaXF1ZShkZltbY29sMl1dKVxuICBub19yb3dzIDwtIG5yb3coZGYpXG4gIGR0IDwtIHRhYmxlKGRmW1tjb2wxXV0sIGRmW1tjb2wyXV0pXG4gIHJlcyA8LSBjaGlzcS50ZXN0KGR0KVxuICByZWFsX2FucyA8LSByZXMkc3RhdGlzdGljXG4gIGFuc192ZWMgPC0gdmVjdG9yKClcbiAgZm9yICh4IGluIDE6aXRlcil7XG5cbiAgICBuZXdfZGF0YSA8LSBzYW1wbGUoeD12YWxzLHNpemU9bm9fcm93cyxyZXBsYWNlID0gVFJVRSlcblxuICAgIGR0X25ldyA8LSB0YWJsZShkZltbY29sMV1dLCBuZXdfZGF0YSlcblxuICAgIHJlc19uZXcgPC0gY2hpc3EudGVzdChkdF9uZXcpXG5cbiAgICBhbnNfdmVjIDwtIGFwcGVuZChhbnNfdmVjLHJlc19uZXckc3RhdGlzdGljKVxuICB9XG4gIGhpc3QoYW5zX3ZlYyxtYWluPVwiUGVybXV0YXRpb24gVGVzdCBmb3IgQ2hpLVNxdWFyZVwiLHhsYWI9XCJDaGktU3F1YXJlIFZhbHVlc1wiKVxuICBwcmludChyZWFsX2FucylcbiAgYWJsaW5lKHY9cmVhbF9hbnMsY29sPVwiYmx1ZVwiLGx3ZD0yKVxufSIsInNhbXBsZSI6IiNkPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3ZcIilcbiNoZWFkKGQpXG5cbmNoaV90ZXN0KGQsXCJNYWpvclwiLFwiR3JhZGVcIiwxMDAwKSJ9 10.3 Snippet 3 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxubW9vZHkkSU48LSdPdXRfU2xpY2UnXG5tb29keVttb29keSRET1pFU19PRkY9PSduZXZlcicgJiBtb29keSRURVhUSU5HX0lOX0NMQVNTPT0nYWx3YXlzJywgXSRJTjwtJ0luX1NsaWNlJ1xuZDwtdGFibGUobW9vZHkkR1JBREUsIG1vb2R5JElOKVxuZFxuY2hpc3EudGVzdChkKSJ9 10.4 Snippet 4 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcbmRhdGE8LXRhYmxlKG1vdmllcyRjb250ZW50LCBtb3ZpZXMkZ2VucmUpXG5jaGlzcS50ZXN0KGRhdGEpIn0= "],["Mtest.html", "Section: 11 üîñ Multiple Hypothesis Testing 11.1 Snippet 1 - Benjamini-Hochberg Algorithm 11.2 Snippet 2 11.3 Additional References", " Section: 11 üîñ Multiple Hypothesis Testing Lecture slides: Multiple Hypothesis Testing Table 11.1: Snippet of Hindex Dataset IDN COUNTRY HAPPINESS 773 80488 Myanmar 7.65 88 85034 Cameroon 8.28 2153 22859 Taiwan 3.32 2051 62934 Djibouti 6.70 5909 96901 Netherlands 9.39 5559 49889 Austria 5.39 3723 35594 Germany 4.47 4787 68356 Tanzania 6.49 35 70265 Qatar 7.34 3027 40460 Cameroon 4.61 11.1 Snippet 1 - Benjamini-Hochberg Algorithm eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwPC1zb3J0KHJvdW5kKHJ1bmlmKDEwMCwgbWluPTAsIG1heD0wLjA1KSwgNCkpXG5wXG5wPC1wKzAuMDAwM1xucFxuI2ltcGxlbWVudCBCZW5qYW1pbmktSG9jaGJlcmcgZm9ybXVsYVxucTwtcmVwKDAuMDUsMTAwKVxucVxucj1jKDE6MTAwKVxucTwtcm91bmQocSpyLzEwMCw0KVxudGVtcDwtcDxxXG4jU2VsZWN0IHAtdmFsdWVzIHdoaWNoIGNvcnJlc3BvbmQgdG8gZGlzY292ZXJpZXMgKHJlamVjdCBOVUxMKVxubWF4aW5kZXg8LW1heCh3aGljaCh0ZW1wPT0nVFJVRScpKVxucFsxOm1heGluZGV4XSJ9 11.2 Snippet 2 Happiness Index synthetic data set which is used in my slides for multiple hypotheses testing How to order by aggregate? First make data frame out of tapply? Use aggregate and list functions. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJIaW5kZXggPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L0hpbmRleC5jc3ZcIikgI3dlYiBsb2FkXG5cbkhpbmRleDwtYWdncmVnYXRlKEhpbmRleCRIQVBQSU5FU1MsIGxpc3QoSGluZGV4JENPVU5UUlkpLCBtZWFuKVxuY29sbmFtZXMoSGluZGV4KTwtIGMoXCJDb3VudHJ5XCIsXCJBdmVyYWdlSFwiKVxuI3JlbmFtZXMgY29sdW1ucyBvZiB0aGUgSGluZGV4IGRhdGEgZnJhbWVcbmNvbG5hbWVzKEhpbmRleClcblxuSGluZGV4W29yZGVyKEhpbmRleCRBdmVyYWdlSCksXSJ9 11.3 Additional References https://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/ "],["cr.html", "Section: 12 Code Review: Exploratory Queries in R 12.1 Movies Dataset Example 12.2 Census Dataset Example", " Section: 12 Code Review: Exploratory Queries in R 12.1 Movies Dataset Example 12.1.1 Snippet 1: What is the mean imdb of low budget comedies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxubWVhbihtb3ZpZXNbbW92aWVzJEJ1ZGdldD09J0xvdycgJiBtb3ZpZXMkZ2VucmU9PSdDb21lZHknLCBdJGltZGJfc2NvcmUpIn0= 12.1.2 Snippet 2: What is standard deviation of imdb score of high gross Family movies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuc2QobW92aWVzW21vdmllcyRHcm9zcz09J0hpZ2gnICYgbW92aWVzJGdlbnJlID09J0ZhbWlseScsXSRpbWRiKSJ9 12.1.3 Snippet 3: What is the lowest imdb score among high budget movies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxubWluKG1vdmllc1ttb3ZpZXMkQnVkZ2V0PT0nSGlnaCcsXSRpbWRiKSJ9 12.1.4 Snippet 4: How many low budget movies generated high gross income? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxubnJvdyhtb3ZpZXNbbW92aWVzJEJ1ZGdldD09J0xvdycgJiBtb3ZpZXMkR3Jvc3MgPT0nSGlnaCcsXSkifQ== 12.1.5 Snippet 5: What is imdb score of the first non-US movie in the movies data frame? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuI1lvdSBjYW4gdXNlIHRoaXMgc2ltcGxlIGNvbW1hbmQgdG8gcXVpY2tseSBmaW5kIG91dFxuaGVhZChtb3ZpZXNbbW92aWVzJGNvdW50cnkhPSdVU0EnLCBdJGltZGJfc2NvcmUpIn0= 12.1.6 Snippet 6: What is the least frequent genre among UK movies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuI1lvdSBjYW4gdXNlIHRoaXMgY29kZSB0byBmaW5kIG91dFxudGFibGUobW92aWVzW21vdmllcyRjb3VudHJ5PT0nVUsnLF0kZ2VucmUsIG1vdmllc1ttb3ZpZXMkY291bnRyeT09J1VLJyxdJGNvdW50cnkpIn0= 12.1.7 Snippet 7: Which content rating has the lowest average imdb score? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuI1lvdSBjYW4gdXNlIHRoaXMgY29kZSB0byBmaW5kIG91dFxudGFwcGx5KG1vdmllcyRpbWRiLCBtb3ZpZXMkY29udGVudCwgbWVhbikifQ== 12.1.8 Snippet 8: Movies from which country have the smallest average imdb score? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuI0JldHRlciBjb21wdXRlIGl0LCBzaW5jZSB0aGVyZSBhcmUgdG9vIG1hbnkgY291bnRyaWVzIGZvciB2aXN1YWwgaW5zcGVjdGlvblxuTUE8LWFnZ3JlZ2F0ZShtb3ZpZXMkaW1kYl9zY29yZSwgbGlzdChtb3ZpZXMkY291bnRyeSksIG1lYW4pXG5jb2xuYW1lcyhNQSk8LWMoXCJDb3VudHJ5XCIsIFwiTWltZGJcIilcbk1BPC1NQVtvcmRlcigtTUEkTWltZGIpLCBdXG5NQVsxLF0ifQ== 12.1.9 Snippet 9: What is the least frequent genre in movies data frame? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuejwtdGFibGUobW92aWVzJGdlbnJlKVxuc29ydCh6LGRlY3JlYXNpbmc9RkFMU0UpWzFdIn0= 12.1.10 Snippet 10: z value = 2.4, whats the p-value? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIxLXBub3JtKDIuNCkifQ== 12.2 Census Dataset Example 12.2.1 Snippet 11: For the individual over 50, which profession has the highest average capital gain? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJjZW5zdXNfZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9DZW5zdXNEYXRhLmNzdlwiKVxuXG5hZ2VfZ3JlYXRlcl90aGFuXzQ5ID0gc3Vic2V0KGNlbnN1c19kYXRhLCBjZW5zdXNfZGF0YSRBR0UgPj0gNTApXG5hZ2VkX2NhcGl0YWxnYWlucyA9IHRhcHBseShhZ2VfZ3JlYXRlcl90aGFuXzQ5JENBUElUQUxHQUlOUywgYWdlX2dyZWF0ZXJfdGhhbl80OSRQUk9GRVNTSU9OLCBtZWFuKVxuYWdlZF9jYXBpdGFsZ2FpbnMifQ== 12.2.2 Snippet 12: Which profession has the highest average capital gains; Sales or Tech-support? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJjZW5zdXNfZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9DZW5zdXNEYXRhLmNzdlwiKVxuXG5leGFtcGxlMTJfZGF0YSA9IHRhcHBseShjZW5zdXNfZGF0YSRDQVBJVEFMR0FJTiwgY2Vuc3VzX2RhdGEkUFJPRkVTU0lPTiwgbWVhbilcbmV4YW1wbGUxMl9kYXRhIn0= 12.2.3 Snippet 13: What is most frequent profession of people with less than 10 years od of education? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJjZW5zdXNfZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9DZW5zdXNEYXRhLmNzdlwiKVxuXG5leGFtcGxlMTNfZGF0YSA9IHRhYmxlKHN1YnNldChjZW5zdXNfZGF0YSwgWUVBUlMgPD0gMTApJFBST0ZFU1NJT04pXG5leGFtcGxlMTNfZGF0YSJ9 12.2.4 Snippet 14: What is minimum number of years of education for people with Exec-managerial specialty? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJjZW5zdXNfZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9DZW5zdXNEYXRhLmNzdlwiKVxuXG5leGFtcGxlMTRfZGF0YSA9IG1pbihzdWJzZXQoY2Vuc3VzX2RhdGEsIFBST0ZFU1NJT04gPT0gXCJFeGVjLW1hbmFnZXJpYWxcIikkWUVBUlMpXG5leGFtcGxlMTRfZGF0YSJ9 12.2.5 Snippet 15: What is the most frequent degree for natives of the United States? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJjZW5zdXNfZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9DZW5zdXNEYXRhLmNzdlwiKVxuXG5leGFtcGxlMTVfZGF0YSA9IHRhYmxlKHN1YnNldChjZW5zdXNfZGF0YSwgTkFUSVZFID09IFwiVW5pdGVkLVN0YXRlc1wiKSRFRFVDQVRJT04pXG5leGFtcGxlMTVfZGF0YSJ9 12.2.6 Snippet 16: What is the least frequent degree for people with at least 12 years of education? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJjZW5zdXNfZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9DZW5zdXNEYXRhLmNzdlwiKVxuXG5leGFtcGxlMTZfZGF0YSA9IHRhYmxlKHN1YnNldChjZW5zdXNfZGF0YSwgWUVBUlMgPj0gMTIpJEVEVUNBVElPTilcbmV4YW1wbGUxNl9kYXRhIn0= "],["common.html", "Section: 13 üîñ Common Sense Judgement and Probability", " Section: 13 üîñ Common Sense Judgement and Probability Lecture slides: Common Sense Judgement and Probability "],["br.html", "Section: 14 üîñ Bayesian Reasoning 14.1 Snippet 1: Covid Odds after positive Home Test. 14.2 Snippet 2: What are the odds that an ‚ÄòF‚Äô student is a freshman? 14.3 Snippet 3: What are the odds that a ‚ÄòA‚Äô student with the score less than 80 is a psychology major?", " Section: 14 üîñ Bayesian Reasoning Lecture slides: Bayesian Reasoning 14.1 Snippet 1: Covid Odds after positive Home Test. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjQmVsaWVmID0gXCJIYXZlIENvdmlkXCJcbiNPYnNlcnZhdGlvbiA9IENvdmlkIFRlc3RcbiNIb3cgbXVjaCB0aGUgcHJvYmFiaWxpdHkgb2YgaGF2aW5nIGNvdmlkIGluY3JlYXNlcyB1cG9uIHBvc2l0aXZlIENPVklELXRlc3Q/XG4jV2UgdXNlIHRoZSBvZGRzIGZvcm11bGF0aW9uIG9mIEJheWVzaWFuIFRoZW9yZW1cbiMgd2UgYmVnaW4gd2l0aCBwcmlvciBvZGRzIG9mIGhhdmluZyBDb3ZpZDogIFAoQ292aWQpLygxLVAoQ292aWQpXG5QcmlvckhhdmVDb3ZpZDwtMC4wMVxuUHJpb3JDb3ZpZE9kZHM8LVByaW9ySGF2ZUNvdmlkLygxLVByaW9ySGF2ZUNvdmlkKVxuUHJpb3JDb3ZpZE9kZHNcbiNUcnVlIHBvc2l0aXZlOiAgUHJvYmFiaWxpdHkgb2YgaGF2aW5nIHBvc2l0aXZlIENvdmlkIHRlc3Qgd2hlbiBoYXZpbmcgY292aWQgID0gUChQb3NpdGl2ZUNvdmlkVGVzdHxIYXZlQ292aWQpXG5UcnVlUG9zaXRpdmU8LTAuOTlcbiNGYWxzZSBwb3NpdGl2ZSA9IFByb2JhYmlsaXR5IG9mIGhhdmluZyBwb3NpdGl2ZSBDb3ZpZCB0ZXN0IHdoZW4gbm90IGhhdmluZyBjb3ZpZCA9IFAoUG9zdGl2ZUNvdmlkVGVzdC9Eb05vdEhhdmVDb3ZpZClcbkZhbHNlUG9zaXRpdmU8LTAuMDAxXG5MaWtlbGlob29kUmF0aW88LVRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlXG5Qb3N0ZXJpb3JDb3ZpZE9kZHM8LUxpa2VsaWhvb2RSYXRpbypQcmlvckNvdmlkT2Rkc1xuUG9zdGVyaW9ySGF2ZUNvdmlkPC0gUG9zdGVyaW9yQ292aWRPZGRzLygxK1Bvc3RlcmlvckNvdmlkT2RkcylcblBvc3RlcmlvckhhdmVDb3ZpZCJ9 14.2 Snippet 2: What are the odds that an ‚ÄòF‚Äô student is a freshman? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3YnKVxuI0JlbGllZiAtIFN0dWRlbnQgaXMgYSBmcmVzaG1hblxuI09ic2VydmF0aW9uIC0gRmFpbGVkIHRoZSBjbGFzc1xuUHJpb3I8LW5yb3cobW9vZHlbbW9vZHkkU2VuaW9yaXR5ID09J0ZyZXNobWFuJyxdKS9ucm93KG1vb2R5KVxuUHJpb3JcblByaW9yT2Rkczwtcm91bmQoUHJpb3IvKDEtUHJpb3IpLDIpXG5Qcmlvck9kZHNcblRydWVQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRHcmFkZT09J0YnICYgbW9vZHkkU2VuaW9yaXR5PT0nRnJlc2htYW4nLF0pL25yb3coXG4gIG1vb2R5W21vb2R5JFNlbmlvcml0eSA9PSdGcmVzaG1hbicsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRHcmFkZT09J0YnJiBtb29keSRTZW5pb3JpdHkgIT0nRnJlc2htYW4nLF0pL25yb3cobW9vZHlbbW9vZHkkU2VuaW9yaXR5ICE9J0ZyZXNobWFuJyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5yb3VuZChQb3N0ZXJpb3IsMikifQ== 14.3 Snippet 3: What are the odds that a ‚ÄòA‚Äô student with the score less than 80 is a psychology major? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjQmVsaWVmIC0gd2hhdCB3ZSBkbyBub3Qga25vdy4gI0lzIGEgc3R1ZGVudCBhIHBzeWNob2xvZ3kgI21ham9yP1xuI09ic2VydmF0aW9uID0gd2hhdCB3ZSBkbyAja25vdy4gVGhleSBnb3QgYW4gQSBhbmQgbGVzcyAjdGhhbiA4MCBpbiBzY29yZVxuXG5tb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3YnKVxuUHJpb3I8LW5yb3cobW9vZHlbbW9vZHkkTWFqb3IgPT0nUHN5Y2hvbG9neScsXSkvbnJvdyhtb29keSlcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3cobW9vZHlbbW9vZHkkU2NvcmUgPDgwICYgbW9vZHkkR3JhZGU9PSdBJyYgbW9vZHkkTWFqb3I9PSdQc3ljaG9sb2d5JyxdKS9ucm93KG1vb2R5W21vb2R5JE1ham9yPT0nUHN5Y2hvbG9neScsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRTY29yZSA8ODAgJiBtb29keSRHcmFkZT09J0EnJiBtb29keSRNYWpvciE9J1BzeWNob2xvZ3knLF0pL25yb3cobW9vZHlbbW9vZHkkTWFqb3IhPSdQc3ljaG9sb2d5JyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5Qb3N0ZXJpb3IifQ== "],["Pc1.html", "Section: 15 üîñ Prediction Challenges 15.1 General Structure of the Prediction Challenges 15.2 Challenge 1 - Freestyle prediction of grades in yet another MOODY data set 15.3 Challenge 2 - Same data but using rpart - decision tree", " Section: 15 üîñ Prediction Challenges Prediction challenges differentiate data 101 class at Rutgers from many similar classes at other universities. Here is how we are different: Typically prediction model building is illustrated using real world data. This sounds very attractive and of course is the ultimate goal - but using real world data for your first prediction models is not a good idea. Here is why: Real data sets needs cleaning - and this requires more elaborate programming skills than in data 101. But even if data is ‚Äúclean‚Äù - that is uploadable to R studio. Even if real data is clean, it is often ‚Äúunrelatable‚Äù - like say some animal laboratory results or drug tests. It takes time to learn the data, it may also require some domain expertize to fully comprehend the data. It may take a lot of effort to discover trends and patterns in real data. Sometimes these trends are very weak. This is why we create our prediction challenges synthetically. Our data is relatable and synthetic. For example we start with data which each student can relate to - Grading. These challenges called Professor Moody prediction challenges call for predicting the grade in class based on attributes such as major, seniority, score in class as well as behavioral characterstics in class - texting, dozing off and asking questions. Data sets for our prediction challenges are generated with hidden patterns which make prediction challenges more fun to work on. They are truly data puzzles. For example, it may be the case that asking a lot of questions in class is negatively correlated with the grade. Professor Moody does not like to be bothered! This suprising pattern could have been injected in one of our data sets. Data generation for our data puzzles is accomplished with our own tool, called Data Puzzle Generator. Using data Puzzle Generator one can generate data sets with embedded patterns in very short time. One can also built on the previous data puzzles and add or remove patterns - creating new data puzzles 15.1 General Structure of the Prediction Challenges The submission will take place on Kaggle which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submission. The datasets provided for each prediction challenge is as follows: Training Dataset It is used for training and cross-validation purpose in the prediction challenge. This data has all the training attributes along and the values of the attribute wich is predicted (so called, Target attribute). Models for prediction are to be trained using this dataset only. Training data set is the set which is used when you build your prediction model - since this is the only data set which has all values of target attribute. Testing Dataset It is used for applying your prediction model to new data. You do it only when you are finished with building your prediction model. Testing data set consists of all the attributes that were used for training, but it does not contain any values of the target attribute. It is disjoint with the training data set - it contains new data and it is missing the target variable. Submission Dataset After prediction using the ‚Äútesting‚Äù dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g.¬†ID or name,etc) and second the predicted attribute column. Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle. To read the datasets use the read.csv() function and for writing the dataset to the file, use the write.csv() function. Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle. To avoid this, you can add the parameter, row.names = F in the write.csv() function. e.g.¬†write.csv(dataframe,fileaddress,row.names = F). 15.2 Challenge 1 - Freestyle prediction of grades in yet another MOODY data set This is the next in the sequence of data puzzles about grading methods of the eccentric professor Moody. Professor Moody found out that his former grading methods were leaked to the student by treacherous TA and changed his grading methods (and the TA). Unfortunately, again the data was leaked to the students (Professor Moody does not use passwords). It indicates that Professor Moody may be tougher on certain majors and also may apply different grading criteria for different student seniority levels Can you build a prediction model which will mimic Moody‚Äôs grading as closely as possible? Attached are three files : One M2022train.csv with original Professor Moody grading data and another, the M2022testS.csv data with missing GRADE column. Finally M2022submissionS.csv is the file you will submit to Kaggle of course after filling up the GRADE column. Your job is to predict the grades in the testing file, adding to it GRADE attribute with predicted grades. This test submission will be handled through Kaggle (just the error computation part) and Canvas just like for any assignments so far (Kaggle submission instructions coming). Kaggle will automatically calculate your prediction error. In this case, of Professor Moody data, it will be a fraction of grades which your prediction model have predicted incorrectly. Data League: https://data101.cs.rutgers.edu/?q=node/155 Kaggle competition: https://www.kaggle.com/competitions/predictive-challenge-1-2022/overview Kaggle submission instructions: https://data101.cs.rutgers.edu/?q=node/150 Canvas HW9: https://rutgers.instructure.com/courses/159918/assignments/1953810 15.3 Challenge 2 - Same data but using rpart - decision tree It is the same DATA as prediction challenge 1. Just use rpart() this time. Lets see if you can do better (certainly faster) with the rpart than with freestyle prediction. You have to use rpart, but you can use it as part of your prediction model and combine it with your model which you submitted for HW9. We will talk about rpart in detail in recitations and lectures next week. FOR THIS PREDICTION CHALLENGE: Have to use rpart function (and predict of course). Have to use and show crossvalidation (use crossvalidate(). Explain in ppts how you used crossvalidation. Use rpart contol functions - like minbucket and minsplit as well as different subsets of attributes - when corssvalidating. Make sure you explain in your ppts what ‚Äúcontrols‚Äù have you tried and eventually used. This test submission will be handled through Kaggle (just the error computation part) and Canvas just like for any assignments so far (Kaggle submission instructions coming). Kaggle will automatically calculate your prediction error. In this case, of Professor Moody data, it will be a fraction of grades which your prediction model have predicted incorrectly. Data League: https://data101.cs.rutgers.edu/?q=node/155 Kaggle competition: https://www.kaggle.com/competitions/predictive-challenge-2-2022/overview Kaggle submission instructions: https://data101.cs.rutgers.edu/?q=node/150 Canvas HW10: https://rutgers.instructure.com/courses/159918/assignments/1961012 "],["P1.html", "Section: 16 üîñ Free Style: Prediction 16.1 Snippet 1: Example of a simple freestyle prediction model 16.2 Snippet 2: How to build a freestyle (your own code) prediction model? 16.3 Snippet 3: One-step crossvalidation 16.4 Snippet 4: Preparing submission.csv for Kaggle", " Section: 16 üîñ Free Style: Prediction Lecture slides: Prediction - Free Style 16.1 Snippet 1: Example of a simple freestyle prediction model eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0ZXN0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3ZcIilcblxuc3VtbWFyeSh0ZXN0KVxuXG5teXByZWRpY3Rpb248LXRlc3RcbmRlY2lzaW9uIDwtIHJlcCgnRicsbnJvdyhteXByZWRpY3Rpb24pKVxuZGVjaXNpb25bbXlwcmVkaWN0aW9uJFNjb3JlPjQwXSA8LSAnRCdcbmRlY2lzaW9uW215cHJlZGljdGlvbiRTY29yZT42MF0gPC0gJ0MnXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU2NvcmU+NzBdIDwtICdCJ1xuZGVjaXNpb25bbXlwcmVkaWN0aW9uJFNjb3JlPjgwXSA8LSAnQSdcbm15cHJlZGljdGlvbiRHcmFkZSA8LWRlY2lzaW9uXG5lcnJvciA8LSBtZWFuKHRlc3QkR3JhZGUhPSBteXByZWRpY3Rpb24kR3JhZGUpXG5lcnJvciJ9 16.2 Snippet 2: How to build a freestyle (your own code) prediction model? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9Nb29keU1hcmNoMjAyMmIuY3N2XCIpXG5cbiMgSG93IGRvIHdlIGJ1aWxkIGEgZnJlZXN0eWxlIHByZWRpY3Rpb24gbW9kZWw/ICBEZWZpbml0ZWx5IHN0YXJ0IHdpdGggcGxvdHMgbGlrZSB0aGUgYm94cGxvdCBmcm9tIHRoZSBzZWN0aW9uIDUgKGRhdGEgZXhwbG9yYXRpb24pLiAgQnV0IHRoZW4gZm9sbG93IHVwIHdpdGggZXhwbG9yYXRvcnkgcXVlcmllcyBhcyBpbiB0aGUgcmVjZW50IHF1aXp6ZXMuIEV4YW1wbGVzIGhlcmUgdXNlIHRhYmxlKCkgIGZ1bmN0b24gYW5kIGxvb2sgZm9yIHNpdHVhdGlvbnMgd2hlbiBvbmUgZ3JhZGUgaXMgYWJzb3V0ZWx5IGRvbWluYW50LiBUaGlzIHdvdWxkIGJlIHlvdXIgcHJlZGljdGlvbi4gVGh1cywgdGhlIGdvYWwgaXMgdG8gc2xpY2UgdGhlIGRhdGEgdXNpbmcgc3Vic2V0dGluZyBpbiBzdWNoIGEgd2F5IHRoYXQgZm9yIGVhY2ggc2xpY2UgeW91IGdldCBhIGNsZWFyIFwid2lubmVyIGdyYWRlXCIuIFRoZW4gY29tYmluZSB0aGVzZSBzdWJzZXQgcnVsZXMgaW50byBkZWNpc2lvbiB2ZWN0b3IgLSBqdXN0IGFzIHdlIGRpZCBpbiBzbmlwcGV0IDE0LjEuXG5cbiMgQmVsb3cgc29tZSBleGFtcGxlcyBvZiBzdWNoIGV4cGxvcmF0b3J5IHF1ZXJpZXMgd2l0aCBjbGVhciBncmFkZSB3aW5uZXJzLlxuXG5zdW1tYXJ5KG1vb2R5KVxudGFibGUobW9vZHkkR3JhZGUpXG50YWJsZShtb29keVttb29keSRTY29yZT44MCxdJEdyYWRlKVxudGFibGUobW9vZHlbbW9vZHkkU2NvcmU+ODAgJiBtb29keSRNYWpvcj09J1BzeWNob2xvZ3knLF0kR3JhZGUpXG50YWJsZShtb29keVttb29keSRTY29yZTw0MCAmIG1vb2R5JE1ham9yPT0nRWNvbm9taWNzJyxdJEdyYWRlKVxudGFibGUobW9vZHlbbW9vZHkkU2NvcmU8NDAgJiBtb29keSRTZW5pb3JpdHk9PSdGcmVzaG1hbicsXSRHcmFkZSkifQ== 16.3 Snippet 3: One-step crossvalidation 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 16.4 Snippet 4: Preparing submission.csv for Kaggle eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEhlcmUgeW91IGp1c3QgbmVlZCB0aGUgdGVzdCB0YWJsZSAod2l0aG91dCBncmFkZXMpIHRvIGFwcGx5IHlvdXIgcHJlZGljdGlvbiBtb2RlbCBhbmQgY2FsY3VsYXRlIHByZWRpY3RlZCBncmFkZXMuIEFuZCBzdWJtaXNzaW9uIGRhdGEgZnJhbWUgdG8gZmlsbCBpdCBpbiB3aXRoIHRoZSBwcmVkaWN0ZWQgI2dyYWRlc1xuXG50ZXN0PC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTTIwMjJ0ZXN0U05vR3JhZGUuY3N2JylcbnN1Ym1pc3Npb248LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMnN1Ym1pc3Npb24uY3N2JylcblxubXlwcmVkaWN0aW9uPC10ZXN0XG4jSGVyZSBpcyB5b3VyIG1vZGVsLiBJIGp1c3Qgc2hvdyBleGFtcGxlIG9mIHRyaXZpYWwgcHJlZGljdGlvbiBtb2RlbFxuZGVjaXNpb24gPC0gcmVwKCdGJyxucm93KG15cHJlZGljdGlvbikpXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU2NvcmU+NDBdIDwtICdEJ1xuZGVjaXNpb25bbXlwcmVkaWN0aW9uJFNjb3JlPjYwXSA8LSAnQydcbmRlY2lzaW9uW215cHJlZGljdGlvbiRTY29yZT43MF0gPC0gJ0InXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU2NvcmU+ODBdIDwtICdBJ1xuI05vdyBtYWtlIHlvdXIgc3VibWlzc2lvbiBmaWxlIC0gaXQgd2lsbCBoYXZlIHRoZSBJRHMgYW5kIG5vdyB0aGUgcHJlZGljdGVkIGdyYWRlc1xuc3VibWlzc2lvbiRHcmFkZTwtZGVjaXNpb25cbnN1Ym1pc3Npb25cbiMgdXNlIHdyaXRlLmNzdihzdWJtaXNzaW9uLCAnc3VibWlzc2lvbi5jc3YnLCByb3cubmFtZXM9RkFMU0UpIHRvIHN0b3JlIHN1Ym1pc3Npb24gYXMgY3N2IGZpbGUgb24geW91ciBtYWNoaW5lIGFuZCBzdWJzZXF1ZW50bHkgc3VibWl0IGl0IG9uIEthZ2dsZSJ9 "],["prpart.html", "Section: 17 üîñ Predictions with rpart 17.1 Use of Rpart 17.2 Visualize the Decision tree 17.3 Rpart Control 17.4 Cross Validation 17.5 Prediction using rpart. 17.6 Snippet 11: Your Model with rpart 17.7 Snippet 12: Freestyle + rpart: Combining rpart prediction models 17.8 Snippet 13: Submission with rpart", " Section: 17 üîñ Predictions with rpart Lecture slides: Prediction with rpart Decision trees are one of the most powerful and popular tools for classification and prediction. The reason decision trees are very popular is that they can generate rules which are easier to understand as compared to other models. They require much less computations for performing modeling and prediction. Both continuous/numerical and categorical variables are handled easily while creating the decision trees. 17.1 Use of Rpart Recursive Partitioning and Regression Tree RPART library is a collection of routines which implements a Decision Tree.The resulting model can be represented as a binary tree. The library associated with this RPART is called rpart. Install this library using install.packages(\"rpart\"). Syntax for building the decision tree using rpart(): rpart( formula , method, data, control,...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... method: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use ‚Äúanova‚Äù for regression, ‚Äúclass‚Äù for classification, etc. data: here we provide the dataset on which we want to fit the decision tree on. control: here we provide the control parameters for the decision tree. Explained more in detail in the section further in this chapter. For more info on the rpart function visit rpart documentation Lets look at an example on the Moody 2022 dataset. We will use the rpart() function with the following inputs: prediction -&gt; GRADE predictors -&gt; SCORE, DOZES_OFF, TEXTING_IN_CLASS, PARTICIPATION data -&gt; moody dataset method -&gt; ‚Äúclass‚Äù for classification. 17.1.1 Snippet 1 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uLlxucnBhcnQoR1JBREUgfiBTQ09SRStET1pFU19PRkYrVEVYVElOR19JTl9DTEFTUytQQVJUSUNJUEFUSU9OLCBkYXRhID0gbW9vZHksbWV0aG9kID0gXCJjbGFzc1wiKSJ9 We can see that the output of the rpart() function is the decision tree with details of, node -&gt; node number split -&gt; split conditions/tests n -&gt; number of records in either branch i.e.¬†subset yval -&gt; output value i.e.¬†the target predicted value. yprob -&gt; probability of obtaining a particular category as the predicted output. Using the output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section 17.5 But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section. 17.2 Visualize the Decision tree To visualize and understand the rpart() tree output in the easiest way possible, we use a library called rpart.plot. The function rpart.plot() of the rpart.plot library is the function used to visualize decision trees. NOTE: The online runnable code block does not support rpart.plot library and functions, thus the output of the following code examples are provided directly. 17.2.1 Snippet 2 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMl9uZXcuY3N2JylcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG5ycGFydChHUkFERSB+IFNDT1JFK0RPWkVTX09GRitURVhUSU5HX0lOX0NMQVNTK1BBUlRJQ0lQQVRJT04sIGRhdGEgPSBtb29keSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgaW1wb3J0IHRoZSBycGFydC5wbG90IGxpYnJhcnkgdG8gdXNlIHRoZSBycGFydC5wbG90KCkgZnVuY3Rpb24uXG4jbGlicmFyeShycGFydC5wbG90KVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQucGxvdCgpIGZ1bmN0aW9uICB0byB2aXN1YWxpemUgdGhlIGRlY2lzaW9uIHRyZWUuXG4jcnBhcnQucGxvdCh0cmVlKSJ9 Output Plot of rpart.plot() function We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about the grade category. the outcome probability of each grade category. the records percentage out of total records. To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides rpart.plot and Plotting with rpart.plot (PDF) NOTE: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment. 17.3 Rpart Control Now let‚Äôs look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function. rpart.control( *minsplit*, *minbucket*, *cp*,...) minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -&gt; the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition. minbucket: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -&gt; the minimum number of observation in the terminal/leaf node of the trees must be 500 or above. cp: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by cp, will not be made in the tree. For more information of the other arguments of the rpart.control() function visit rpart.control Let look at few examples. Suppose you want to set the control parameter minsplit=200. 17.3.1 Snippet 3: Minsplit = 200 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIG1pbnNwbGl0PTIwMFxudHJlZSA8LSBycGFydChHUkFERSB+IFNDT1JFK0RPWkVTX09GRitURVhUSU5HX0lOX0NMQVNTK1BBUlRJQ0lQQVRJT04sIGRhdGEgPSBtb29keSwgbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5zcGxpdCA9IDIwMCkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=200 in rpart.control() function 17.3.2 Snippet 4: Minsplit = 100 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIG1pbnNwbGl0PTEwMFxudHJlZSA8LSBycGFydChHUkFERSB+IFNDT1JFK0RPWkVTX09GRitURVhUSU5HX0lOX0NMQVNTK1BBUlRJQ0lQQVRJT04sIGRhdGEgPSBtb29keSwgbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5zcGxpdCA9IDEwMCkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=100 in rpart.control() function We can see from the output of tree$splits and the tree plot, that at each split the total amount of observations are above 200 and 100. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits. Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters. 17.3.3 Snippet 5: Minbucket = 100 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIE1pbmJ1Y2tldD0xMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStET1pFU19PRkYrVEVYVElOR19JTl9DTEFTUytQQVJUSUNJUEFUSU9OLCBkYXRhID0gbW9vZHksIG1ldGhvZCA9IFwiY2xhc3NcIixjb250cm9sPXJwYXJ0LmNvbnRyb2wobWluYnVja2V0ID0gMTAwKSlcblxudHJlZVxuXG4jbGlicmFyeShycGFydC5wbG90KVxuI3JwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=100 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. 17.3.4 Snippet 6: Minbucket = 200 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIE1pbmJ1Y2tldD0yMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStET1pFU19PRkYrVEVYVElOR19JTl9DTEFTUytQQVJUSUNJUEFUSU9OLCBkYXRhID0gbW9vZHksIG1ldGhvZCA9IFwiY2xhc3NcIixjb250cm9sPXJwYXJ0LmNvbnRyb2wobWluYnVja2V0ID0gMjAwKSlcblxudHJlZVxuXG4jbGlicmFyeShycGFydC5wbG90KVxuI3JwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=200 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 200. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. Lets now use the cp parameter and see its effect on the tree. 17.3.5 Snippet 7: cp = 0.05 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIGNwPTAuMlxudHJlZSA8LSBycGFydChHUkFERSB+IC4sIGRhdGEgPSBtb29keSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKGNwID0gMC4wNSkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUpIn0= Output tree plot of after setting cp=0.05 in rpart.control() function 17.3.6 Snippet 8: cp = 0.005 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIGNwPTAuMDA1XG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gLiwgZGF0YSA9IG1vb2R5LG1ldGhvZCA9IFwiY2xhc3NcIixjb250cm9sPXJwYXJ0LmNvbnRyb2woY3AgPSAwLjAwNSkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUpIn0= We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005. 17.4 Cross Validation Overfitting takes place when you have a high accuracy on training dataset, but a low accuracy on the test dataset. But how do you know whether you are overfitting or not? Especially since you cannot determine accuracy on the test dataset? That is where cross-validation comes into play. Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. The partition is defined by split ratio. If split ratio =0.7, 70% of the training dataset will be used for the actual training of your model (rpart or lm), and 30 % will be used for validation (or testing). The accuracy of this validation data is called cross-validation accuracy. To know if you are overfitting or not, compare the training accuracy with the cross-validation accuracy. If your training accuracy is high, and cross-validation accuracy is low, that means you are overfitting. cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*) data: The dataset on which cross validation is to be performed. tree: The decision tree generated using rpart. n_iter: Number of iterations. split_ratio: The splitting ratio of the data into train data and validation data. method: Method of the prediction. ‚Äúclass‚Äù for classification. The way the function works is as follows: It randomly partitions your data into training and validation. It then constructs the following two decision trees on training partition: The tree that you pass to the function. The tree is constructed on all attributes as predictors and with no control parameters. -It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees. The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting. We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired. The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant. Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration. Let‚Äôs look at the cross_validate() function in action in the example below. We will pass the tree with formula as GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, and control parameter, with minsplit=100. And for cross_validate() function, we will usen_iter=5, and split_raitio=0.7 NOTE: Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use install.packages(&quot;devtools&quot;) devtools::install_github(&quot;devanshagr/CrossValidation&quot;) CrossValidation::cross_validate() 17.4.1 Snippet 9 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 You can see that the cross-validation accuracies for the tree that was passed (accuracy_subset) are fairly high and close to our training accuracy of 84%. This means we are not overfitting. Also observe that accuracy_subset and accuracy_all have the same values, which means that the only relevant attributes are score and participation, and adding more attributes doesn‚Äôt make any difference to the tree. Finally, the values in accuracy_subset are reasonably close to each other, which mean low variance. 17.5 Prediction using rpart. Now that we have seen the process to create a decision tree and also plot it, we will like to use the output tree to predict the required attribute. From the moody example, we are trying to predict the grade of students. Lets look at the predict() function to predict the outcomes. predict(*object*,*data*,*type*,...) object: the generated tree from the rpart function. data: the data on which the prediction is to be performed. type: the type of prediction required. One of ‚Äúvector‚Äù, ‚Äúprob‚Äù, ‚Äúclass‚Äù or ‚Äúmatrix‚Äù. Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset. 17.5.1 Snippet 10 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMl9uZXcuY3N2JylcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gU0NPUkUrRE9aRVNfT0ZGK1RFWFRJTkdfSU5fQ0xBU1MrUEFSVElDSVBBVElPTiwgZGF0YSA9IG1vb2R5ICxtZXRob2QgPSBcImNsYXNzXCIpXG50cmVlXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZCkifQ== 17.6 Snippet 11: Your Model with rpart eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjSG93IHRvIGNvbWJpbmUgeW91ciBmcmVlc3R5bGUgcHJlZGljdGlvbiBtb2RlbCB3aXRoIHRoZSBycGFydD8gXG5cbiNPbmUgd2F5IG9mIGRvaW5nIGl0IGlzIHRvIGRpdmlkZSB0aGUgZGF0YSBzZXRzIGludG8gdHdvIG11dHVhbGx5IGV4Y2x1c2l2ZSBzdWJzZXRzICh3aGljaCBjb3ZlciBhbGwgZGF0YSBhbHNvKS4gIEhvdyBkbyB5b3UgbWFrZSB0aGVzZSBzdWJzZXRzPyAgVW5mb3J0dW5hdGVseSB0aGVyZSBpcyBubyBhbGdvcml0aG0gZm9yIHRoaXMgYW5kIGl0IGlzIG1vcmUgcmVseWluZyBvbiBob3cgd2VsbCBpcyB5b3VyIG1vZGVsIGRvaW5nIGZvciBkaWZmZXJlbnQgc2xpY2VzIG9mIHRoZSBkYXRhLiAgXG5cbiNJbiB0aGlzIGV4YW1wbGUgKHNpbWlsYXJseSB0byBzbmlwcGV0IDE2Ljcgd2hlcmUgd2UgY29tYmluZSB0d28gcnBhcnQgbW9kZWxzLCB3ZSBhc3N1bWUgdGhhdCBpbml0aWFsIHNwbGl0IHdlIGRlY2lkZWQgb24gaXMgYmFzZWQgb24gU0NPUkUuIEJ1dCBpbnN0ZWFkIG9mIGhhdmluZyB0d28gcnBhcnQgbW9kZWxzICAoMTYuNyksIHdlIHdpbGwgdXNlIG91ciBwcmVkaWN0aW9uICBtb2RlbCBmcm9tIHByZWRpY3Rpb24gY2hhbGxlbmdlIDEgIGZvciBTQ09SRSA+NTAgYW5kIHJwYXJ0IGZvciBTQ09SRSA8PTUwLlxuXG4jTGV0cyBhc3N1bWUgdGhhdCB5b3VyUHJlZGljdGlvbiBpcyBvdXIgbW9kZWwgZnJvbSBQcmVkaWN0aW9uIENoYWxsZW5nZSAxICh5b3VyIGVudGlyZSBjb2RlIGhhcyB0byBiZSBhcHBsaWVkIGhlcmUgdG8gdGhlIGRhdGEgc2V0IChtb29keSwgYmVsb3cpXG5cbm1vb2R5PC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIyX25ldy5jc3YnKVxuXG4jcnBhcnRNb2RlbDwtcnBhcnQoR1JBREV+LiwgZGF0YT1tb29keVttb29keSRTQ09SRTw9NTAsXSk7XG4jcHJlZF9ycGFydE1vZGVsIDwtIHByZWRpY3QocnBhcnRNb2RlbCwgbmV3ZGF0YT1tb29keVttb29keSRTQ09SRTw9NTAsXSwgdHlwZT1cImNsYXNzXCIpXG4jcHJlZF95b3VyTW9kZWwgPC0geW91clByZWRpY3Rpb25bbW9vZHkkU0NPUkU8PTUwXVxuI215cHJlZGljdGlvbjwtbW9vZHlcblxuIyMgSGVyZSB3ZSBjb21iaW5lIHR3byBtb2RlbHMgLSBvdXIgbW9kZWwgZnJvbSBwcmVkaWN0aW9uIDEgY2hhbGxlbmdlIGFuZCBycGFydC5cblxuI2RlY2lzaW9uIDwtIHJlcCgnRicsbnJvdyhteXByZWRpY3Rpb24pKVxuI2RlY2lzaW9uW215cHJlZGljdGlvbiRTQ09SRT41MF0gPC0gcHJlZF95b3VyTW9kZWxcbiNkZWNpc2lvbltteXByZWRpY3Rpb24kU0NPUkU8PTUwXSA8LWFzLmNoYXJhY3RlcihwcmVkX3JwYXJ0TW9kZWwgKVxuI215cHJlZGljdGlvbiRHUkFERSA8LWRlY2lzaW9uXG4jZXJyb3IgPC0gbWVhbihtb29keSRHUkFERSE9IG15cHJlZGljdGlvbiRHUkFERVxuI2Vycm9yIn0= 17.7 Snippet 12: Freestyle + rpart: Combining rpart prediction models eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxuIyBJbXBvcnQgZGF0YXNldFxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5tb2RlbDE8LXJwYXJ0KEdSQURFfi4sIGRhdGE9bW9vZHlbbW9vZHkkU0NPUkU+NTAsXSk7XG5tb2RlbDI8LXJwYXJ0KEdSQURFfi4sIGRhdGE9bW9vZHlbbW9vZHkkU0NPUkU8PTUwLF0pO1xubW9kZWwxXG5tb2RlbDJcbnByZWQxIDwtIHByZWRpY3QobW9kZWwxLCBuZXdkYXRhPW1vb2R5W21vb2R5JFNDT1JFPjUwLF0sIHR5cGU9XCJjbGFzc1wiKVxucHJlZDIgPC0gcHJlZGljdChtb2RlbDIsIG5ld2RhdGE9bW9vZHlbbW9vZHkkU0NPUkU8PTUwLF0sIHR5cGU9XCJjbGFzc1wiKVxubXlwcmVkaWN0aW9uPC1tb29keVxuZGVjaXNpb24gPC0gcmVwKCdGJyxucm93KG15cHJlZGljdGlvbikpXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU0NPUkU+NTBdIDwtIGFzLmNoYXJhY3RlcihwcmVkMSlcbmRlY2lzaW9uW215cHJlZGljdGlvbiRTQ09SRTw9NTBdIDwtYXMuY2hhcmFjdGVyKHByZWQyKVxubXlwcmVkaWN0aW9uJEdSQURFIDwtZGVjaXNpb25cbmVycm9yIDwtIG1lYW4obW9vZHkkR1JBREUhPSBteXByZWRpY3Rpb24kR1JBREUpXG5lcnJvciJ9 17.8 Snippet 13: Submission with rpart eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxudGVzdDwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L00yMDIydGVzdFNOb0dyYWRlLmNzdicpXG5zdWJtaXNzaW9uPC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTTIwMjJzdWJtaXNzaW9uLmNzdicpXG50cmFpbiA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L00yMDIydHJhaW4uY3N2XCIpXG5cbnRyZWUgPC0gcnBhcnQoR3JhZGUgfiBNYWpvcitTY29yZStTZW5pb3JpdHksIGRhdGEgPSB0cmFpbiwgbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5idWNrZXQgPSAyMDApKVxudHJlZVxuXG5wcmVkaWN0aW9uIDwtIHByZWRpY3QodHJlZSwgdGVzdCwgdHlwZT1cImNsYXNzXCIpXG5cbiNOb3cgbWFrZSB5b3VyIHN1Ym1pc3Npb24gZmlsZSAtIGl0IHdpbGwgaGF2ZSB0aGUgSURzIGFuZCBub3cgdGhlIHByZWRpY3RlZCBncmFkZXNcbnN1Ym1pc3Npb24kR3JhZGU8LXByZWRpY3Rpb24gXG5cbiMgdXNlIHdyaXRlLmNzdihzdWJtaXNzaW9uLCAnc3VibWlzc2lvbi5jc3YnLCByb3cubmFtZXM9RkFMU0UpIHRvIHN0b3JlIHN1Ym1pc3Npb24gYXMgY3N2IGZpbGUgb24geW91ciBtYWNoaW5lIGFuZCBzdWJzZXF1ZW50bHkgc3VibWl0IGl0IG9uIEthZ2dsZSJ9 "],["lr.html", "Section: 18 üîñ Linear Regression 18.1 Linear regression using lm() function 18.2 Calculating the Error using mse() 18.3 Snippet 2: Cross Validate your prediction 18.4 Snippet 3: Submission with lm", " Section: 18 üîñ Linear Regression Lecture slides: Linear Regression Linear regression is a linear approach to modeling the relationship between a numerical response (\\(Y\\)) and one or more independent variables (\\(X_i\\)). Usually in linear regression, models are used to predict only one scalar variable. But there are two subtype if these models: - First when there is only one explanatory variable and one output variable. This type of linear regression model known as simple linear regression. - Second, when there are multiple predictors, i.e.¬†explanatory/dependent variables for the output variable. This type of linear regression model known as multiple linear regression. Linear models fitted to various different type of data spread. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. Credits: Wikipedia. 18.1 Linear regression using lm() function Syntax for building the regression model using the lm() function is as follows: lm(formula, data, ...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the linear regression model is to be trained. For more info on the lm() function visit lm() Lets look at the example on the Moody dataset. Table 18.1: Snippet of Moody Num Dataset Midterm Project FinalExam ClassScore 73 8 70 39.60000 61 100 20 68.20000 58 88 38 67.00000 93 41 46 52.47565 85 52 85 68.50000 97 48 19 49.10000 26 59 22 41.30000 58 62 25 50.10000 53 56 27 46.70000 66 27 17 34.80494 Now we can build a simple linear regression model to predict the ClassScore attribute based on the various other attributes present in the dataset, as shown above. Since we will be predicting only one attribute values, this model will be called simple linear regression model. 18.1.1 Snippet 1: How much do Midterm, Project and Final Exam count? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keU5VTTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TlVNLmNzdicpXG5zcGxpdDwtMC43Km5yb3cobW9vZHlOVU0pXG5zcGxpdFxubW9vZHlOVU1UcjwtbW9vZHlOVU1bMTpzcGxpdCxdXG5tb29keU5VTVRyXG5tb29keU5VTVRzPC1tb29keU5VTVtzcGxpdDpucm93KG1vb2R5TlVNKSxdXG4jV2UgdXNlIGxpbmVhciByZWdyZXNzaW9uIHRvICNmaW5kIG91dCB0aGUgd2VpZ2h0cyBvZiAjTWlkdGVybSwgUHJvamVjdCBhbmQgRmluYWwgI0V4YW0gaW4gY2FsY3VsYXRpb24gb2YgdGhlICNmaW5hbCBjbGFzcyBzY29yZS4gRWFjaCBvZiAjdGhlbSBhcmUgc2NvcmVkIG91dCBvZiAxMDAgYW5kICN0aGUgZmluYWwgY2xhc3Mgc2NvcmUgaXMgYWxzbyAjc2NvcmVkIG91dCBvZiAxMDAgYXMgd2VpZ2h0ZWQgI3N1bSBvZiBNaWR0ZXJtLCBQcm9qZWN0IGFuZCAjRmluYWwgRXhhbSBzY29yZXMuXG50cmFpbiA8LSBsbShDbGFzc1Njb3Jlfi4sICBkYXRhPW1vb2R5TlVNVHIpXG50cmFpblxucHJlZCA8LSBwcmVkaWN0KHRyYWluLG5ld2RhdGE9bW9vZHlOVU1Ucylcbm1lYW4oKHByZWQgLSBtb29keU5VTVRzJENsYXNzU2NvcmUpXjIpIn0= We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained from the predict function using the trained model and the test data. 18.2 Calculating the Error using mse() As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test won‚Äôt prove useful now in our numerical predictions scenario. We don‚Äôt want to eyeball every time we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique. To do this we will use the Mean Squared Error(MSE). The MSE is a measure of the quality of an predictor/estimator It is always non-negative Values closer to zero are better. The equation to calculate the MSE is as follows: \\[\\begin{equation} MSE=\\frac{1}{n} \\sum_{i=1}^{n}{(Y_i - \\hat{Y_i})^2} \\\\ \\text{where $n$ is the number of data points, $Y_i$ are the observed value}\\\\ \\text{and $\\hat{Y_i}$ are the predicted values} \\end{equation}\\] To implement this, we will use the mse() function present in the Metrics Package, so remember to install the Metrics package and use library(Metrics) in the code for local use. The syntax for mse() function is very simple: mse(actual,predicted) actual: vector of the actual values of the attribute we want to predict. predicted: vector of the predicted values obtained using our model. 18.3 Snippet 2: Cross Validate your prediction eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxudHJhaW4gPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9Nb29keU5VTS5jc3ZcIilcbiNzY3JhbWJsZSB0aGUgdHJhaW4gZnJhbWVcbnY8LXNhbXBsZSgxOm5yb3codHJhaW4pKVxudlsxOjVdXG50cmFpblNjcmFtYmxlZDwtdHJhaW5bdiwgXVxuXG4jb25lIHN0ZXAgY3Jvc3N2YWxpZGF0aW9uXG5uIDwtIDEwMFxudHJhaW5TYW1wbGU8LXRyYWluU2NyYW1ibGVkW25yb3codHJhaW5TY3JhbWJsZWQpLW46bnJvdyh0cmFpblNjcmFtYmxlZCksIF1cbnRlc3RTYW1wbGUgPC0gdHJhaW5TY3JhbWJsZWRbMTpuLF1cblxubG0udHJlZSA8LSBsbShDbGFzc1Njb3Jlfi4sICBkYXRhPXRyYWluU2FtcGxlKVxubG0udHJlZVxuXG5wcmVkIDwtIHByZWRpY3QobG0udHJlZSxuZXdkYXRhPXRlc3RTYW1wbGUpXG5wcmVkXG5cbm1zZSh0ZXN0U2FtcGxlJENsYXNzU2NvcmUscHJlZCkifQ== We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the previous model we are using the cross validation technique to check that if we have more accurate predictions, thus increasing the overall accuracy of the model. 18.4 Snippet 3: Submission with lm eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxudGVzdDwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TlVNX3Rlc3QuY3N2JylcbnN1Ym1pc3Npb248LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMnN1Ym1pc3Npb24uY3N2JylcbnRyYWluIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW9vZHlOVU0uY3N2XCIpXG5cbnRyZWUgPC0gbG0oQ2xhc3NTY29yZX4uLCAgZGF0YT10cmFpbilcbnRyZWVcblxucHJlZGljdGlvbiA8LSBwcmVkaWN0KHRyZWUsIG5ld2RhdGE9dGVzdClcblxuI05vdyBtYWtlIHlvdXIgc3VibWlzc2lvbiBmaWxlIC0gaXQgd2lsbCBoYXZlIHRoZSBJRHMgYW5kIG5vdyB0aGUgcHJlZGljdGVkIGdyYWRlc1xuc3VibWlzc2lvbiRHcmFkZTwtcHJlZGljdGlvbiBcblxuIyB1c2Ugd3JpdGUuY3N2KHN1Ym1pc3Npb24sICdzdWJtaXNzaW9uLmNzdicsIHJvdy5uYW1lcz1GQUxTRSkgdG8gc3RvcmUgc3VibWlzc2lvbiBhcyBjc3YgZmlsZSBvbiB5b3VyIG1hY2hpbmUgYW5kIHN1YnNlcXVlbnRseSBzdWJtaXQgaXQgb24gS2FnZ2xlIn0= "],["MLP.html", "Section: 19 üîñ Machine Learning-Prediction Loop", " Section: 19 üîñ Machine Learning-Prediction Loop Lecture slides: Prediction Loop "],["BA.html", "Section: 20 üîñ Boundless Analytics 20.1 Snippet 1: Chi square hunt", " Section: 20 üîñ Boundless Analytics 20.1 Snippet 1: Chi square hunt eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFNheSwgdGhlIEJvdW5kbGVzcyBhbmFseXRpY3MgcHJvdmlkZXMgdXMgd2l0aCB0aGUgc2xpY2U6ICBCZWVyID09J0xhZ2VyJyAmICBEYXkgPT0nV2Vla2VuZCcgYW5kIFNuYWNrcyA9J0NyYWNrZXJzJyBhbmQgYW5jaG9yIGF0dHJpYnV0ZSBpcyBMb2NhdGlvbi4gIFlvdSBjYW4gY2FsY3VsYXRlIENoaXNxIGZvciB0aGlzIHNsaWNlIGFuZCB0aGUgTG9jYXRpb24gYXR0cmlidXRlIHRvIHRlc3QgaWYgZGlzdHJpYnV0aW9uIG9mIGxvY2F0aW9ucyBpcyBhZmZlY3RlZCBpZiB3ZSBsaW1pdCBvdXJzZWx2ZXMgb25seSB0byB0cmFuc2FjdGlvbnMgc2VsbGluZyBMYWdlciBhbmQgQ3JhY2tlcnMgb24gV2Vla2VuZHM/ICBcblxuIyBUaGUgbW9zdCBpbnRlcmVzdGluZyBzbGljZS1hbmNob3IgYXR0cmlidXRlIGNvbWJpbmF0aW9ucyBhcmUgdGhlIG9uZXMgd2l0aCB0aGUgbGFyZ2VzdCBjaGlzcSB0ZXN0IGFuZCBsb3dlc3QgcC12YWx1ZS4gTmV2ZXJ0aGVsZXNzIGRvIG5vdCBmb3JnZXQgYWJvdXQgbXVsdGlwbGUgaHlwb3RoZXNpcyBjb3JyZWN0aW9uIC0gc2luY2Ugd2UgY2FuIG9uIGNoaS1zcXVhcmUgaHVudCBoZXJlIVxuXG5NaW5pbWFya2V0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L0hvbWV3b3JrTWFya2V0MjAyMi5jc3ZcIilcblxuTWluaW1hcmtldCRJTjwtJ091dF9TbGljZSdcbk1pbmltYXJrZXRbTWluaW1hcmtldCRCZWVyPT0nTGFnZXInICYgTWluaW1hcmtldCREYXk9PSdXZWVrZW5kJyAmICBNaW5pbWFya2V0JFNuYWNrcyA9PSdDcmFja2VycycsIF0kSU48LSdJbl9TbGljZSdcbmQ8LXRhYmxlKE1pbmltYXJrZXQkTG9jYXRpb24sIE1pbmltYXJrZXQkSU4pXG5jaGlzcS50ZXN0KGQpIn0= "],["MAD.html", "Section: 21 üîñ How can data fool us?", " Section: 21 üîñ How can data fool us? Lecture Slides: Unlock Mysteries "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
