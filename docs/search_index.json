[["intro.html", "Section: 1 Introduction", " Section: 1 Introduction The objective of this textbook is to provide you with the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. In other words, to introduce you to data science. We call it an active textbook, since students can interact with the book by running and modifying snippets of R code. Students can also test themselves using Query and Code Roulette - on questions and simple coding tasks. Thus, an active textbook interacts with its reader, helps to run code and also asks students questions and gives them simple coding tasks. Concepts which we discuss in the book are widely covered on the web with countless youtube video tutorials. We briefly introduce the basic concepts here as well, but our focus is on problem solving and simple coding. In other words, honing the skills to do something with the data, not just talk about it. To learn a concept you have to know how to code it. To put it bluntly, no coding, no learning! Active textbook is data-centric. We stress that prior to data analysis and exploration, you have to know your data. Paraphrasing the famous saying that real estate is about “location, location and location”, data science is about data, data and data. Using numerous data sets we guide the student through the process of getting acquainted with their data. We call these data sets - data puzzles, since each of them is synthetically created and has hidden patterns embedded in the process of data generation. For each of our data puzzles we show the process of getting familiar with the data beginning from simple scripts called queries and proceeding through as-hoc hypothesis testing as well as Bayesian reasoning. As we said, each data puzzle hides interesting and non-trivial patterns which are to be discovered. This process of discovery makes data science similar to the work of a detective. Discoveries range from grading methods of Professor Moody, factors influencing quality of a party, voter profiles in local town elections or quality of sleep determinants. Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in a statistically convincing manner, perhaps in a colorful and visually appealing way. Finally, you will be able to apply some basic machine learning methods to build, train and test prediction models. All of this will be accomplished in a succinct and crisp way using a small subset of R instructions. We assume no prior programming background. We will teach you as little R as necessary to achieve the goals of this book: explore the data, visualize it, verify hypotheses and build prediction models. Thus, you will be able to do a good chunk of work which data scientists do. We will accomplish this goal through active snippets of executable code. These are examples of R code (around 100 executable snippets of code) embedded in the textbook itself. More importantly, you will be able to modify the code and execute the modified code without need to install any application on your machine. This will allow you to understand the code in the book through the “what if” exploratory process. Thus, every code snippet is just an invitation to endless modifications. This is why we call this textbook - active. Another unique aspect of this textbook is its reliance on data puzzles. These are synthetic data sets with embedded patterns and rules generated by our tool called DataMaker. We will present our data puzzles (dynamic list, may vary from year to year) in section 8 following introduction to plots, then we will proceed to freestyle data exploration. This will allow us to learn more about our data, form the leads, and finally state our hypotheses. We will follow up by an elementary introduction to hypothesis testing through a permutation test. We will learn how to calculate p-values and how to use them to defend our findings against the randomness trap. This will be particularly important in case of multiple hypotheses when one has to be particularly careful to avoid “false positives” . We will introduce Bayesian reasoning and learn how to compute posterior odds of a belief given an observation. All these important concepts will be introduced via executable snippets of code and “what if” practicing. We will then enter the key section of the book - the data puzzles section. In the data puzzle section, for each data set we will go through the process of getting to know the data and using the concepts learned so far by executing code tailored to each of the data puzzles. In the second part of the book we discuss prediction models. We focus on decision trees (rpart() - recursive partitioning) and linear regression. But we also show how to use other machine learning methods from the rich R-library. We go over cross-validation and show how to build prediction models which combine multiple machine learning models. We stress the importance of knowing your data first, instead of just blind application of machine learning packages. Humans in the loop is very important and prior data exploration and visualization leads to improved quality prediction models. Students can practice prediction model building on especially prediction snippets to make themselves prepared for Kaggle based prediction challenge competition which takes the last months of the data 101 class. The last leaderboard of 2022 challenge is presented here LeaderBoard . We will use as few R functions as possible to achieve our goals. In fact we will demonstrate how using less than ten R functions is sufficient for us. In the appendix, we show many more useful commands of R which eventually you would have to use. However, our goal in this short textbook, is to present the shortest path to data analysis which will let you import the data, plot it, make some analysis yourself and use R-libraries to build machine learning models. In this textbook and in this class we do not teach how to clean the data (data wrangling) and how to deal with a wide variety of data types. We also do not address complex data transformations such as multi-frame operations like merge function. We also do not explain how different machine learning methods work, we only show you how to use them. It is similar to teaching one how to drive a car without knowing how a car engine works. Sections 2.5 and 2.6 provide the lists of all concepts which we cover in our active textbook and all R functions which are needed. Notice how small the set of R functions is. It is important for programming novices to start small and also see how far this small set of functions can get you. Our question roulette allows self-testing on nearly 100 questions relevant to the material. Each question is answered, but students are encouraged first to answer questions themselves and only then follow it with checking the correct answer. The code roulette, on the other hand, consists of around 100 of simple common data science coding tasks. "],["Setting_up_R.html", "Section: 2 Setting Up R 2.1 Create New Project 2.2 How to upload a data set? 2.3 Saving your work 2.4 General R References 2.5 Textbook Concepts 2.6 R functions used in this class", " Section: 2 Setting Up R Important Instructions Installation of R is required before installing RStudio “R” is a programming language, and, “RStudio” is an Integrated Development Environment (IDE) which provides you a platform to code in R. How to download and install R &amp; RStudio? Downloading and installing R. For Windows Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cran.r-project.org/bin/windows/base/ Click on the link at top left where it says “Download R 4.0.3 for windows” or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. For MAC Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cloud.r-project.org/bin/macosx/ Under “Latest release”, click on “R-4.0.3.pkg” or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. Downloading and installing RStudio. For Windows Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named “All Installers”. Click on the download link beside “Windows 10/8/7” to download the windows version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. For MAC Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named “All Installers”. Click on the link beside “macOS 10.13+” to start your download the MAC version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. 2.1 Create New Project After installing R studio successfully the first step is to create a project R studio. Step 1: Go to File -&gt; New Project New Project Step 2: Select New Directory New Directory Step 3: Select New Project New Project Step 4: Give your preferred directory name like “Data101_Assignmnets” Directory Name Step 5: Click on Create Project and finally the R studio should look like Rstudio 2.2 How to upload a data set? To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon. There are two options while accessing the dataset from your local machine: To avoid giving long directory paths for accessing the dataset, one should use the command getwd() to get the current working directory and store the dataset in the same directory. Getwd To access the dataset stored in the same directory one can use the following: read.csv(“MOODY_DATA.csv”). Store the moody dataset in the same directory One can also store the dataset at a different location and can access it using the following command: (Suppose the dataset is stored inside the folder Data101_Tutorials on the desktop) - For Windows Users. - Example: read.csv(&quot;C:/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv&quot;) - For MAC Users. - Example: read.csv(&quot;/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv&quot;) Note: The directory path given here is the current working directory hosted on Github where the dataset has been stored. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFJlYWQgaW4gdGhlIGRhdGFcbmRmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIilcblxuIyBQcmludCBvdXQgYGRmYFxuaGVhZChkZikifQ== 2.3 Saving your work To save your work go to File -&gt; Save. It will ask you to give a name for your .R file and then click on Save. Save After making modifications to your saved file, you will need to save the file again. If the name of the file on the top is in Red Color indicates that the file have unsaved changes. Unsaved File Go to File -&gt; Save to save your .R file again. After saving the file the color of the file name i.e. HW1.R will again change back to black. Saved File Note: You can create multiple files inside the same project such as for your each homework assignments 2.4 General R References https://www.w3schools.com/r/ https://cran.r-project.org/doc/contrib/Short-refcard.pdf https://www.amazon.com/Statistics-Engineers-Scientists-William-Navidi/dp/0073376337/ref=pd_lpo_3?pd_rd_i=0073376337&amp;psc=1 https://data101.cs.rutgers.edu/laboratory/ 2.5 Textbook Concepts Hypothesis testing: 5 Difference of means hypothesis testing: 5 Null Hypothesis: 5 Alternative Hypothesis: 5 z-value: 5 critical value: 5 significance level: 5 p-value: 5 Bonferroni correction: 7 Chi square test: 6 Independence: 6 Multiple Hypothesis testing: 7 False Discovery Proportion: 7 Contingency Matrix: 6 Bayesian Reasoning: 10 Prior odds: 10 Posterior odds: 10 Likelihood ratio: 10 False positive: 10 True positive: 10 Crossvalidation: 12.5 Decision trees: 12 Linear regression: 13 Recursive partitioning: 13 MSE: 13 Prediction accuracy: 13 Training: 13 Testing: 13 2.6 R functions used in this class Elementary instructions: c() 3.1, mean() 3.4.1, nrow() 3.5.1, rep(), sd() 3.4.5, cut() ?? Plots: plot() 4.1, barplot() 4.2, boxplot() 4.3 mosaicplot() 4.4 Data Transformations: subset() 3.5, tapply() 3.6, table() 3.3, aggregate() Library functions: chisq.test() 6, pnorm() 5.2, Permutation() 5.2, rpart() 12, predict() 12.6, lm() 13.2, crossvalidation() 12.5 Parameters of rpart: minsplit 12.4, minbucket 12.4, cp 12.4 "],["tapply_subsetting.html", "Section: 3 🔖 Basic R Intructions 3.1 Vector 3.2 Data Frames 3.3 Table 3.4 Basic Functions 3.5 Subset 3.6 tapply 3.7 Additional references", " Section: 3 🔖 Basic R Intructions In this section we introduce the absolutely basic R instructions, call it R101, which will be sufficient for the entire data 101 class. This is a very small subset of the entire R. The good news is that using this very small subset of R we can accomplish all coding objectives for data 101! The set we present below is a mix of simple arithmetic aggregate functions such as mean() 3.4.1, max() 3.4.3, sum() , basic data structures such as vectors and data frames and finally, two core functions defined for data frames: subset() 3.5, tapply() 3.6 and table() 3.3 function defined on vectors. 3.1 Vector A vector is simply a list of items that are of the same type. 3.1.1 Snippet 1 Lets look at example of creating a vector: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgMyB2ZWN0b3JzIHdpdGggdGl0bGUsIGF1dGhvciBhbmQgeWVhci5cbmNvbG9yIDwtIGMoJ1JlZCcsJ0JsdWUnLCdZZWxsb3cnLCdHcmVlbicpXG5cbiNMZXRzIGxvb2sgYXQgaG93IHRoZSBjcmVhdGVkIHZlY3RvcnMgbG9vay5cbmNvbG9yIn0= 3.1.2 Snippet 2 Create a vector with numerical values in a sequence, use the : operator: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgYSB2ZWN0b3JzIHdpdGggbnVtZXJpY2FsIHNlcXVlbmNlLlxueWVhciA8LSAyMDE4OjIwMjJcblxuI0xldHMgbG9vayBhdCBob3cgdGhlIGNyZWF0ZWQgdmVjdG9ycyBsb29rLlxueWVhciJ9 3.2 Data Frames Data Frames are data displayed in a format as a table. 3.2.1 Snippet 1 Data frames will serve as containers of imported data - typically data provided in csv format, like the moody data set above. Snippet 4.21 shows how to populate a data frame using read.csv() instruction. Notice that the moody data frame which is the container for the imported data set will automatically inherit attribute names (columns) of the underlying data set. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQgaW50byB0aGUgbW9vZHkgdmFyaWFibGVcbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMi5jc3ZcIilcblxuIyBOb3cgbGV0cyB2aWV3IHRoZSBkYXRhZnJhbWUgbW9vZHkgd2l0aCBqdXN0IDUtNiB0dXBsZXNcbmhlYWQobW9vZHkpIn0= 3.2.2 Snippet 2 Get the summary of the dataframe: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQgaW50byB0aGUgbW9vZHkgdmFyaWFibGVcbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMi5jc3ZcIilcblxuIyBVc2UgdGhlIHN1bW1hcnkoKSBmdW5jdGlvbiB0byBzdW1tYXJpemUgdGhlIGRhdGEgZnJvbSBhIERhdGEgRnJhbWU6XG5zdW1tYXJ5KG1vb2R5KSJ9 3.2.3 Snippet 3 We can select subsets of columns and subsets of rows for a data frame using the following the notation data[rows, columns]: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQgaW50byB0aGUgbW9vZHkgdmFyaWFibGVcbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMi5jc3ZcIilcblxuIyBSZXR1cm4gcm93IDFcbm1vb2R5WzEsIF1cblxuIyBSZXR1cm4gY29sdW1uIDVcbm1vb2R5WywgNV1cblxuIyBSb3dzIDE6NSBhbmQgY29sdW1uIDJcbm1vb2R5WzE6NSwgMl1cblxuIyBHaXZlIG1lIHJvd3MgMS0zIGFuZCBjb2x1bW5zIDIgYW5kIDQgb2YgbW9vZHlcbm1vb2R5WzE6MywgYygyOjQpXSJ9 3.3 Table The function table() displays frequency distribution of its arguments. table() function is fundamental for plots as well as a tool to “know your data”. 3.3.1 Snippet 1 The below examples show how to use this function: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpICN3ZWIgbG9hZFxuXG4jbGV0cyBtYWtlIGEgdGFibGUgZm9yIHRoZSBncmFkZXMgb2Ygc3R1ZGVudHMgYW5kIGNvdW50cyBvZiBzdHVkZW50cyBmb3IgZWFjaCBHcmFkZS4gXG5ncmFkZXMgPC0gdGFibGUobW9vZHkkR3JhZGUpXG5cbiNKb2ludCBkaXN0cmlidXRpb24gb2YgZ3JhZGUgYW5kIG1ham9yXG50YWJsZShtb29keSRHcmFkZSwgbW9vZHkkTWFqb3IpIn0= 3.4 Basic Functions Table 3.1: Snippet of moody Dataset Major Score Seniority GPA Grade 905 Statistics 14 Junior 4 F 247 Statistics 10 Senior 2 F 869 CS 98 Junior 1 A 50 CS 36 Senior 1 F 698 Economics 95 Junior 4 A 3.4.1 mean() mean() function is used to find the average of values in a numerical vector. dc_light_exercise_unnamed-chunk-34 ## [1] 56.398 3.4.2 length() length() function is used to get the number of elements in any vector dc_light_exercise_unnamed-chunk-35 ## [1] 1000 3.4.3 max() max() function is used to get the maximum value in a numerical vector. dc_light_exercise_unnamed-chunk-36 ## [1] 100 3.4.4 min() min() function is used to get the minimum value in a numerical vector dc_light_exercise_unnamed-chunk-37 ## [1] 0 3.4.5 sd() sd() function is used to find the standard deviation of numerical vector dc_light_exercise_unnamed-chunk-38 ## [1] 29.02775 Now we are ready to introduce basic data transformation techniques such as slicing and dicing. Slicing, otherwise known as subsetting, allows the selection of data frame subsets. These subsets are defined by boolean conditions built from Attribute op value pairs where op is one of the arithmetic operators such as =, !=, &lt; etc. For example (Score &gt;70)&amp; (Grade ==’A’) refers to a subset of a data frame describing students who scored more than 70 points and got an A. Dicing refers to eliminating some of the attributes from a data frame - it is vertical slicing - which results in a more “narrow” frame. Finally we can also expand our data frame with new, so called derived, attributes. This is a very useful operation in data analysis since it allows so-called “feature engineering”. These new user-defined features can lead to totally new insights into the data. 3.5 Subset The following snippets demonstrate two ways of subsetting a data frame: first through explicit function subset() and second through the native sub-data frame notation df[ ]. 3.5.1 Snippet 1 Subsetting data frame via subset() function dc_light_exercise_unnamed-chunk-39 ## [1] 1000 ## [1] 229 3.5.2 Snippet 2 Example of subset function dc_light_exercise_unnamed-chunk-40 ## Major Score Seniority GPA Grade ## 5 Psychology 35 Freshman 2.8 F ## 10 Psychology 52 Freshman 2.8 D ## 14 Psychology 15 Freshman 3.8 F ## 15 Psychology 76 Sophomore 1.8 A ## 16 Psychology 36 Senior 2.8 F ## 23 Psychology 72 Senior 2.8 A ## 26 Psychology 78 Sophomore 2.8 A ## 28 Psychology 14 Sophomore 4.0 F ## 34 Psychology 95 Sophomore 3.8 A ## 36 Psychology 65 Senior 1.8 C ## 38 Psychology 16 Junior 2.8 F ## 40 Psychology 72 Sophomore 2.8 A ## 44 Psychology 8 Junior 1.8 F ## 47 Psychology 98 Sophomore 1.8 A ## 52 Psychology 5 Junior 3.8 F ## 54 Psychology 4 Junior 4.0 F ## 56 Psychology 59 Freshman 3.8 C ## 61 Psychology 47 Senior 2.8 D ## 62 Psychology 18 Freshman 4.0 F ## 65 Psychology 95 Senior 3.8 A ## 72 Psychology 18 Junior 3.0 F ## 75 Psychology 47 Junior 2.8 D ## 86 Psychology 99 Sophomore 4.0 A ## 88 Psychology 7 Sophomore 3.8 F ## 97 Psychology 66 Sophomore 1.8 C ## 100 Psychology 34 Freshman 3.8 F ## 102 Psychology 9 Senior 3.8 F ## 104 Psychology 64 Sophomore 3.8 C ## 107 Psychology 1 Junior 4.0 F ## 108 Psychology 69 Junior 2.0 C ## 118 Psychology 50 Freshman 4.0 D ## 122 Psychology 97 Senior 2.8 A ## 123 Psychology 71 Sophomore 1.8 A ## 127 Psychology 76 Freshman 1.8 A ## 128 Psychology 71 Freshman 4.0 B ## 132 Psychology 2 Senior 4.0 F ## 141 Psychology 19 Sophomore 3.8 F ## 143 Psychology 13 Freshman 2.8 F ## 151 Psychology 97 Senior 4.0 A ## 154 Psychology 18 Senior 3.8 F ## 164 Psychology 99 Freshman 3.8 A ## 167 Psychology 62 Freshman 1.8 C ## 170 Psychology 93 Senior 3.8 A ## 173 Psychology 33 Sophomore 1.8 F ## 177 Psychology 29 Senior 4.0 F ## 178 Psychology 84 Senior 1.0 B ## 204 Psychology 14 Junior 1.8 F ## 208 Psychology 60 Sophomore 2.8 C ## 224 Psychology 51 Junior 1.8 D ## 229 Psychology 6 Senior 2.8 F ## 230 Psychology 47 Freshman 1.8 D ## 231 Psychology 97 Freshman 4.0 A ## 236 Psychology 38 Sophomore 2.8 F ## 237 Psychology 35 Sophomore 3.0 F ## 242 Psychology 15 Junior 2.8 F ## 244 Psychology 26 Senior 2.8 F ## 245 Psychology 30 Senior 2.8 F ## 246 Psychology 6 Senior 1.8 F ## 265 Psychology 25 Senior 4.0 F ## 276 Psychology 50 Senior 1.8 D ## 278 Psychology 1 Junior 4.0 F ## 279 Psychology 69 Sophomore 2.8 C ## 286 Psychology 56 Junior 3.8 C ## 299 Psychology 63 Senior 1.8 C ## 303 Psychology 53 Junior 1.8 D ## 305 Psychology 94 Junior 4.0 A ## 307 Psychology 20 Sophomore 4.0 F ## 309 Psychology 91 Junior 2.8 A ## 323 Psychology 43 Junior 1.8 D ## 327 Psychology 50 Sophomore 1.8 D ## 328 Psychology 58 Sophomore 1.8 C ## 332 Psychology 33 Sophomore 2.8 F ## 334 Psychology 4 Sophomore 4.0 F ## 335 Psychology 32 Junior 4.0 F ## 337 Psychology 54 Senior 1.8 D ## 342 Psychology 74 Sophomore 4.0 A ## 344 Psychology 80 Senior 1.8 A ## 357 Psychology 96 Senior 3.8 A ## 358 Psychology 67 Senior 4.0 C ## 366 Psychology 89 Senior 4.0 A ## 370 Psychology 93 Freshman 3.8 A ## 389 Psychology 95 Sophomore 2.8 A ## 390 Psychology 15 Freshman 3.8 F ## 408 Psychology 68 Senior 4.0 C ## 419 Psychology 68 Junior 3.8 C ## 420 Psychology 63 Sophomore 1.8 C ## 424 Psychology 31 Junior 3.8 F ## 426 Psychology 80 Senior 3.8 A ## 429 Psychology 10 Sophomore 2.8 F ## 430 Psychology 11 Sophomore 2.8 F ## 438 Psychology 18 Junior 2.8 F ## 441 Psychology 25 Senior 4.0 F ## 452 Psychology 36 Freshman 1.8 F ## 459 Psychology 2 Freshman 2.8 F ## 469 Psychology 13 Senior 4.0 F ## 471 Psychology 94 Sophomore 4.0 A ## 474 Psychology 30 Senior 1.8 F ## 479 Psychology 37 Sophomore 2.8 F ## 481 Psychology 18 Sophomore 2.8 F ## 485 Psychology 80 Sophomore 4.0 A ## 486 Psychology 47 Sophomore 1.8 D ## 487 Psychology 95 Senior 3.8 A ## 488 Psychology 46 Junior 1.8 D ## 491 Psychology 83 Sophomore 3.8 A ## 497 Psychology 100 Junior 3.8 A ## 500 Psychology 33 Freshman 3.8 F ## 505 Psychology 32 Senior 3.8 F ## 508 Psychology 16 Freshman 3.8 F ## 511 Psychology 71 Junior 1.8 A ## 512 Psychology 98 Freshman 1.8 A ## 520 Psychology 17 Junior 4.0 F ## 522 Psychology 82 Junior 3.8 A ## 523 Psychology 5 Junior 2.8 F ## 524 Psychology 12 Junior 4.0 F ## 525 Psychology 10 Senior 2.8 F ## 526 Psychology 73 Sophomore 2.8 A ## 528 Psychology 93 Junior 4.0 A ## 529 Psychology 57 Freshman 2.8 C ## 531 Psychology 61 Senior 3.8 C ## 532 Psychology 81 Sophomore 3.8 A ## 542 Psychology 86 Senior 1.8 A ## 545 Psychology 17 Senior 3.8 F ## 547 Psychology 49 Freshman 2.8 D ## 549 Psychology 4 Sophomore 2.8 F ## 551 Psychology 21 Senior 1.8 F ## 556 Psychology 95 Freshman 1.8 A ## 562 Psychology 38 Sophomore 2.8 F ## 564 Psychology 70 Freshman 3.8 C ## 565 Psychology 54 Junior 3.8 D ## 568 Psychology 68 Freshman 3.8 C ## 569 Psychology 51 Senior 2.8 D ## 570 Psychology 94 Freshman 3.8 A ## 575 Psychology 32 Sophomore 2.8 F ## 586 Psychology 91 Sophomore 4.0 A ## 598 Psychology 35 Sophomore 1.0 F ## 601 Psychology 60 Sophomore 4.0 C ## 608 Psychology 36 Senior 3.8 F ## 614 Psychology 45 Senior 1.0 F ## 618 Psychology 91 Sophomore 1.8 A ## 624 Psychology 23 Senior 1.8 F ## 629 Psychology 25 Freshman 4.0 F ## 632 Psychology 41 Sophomore 1.8 D ## 633 Psychology 64 Freshman 1.8 C ## 635 Psychology 38 Freshman 2.8 F ## 641 Psychology 87 Freshman 2.8 A ## 642 Psychology 29 Sophomore 4.0 F ## 645 Psychology 98 Freshman 1.8 A ## 646 Psychology 59 Senior 1.8 C ## 659 Psychology 100 Freshman 4.0 A ## 660 Psychology 75 Senior 3.8 A ## 663 Psychology 81 Junior 4.0 A ## 664 Psychology 5 Senior 4.0 F ## 671 Psychology 23 Senior 2.8 F ## 673 Psychology 92 Junior 3.8 A ## 678 Psychology 82 Senior 3.8 A ## 684 Psychology 97 Sophomore 3.8 A ## 689 Psychology 1 Sophomore 1.8 F ## 701 Psychology 45 Senior 2.8 D ## 709 Psychology 37 Freshman 2.8 F ## 719 Psychology 62 Freshman 1.0 D ## 720 Psychology 90 Senior 3.8 A ## 721 Psychology 7 Senior 4.0 F ## 722 Psychology 45 Freshman 3.8 D ## 727 Psychology 99 Sophomore 2.8 A ## 735 Psychology 84 Junior 4.0 A ## 737 Psychology 28 Junior 4.0 F ## 740 Psychology 30 Junior 4.0 F ## 742 Psychology 5 Freshman 3.0 F ## 743 Psychology 80 Sophomore 2.8 A ## 748 Psychology 55 Senior 1.8 D ## 763 Psychology 27 Sophomore 1.8 F ## 764 Psychology 45 Freshman 2.8 D ## 769 Psychology 10 Freshman 3.0 F ## 772 Psychology 99 Freshman 4.0 A ## 777 Psychology 13 Junior 1.8 F ## 779 Psychology 85 Sophomore 2.8 A ## 781 Psychology 32 Freshman 2.8 F ## 785 Psychology 58 Junior 2.8 C ## 792 Psychology 93 Sophomore 4.0 A ## 794 Psychology 38 Senior 3.8 F ## 799 Psychology 42 Senior 4.0 D ## 803 Psychology 49 Junior 2.8 D ## 807 Psychology 77 Senior 3.8 A ## 808 Psychology 39 Sophomore 2.8 F ## 810 Psychology 9 Sophomore 4.0 F ## 815 Psychology 70 Junior 2.8 C ## 820 Psychology 93 Sophomore 4.0 A ## 828 Psychology 4 Junior 3.8 F ## 829 Psychology 68 Freshman 2.8 C ## 835 Psychology 44 Junior 1.8 D ## 840 Psychology 70 Freshman 1.8 C ## 844 Psychology 5 Sophomore 4.0 F ## 849 Psychology 29 Sophomore 4.0 F ## 855 Psychology 68 Freshman 4.0 C ## 860 Psychology 8 Senior 1.8 F ## 861 Psychology 60 Senior 2.8 C ## 867 Psychology 70 Senior 3.8 C ## 878 Psychology 89 Junior 4.0 A ## 882 Psychology 82 Freshman 4.0 A ## 887 Psychology 99 Senior 2.8 A ## 892 Psychology 74 Junior 3.8 A ## 896 Psychology 37 Senior 3.8 F ## 897 Psychology 34 Freshman 1.8 F ## 907 Psychology 31 Senior 1.8 F ## 909 Psychology 52 Senior 3.8 D ## 919 Psychology 54 Sophomore 1.8 D ## 923 Psychology 40 Junior 3.8 F ## 927 Psychology 3 Sophomore 1.8 F ## 930 Psychology 11 Sophomore 2.8 F ## 931 Psychology 90 Sophomore 1.8 A ## 935 Psychology 18 Junior 2.8 F ## 936 Psychology 99 Freshman 3.8 A ## 940 Psychology 48 Sophomore 4.0 D ## 943 Psychology 45 Junior 4.0 D ## 945 Psychology 67 Sophomore 3.8 C ## 948 Psychology 73 Sophomore 4.0 A ## 949 Psychology 74 Junior 2.8 A ## 958 Psychology 85 Sophomore 1.8 A ## 960 Psychology 15 Junior 1.8 F ## 963 Psychology 2 Senior 4.0 F ## 964 Psychology 26 Senior 4.0 F ## 966 Psychology 98 Senior 2.8 A ## 977 Psychology 21 Freshman 2.0 F ## 978 Psychology 73 Sophomore 3.8 A ## 979 Psychology 34 Sophomore 1.8 F ## 980 Psychology 15 Senior 3.8 F ## 983 Psychology 7 Senior 1.8 F ## 984 Psychology 84 Freshman 2.8 A ## 1000 Psychology 39 Junior 1.8 F ## Major Score Seniority GPA Grade ## 1 Statistics 56 Senior 2.0 D ## 2 Statistics 43 Sophomore 2.0 F ## 3 Economics 26 Junior 1.0 F ## 4 Economics 70 Sophomore 4.0 C ## 6 Economics 31 Sophomore 4.0 F ## 7 Statistics 11 Sophomore 3.0 F ## 8 CS 98 Junior 4.0 B ## 9 Statistics 100 Sophomore 4.0 A ## 11 CS 98 Junior 3.0 A ## 12 CS 45 Junior 4.0 F ## 13 CS 96 Junior 3.0 A ## 17 Economics 17 Senior 3.0 F ## 18 Economics 8 Junior 4.0 F ## 19 Statistics 97 Senior 2.0 A ## 20 Economics 63 Sophomore 2.0 C ## 21 Economics 34 Sophomore 1.0 F ## 22 CS 59 Senior 3.0 D ## 24 CS 33 Freshman 1.0 F ## 25 Economics 29 Junior 2.0 F ## 27 Economics 40 Junior 2.0 F ## 29 Statistics 95 Sophomore 3.0 A ## 30 Economics 11 Sophomore 1.0 F ## 31 Statistics 79 Sophomore 2.0 C ## 32 Statistics 10 Senior 4.0 F ## 33 CS 97 Sophomore 4.0 A ## 35 Statistics 52 Freshman 2.0 D ## 37 CS 28 Junior 3.0 F ## 39 CS 98 Junior 4.0 A ## 41 Statistics 90 Senior 4.0 B ## 42 CS 85 Senior 1.0 B ## 43 CS 80 Junior 3.0 C ## 45 CS 74 Freshman 4.0 C ## 46 Economics 21 Sophomore 4.0 F ## 48 Economics 50 Freshman 3.0 D ## 49 Statistics 48 Freshman 1.0 F ## 50 CS 36 Senior 1.0 F ## 51 CS 93 Junior 4.0 B ## 53 Economics 48 Junior 2.0 D ## 55 CS 95 Sophomore 4.0 A ## 57 Statistics 35 Senior 4.0 F ## 58 Statistics 12 Sophomore 4.0 F ## 59 Economics 57 Freshman 4.0 C ## 60 CS 62 Freshman 1.0 D ## 63 CS 48 Senior 4.0 F ## 64 CS 93 Junior 1.0 B ## 66 Statistics 90 Sophomore 2.0 B ## 67 Economics 35 Senior 3.0 F ## 68 Economics 35 Junior 1.0 F ## 69 Statistics 80 Sophomore 4.0 C ## 70 CS 95 Freshman 3.0 A ## 71 Statistics 37 Senior 3.0 F ## 73 CS 50 Sophomore 3.0 F ## 74 CS 95 Junior 3.0 B ## 76 Economics 43 Freshman 2.0 D ## 77 Statistics 88 Senior 4.0 B ## 78 Economics 40 Junior 4.0 F ## 79 Economics 27 Senior 4.0 F ## 80 Economics 87 Senior 4.0 A ## 81 Economics 12 Junior 4.0 F ## 82 Statistics 21 Sophomore 4.0 F ## 83 Economics 43 Freshman 4.0 D ## 84 Statistics 71 Junior 4.0 C ## 85 CS 79 Freshman 2.0 C ## 87 CS 80 Freshman 1.0 C ## 89 Economics 30 Sophomore 4.0 F ## 90 Economics 18 Senior 4.0 F ## 91 Economics 85 Freshman 4.0 A ## 92 Statistics 93 Junior 3.0 A ## 93 CS 90 Junior 2.0 B ## 94 Economics 49 Senior 4.0 D ## 95 Statistics 0 Senior 1.8 F ## 96 Statistics 31 Senior 2.0 F ## 98 CS 95 Freshman 4.0 A ## 99 Statistics 27 Sophomore 1.0 F ## 101 CS 45 Sophomore 3.0 F ## 103 Economics 56 Sophomore 1.0 C ## 105 Economics 1 Freshman 3.0 F ## 106 Economics 94 Senior 4.0 A ## 109 CS 62 Senior 4.0 D ## 110 CS 31 Freshman 4.0 F ## 111 CS 75 Junior 1.0 C ## 112 Economics 51 Freshman 2.0 D ## 113 CS 23 Sophomore 1.0 F ## 114 CS 68 Sophomore 1.0 D ## 115 Economics 78 Senior 4.0 A ## 116 Statistics 56 Freshman 1.0 D ## 117 CS 41 Sophomore 1.0 F ## 119 Economics 13 Junior 3.0 F ## 120 CS 59 Junior 2.0 D ## 121 CS 17 Sophomore 2.0 F ## 124 Economics 12 Freshman 2.0 F ## 125 CS 57 Junior 2.0 D ## 126 Statistics 31 Senior 4.0 F ## 129 CS 61 Senior 3.0 D ## 130 Statistics 55 Freshman 3.0 D ## 131 Economics 66 Senior 2.0 C ## 133 CS 99 Junior 4.0 B ## 134 CS 98 Senior 3.0 A ## 135 CS 98 Senior 4.0 A ## 136 CS 85 Senior 4.0 B ## 137 CS 70 Junior 4.0 D ## 138 CS 97 Junior 1.0 B ## 139 CS 18 Senior 2.0 F ## 140 Statistics 41 Senior 3.0 F ## 142 Statistics 76 Senior 1.0 C ## 144 Statistics 96 Junior 3.0 A ## 145 CS 49 Freshman 4.0 F ## 146 Economics 70 Sophomore 1.0 C ## 147 Economics 75 Junior 3.0 B ## 148 Statistics 73 Sophomore 2.0 C ## 149 CS 65 Freshman 4.0 D ## 150 Statistics 39 Freshman 3.0 F ## 152 CS 44 Freshman 3.0 F ## 153 Statistics 71 Freshman 4.0 C ## 155 Statistics 81 Junior 3.0 B ## 156 CS 82 Senior 3.0 C ## 157 CS 66 Sophomore 4.0 D ## 158 Economics 79 Senior 4.0 A ## 159 CS 34 Junior 3.0 F ## 160 CS 52 Sophomore 4.0 F ## 161 Economics 66 Freshman 4.0 C ## 162 Economics 97 Sophomore 4.0 A ## 163 Economics 71 Senior 1.0 B ## 165 Statistics 58 Junior 1.0 D ## 166 Statistics 76 Junior 4.0 C ## 168 Statistics 28 Freshman 2.0 F ## 169 Statistics 95 Sophomore 1.0 A ## 171 CS 25 Junior 4.0 F ## 172 Statistics 81 Sophomore 4.0 B ## 174 Statistics 95 Junior 2.0 A ## 175 CS 52 Freshman 1.0 F ## 176 Economics 100 Sophomore 4.0 A ## 179 CS 92 Sophomore 3.0 A ## 180 Economics 66 Freshman 3.0 C ## 181 CS 23 Sophomore 1.0 F ## 182 CS 87 Junior 3.0 B ## 183 CS 42 Freshman 2.0 F ## 184 Statistics 97 Freshman 3.0 A ## 185 CS 59 Senior 4.0 D ## 186 Statistics 95 Senior 4.0 B ## 187 Statistics 63 Freshman 4.0 D ## 188 CS 82 Freshman 2.0 C ## 189 CS 93 Freshman 1.0 B ## 190 Economics 51 Freshman 1.0 D ## 191 Statistics 44 Freshman 3.0 F ## 192 CS 29 Junior 4.0 F ## 193 Statistics 80 Freshman 2.0 C ## 194 CS 43 Junior 4.0 D ## 195 Statistics 93 Senior 4.0 B ## 196 Statistics 25 Freshman 4.0 F ## 197 Statistics 97 Junior 2.0 A ## 198 Economics 23 Junior 4.0 F ## 199 Statistics 94 Sophomore 2.0 A ## 200 Statistics 92 Sophomore 4.0 A ## 201 CS 92 Senior 1.0 B ## 202 CS 60 Senior 2.0 D ## 203 Economics 38 Senior 4.0 F ## 205 Statistics 34 Sophomore 4.0 F ## 206 CS 93 Sophomore 4.0 A ## 207 Statistics 52 Junior 3.0 D ## 209 CS 84 Sophomore 3.0 C ## 210 Statistics 97 Freshman 3.0 A ## 211 CS 86 Freshman 2.0 A ## 212 Economics 45 Sophomore 3.0 D ## 213 CS 55 Senior 1.0 F ## 214 Statistics 73 Freshman 2.0 C ## 215 Economics 88 Freshman 4.0 A ## 216 CS 91 Junior 1.0 B ## 217 Economics 7 Freshman 1.0 F ## 218 Statistics 59 Freshman 4.0 D ## 219 Statistics 91 Senior 3.0 B ## 220 CS 28 Freshman 1.0 F ## 221 CS 87 Senior 3.0 B ## 222 CS 34 Freshman 3.0 F ## 223 Statistics 24 Freshman 1.0 F ## 225 Statistics 81 Freshman 4.0 B ## 226 Statistics 18 Freshman 1.0 F ## 227 Statistics 89 Freshman 4.0 B ## 228 Economics 89 Junior 2.0 B ## 232 CS 46 Freshman 4.0 F ## 233 Economics 19 Sophomore 2.0 F ## 234 CS 20 Sophomore 4.0 F ## 235 Economics 54 Sophomore 3.0 D ## 238 Economics 34 Freshman 4.0 F ## 239 Statistics 99 Sophomore 4.0 A ## 240 CS 35 Junior 4.0 F ## 241 Economics 94 Junior 4.0 A ## 243 Statistics 93 Senior 4.0 A ## 247 Statistics 10 Senior 2.0 F ## 248 CS 72 Sophomore 3.0 C ## 249 CS 75 Junior 1.0 C ## 250 Statistics 77 Sophomore 1.0 C ## 251 Economics 44 Junior 4.0 F ## 252 CS 86 Senior 4.0 B ## 253 Economics 65 Freshman 2.0 C ## 254 CS 84 Sophomore 4.0 C ## 255 CS 66 Sophomore 1.0 D ## 256 Statistics 25 Freshman 1.0 F ## 257 Economics 61 Junior 3.0 C ## 258 Statistics 93 Freshman 2.0 A ## 259 Statistics 32 Sophomore 1.0 F ## 260 CS 79 Senior 2.0 C ## 261 Economics 53 Freshman 3.0 D ## 262 CS 44 Junior 3.0 F ## 263 CS 99 Junior 2.0 A ## 264 CS 48 Sophomore 1.0 F ## 266 CS 81 Junior 1.0 C ## 267 CS 28 Junior 4.0 F ## 268 Statistics 93 Senior 1.0 B ## 269 CS 100 Junior 2.0 A ## 270 CS 70 Freshman 1.0 D ## 271 Statistics 16 Freshman 3.0 F ## 272 Statistics 92 Freshman 2.0 A ## 273 Economics 67 Junior 1.0 C ## 274 CS 79 Freshman 4.0 C ## 275 Economics 42 Sophomore 4.0 D ## 277 CS 37 Senior 4.0 F ## 280 CS 72 Junior 3.0 C ## 281 CS 40 Sophomore 3.0 F ## 282 Economics 35 Junior 1.0 F ## 283 CS 15 Senior 4.0 F ## 284 CS 90 Freshman 3.0 A ## 285 Economics 38 Sophomore 2.0 F ## 287 Statistics 22 Sophomore 4.0 F ## 288 Statistics 38 Senior 3.8 F ## 289 Statistics 67 Senior 3.0 C ## 290 Statistics 56 Freshman 1.0 D ## 291 Statistics 85 Senior 3.0 B ## 292 CS 69 Senior 1.0 D ## 293 CS 99 Sophomore 2.0 A ## 294 Economics 98 Freshman 4.0 A ## 295 CS 93 Senior 4.0 B ## 296 CS 62 Freshman 2.0 D ## 297 Statistics 41 Sophomore 3.0 F ## 298 Statistics 84 Freshman 2.0 B ## 300 Economics 56 Senior 4.0 C ## 301 CS 39 Freshman 4.0 F ## 302 CS 43 Freshman 4.0 F ## 304 CS 95 Senior 1.0 A ## 306 Statistics 76 Senior 4.0 C ## 308 CS 85 Sophomore 2.0 C ## 310 Economics 98 Junior 4.0 A ## 311 CS 45 Sophomore 1.0 F ## 312 Economics 6 Senior 3.0 F ## 313 CS 46 Junior 1.0 F ## 314 CS 23 Senior 1.0 F ## 315 Statistics 94 Freshman 1.0 A ## 316 Statistics 92 Freshman 4.0 A ## 317 CS 58 Freshman 4.0 D ## 318 Economics 35 Freshman 1.0 F ## 319 Economics 98 Freshman 4.0 A ## 320 CS 44 Freshman 3.0 F ## 321 CS 93 Freshman 4.0 B ## 322 Statistics 84 Junior 2.0 B ## 324 CS 35 Freshman 3.0 F ## 325 Statistics 38 Junior 1.0 F ## 326 Statistics 33 Junior 1.0 F ## 329 CS 90 Senior 3.0 B ## 330 Economics 24 Senior 2.0 F ## 331 Economics 27 Sophomore 2.0 F ## 333 CS 57 Freshman 2.0 D ## 336 Economics 83 Junior 4.0 A ## 338 Statistics 21 Sophomore 4.0 F ## 339 CS 74 Junior 1.0 C ## 340 CS 21 Sophomore 1.0 F ## 341 CS 79 Sophomore 4.0 C ## 343 Economics 8 Junior 2.0 F ## 345 CS 96 Senior 4.0 A ## 346 Economics 93 Junior 4.0 A ## 347 Statistics 12 Junior 4.0 F ## 348 Economics 80 Junior 4.0 A ## 349 Economics 54 Junior 1.0 D ## 350 CS 86 Senior 1.0 B ## 351 Economics 87 Junior 4.0 A ## 352 Economics 53 Freshman 2.0 D ## 353 Statistics 94 Junior 3.0 A ## 354 Statistics 12 Sophomore 1.0 F ## 355 Statistics 60 Junior 1.0 D ## 356 Statistics 26 Junior 2.0 F ## 359 CS 55 Junior 2.0 D ## 360 Economics 77 Sophomore 4.0 A ## 361 Statistics 17 Senior 4.0 F ## 362 Statistics 84 Sophomore 1.0 B ## 363 Economics 70 Senior 1.0 C ## 364 Economics 9 Senior 1.0 F ## 365 CS 19 Sophomore 3.0 F ## 367 CS 97 Sophomore 1.0 A ## 368 Economics 16 Junior 4.0 F ## 369 CS 88 Senior 2.0 B ## 371 Statistics 88 Sophomore 2.0 B ## 372 Statistics 88 Senior 4.0 B ## 373 Economics 26 Senior 4.0 F ## 374 Statistics 13 Sophomore 2.0 F ## 375 Economics 13 Freshman 3.0 F ## 376 CS 92 Sophomore 1.0 A ## 377 CS 91 Junior 2.0 B ## 378 Economics 9 Sophomore 2.0 F ## 379 Statistics 79 Junior 3.0 C ## 380 Economics 9 Senior 1.0 F ## 381 CS 95 Senior 4.0 B ## 382 Economics 19 Junior 2.0 F ## 383 Statistics 81 Sophomore 2.0 B ## 384 Economics 93 Senior 4.0 A ## 385 CS 64 Freshman 1.0 D ## 386 Economics 13 Freshman 1.0 F ## 387 Economics 94 Senior 4.0 A ## 388 Economics 55 Junior 3.0 D ## 391 CS 85 Freshman 4.0 A ## 392 Economics 92 Freshman 4.0 A ## 393 Statistics 77 Sophomore 1.0 C ## 394 CS 88 Senior 4.0 A ## 395 CS 32 Senior 4.0 F ## 396 CS 36 Senior 1.0 F ## 397 CS 89 Senior 1.0 B ## 398 Statistics 54 Freshman 3.0 D ## 399 Economics 33 Freshman 3.0 F ## 400 Economics 31 Senior 2.0 F ## 401 Statistics 79 Sophomore 4.0 C ## 402 Statistics 47 Junior 3.0 F ## 403 CS 96 Senior 4.0 A ## 404 CS 18 Senior 2.0 F ## 405 Statistics 32 Junior 2.0 F ## 406 Statistics 66 Sophomore 4.0 C ## 407 Economics 45 Freshman 4.0 D ## 409 CS 70 Freshman 2.0 D ## 410 Statistics 59 Junior 1.0 D ## 411 Economics 34 Senior 2.0 F ## 412 Economics 66 Junior 2.0 C ## 413 Economics 15 Senior 2.0 F ## 414 Statistics 35 Junior 1.0 F ## 415 Statistics 78 Freshman 2.0 C ## 416 Statistics 68 Senior 4.0 C ## 417 Statistics 92 Junior 4.0 A ## 418 CS 83 Junior 3.0 C ## 421 CS 74 Senior 2.0 C ## 422 CS 60 Senior 4.0 D ## 423 CS 93 Senior 3.0 B ## 425 Economics 52 Freshman 3.0 D ## 427 Economics 66 Freshman 2.8 C ## 428 Economics 55 Sophomore 2.0 D ## 431 Statistics 13 Sophomore 3.0 F ## 432 Statistics 96 Freshman 1.0 A ## 433 Economics 85 Junior 4.0 A ## 434 Statistics 36 Senior 4.0 F ## 435 Economics 70 Junior 1.0 D ## 436 CS 99 Junior 3.0 B ## 437 Statistics 19 Sophomore 1.0 F ## 439 CS 95 Freshman 3.0 B ## 440 Statistics 64 Junior 4.0 D ## 442 Statistics 26 Sophomore 4.0 F ## 443 Statistics 95 Sophomore 1.0 A ## 444 Statistics 95 Junior 2.0 A ## 445 CS 33 Sophomore 1.0 F ## 446 Economics 92 Sophomore 4.0 A ## 447 Economics 66 Senior 2.0 C ## 448 CS 76 Freshman 3.0 C ## 449 Economics 51 Junior 1.0 D ## 450 CS 98 Junior 2.0 A ## 451 Statistics 39 Sophomore 3.0 F ## 453 Economics 52 Senior 3.0 D ## 454 CS 93 Sophomore 3.8 A ## 455 Economics 81 Freshman 4.0 A ## 456 Statistics 43 Senior 2.0 F ## 457 CS 90 Junior 1.0 B ## 458 CS 41 Senior 3.0 F ## 460 Economics 56 Senior 1.0 C ## 461 CS 96 Senior 3.0 B ## 462 CS 63 Senior 1.0 D ## 463 CS 82 Junior 2.0 C ## 464 CS 97 Junior 1.0 A ## 465 CS 97 Sophomore 1.0 A ## 466 Economics 29 Junior 3.0 F ## 467 Economics 31 Senior 1.0 F ## 468 CS 25 Freshman 1.0 F ## 470 Economics 64 Senior 3.8 C ## 472 Economics 56 Junior 4.0 C ## 473 Economics 21 Freshman 1.0 F ## 475 Statistics 34 Senior 2.0 F ## 476 Statistics 83 Sophomore 2.0 B ## 477 CS 94 Freshman 2.0 A ## 478 Economics 34 Sophomore 2.0 F ## 480 Economics 13 Freshman 4.0 F ## 482 Statistics 64 Freshman 3.0 D ## 483 Statistics 58 Freshman 2.0 D ## 484 Statistics 99 Junior 3.0 A ## 489 CS 74 Senior 3.0 C ## 490 Economics 25 Freshman 4.0 F ## 492 Statistics 56 Freshman 2.0 D ## 493 Statistics 75 Senior 1.0 C ## 494 CS 37 Sophomore 4.0 F ## 495 Statistics 17 Freshman 3.0 F ## 496 CS 96 Junior 1.0 A ## 498 CS 49 Senior 2.0 F ## 499 Economics 72 Freshman 1.0 B ## 501 Statistics 79 Freshman 1.0 C ## 502 Economics 49 Junior 2.0 D ## 503 CS 60 Junior 4.0 D ## 504 Economics 60 Sophomore 3.0 C ## 506 Economics 18 Junior 2.0 F ## 507 Statistics 75 Senior 1.0 C ## 509 Economics 52 Sophomore 4.0 D ## 510 Statistics 46 Freshman 4.0 F ## 513 Economics 24 Junior 4.0 F ## 514 CS 64 Junior 2.0 C ## 515 Economics 24 Junior 3.0 F ## 516 Statistics 14 Freshman 4.0 F ## 517 Economics 29 Senior 1.0 F ## 518 Statistics 56 Junior 4.0 D ## 519 Statistics 29 Senior 3.0 F ## 521 Economics 47 Freshman 1.0 D ## 527 Statistics 51 Freshman 3.0 D ## 530 CS 66 Junior 2.0 D ## 533 Economics 92 Sophomore 4.0 A ## 534 CS 44 Freshman 3.0 F ## 535 Economics 57 Senior 3.0 C ## 536 CS 68 Freshman 2.0 D ## 537 Statistics 88 Freshman 3.0 B ## 538 Statistics 65 Junior 3.0 D ## 539 CS 41 Senior 3.0 F ## 540 CS 28 Freshman 3.0 F ## 541 CS 62 Senior 2.0 D ## 543 Economics 51 Junior 1.0 D ## 544 CS 91 Freshman 1.0 A ## 546 Economics 91 Freshman 4.0 A ## 548 Statistics 47 Sophomore 1.0 F ## 550 CS 18 Freshman 1.0 F ## 552 Economics 24 Sophomore 3.0 F ## 553 CS 60 Senior 3.0 D ## 554 CS 86 Sophomore 4.0 B ## 555 Economics 12 Freshman 4.0 F ## 557 CS 52 Freshman 4.0 F ## 558 CS 78 Sophomore 3.0 C ## 559 Statistics 12 Junior 2.0 F ## 560 CS 17 Freshman 2.0 F ## 561 CS 44 Junior 2.0 F ## 563 Statistics 36 Freshman 4.0 F ## 566 Statistics 56 Junior 4.0 D ## 567 CS 55 Senior 2.0 F ## 571 Economics 23 Sophomore 4.0 F ## 572 CS 18 Junior 3.0 F ## 573 Economics 72 Junior 4.0 B ## 574 CS 86 Sophomore 4.0 B ## 576 Statistics 84 Senior 2.0 B ## 577 CS 70 Senior 4.0 D ## 578 Economics 20 Sophomore 1.0 F ## 579 CS 21 Freshman 4.0 F ## 580 CS 75 Senior 4.0 C ## 581 Statistics 62 Sophomore 1.0 D ## 582 Economics 28 Sophomore 4.0 F ## 583 Statistics 43 Senior 4.0 D ## 584 Statistics 83 Sophomore 2.0 B ## 585 Statistics 42 Senior 2.0 F ## 587 Economics 11 Junior 3.0 F ## 588 Economics 75 Freshman 3.0 B ## 589 Economics 79 Senior 4.0 A ## 590 CS 78 Freshman 1.0 C ## 591 Economics 83 Junior 4.0 A ## 592 CS 98 Junior 2.0 A ## 593 Statistics 98 Freshman 3.0 A ## 594 CS 47 Freshman 1.0 F ## 595 Statistics 97 Senior 2.0 B ## 596 Economics 88 Senior 4.0 A ## 597 Statistics 21 Junior 4.0 F ## 599 CS 58 Junior 4.0 D ## 600 CS 81 Senior 4.0 C ## 602 CS 51 Senior 2.0 D ## 603 CS 35 Sophomore 2.0 F ## 604 Statistics 25 Senior 1.0 F ## 605 CS 87 Sophomore 4.0 A ## 606 CS 37 Junior 3.0 F ## 607 CS 97 Sophomore 4.0 A ## 609 Statistics 37 Sophomore 4.0 F ## 610 CS 28 Freshman 1.0 F ## 611 Statistics 15 Senior 1.0 F ## 612 CS 55 Junior 4.0 F ## 613 CS 75 Sophomore 3.0 C ## 615 Statistics 52 Senior 1.0 D ## 616 Economics 71 Sophomore 1.0 B ## 617 Statistics 81 Senior 2.0 B ## 619 CS 15 Senior 4.0 F ## 620 Economics 78 Junior 4.0 A ## 621 Economics 21 Senior 1.0 F ## 622 Statistics 60 Junior 4.0 D ## 623 CS 71 Junior 2.0 C ## 625 CS 44 Senior 4.0 F ## 626 CS 45 Junior 1.0 F ## 627 CS 84 Junior 4.0 C ## 628 Statistics 40 Senior 3.0 F ## 630 CS 83 Freshman 1.0 C ## 631 Economics 87 Freshman 4.0 A ## 634 Statistics 74 Senior 2.0 C ## 636 Statistics 33 Junior 4.0 F ## 637 Statistics 37 Junior 4.0 F ## 638 Economics 23 Freshman 1.0 F ## 639 Statistics 54 Senior 3.0 D ## 640 CS 29 Sophomore 3.0 F ## 643 CS 72 Freshman 2.0 C ## 644 Economics 3 Sophomore 4.0 F ## 647 CS 17 Senior 4.0 F ## 648 CS 84 Senior 4.0 C ## 649 Statistics 84 Freshman 4.0 B ## 650 Statistics 57 Senior 1.0 D ## 651 Statistics 98 Senior 1.0 A ## 652 CS 25 Freshman 3.0 F ## 653 Economics 50 Sophomore 3.0 D ## 654 Economics 69 Freshman 3.0 C ## 655 Economics 18 Junior 1.0 F ## 656 Statistics 32 Senior 2.0 F ## 657 Statistics 43 Freshman 4.0 F ## 658 Statistics 53 Senior 2.0 D ## 661 CS 85 Freshman 4.0 A ## 662 Economics 38 Junior 4.0 F ## 665 CS 99 Freshman 3.0 A ## 666 Economics 47 Freshman 1.0 D ## 667 Statistics 99 Sophomore 2.0 A ## 668 Economics 20 Freshman 3.0 F ## 669 CS 30 Senior 2.0 F ## 670 CS 71 Freshman 1.0 C ## 672 CS 16 Senior 2.0 F ## 674 Economics 5 Junior 3.0 F ## 675 Statistics 24 Junior 2.0 F ## 676 Statistics 75 Junior 2.0 C ## 677 Economics 8 Junior 2.0 F ## 679 Statistics 35 Sophomore 2.0 F ## 680 CS 17 Freshman 3.0 F ## 681 CS 73 Senior 1.0 C ## 682 Economics 53 Freshman 1.0 D ## 683 CS 83 Sophomore 4.0 C ## 685 Statistics 79 Freshman 2.0 C ## 686 Economics 3 Senior 4.0 F ## 687 Statistics 24 Sophomore 1.0 F ## 688 CS 19 Senior 4.0 F ## 690 CS 81 Sophomore 3.0 C ## 691 Economics 12 Sophomore 1.0 F ## 692 CS 65 Sophomore 3.0 C ## 693 CS 35 Freshman 3.0 F ## 694 Statistics 93 Senior 4.0 B ## 695 CS 34 Sophomore 4.0 F ## 696 Statistics 38 Freshman 2.0 F ## 697 Statistics 99 Sophomore 1.0 A ## 698 Economics 95 Junior 4.0 A ## 699 CS 23 Freshman 1.0 F ## 700 CS 99 Sophomore 1.0 A ## 702 Economics 28 Junior 4.0 F ## 703 Economics 12 Senior 4.0 F ## 704 Statistics 100 Junior 1.0 A ## 705 Economics 52 Sophomore 4.0 D ## 706 CS 69 Junior 3.0 C ## 707 CS 30 Freshman 3.0 F ## 708 Economics 93 Sophomore 2.0 A ## 710 Statistics 80 Senior 1.0 C ## 711 Statistics 67 Sophomore 1.0 C ## 712 Economics 92 Junior 4.0 A ## 713 CS 53 Junior 1.0 F ## 714 Statistics 98 Freshman 3.0 A ## 715 Statistics 43 Junior 3.0 F ## 716 Economics 93 Senior 4.0 A ## 717 Statistics 30 Junior 2.0 F ## 718 Economics 69 Freshman 4.0 C ## 723 Statistics 96 Junior 3.0 A ## 724 CS 18 Senior 2.0 F ## 725 Statistics 28 Sophomore 4.0 F ## 726 Economics 29 Sophomore 2.0 F ## 728 Economics 38 Freshman 1.0 F ## 729 CS 66 Sophomore 2.0 D ## 730 CS 41 Freshman 1.0 F ## 731 CS 54 Senior 4.0 F ## 732 Economics 85 Sophomore 4.0 A ## 733 Economics 26 Junior 2.0 F ## 734 Economics 97 Freshman 4.0 A ## 736 CS 83 Junior 2.0 C ## 738 Economics 89 Sophomore 4.0 A ## 739 CS 95 Senior 4.0 B ## 741 Statistics 83 Sophomore 1.0 B ## 744 Statistics 49 Senior 4.0 F ## 745 CS 94 Senior 1.0 B ## 746 Statistics 31 Junior 4.0 F ## 747 Statistics 38 Freshman 4.0 F ## 749 Statistics 18 Junior 2.0 F ## 750 Statistics 91 Sophomore 2.0 A ## 751 Economics 19 Sophomore 2.0 F ## 752 Economics 38 Junior 3.0 F ## 753 Economics 11 Junior 4.0 F ## 754 Economics 72 Senior 2.8 A ## 755 Economics 88 Sophomore 4.0 A ## 756 CS 74 Junior 2.0 C ## 757 Statistics 33 Junior 3.0 F ## 758 Economics 86 Sophomore 4.0 A ## 759 CS 86 Sophomore 2.0 A ## 760 Economics 30 Senior 2.0 F ## 761 CS 57 Freshman 2.0 D ## 762 Economics 47 Junior 4.0 D ## 765 CS 88 Sophomore 2.0 A ## 766 Economics 90 Junior 4.0 A ## 767 Statistics 18 Senior 4.0 F ## 768 CS 80 Senior 3.0 C ## 770 Statistics 45 Sophomore 1.0 F ## 771 Economics 14 Freshman 3.0 F ## 773 Economics 86 Freshman 4.0 A ## 774 Statistics 87 Freshman 3.0 B ## 775 Economics 16 Junior 1.0 F ## 776 Economics 46 Freshman 1.0 D ## 778 Economics 29 Senior 3.0 F ## 780 CS 86 Freshman 1.0 B ## 782 Economics 33 Sophomore 1.0 F ## 783 Statistics 37 Junior 4.0 F ## 784 Economics 10 Freshman 4.0 F ## 786 Statistics 22 Sophomore 4.0 F ## 787 CS 96 Senior 1.0 A ## 788 CS 99 Sophomore 1.0 A ## 789 Economics 42 Freshman 4.0 D ## 790 Economics 39 Junior 2.0 F ## 791 CS 92 Senior 4.0 B ## 793 CS 30 Freshman 4.0 F ## 795 Statistics 96 Senior 3.0 A ## 796 Economics 58 Freshman 1.0 C ## 797 Economics 16 Freshman 2.0 F ## 798 CS 62 Senior 3.0 D ## 800 CS 43 Junior 4.0 F ## 801 Statistics 27 Sophomore 3.0 F ## 802 Statistics 42 Sophomore 2.0 F ## 804 Statistics 51 Sophomore 2.0 D ## 805 CS 90 Senior 2.0 B ## 806 Economics 4 Sophomore 3.0 F ## 809 Economics 44 Sophomore 2.0 D ## 811 CS 67 Freshman 2.0 D ## 812 CS 97 Freshman 2.8 A ## 813 Economics 59 Junior 1.0 C ## 814 Economics 100 Sophomore 4.0 A ## 816 Statistics 75 Senior 2.0 C ## 817 CS 20 Junior 4.0 F ## 818 Economics 8 Junior 1.0 F ## 819 CS 45 Junior 1.0 F ## 821 Economics 94 Senior 4.0 A ## 822 Statistics 91 Senior 4.0 B ## 823 Statistics 83 Senior 3.0 B ## 824 Statistics 97 Freshman 2.0 A ## 825 Economics 44 Junior 3.0 D ## 826 CS 20 Senior 2.0 F ## 827 CS 37 Senior 4.0 F ## 830 Economics 48 Junior 4.0 D ## 831 CS 99 Senior 3.0 A ## 832 CS 6 Sophomore 3.0 F ## 833 CS 97 Sophomore 4.0 A ## 834 CS 92 Senior 4.0 B ## 836 Economics 90 Senior 4.0 A ## 837 Economics 67 Freshman 4.0 C ## 838 CS 88 Freshman 4.0 B ## 839 Economics 92 Freshman 4.0 A ## 841 Statistics 35 Junior 2.0 F ## 842 Statistics 20 Junior 4.0 F ## 843 CS 52 Freshman 1.0 F ## 845 Statistics 77 Sophomore 1.0 C ## 846 Statistics 13 Junior 1.0 F ## 847 Economics 87 Sophomore 4.0 A ## 848 Economics 30 Freshman 4.0 F ## 850 CS 19 Freshman 4.0 F ## 851 CS 27 Senior 3.0 F ## 852 Economics 73 Sophomore 4.0 B ## 853 Economics 15 Freshman 3.0 F ## 854 Statistics 16 Senior 4.0 F ## 856 Economics 57 Senior 4.0 C ## 857 Economics 10 Freshman 1.0 F ## 858 Statistics 34 Sophomore 3.0 F ## 859 Statistics 14 Senior 4.0 F ## 862 CS 90 Junior 4.0 B ## 863 Statistics 62 Senior 4.0 D ## 864 CS 23 Senior 1.0 F ## 865 Statistics 54 Junior 4.0 D ## 866 Economics 35 Junior 1.0 F ## 868 Statistics 66 Senior 2.8 C ## 869 CS 98 Junior 1.0 A ## 870 CS 74 Freshman 3.0 C ## 871 Statistics 20 Sophomore 1.0 F ## 872 CS 92 Junior 4.0 B ## 873 Statistics 81 Freshman 2.0 B ## 874 CS 63 Junior 2.0 D ## 875 Statistics 94 Senior 4.0 B ## 876 Statistics 68 Sophomore 3.0 C ## 877 CS 90 Senior 4.0 B ## 879 Economics 56 Junior 2.0 C ## 880 CS 37 Freshman 3.0 F ## 881 CS 50 Junior 2.0 F ## 883 CS 97 Senior 4.0 B ## 884 Economics 21 Junior 4.0 F ## 885 Economics 41 Senior 4.0 D ## 886 Economics 75 Junior 2.0 B ## 888 Statistics 11 Junior 3.0 F ## 889 CS 82 Sophomore 1.0 C ## 890 CS 42 Junior 1.0 F ## 891 Statistics 10 Junior 4.0 F ## 893 CS 64 Junior 4.0 D ## 894 Statistics 92 Freshman 3.0 A ## 895 Economics 78 Freshman 4.0 A ## 898 Economics 58 Freshman 4.0 C ## 899 Economics 24 Senior 4.0 F ## 900 Economics 86 Freshman 4.0 A ## 901 CS 80 Senior 3.0 C ## 902 Economics 72 Freshman 4.0 B ## 903 Statistics 94 Junior 3.0 A ## 904 Statistics 62 Senior 4.0 D ## 905 Statistics 14 Junior 4.0 F ## 906 Statistics 92 Senior 4.0 B ## 908 CS 88 Freshman 4.0 B ## 910 Statistics 82 Freshman 2.0 B ## 911 CS 74 Junior 2.0 C ## 912 Statistics 52 Junior 1.0 D ## 913 CS 84 Junior 4.0 C ## 914 CS 88 Junior 3.0 B ## 915 Statistics 17 Sophomore 1.0 F ## 916 Statistics 28 Senior 1.0 F ## 917 Statistics 92 Senior 4.0 B ## 918 Statistics 13 Senior 3.0 F ## 920 Economics 99 Junior 4.0 A ## 921 CS 87 Freshman 1.0 B ## 922 CS 50 Senior 1.0 F ## 924 CS 18 Sophomore 4.0 F ## 925 Economics 48 Freshman 1.0 D ## 926 CS 81 Junior 1.0 C ## 928 Economics 84 Senior 4.0 A ## 929 Statistics 21 Sophomore 3.0 F ## 932 CS 91 Senior 4.0 B ## 933 Statistics 32 Sophomore 1.0 F ## 934 Statistics 78 Freshman 3.0 C ## 937 CS 76 Freshman 4.0 C ## 938 Statistics 93 Junior 3.0 A ## 939 CS 40 Senior 3.0 F ## 941 Statistics 63 Sophomore 2.0 D ## 942 Economics 100 Sophomore 4.0 A ## 944 Economics 56 Senior 1.0 C ## 946 CS 94 Sophomore 2.0 A ## 947 CS 40 Sophomore 3.0 F ## 950 Statistics 74 Senior 3.0 C ## 951 Statistics 96 Sophomore 2.0 A ## 952 Statistics 40 Freshman 1.0 F ## 953 Economics 83 Senior 4.0 A ## 954 Economics 91 Senior 4.0 A ## 955 Economics 20 Sophomore 1.0 F ## 956 Economics 59 Senior 3.0 C ## 957 Statistics 74 Freshman 3.0 C ## 959 CS 98 Freshman 1.0 A ## 961 Statistics 17 Junior 1.0 F ## 962 CS 57 Senior 1.0 D ## 965 CS 30 Junior 4.0 F ## 967 Statistics 94 Freshman 2.0 A ## 968 Statistics 38 Freshman 3.0 F ## 969 Statistics 99 Freshman 3.0 A ## 970 CS 83 Freshman 2.0 C ## 971 CS 37 Junior 4.0 F ## 972 Statistics 29 Senior 2.0 F ## 973 Economics 36 Sophomore 4.0 F ## 974 Statistics 62 Senior 3.0 D ## 975 CS 38 Junior 1.0 F ## 976 Economics 45 Sophomore 1.0 D ## 981 Economics 8 Junior 1.0 F ## 982 Economics 74 Freshman 3.0 B ## 985 CS 52 Senior 2.0 F ## 986 CS 19 Sophomore 1.0 F ## 987 CS 30 Senior 1.0 F ## 988 Economics 35 Junior 3.0 F ## 989 Statistics 99 Junior 1.8 A ## 990 Economics 71 Freshman 3.0 B ## 991 Statistics 98 Junior 2.0 A ## 992 Economics 65 Sophomore 3.0 C ## 993 Economics 40 Sophomore 2.0 F ## 994 CS 60 Sophomore 3.0 D ## 995 CS 36 Freshman 3.0 F ## 996 CS 91 Junior 2.0 B ## 997 Statistics 73 Junior 1.0 C ## 998 Economics 61 Junior 4.0 C ## 999 CS 66 Sophomore 3.0 D ## Major Score Seniority GPA Grade ## 8 CS 98 Junior 4.0 B ## 9 Statistics 100 Sophomore 4.0 A ## 11 CS 98 Junior 3.0 A ## 13 CS 96 Junior 3.0 A ## 19 Statistics 97 Senior 2.0 A ## 29 Statistics 95 Sophomore 3.0 A ## 33 CS 97 Sophomore 4.0 A ## 34 Psychology 95 Sophomore 3.8 A ## 39 CS 98 Junior 4.0 A ## 41 Statistics 90 Senior 4.0 B ## 42 CS 85 Senior 1.0 B ## 47 Psychology 98 Sophomore 1.8 A ## 51 CS 93 Junior 4.0 B ## 55 CS 95 Sophomore 4.0 A ## 64 CS 93 Junior 1.0 B ## 65 Psychology 95 Senior 3.8 A ## 66 Statistics 90 Sophomore 2.0 B ## 70 CS 95 Freshman 3.0 A ## 74 CS 95 Junior 3.0 B ## 77 Statistics 88 Senior 4.0 B ## 80 Economics 87 Senior 4.0 A ## 86 Psychology 99 Sophomore 4.0 A ## 91 Economics 85 Freshman 4.0 A ## 92 Statistics 93 Junior 3.0 A ## 93 CS 90 Junior 2.0 B ## 98 CS 95 Freshman 4.0 A ## 106 Economics 94 Senior 4.0 A ## 122 Psychology 97 Senior 2.8 A ## 133 CS 99 Junior 4.0 B ## 134 CS 98 Senior 3.0 A ## 135 CS 98 Senior 4.0 A ## 136 CS 85 Senior 4.0 B ## 138 CS 97 Junior 1.0 B ## 144 Statistics 96 Junior 3.0 A ## 151 Psychology 97 Senior 4.0 A ## 155 Statistics 81 Junior 3.0 B ## 156 CS 82 Senior 3.0 C ## 162 Economics 97 Sophomore 4.0 A ## 164 Psychology 99 Freshman 3.8 A ## 169 Statistics 95 Sophomore 1.0 A ## 170 Psychology 93 Senior 3.8 A ## 172 Statistics 81 Sophomore 4.0 B ## 174 Statistics 95 Junior 2.0 A ## 176 Economics 100 Sophomore 4.0 A ## 178 Psychology 84 Senior 1.0 B ## 179 CS 92 Sophomore 3.0 A ## 182 CS 87 Junior 3.0 B ## 184 Statistics 97 Freshman 3.0 A ## 186 Statistics 95 Senior 4.0 B ## 188 CS 82 Freshman 2.0 C ## 189 CS 93 Freshman 1.0 B ## 195 Statistics 93 Senior 4.0 B ## 197 Statistics 97 Junior 2.0 A ## 199 Statistics 94 Sophomore 2.0 A ## 200 Statistics 92 Sophomore 4.0 A ## 201 CS 92 Senior 1.0 B ## 206 CS 93 Sophomore 4.0 A ## 209 CS 84 Sophomore 3.0 C ## 210 Statistics 97 Freshman 3.0 A ## 211 CS 86 Freshman 2.0 A ## 215 Economics 88 Freshman 4.0 A ## 216 CS 91 Junior 1.0 B ## 219 Statistics 91 Senior 3.0 B ## 221 CS 87 Senior 3.0 B ## 225 Statistics 81 Freshman 4.0 B ## 227 Statistics 89 Freshman 4.0 B ## 228 Economics 89 Junior 2.0 B ## 231 Psychology 97 Freshman 4.0 A ## 239 Statistics 99 Sophomore 4.0 A ## 241 Economics 94 Junior 4.0 A ## 243 Statistics 93 Senior 4.0 A ## 252 CS 86 Senior 4.0 B ## 254 CS 84 Sophomore 4.0 C ## 258 Statistics 93 Freshman 2.0 A ## 263 CS 99 Junior 2.0 A ## 266 CS 81 Junior 1.0 C ## 268 Statistics 93 Senior 1.0 B ## 269 CS 100 Junior 2.0 A ## 272 Statistics 92 Freshman 2.0 A ## 284 CS 90 Freshman 3.0 A ## 291 Statistics 85 Senior 3.0 B ## 293 CS 99 Sophomore 2.0 A ## 294 Economics 98 Freshman 4.0 A ## 295 CS 93 Senior 4.0 B ## 298 Statistics 84 Freshman 2.0 B ## 304 CS 95 Senior 1.0 A ## 305 Psychology 94 Junior 4.0 A ## 308 CS 85 Sophomore 2.0 C ## 309 Psychology 91 Junior 2.8 A ## 310 Economics 98 Junior 4.0 A ## 315 Statistics 94 Freshman 1.0 A ## 316 Statistics 92 Freshman 4.0 A ## 319 Economics 98 Freshman 4.0 A ## 321 CS 93 Freshman 4.0 B ## 322 Statistics 84 Junior 2.0 B ## 329 CS 90 Senior 3.0 B ## 336 Economics 83 Junior 4.0 A ## 345 CS 96 Senior 4.0 A ## 346 Economics 93 Junior 4.0 A ## 350 CS 86 Senior 1.0 B ## 351 Economics 87 Junior 4.0 A ## 353 Statistics 94 Junior 3.0 A ## 357 Psychology 96 Senior 3.8 A ## 362 Statistics 84 Sophomore 1.0 B ## 366 Psychology 89 Senior 4.0 A ## 367 CS 97 Sophomore 1.0 A ## 369 CS 88 Senior 2.0 B ## 370 Psychology 93 Freshman 3.8 A ## 371 Statistics 88 Sophomore 2.0 B ## 372 Statistics 88 Senior 4.0 B ## 376 CS 92 Sophomore 1.0 A ## 377 CS 91 Junior 2.0 B ## 381 CS 95 Senior 4.0 B ## 383 Statistics 81 Sophomore 2.0 B ## 384 Economics 93 Senior 4.0 A ## 387 Economics 94 Senior 4.0 A ## 389 Psychology 95 Sophomore 2.8 A ## 391 CS 85 Freshman 4.0 A ## 392 Economics 92 Freshman 4.0 A ## 394 CS 88 Senior 4.0 A ## 397 CS 89 Senior 1.0 B ## 403 CS 96 Senior 4.0 A ## 417 Statistics 92 Junior 4.0 A ## 418 CS 83 Junior 3.0 C ## 423 CS 93 Senior 3.0 B ## 432 Statistics 96 Freshman 1.0 A ## 433 Economics 85 Junior 4.0 A ## 436 CS 99 Junior 3.0 B ## 439 CS 95 Freshman 3.0 B ## 443 Statistics 95 Sophomore 1.0 A ## 444 Statistics 95 Junior 2.0 A ## 446 Economics 92 Sophomore 4.0 A ## 450 CS 98 Junior 2.0 A ## 454 CS 93 Sophomore 3.8 A ## 455 Economics 81 Freshman 4.0 A ## 457 CS 90 Junior 1.0 B ## 461 CS 96 Senior 3.0 B ## 463 CS 82 Junior 2.0 C ## 464 CS 97 Junior 1.0 A ## 465 CS 97 Sophomore 1.0 A ## 471 Psychology 94 Sophomore 4.0 A ## 476 Statistics 83 Sophomore 2.0 B ## 477 CS 94 Freshman 2.0 A ## 484 Statistics 99 Junior 3.0 A ## 487 Psychology 95 Senior 3.8 A ## 491 Psychology 83 Sophomore 3.8 A ## 496 CS 96 Junior 1.0 A ## 497 Psychology 100 Junior 3.8 A ## 512 Psychology 98 Freshman 1.8 A ## 522 Psychology 82 Junior 3.8 A ## 528 Psychology 93 Junior 4.0 A ## 532 Psychology 81 Sophomore 3.8 A ## 533 Economics 92 Sophomore 4.0 A ## 537 Statistics 88 Freshman 3.0 B ## 542 Psychology 86 Senior 1.8 A ## 544 CS 91 Freshman 1.0 A ## 546 Economics 91 Freshman 4.0 A ## 554 CS 86 Sophomore 4.0 B ## 556 Psychology 95 Freshman 1.8 A ## 570 Psychology 94 Freshman 3.8 A ## 574 CS 86 Sophomore 4.0 B ## 576 Statistics 84 Senior 2.0 B ## 584 Statistics 83 Sophomore 2.0 B ## 586 Psychology 91 Sophomore 4.0 A ## 591 Economics 83 Junior 4.0 A ## 592 CS 98 Junior 2.0 A ## 593 Statistics 98 Freshman 3.0 A ## 595 Statistics 97 Senior 2.0 B ## 596 Economics 88 Senior 4.0 A ## 600 CS 81 Senior 4.0 C ## 605 CS 87 Sophomore 4.0 A ## 607 CS 97 Sophomore 4.0 A ## 617 Statistics 81 Senior 2.0 B ## 618 Psychology 91 Sophomore 1.8 A ## 627 CS 84 Junior 4.0 C ## 630 CS 83 Freshman 1.0 C ## 631 Economics 87 Freshman 4.0 A ## 641 Psychology 87 Freshman 2.8 A ## 645 Psychology 98 Freshman 1.8 A ## 648 CS 84 Senior 4.0 C ## 649 Statistics 84 Freshman 4.0 B ## 651 Statistics 98 Senior 1.0 A ## 659 Psychology 100 Freshman 4.0 A ## 661 CS 85 Freshman 4.0 A ## 663 Psychology 81 Junior 4.0 A ## 665 CS 99 Freshman 3.0 A ## 667 Statistics 99 Sophomore 2.0 A ## 673 Psychology 92 Junior 3.8 A ## 678 Psychology 82 Senior 3.8 A ## 683 CS 83 Sophomore 4.0 C ## 684 Psychology 97 Sophomore 3.8 A ## 690 CS 81 Sophomore 3.0 C ## 694 Statistics 93 Senior 4.0 B ## 697 Statistics 99 Sophomore 1.0 A ## 698 Economics 95 Junior 4.0 A ## 700 CS 99 Sophomore 1.0 A ## 704 Statistics 100 Junior 1.0 A ## 708 Economics 93 Sophomore 2.0 A ## 712 Economics 92 Junior 4.0 A ## 714 Statistics 98 Freshman 3.0 A ## 716 Economics 93 Senior 4.0 A ## 720 Psychology 90 Senior 3.8 A ## 723 Statistics 96 Junior 3.0 A ## 727 Psychology 99 Sophomore 2.8 A ## 732 Economics 85 Sophomore 4.0 A ## 734 Economics 97 Freshman 4.0 A ## 735 Psychology 84 Junior 4.0 A ## 736 CS 83 Junior 2.0 C ## 738 Economics 89 Sophomore 4.0 A ## 739 CS 95 Senior 4.0 B ## 741 Statistics 83 Sophomore 1.0 B ## 745 CS 94 Senior 1.0 B ## 750 Statistics 91 Sophomore 2.0 A ## 755 Economics 88 Sophomore 4.0 A ## 758 Economics 86 Sophomore 4.0 A ## 759 CS 86 Sophomore 2.0 A ## 765 CS 88 Sophomore 2.0 A ## 766 Economics 90 Junior 4.0 A ## 772 Psychology 99 Freshman 4.0 A ## 773 Economics 86 Freshman 4.0 A ## 774 Statistics 87 Freshman 3.0 B ## 779 Psychology 85 Sophomore 2.8 A ## 780 CS 86 Freshman 1.0 B ## 787 CS 96 Senior 1.0 A ## 788 CS 99 Sophomore 1.0 A ## 791 CS 92 Senior 4.0 B ## 792 Psychology 93 Sophomore 4.0 A ## 795 Statistics 96 Senior 3.0 A ## 805 CS 90 Senior 2.0 B ## 812 CS 97 Freshman 2.8 A ## 814 Economics 100 Sophomore 4.0 A ## 820 Psychology 93 Sophomore 4.0 A ## 821 Economics 94 Senior 4.0 A ## 822 Statistics 91 Senior 4.0 B ## 823 Statistics 83 Senior 3.0 B ## 824 Statistics 97 Freshman 2.0 A ## 831 CS 99 Senior 3.0 A ## 833 CS 97 Sophomore 4.0 A ## 834 CS 92 Senior 4.0 B ## 836 Economics 90 Senior 4.0 A ## 838 CS 88 Freshman 4.0 B ## 839 Economics 92 Freshman 4.0 A ## 847 Economics 87 Sophomore 4.0 A ## 862 CS 90 Junior 4.0 B ## 869 CS 98 Junior 1.0 A ## 872 CS 92 Junior 4.0 B ## 873 Statistics 81 Freshman 2.0 B ## 875 Statistics 94 Senior 4.0 B ## 877 CS 90 Senior 4.0 B ## 878 Psychology 89 Junior 4.0 A ## 882 Psychology 82 Freshman 4.0 A ## 883 CS 97 Senior 4.0 B ## 887 Psychology 99 Senior 2.8 A ## 889 CS 82 Sophomore 1.0 C ## 894 Statistics 92 Freshman 3.0 A ## 900 Economics 86 Freshman 4.0 A ## 903 Statistics 94 Junior 3.0 A ## 906 Statistics 92 Senior 4.0 B ## 908 CS 88 Freshman 4.0 B ## 910 Statistics 82 Freshman 2.0 B ## 913 CS 84 Junior 4.0 C ## 914 CS 88 Junior 3.0 B ## 917 Statistics 92 Senior 4.0 B ## 920 Economics 99 Junior 4.0 A ## 921 CS 87 Freshman 1.0 B ## 926 CS 81 Junior 1.0 C ## 928 Economics 84 Senior 4.0 A ## 931 Psychology 90 Sophomore 1.8 A ## 932 CS 91 Senior 4.0 B ## 936 Psychology 99 Freshman 3.8 A ## 938 Statistics 93 Junior 3.0 A ## 942 Economics 100 Sophomore 4.0 A ## 946 CS 94 Sophomore 2.0 A ## 951 Statistics 96 Sophomore 2.0 A ## 953 Economics 83 Senior 4.0 A ## 954 Economics 91 Senior 4.0 A ## 958 Psychology 85 Sophomore 1.8 A ## 959 CS 98 Freshman 1.0 A ## 966 Psychology 98 Senior 2.8 A ## 967 Statistics 94 Freshman 2.0 A ## 969 Statistics 99 Freshman 3.0 A ## 970 CS 83 Freshman 2.0 C ## 984 Psychology 84 Freshman 2.8 A ## 989 Statistics 99 Junior 1.8 A ## 991 Statistics 98 Junior 2.0 A ## 996 CS 91 Junior 2.0 B ## Major Score Seniority GPA Grade ## 8 CS 98 Junior 4 B ## 41 Statistics 90 Senior 4 B ## 42 CS 85 Senior 1 B ## 51 CS 93 Junior 4 B ## 64 CS 93 Junior 1 B ## 66 Statistics 90 Sophomore 2 B ## 74 CS 95 Junior 3 B ## 77 Statistics 88 Senior 4 B ## 93 CS 90 Junior 2 B ## 133 CS 99 Junior 4 B ## 136 CS 85 Senior 4 B ## 138 CS 97 Junior 1 B ## 155 Statistics 81 Junior 3 B ## 172 Statistics 81 Sophomore 4 B ## 178 Psychology 84 Senior 1 B ## 182 CS 87 Junior 3 B ## 186 Statistics 95 Senior 4 B ## 189 CS 93 Freshman 1 B ## 195 Statistics 93 Senior 4 B ## 201 CS 92 Senior 1 B ## 216 CS 91 Junior 1 B ## 219 Statistics 91 Senior 3 B ## 221 CS 87 Senior 3 B ## 225 Statistics 81 Freshman 4 B ## 227 Statistics 89 Freshman 4 B ## 228 Economics 89 Junior 2 B ## 252 CS 86 Senior 4 B ## 268 Statistics 93 Senior 1 B ## 291 Statistics 85 Senior 3 B ## 295 CS 93 Senior 4 B ## 298 Statistics 84 Freshman 2 B ## 321 CS 93 Freshman 4 B ## 322 Statistics 84 Junior 2 B ## 329 CS 90 Senior 3 B ## 350 CS 86 Senior 1 B ## 362 Statistics 84 Sophomore 1 B ## 369 CS 88 Senior 2 B ## 371 Statistics 88 Sophomore 2 B ## 372 Statistics 88 Senior 4 B ## 377 CS 91 Junior 2 B ## 381 CS 95 Senior 4 B ## 383 Statistics 81 Sophomore 2 B ## 397 CS 89 Senior 1 B ## 423 CS 93 Senior 3 B ## 436 CS 99 Junior 3 B ## 439 CS 95 Freshman 3 B ## 457 CS 90 Junior 1 B ## 461 CS 96 Senior 3 B ## 476 Statistics 83 Sophomore 2 B ## 537 Statistics 88 Freshman 3 B ## 554 CS 86 Sophomore 4 B ## 574 CS 86 Sophomore 4 B ## 576 Statistics 84 Senior 2 B ## 584 Statistics 83 Sophomore 2 B ## 595 Statistics 97 Senior 2 B ## 617 Statistics 81 Senior 2 B ## 649 Statistics 84 Freshman 4 B ## 694 Statistics 93 Senior 4 B ## 739 CS 95 Senior 4 B ## 741 Statistics 83 Sophomore 1 B ## 745 CS 94 Senior 1 B ## 774 Statistics 87 Freshman 3 B ## 780 CS 86 Freshman 1 B ## 791 CS 92 Senior 4 B ## 805 CS 90 Senior 2 B ## 822 Statistics 91 Senior 4 B ## 823 Statistics 83 Senior 3 B ## 834 CS 92 Senior 4 B ## 838 CS 88 Freshman 4 B ## 862 CS 90 Junior 4 B ## 872 CS 92 Junior 4 B ## 873 Statistics 81 Freshman 2 B ## 875 Statistics 94 Senior 4 B ## 877 CS 90 Senior 4 B ## 883 CS 97 Senior 4 B ## 906 Statistics 92 Senior 4 B ## 908 CS 88 Freshman 4 B ## 910 Statistics 82 Freshman 2 B ## 914 CS 88 Junior 3 B ## 917 Statistics 92 Senior 4 B ## 921 CS 87 Freshman 1 B ## 932 CS 91 Senior 4 B ## 996 CS 91 Junior 2 B 3.5.3 Snippet 3 Subsetting columns dc_light_exercise_unnamed-chunk-41 ## [1] &quot;Major&quot; &quot;Score&quot; &quot;Seniority&quot; &quot;GPA&quot; &quot;Grade&quot; ## [1] 4 ## [1] 4 3.5.4 Snippet 4 Sub-setting rows and columns dc_light_exercise_unnamed-chunk-42 ## [1] &quot;Score&quot; &quot;Seniority&quot; &quot;GPA&quot; ## [1] 229 3 One of the most important R instructions is tapply. It allows parallel execution of an aggregate function for different values of a categorical variable. 3.6 tapply tapply() has four arguments: the data frame (df), numerical attribute of df, categorical attribute of df and aggregate function (mean, max, min etc). Syntax of df is as follows: tapply(df$numerical attribute, df$categorical attribute, aggregate function) tapply() first slices data frame df by different values of a categorical attribute and then computes an aggregate (mean, median, min, max, etc..) of a numerical attribute to each slice. 3.6.1 Snippet 1 Example of tapply followed by plot dc_light_exercise_unnamed-chunk-43 ## A B C D F ## 91.85714 89.09091 71.38235 54.50000 25.76190 3.6.2 Snippet 2 Combining table and subset dc_light_exercise_unnamed-chunk-44 ## ## A B C D F ## 49 22 34 40 105 ## ## A B C D F ## 15 1 10 3 21 3.7 Additional references Data Transformation "],["plots.html", "Section: 4 🔖 Plots 4.1 Scatter Plot 4.2 Bar Plot 4.3 Box Plot 4.4 Mosaic Plot 4.5 Additional References", " Section: 4 🔖 Plots When you import your data to R studio one of the first things you do is plot. Data visualization is a key components of data analysis. Before we talk about plots, we introduce some very basis data structures in R: vectors, data frames and tables. These are introduced below in the form of code snippets that you can run and modify. Then we are ready to plot! We will introduce several basic plots such as scatter plot, bar plot, boxplot and mosaic plot. How do we know which plot to apply? It depends on whether the variables to be plotted are categorical or numerical. Below we show a simple table which can serve as a guide which plot to use depending on types of variables to be plotted. NUM x NUM scatter plot CAT x CAT mosaic plot CAT x NUM box plot NUM box plot, histogram CAT bargraph 4.1 Scatter Plot Scatter Plot are used to plot two numerical variables. Hence it is used when both the labels are numerical values. Lets look at example of scatter plot using Moody. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExldCdzIGxvb2sgYXQgYSAyIGF0dHJpYnV0ZSBzY2F0dGVyIHBsb3QuXG4jIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbnBsb3QobW9vZHkkcGFydGljaXBhdGlvbixtb29keSRzY29yZSx5bGFiPVwic2NvcmVcIix4bGFiPVwicGFydGljaXBhdGlvblwiLG1haW49XCIgUGFydGljaXBhdGlvbiB2cyBTY29yZVwiLGNvbD1cInJlZFwiKSJ9 4.2 Bar Plot A bar plot are used to plot a categorical variable. This rectangle height is proportional to the value of the variable in the vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbmNvbG9yczwtIGMoJ3JlZCcsJ2JsdWUnLCdjeWFuJywneWVsbG93JywnZ3JlZW4nKSAjIEFzc2lnbmluZyBkaWZmZXJlbnQgY29sb3JzIHRvIGJhcnNcblxuI2xldHMgbWFrZSBhIHRhYmxlIGZvciB0aGUgZ3JhZGVzIG9mIHN0dWRlbnRzIGFuZCBjb3VudHMgb2Ygc3R1ZGVudHMgZm9yIGVhY2ggR3JhZGUuIFxuXG50PC10YWJsZShtb29keSRncmFkZSlcblxuI29uY2Ugd2UgaGF2ZSB0aGUgdGFibGUgbGV0cyBjcmVhdGUgYSBiYXJwbG90IGZvciBpdC5cblxuYmFycGxvdCh0LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJOdW1iZXIgb2YgU3R1ZGVudHNcIixjb2w9Y29sb3JzLCBcbiAgICAgICAgbWFpbj1cIkJhcnBsb3QgZm9yIHN0dWRlbnQgZ3JhZGUgZGlzdHJpYnV0aW9uXCIsYm9yZGVyPVwiYmxhY2tcIikifQ== 4.3 Box Plot A boxplot is used to display a numerical variable. A boxplot shows the distribution of data in a dataset. A boxplot shows the following things: Minimum Maximum Median First quartile Third quartile Outliers eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbmNvbG9yczwtIGMoJ3JlZCcsJ2JsdWUnLCdjeWFuJywneWVsbG93JywnZ3JlZW4nKSAjIEFzc2lnbmluZyBkaWZmZXJlbnQgY29sb3JzIHRvIGJhcnNcblxuI1N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCB0aGUgZGlzdHJpYnV0aW9uIG9mIHN0dWRlbnRzIHNjb3JlIHBlciBHcmFkZS4gV2UgdXNlIGJveCBwbG90IGZvciBnZXR0aW5nIHRoYXQuIFxuYm94cGxvdChzY29yZX5ncmFkZSxkYXRhPW1vb2R5LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJTY29yZVwiLCBtYWluPVwiQm94cGxvdCBvZiBncmFkZSB2cyBzY29yZVwiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIilcblxuIyB0aGUgY2lyY2xlcyByZXByZXNlbnQgb3V0bGllcnMuIn0= 4.4 Mosaic Plot Mosaic plot is used to visualize two categorical variables. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbmNvbG9yczwtIGMoJ3JlZCcsJ2JsdWUnLCdjeWFuJywneWVsbG93JywnZ3JlZW4nKSAjIEFzc2lnbmluZyBkaWZmZXJlbnQgY29sb3JzIHRvIGJhcnNcblxuI3N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCBudW1iZXJzIG9mIHN0dWRlbnRzIHdpdGggYSBwYXJ0aWN1bGFyIGdyYWRlIGJhc2VkIG9uIHRoZWlyIHRleHRpbmcgaGFiaXRzLiBVc2UgTW9zaWFjLXBsb3QuXG5cbm1vc2FpY3Bsb3QobW9vZHkkZ3JhZGV+bW9vZHkkdGV4dGluZyx4bGFiID0gJ0dyYWRlJyx5bGFiID0gJ1RleHRpbmcgaGFiaXQnLCBtYWluID0gXCJNb3NpYWMgb2YgZ3JhZGUgdnMgdGV4aW5nIGhhYml0IGluIGNsYXNzXCIsY29sPWNvbG9ycyxib3JkZXI9XCJibGFja1wiKSJ9 4.5 Additional References Plots https://www.datamentor.io/r-programming/plot-function/ "],["hypothesis_testing.html", "Section: 5 🔖 Hypothesis Testing 5.1 Introduction 5.2 Snippet 1: Permutation test 5.3 Snippet 2: z-test 5.4 Snippet 3: Make your own data and see how p-value changes 5.5 Additional References", " Section: 5 🔖 Hypothesis Testing 5.1 Introduction Randomness is the biggest enemy of data scientists. How to distinguish what’s real from what’s random? This is the goal of hypothesis testing. We will introduce hypothesis testing through the permutation test. To illustrate the permutation test, let us start with a simple example of a dataset storing information about traffic in Lincoln and Holland tunnels. Table 5.1: Snippet of Traffic Dataset TUNNEL DAY VOLUME_PER_MINUTE 2694 Lincoln weekend 77.0 263 Holland weekday 46.5 992 Holland weekday 80.5 2275 Lincoln weekday 70.0 1348 Holland weekend 71.5 315 Holland weekday 77.5 1866 Lincoln weekday 93.0 1315 Holland weekend 67.5 1044 Holland weekend 69.5 2724 Lincoln weekend 69.0 We observe that Lincoln traffic is higher than Holland tunnel traffic by calculating average traffic volume per minute for each of the tunnels using the provided data. We conclude with 68.54 mean volume per minute for Lincoln and 67.71 mean volume per minute for the Holland tunnel. This seems to indicate that Lincoln traffic is higher than Holland traffic. But is it? Or is it just random deviation? Perhaps if we took more measurements, the trend would be reversed? This is where the permutation test comes handy. First, let us talk about the null hypothesis and the alternative hypothesis. Null hypothesis for the Lincoln-Holland tunnel observation is that, not surprisingly, there is no difference in traffic between the two tunnels. The alternative hypothesis states that Lincoln tunnel is more busy than Holland tunnel. Does observed data (observed traffic difference) provide us with enough evidence to reject null hypothesis and in fact support the alternative hypothesis? To answer this question we need to decide whether the observed result is reasonably likely to come up randomly under the condition that NULL hypothesis holds. How likely it is that observed difference (D=0.83) comes randomly? Permutation test helps us to estimate the chance that D=0.83 will come up randomly under the condition that traffic in Holland and Lincoln tunnels is equal. In each permutation of the permutation test we randomly scramble the traffic table once. Permutation test is run many times, typically 10,000, even 100,000 times, and each permutation simulates a random process by simply reassigning the traffic volume values randomly between tunnels. The numbers of traffic measurements in Holland and Lincoln tunnels respectively remain the same. Existing values are scrambled though - breaking any relationship between volume numbers and tunnel names. Each permutation is like rolling a dice. How often will this random process produce the result which is at least as extreme as D=0.83 that we have observed? The less often it happens the more likely it is that what we have observed is NOT random. For example, if we can get our observed result only 3 times in 1000 rolls of a dice (permutation test) it means that with probability of 99.7% our observed result cannot be random. Permutation test provides a palpable experience of randomness. Just roll the dice many times and see how often you can get the observed result or more. If you can get D&gt;0.83 relatively often (above what is called significance level usually it is at least 5% of the time), then you cannot reject the null hypothesis. In other words the conclusion that your observation appeared RANDOMLY. Otherwise, we can conclude that observation was not random - and we reject the null hypothesis. Notice, that every time we run the permutation test function we may get slightly different p-values. This is because permutations are random. The more times we run a permutation test, the closer it will approximate the “real” p-value. Snippet 6.1 shows permutation test results for the Traffic data set. Another test which is often used for difference of means hypothesis testing is the z-test. It is described very well in the attached link to the Khan Academy lecture. Here we run z-test function in one of the following snippets. 5.2 Snippet 1: Permutation test The following snippet 5.2 shows the code for hypothesis test of difference of means. Is the mean traffic (VOLUME_PER_MINUTE) in the Holland tunnel bigger than mean traffic (VOLUME_PER_MINUTE) in the Lincoln? Do this in your R studio, since we cannot install our package in data camp service we are using to run the code snippets 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 5.3 Snippet 2: z-test Null Hypothesis - Traffic in Holland tunnel is the same as traffic in Lincoln tunnel. Alternative Hypothesis - Traffic in the Holland Tunnel is larger than traffic in the Lincoln tunnel. In the snippet 5.3 we end up calculating the p-value which leads to rejection of Null hypothesis (good news for data scientist, bad for the sceptic). Indeed, p-value is less than the significance level of 5%. This means, that under null hypothesis it is extremely unlikely (less than 5% chance) to see the result which is at least as big as the observed difference of means. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InpfdGVzdCA8LSBmdW5jdGlvbihkYXRhLGNvbDEsY29sMixzdWIxLHN1YjIpIHtcbiAgZGF0YSA8LSBhcy5kYXRhLmZyYW1lKGRhdGEpXG4gIFYxPC1kYXRhWyxjb2wxXVxuICBWMjwtZGF0YVssY29sMl1cbiAgI2RhdGEgY2xlYW4gYW5kIHN1YnNldCwgZWl0aGVyXG4gIGxpbmNvbG4uZGF0YSA8LSBzdWJzZXQoZGF0YSwgVjEgPT0gc3ViMSlcbiAgaG9sbGFuZC5kYXRhIDwtIHN1YnNldChkYXRhLCBWMSA9PSBzdWIyKVxuICBcbiAgI3RyYWZmaWMgYXQgbGluY29sblxuICBsaW5jb2xuLnRyYWZmaWMgPC0gbGluY29sbi5kYXRhWyxjb2wyXVxuICAjdHJhZmZpYyBhdCBob2xsYW5kXG4gIGhvbGxhbmQudHJhZmZpYyA8LSBob2xsYW5kLmRhdGFbLGNvbDJdXG4gIFxuICAjIHN0YW5kYXJkIGRldmlhdGlvbiBvZiB0d28gc2FtcGxlcy5cbiAgc2QubGluY29sbiA8LSBzZChsaW5jb2xuLnRyYWZmaWMpXG4gIHNkLmhvbGxhbmQgPC0gc2QoaG9sbGFuZC50cmFmZmljKVxuICBcbiAgI2xlbmd0aCBvZiBsaW5jb2xuIGFuZCBob2xsYW5kXG4gIGxlbl9saW5jb2xuIDwtIGxlbmd0aChsaW5jb2xuLnRyYWZmaWMpXG4gIGxlbl9ob2xsYW5kIDwtIGxlbmd0aChob2xsYW5kLnRyYWZmaWMpXG4gIGxlbl9saW5jb2xuXG4gIGxlbl9ob2xsYW5kXG4gIFxuICAjc3RhbmRhcmQgZGV2aWF0aW9uIG9mIGRpZmZlcmVuY2UgdHJhZmZpY1xuICBzZC5saW4uaG9sIDwtIHNxcnQoc2QubGluY29sbl4yL2xlbl9saW5jb2xuICsgc2QuaG9sbGFuZF4yL2xlbl9ob2xsYW5kKVxuICBzZC5saW4uaG9sXG4gIFxuICAjbWVhbnMgb2YgdHdvIHNhbXBsZXNcbiAgbWVhbi5saW5jb2xuIDwtIG1lYW4obGluY29sbi50cmFmZmljKVxuICBtZWFuLmhvbGxhbmQgPC0gbWVhbihob2xsYW5kLnRyYWZmaWMpXG4gIG1lYW4ubGluY29sblxuICBtZWFuLmhvbGxhbmRcbiAgXG4gICN6IHNjb3JlXG4gIHpldGEgPC0gKG1lYW4ubGluY29sbiAtIG1lYW4uaG9sbGFuZCkvc2QubGluLmhvbFxuICBwcmludChwYXN0ZSh6ZXRhLFwiIGlzIHRoZSB6LXZhbHVlXCIpKVxuICBcbiAgI3Bsb3QgcmVkIGxpbmVcbiAgcGxvdCh4PXNlcShmcm9tID0gLTUsIHRvPSA1LCBieT0wLjEpLHk9ZG5vcm0oc2VxKGZyb20gPSAtNSwgdG89IDUsICBieT0wLjEpLG1lYW49MCksdHlwZT0nbCcseGxhYiA9ICdtZWFuIGRpZmZlcmVuY2UnLCAgeWxhYj0ncG9zc2liaWxpdHknKVxuICBhYmxpbmUodj16ZXRhLCBjb2w9J3JlZCcpXG4gIFxuICAjZ2V0IHBcbiAgcCA9IDEtcG5vcm0oemV0YSlcbiAgcHJpbnQocGFzdGUocCwgXCIgaXMgdGhlIHAtdmFsdWVcIikpXG59Iiwic2FtcGxlIjoiVFJBRkZJQzwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1RyYWZmaWMyMDIyLmNzdicpXG5cbnpfdGVzdChUUkFGRklDLFwiVFVOTkVMXCIsIFwiVk9MVU1FX1BFUl9NSU5VVEVcIixcIkxpbmNvbG5cIiwgXCJIb2xsYW5kXCIpIn0= 5.4 Snippet 3: Make your own data and see how p-value changes For students familiar with basic descriptive statistics (mean, standard deviation)we build a synthetic data set ourselves and see how difference of means and difference of standard deviations affects the p-value. We will build our two distributions ourselves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. Build the data frame with two attributes: Cat and Val, using rnorm() function. Our null hypothesis is that Group A and Group B have identical mean(Val). The alternative hypothesis is that the mean(Val) for Group B is higher than mean(Val) for Group A. We will change the mean and standard deviation of the data distributions for Group A and Group B and see how these changes affect the p-value. We will first use a permutation test and a single-step permutation test (just to illustrate what happens each single step when we run a permutation test). Then we finish off with the z-test. 5.4.1 Permuation test Exercise - How p-value is affected by difference of means and standard deviations We will build our two distributions ourseleves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use permutation test (can be z-test as well) to test difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. Build the data frame with two attributes: Cat and Val, using rnorm() function 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 5.4.2 One permutation at a time eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0cmFmZmljPC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvVHJhZmZpYzIwMjIuY3N2JylcblxucmFuTnVtIDwtIHNhbXBsZSgxOm5yb3codHJhZmZpYyksbnJvdyh0cmFmZmljKSlcbnJhbk51bVsxOjVdXG5cblZPTFVNRV9QRVJfTUlOVVRFPC10cmFmZmljJFZPTFVNRV9QRVJfTUlOVVRFW3Jhbk51bV1cblRVTk5FTDwtdHJhZmZpYyRUVU5ORUxcblxuUGVybXV0ZWRfdHJhZmZpYzwtZGF0YS5mcmFtZShUVU5ORUwsIFZPTFVNRV9QRVJfTUlOVVRFKVxuXG5tZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywgXSRWT0xVTUVfUEVSX01JTlVURSkgLW1lYW4odHJhZmZpY1t0cmFmZmljJFRVTk5FTD09J0hvbGxhbmQnLCBdJFZPTFVNRV9QRVJfTUlOVVRFKVxuXG5tZWFuKFBlcm11dGVkX3RyYWZmaWNbUGVybXV0ZWRfdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywgXSRWT0xVTUVfUEVSX01JTlVURSktbWVhbihQZXJtdXRlZF90cmFmZmljW1Blcm11dGVkX3RyYWZmaWMkVFVOTkVMPT0nSG9sbGFuZCcsIF0kVk9MVU1FX1BFUl9NSU5VVEUpIn0= 5.4.3 z-test How p-value is affected by difference of means and standard deviations. We will build two distributions ourselves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. You can do it by changing values of mean and standard deviation in the rnorm() function. Clearly the further apart the mean values are - the lower the p-value. But how do standard deviations affect the p-value? See for yourself. Build the data frame with two attributes: Cat and Val, using rnorm() function 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 5.5 Additional References Hypothesis Testing Permutation Test Khan Academy Video https://www.khanacademy.org/math/statistics-probability/significance-tests-confidence-intervals-two-samples/comparing-two-means/v/hypothesis-test-for-difference-of-means https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/p-value/ http://www.z-table.com/ https://www.statisticshowto.com/probability-and-statistics/z-score/ https://sixsigmastudyguide.com/z-scores-z-table-z-transformations/ "],["Chisquare.html", "Section: 6 🔖 Test of Independence 6.1 Introduction 6.2 Snippet 1 6.3 Snippet 2 6.4 Snippet 3 6.5 Snippet 4 6.6 Additional Reference", " Section: 6 🔖 Test of Independence 6.1 Introduction We would like to test if a student’s final grade in Professor Moody’s class depends on the student’s major. The null hypothesis in this case is the hypothesis of independence. Independence means that the distribution of final grades is the same no matter what the major is. The alternative hypothesis is that the distribution of final grades changes from major to major, rejecting the null hypothesis of independence. Notice that we do not specify how the grades depend on students’ majors. Do CS students get better grades than Psychology majors? Do Economics majors get lower grades than Statistics majors? We do not care about this. We are only testing here whether there is a relationship between Major and the final grade distribution. We will describe the permutation test which scrambles our data randomly in such a way that any relationship between grades and major (if it ever existed) is broken. We will run a permutation test a large number of times - possibly tens of thousands of times. Then we will determine how likely it is to randomly obtain the observed result. But what is the observed result? It is a bit more complex than the difference of means which were observing the difference of means hypothesis testing. The observed result in our case is calculated by so called chi-square statistic which is calculated on the contingency table, table(moody$Grade, moody$Major) To explain it, let us first define two tables: The observed contingency table and the expected contingency table. It is calculated by table() function. OBSERVED CONTINGENCY TABLE Grade/Major CS Economics Psychology Statistics A 46 54 69 42 B 46 12 2 35 C 51 33 30 34 D 41 37 29 34 F 108 99 99 99 The second table we need is called the expected table. This is the hypothetical table of the relationship between grade and major, assuming grade and major are completely independent. Such a table would be the result of the same distribution of grades for each of the majors. Notice that we 1000 students in the data set the expected table (i.e. table which have grades completely independent from majors) would have the same distribution of grades for each major that over all students - which is shown by the TOTAL column. EXPECTED CONTINGENCY TABLE Grade/Major CS Economics Psychology Statistics A 61 50 48 51 B 28 22 22 23 C 43 35 34 36 D 41 33 32 34 F 118 96 93 99 TOTAL 292 236 229 244 1000 We kept fractions - although these are number of students - therefore would have to be rounded up to integers The chi-square formula calculates the “distance” between the observed contingency table and the expected contingency table. \\[\\begin{equation} \\sum \\frac{(O_i - E_i)^2}{E_i}\\\\ \\text{where:}\\\\ \\text{O = observed values}\\\\ \\text{E = expected values}\\\\ \\end{equation}\\] For the two tables above the, \\[\\begin{equation} \\sum \\frac{(O_i - E_i)^2}{E_i} = 60.03\\\\ \\end{equation}\\] To evaluate how far off is this observed result assuming that Grades are independent from Major, we run a permutation test which scrambles Grades and Majors randomly and every time computes the chi-square formula with the new observed table (the expected table is always the same). Then, we see how many times out of, say 10,000 iterations of permutation test we obtain a result larger than the observed result of 60.03? This is the p-value. Permutation test for independence hypothesis gives us again a better feeling about the impact of randomness and whether the observed chi-square result for “similarity” of grade distributions for different majors can be obtained randomly. In the following snippet we run the chisq test which is based on the so-called chi square distribution. Here we simply show you a function which can calculate p-value, just like the z-test function does. The explanation of the chi-square test is provided in attached link to the excellent Khan Academy video. Permutation tests in both cases of difference of means and independence hypotheses give a better intuitive sense of how we answer the question - can the observed result be obtained randomly? Notice that the independence test is looking globally at two vectors and whether one depends on another. If we wanted to be more specific and know if psychology majors are more likely to get an A than CS majors, we can frame this as a difference of means hypothesis. Testing this hypothesis will be using the difference of means of frequencies of A’s among CS majors and psychology majors. This could be done again by permutation test in section 8 or the z-test. 6.2 Snippet 1 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJFeHBlY3RlZCA8LW1hdHJpeChjKDIwMCw0MjAsMTgwLCA0MCwxMjAsNDApLCBucm93PTMsIG5jb2w9Milcbk9ic2VydmVkPC1tYXRyaXgoYygyMDAsNDIwLDE4MCwzNSwxMjAsNDUpLCBucm93PTMsIG5jb2w9MilcbkV4cGVjdGVkXG5PYnNlcnZlZFxuY2hpc3EudGVzdChPYnNlcnZlZCkifQ== 6.3 Snippet 2 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZHBseXIpXG5vcHRpb25zKHdhcm49LTEpXG5jaGlfdGVzdCA8LSBmdW5jdGlvbihkYXRhLGNvbDEsY29sMixpdGVyKSB7XG4gIFxuICBkZiA8LSBkYXRhLmZyYW1lKGRhdGEpXG4gIHZhbHM8LSB1bmlxdWUoZGZbW2NvbDJdXSlcbiAgbm9fcm93cyA8LSBucm93KGRmKVxuICBkdCA8LSB0YWJsZShkZltbY29sMV1dLCBkZltbY29sMl1dKVxuICByZXMgPC0gY2hpc3EudGVzdChkdClcbiAgcmVhbF9hbnMgPC0gcmVzJHN0YXRpc3RpY1xuICBwX3ZhbHVlIDwtIHJlcyRwLnZhbHVlXG4gIGFuc192ZWMgPC0gdmVjdG9yKClcbiAgZm9yICh4IGluIDE6aXRlcil7XG5cbiAgICBuZXdfZGF0YSA8LSBzYW1wbGUoeD12YWxzLHNpemU9bm9fcm93cyxyZXBsYWNlID0gVFJVRSlcblxuICAgIGR0X25ldyA8LSB0YWJsZShkZltbY29sMV1dLCBuZXdfZGF0YSlcblxuICAgIHJlc19uZXcgPC0gY2hpc3EudGVzdChkdF9uZXcpXG5cbiAgICBhbnNfdmVjIDwtIGFwcGVuZChhbnNfdmVjLHJlc19uZXckc3RhdGlzdGljKVxuICB9XG4gIGhpc3QoYW5zX3ZlYyxtYWluPVwiUGVybXV0YXRpb24gVGVzdCBmb3IgQ2hpLVNxdWFyZVwiLHhsYWI9XCJDaGktU3F1YXJlIFZhbHVlc1wiLGJyZWFrcyA9IDEwMClcbiAgcHJpbnQocmVhbF9hbnMpXG4gIGFibGluZSh2PXJlYWxfYW5zLGNvbD1cImJsdWVcIixsd2Q9MilcbiAgcmV0dXJuIChwX3ZhbHVlKVxufSIsInNhbXBsZSI6ImQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW9vZHlNYXJjaDIwMjJiLmNzdlwiKVxuaGVhZChkKVxuXG5jaGlfdGVzdChkLFwiTWFqb3JcIixcIkdyYWRlXCIsMTAwMCkifQ== 6.4 Snippet 3 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxubW9vZHkkSU48LSdPdXRfU2xpY2UnXG5tb29keVttb29keSRET1pFU19PRkY9PSduZXZlcicgJiBtb29keSRURVhUSU5HX0lOX0NMQVNTPT0nYWx3YXlzJywgXSRJTjwtJ0luX1NsaWNlJ1xuZDwtdGFibGUobW9vZHkkR1JBREUsIG1vb2R5JElOKVxuZFxuY2hpc3EudGVzdChkKSJ9 6.5 Snippet 4 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcbmRhdGE8LXRhYmxlKG1vdmllcyRjb250ZW50LCBtb3ZpZXMkZ2VucmUpXG5jaGlzcS50ZXN0KGRhdGEpIn0= 6.6 Additional Reference Chi Square Khan Academy Video https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-goodness-fit/v/chi-square-statistic "],["Multiple_Hypothesis.html", "Section: 7 🔖 Multiple Hypothesis Testing 7.1 Introduction 7.2 Additional References", " Section: 7 🔖 Multiple Hypothesis Testing 7.1 Introduction We often consider multiple possible hypotheses in our search for discovery to find one with lowest possible p-value. Consciously or subconsciously we are engaging, what is often called, p-value hunting. We have to be very careful! We may “discover” what is simply random even if we correctly calculate p-value and compare it with the significance level. It is very important to learn about multiple hypothesis traps very early in the process of learning data science. For example assume that we are looking for associations between sales of individual items in a supermarket. Does bread sell with butter? Does coffee sell with spring water? There is an exponential number of possible combinations (N choose 2 to be exact, where N is the number of items). For each such pair we perform hypothesis testing. If one test is performed at the 5% significance level and the corresponding null hypothesis is true, there is only a 5% chance of incorrectly rejecting the null hypothesis. However, if 100 tests are each conducted at the 5% significance level and all corresponding null hypotheses are true, the expected number of incorrect rejections (also known as false positives or Type I errors) is 5. If the tests are statistically independent from each other, the probability of at least one incorrect rejection is approximately 99.4%. Thus, we will almost surely find one false positive! In other words we will be fooled by data. Bonferroni correction is a method to counteract the multiple hypothesis (often called multiple comparison problem. Make it harder to reject null hypotheses by dividing the significance level by number of hypotheses. The Bonferroni correction compensates for that increase by testing each individual hypothesis at a significance level of α / m where m is the number of hypotheses. For example, if a trial is testing me = 20 hypotheses with a desired α = 0.05, then the Bonferroni correction would test each individual hypothesis at \\[\\begin{equation} \\alpha = \\frac{0.05} {20} = 0.0025 \\end{equation}\\] Thus, there is a very simple remedy for multiple hypothesis traps. Just divide the significance level by the number of (potential) hypotheses tested. This will make it harder, often much much harder to reject the null hypothesis and yell Eureka! Critics say that in fact Bonferroni correction is too conservative and too “pro-null” and tough on alternative hypotheses to be acceptable. The unwanted side effect of Bonferroni correction is that we may fail to reject the null hypothesis too often. Bonferroni correction makes discovery sometimes too hard, making data scientists too conservative and accepting null hypothesis when they should be rejected. It may also be the case that even Bonferroni correction will not protect us, as we will show in our example below. But at least we will be much less likely to make fools of ourselves coming with false discoveries leading potentially to very wrong business decisions. There are other less conservative methods of correcting for multiple hypotheses - such as the Benjamini-Hochberg method described in the attached slides. The ?? describes the data set based on a hypothetical temperature readings in various municipalities of New Jersey over summer. Is one city experiencing higher average temperatures than another? Can we find such a pair of cities? This is the ultimate p-value hunt. Let’s compare townshiships pair by pair, until we find a pair with sufficiently large differences of mean temperatures and sufficiently low p-value. Careful! You may come up with false discovery if you do not correct for multiple hypotheses! ?? shows several permutation tests for different pairs of townships and difference of means of temperatures hypothesis test. Two of four pairs show p–values less than customary significance level of 5%. Should we then reject the null hypothesis and conclude that indeed Ocean Grove is warmer than New Brunswick and that New Brunswick is warmer than Holmdel? Indeed, both pairs result in p-values significantly lower than 5%. If we incorrectly disregard the number of hypotheses considered, we may come to wrong conclusions supporting these two alternative hypotheses. But there are around 20 townships in the Temp data set. Thus there are around 200 possible hypotheses (200 pairs of townships) which we may consider in our p-value hunt. If we apply Bonferroni correction for N=200, the significance level will be 200 times lower, instead of 5%, it will be 0.025%. None of the two hypotheses (Ocean Grove vs New Brunswick and New Brunswick vs Holmdel) meets the new significance level. Indeed in both cases p-values are significantly larger than 0.025%. Thus, for none of the four pairs we can reject null hypotheses. Now we can disclose that we have created our Temp data set completely randomly - assigning random temperatures between 50 and 100 degrees to each township. Thus, without Bonferroni coefficient we would be fooled by data, not once, but twice in our four tests. We would find a trend when it does not exist - it is simply random deviation. It turns out however that even Bonferroni correction is not sufficient to protect us against incorrectly rejecting null hypothesis. Indeed, for Red Bank and Holmdel, we conclude that Red Band is warmer than Holmdel with p-value of 0.01%! (see the last permutation test in the snippet 1). This p-value falls even below significance level adjusted with Bonferroni correction (0.025%). It only shows that dealing with multiple hypotheses is a risky adventure. We may end up being fooled by data even when we apply Bonferroni correction. But at least we are less likely to fall into the trap of multiple hypotheses when we apply Bonferroni Correction. Table 7.1: Snippet of Hindex Dataset IDN COUNTRY HAPPINESS 1270 49450 Latvia 4.61 4455 15743 North Cyprus 2.57 5276 19061 Tajikistan 2.49 4390 37027 Azerbaijan 4.62 3609 89386 Czech Republic 8.55 3846 18075 Iraq 2.60 5807 86933 Croatia 8.56 2318 21655 Finland 3.30 3369 66225 Armenia 6.70 4762 69650 Kyrgyzstan 7.47 7.2 Additional References Multiple Hypothesis Testing https://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/ "],["FreeStyle.html", "Section: 8 🔖 Data puzzles 8.1 Strange grading methods of Professor Moody Data Puzzle 8.2 How to predict a good party? Data puzzle 8.3 When election is truly local - data puzzle 8.4 Secrets of good sleep Data Puzzle 8.5 Let’s go to the movies: Data Puzzle 8.6 When canvas goes wild data puzzle 8.7 Very local minimarket data puzzle 8.8 Airbnb data puzzle 8.9 Titanic data puzzle 8.10 Addiotional Reference", " Section: 8 🔖 Data puzzles Data Puzzles are synthetically generated datasets with some embedded patterns. Patterns have various forms from relationships between attributes to rules of the form “if condition then value” between specific attribute-value pairs. These patterns are stochastic and embedded in datasets using DataMaker - our Data Puzzle Generation Tool. We use data puzzles extensively in the class assignments. These range from data exploration and plotting through hypothesis testing to prediction and machine learning. After the assignment is completed we reveal the data secrets - the patterns which were embedded by DataMaker. Students do not have to find exactly the embedded patterns, often they find related patterns which makes the “game” even more fun. In the following we provide the list of data puzzles along with the underlying data sets. Using DataMaker we change the patterns and even data sets from academic year to academic year.. We can also provide data puzzles of different levels of difficulty from the one star (easy) to five star (most difficult) ones. 8.1 Strange grading methods of Professor Moody Data Puzzle Download: moody2022_new.csv How to get a good grade in Professor Moody’s class? Professor Moody does not give final grades just on the basis of your total score alone. Our data shows that two students with the same total score may get widely varying final grades. Can you believe that you can even fail his class with a score as high as 82%? This is outrageous, isn’t it? DataMaker has generated thousands of tuples which in addition to the total score and final grade also store bizarre information about student behaviors in the class - do they often doze off? Does a student text a lot? Does s/he ask a lot of questions? Does it help if you ask a lot of questions? Does it hurt if you doze off a lot? Comment: There is a series of Professor Moody’s puzzles which we have used over the years. We have used different attributes including student’s major, , seniority, class participation etc. Table 8.1: Snippet of Moody Dataset SCORE GRADE DOZES_OFF TEXTING_IN_CLASS PARTICIPATION 21.33 F never never 0.29 71.57 C always rarely 0.11 90.11 A always never 0.26 31.52 D sometimes rarely 0.03 95.94 A always rarely 0.21 8.1.1 Practice Snippets 8.1.1.1 Snippet 1: Get familiar with the data set eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5jb2xuYW1lcyhtb29keSlcbnN1bW1hcnkobW9vZHkpXG51bmlxdWUobW9vZHkkR1JBREUpXG5ucm93KG1vb2R5KVxudW5pcXVlKG1vb2R5JERPWkVTX09GRilcbnVuaXF1ZShtb29keSRURVhUSU5HX0lOX0NMQVNTKVxudGFibGUobW9vZHkkR1JBREUpXG50YWJsZShtb29keSRET1pFU19PRkYpXG50YWJsZShtb29keSRURVhUSU5HX0lOX0NMQVNTKVxudGFwcGx5KG1vb2R5JFNDT1JFLCBtb29keSRHUkFERSwgbWVhbilcbnRhcHBseShtb29keSRQQVJUSUNJUEFUSU9OLCBtb29keSRHUkFERSwgbWVhbilcbnRhcHBseShtb29keSRTQ09SRSwgbW9vZHkkRE9aRVNfT0ZGLCBtZWFuKVxudGFwcGx5KG1vb2R5JFBBUlRJQ0lQQVRJT04sIG1vb2R5JERPWkVTX09GRiwgbWVhbilcbnRhcHBseShtb29keSRTQ09SRSwgbW9vZHkkVEVYVElOR19JTl9DTEFTUywgbWVhbilcbnRhcHBseShtb29keSRQQVJUSUNJUEFUSU9OLCBtb29keSRURVhUSU5HX0lOX0NMQVNTLCBtZWFuKSJ9 8.1.1.2 Snippet 2 Q: Did you know that students who never doze off in class have more than twice as many A’s than students who sometimes doze off? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG50YWJsZShtb29keSRET1pFU19PRkYsIG1vb2R5JEdSQURFKSJ9 8.1.1.3 Snippet 3 Q: Did you know that the students who scored over 85 and still received a B almost always dozed off all the time during class? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5tb29keVsobW9vZHkkU0NPUkU+ODUpICYgKG1vb2R5JEdSQURFPT0nQicpLCBdJERPWkVTX09GRiJ9 8.1.1.4 Snippet 4 Q: What gives a higher chance of failing, texting all the time or always dozing off during class? A: Always texting in class! Almost 40% chance of failing! eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG4jUHJvYmFiaWxpdHkgb2YgZmFpbGluZyB0aGUgY2xhc3Mgd2hlbiBkb3ppbmcgb2ZmIGFsbCB0aGUgdGltZVxubnJvdyhtb29keVttb29keSRET1pFU19PRkY9PSdhbHdheXMnICYgbW9vZHkkR1JBREU9PSdGJyxdKS9ucm93KG1vb2R5W21vb2R5JERPWkVTX09GRj09J2Fsd2F5cycsXSlcbiNQcm9iYWJpbGl0eSBvZiBmYWlsaW5nIHRoZSBjbGFzcyB3aGVuICBhbHdheXMgdGV4dGluZyBpbiBjbGFzcyBcbm5yb3cobW9vZHlbbW9vZHkkVEVYVElOR19JTl9DTEFTUz09J2Fsd2F5cycgJiBtb29keSRHUkFERT09J0YnLF0pL25yb3cobW9vZHlbbW9vZHkkVEVYVElOR19JTl9DTEFTUz09J2Fsd2F5cycsXSkifQ== 8.1.1.5 Snippet 5 Q: What grade did a student who scores 39.57 and always dozed off received? A: D eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5tb29keVttb29keSRTQ09SRT09JzM5LjU3JyZtb29keSRET1pFU19PRkY9PSdhbHdheXMnLF0kR1JBREUifQ== 8.1.1.6 Snippet 6 Q: What are posterior odds that an A student never dozes off? A: Posterior Odds = 2.66 Likelihood Ratio = 4 Prior Odds =0.56 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5QcmlvcjwtbnJvdyhtb29keVttb29keSRET1pFU19PRkYgPT0nbmV2ZXInLF0pL25yb3cobW9vZHkpXG5QcmlvclxuUHJpb3JPZGRzPC1yb3VuZChQcmlvci8oMS1QcmlvciksMilcblByaW9yT2Rkc1xuVHJ1ZVBvc2l0aXZlPC1yb3VuZChucm93KG1vb2R5W21vb2R5JEdSQURFPT0nQScmIG1vb2R5JERPWkVTX09GRj09J25ldmVyJyxdKS9ucm93KG1vb2R5W21vb2R5JERPWkVTX09GRj09J2Fsd2F5cycsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRHUkFERT09J0EnJiBtb29keSRET1pFU19PRkYhPSduZXZlcicsXSkvbnJvdyhtb29keVttb29keSRET1pFU19PRkYhPSdhbHdheXMnLF0pLDIpXG5GYWxzZVBvc2l0aXZlXG5MaWtlbGlob29kUmF0aW88LXJvdW5kKFRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlLDIpXG5MaWtlbGlob29kUmF0aW9cblBvc3Rlcmlvck9kZHMgPC1MaWtlbGlob29kUmF0aW8gKiBQcmlvck9kZHNcblBvc3Rlcmlvck9kZHNcblBvc3RlcmlvciA8LVBvc3Rlcmlvck9kZHMvKDErUG9zdGVyaW9yT2RkcylcblBvc3RlcmlvciJ9 8.1.1.7 Snippet 7 Q: Verify the hypothesis that C students have higher mean participation than F students? What is the p-value? A: Negative. Fail to reject null hypothesis that mean participations of C and F students are the same with p=0.11 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiIjaW5zdGFsbC5wYWNrYWdlcyhcImRldnRvb2xzXCIpXG4jZGV2dG9vbHM6Omluc3RhbGxfZ2l0aHViKFwiZGV2YW5zaGFnci9QZXJtdXRhdGlvblRlc3RTZWNvbmRcIilcblxuI1Blcm11dGF0aW9uVGVzdFNlY29uZDo6UGVybXV0YXRpb24oZCwgXCJDYXRcIiwgXCJWYWxcIiwxMDAwMCwgXCJHcm91cEFcIiwgXCJHcm91cEJcIikgXG5cbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMl9uZXcuY3N2XCIpXG5cblBlcm11dGF0aW9uVGVzdFNlY29uZDo6UGVybXV0YXRpb24obW9vZHksIFwiR1JBREVcIiwgXCJQQVJUSUNJUEFUSU9OXCIsMTAwMDAsIFwiQ1wiLCBcIkZcIikifQ== 8.1.1.8 Snippet 8 Q: What is the mean score of students who always doze off in class and what is the most frequent grade that they received? A: The mean score is 50.26 and the most frequent grade is D. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5tZWFuKG1vb2R5W21vb2R5JERPWkVTX09GRj09J2Fsd2F5cycsXSRTQ09SRSlcbnRhYmxlKG1vb2R5W21vb2R5JERPWkVTX09GRj09J2Fsd2F5cycsXSRHUkFERSkifQ== Great job!! You have made it this far. You are now familiar with the moody dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 8.1.3. 8.1.2 Moody Data Quiz Quiz Time 8.1.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5zdW1tYXJ5KG1vb2R5KSJ9 8.2 How to predict a good party? Data puzzle Download: Partyb.csv DataMaker has generated data about thousands of parties, some fun parties, others which were OK or simply boring. Your goal is to discover secrets of a fun party. Is it music? Dancing? Does the host matter? Or who was present at a party? Maybe who was NOT present at the party? All this data is stored in this data puzzle. Table 8.2: Snippet of Party Dataset Party Music Host WasThere WasNotThere CaloriesDanc 795 Boring Classical Sonya Eva Angela 58 2101 Fun HipHop Sonya Qiong Manny 435 2721 Boring Classical Janek Qiong Joe 173 2228 ItwasOK Rock Alex Billy Joe 127 1629 Boring Classical Katya Ada Angela 141 8.2.1 Practice Snippets 8.2.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbmNvbG5hbWVzKHBhcnR5KVxubnJvdyhwYXJ0eSlcbnN1bW1hcnkocGFydHkpXG51bmlxdWUocGFydHkkUGFydHkpXG51bmlxdWUocGFydHkkTXVzaWMpXG51bmlxdWUocGFydHkkSG9zdClcbnVuaXF1ZShwYXJ0eSRXYXNUaGVyZSlcbnVuaXF1ZShwYXJ0eSRXYXNOb3RUaGVyZSlcbnRhYmxlKHBhcnR5JFBhcnR5KVxudGFibGUocGFydHkkTXVzaWMpXG50YWJsZShwYXJ0eSRIb3N0KVxudGFibGUocGFydHkkV2FzVGhlcmUpXG50YWJsZShwYXJ0eSRXYXNOb3RUaGVyZSlcbmNvbG5hbWVzKHBhcnR5KVxudGFwcGx5KHBhcnR5JENhbG9yaWVzRGFuYywgcGFydHkkUGFydHksIG1lYW4pXG50YXBwbHkocGFydHkkQ2Fsb3JpZXNEYW5jLCBwYXJ0eSRNdXNpYywgbWVhbilcbnRhcHBseShwYXJ0eSRDYWxvcmllc0RhbmMsIHBhcnR5JEhvc3QsIG1lYW4pXG50YXBwbHkocGFydHkkQ2Fsb3JpZXNEYW5jLCBwYXJ0eSRXYXNUaGVyZSwgbWVhbilcbnRhcHBseShwYXJ0eSRDYWxvcmllc0RhbmMsIHBhcnR5JFdhc05vdFRoZXJlLCBtZWFuKSJ9 8.2.1.2 Snippet 2 Q: Did you know that a party is often boring when Angela is not there? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbnRhYmxlKHBhcnR5W3BhcnR5JFdhc05vdFRoZXJlPT0nQW5nZWxhJyxdJFBhcnR5KSJ9 8.2.1.3 Snippet 3 Q: What are the odds of the party being fun when Vladimir is not there? A: PosteriorOdds = 2.91 LikelihoodRatio = 3.83 Prior Odds = 0.76 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cblByaW9yPC1ucm93KHBhcnR5W3BhcnR5JFBhcnR5ID09J0Z1bicsXSkvbnJvdyhwYXJ0eSlcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3cocGFydHlbcGFydHkkUGFydHk9PSdGdW4nJiBwYXJ0eSRXYXNOb3RUaGVyZT09J1ZsYWRpbWlyJyxdKS9ucm93KHBhcnR5W3BhcnR5JFBhcnR5PT0nRnVuJyxdKSwyKVxuVHJ1ZVBvc2l0aXZlXG5GYWxzZVBvc2l0aXZlPC1yb3VuZChucm93KHBhcnR5W3BhcnR5JFBhcnR5IT0nRnVuJyYgcGFydHkkV2FzTm90VGhlcmU9PSdWbGFkaW1pcicsXSkvbnJvdyhwYXJ0eVtwYXJ0eSRQYXJ0eSE9J0Z1bicsXSksMilcbkZhbHNlUG9zaXRpdmVcbkxpa2VsaWhvb2RSYXRpbzwtcm91bmQoVHJ1ZVBvc2l0aXZlL0ZhbHNlUG9zaXRpdmUsMilcbkxpa2VsaWhvb2RSYXRpb1xuUG9zdGVyaW9yT2RkcyA8LUxpa2VsaWhvb2RSYXRpbyAqIFByaW9yT2Rkc1xuUG9zdGVyaW9yT2Rkc1xuUG9zdGVyaW9yIDwtUG9zdGVyaW9yT2Rkcy8oMStQb3N0ZXJpb3JPZGRzKVxuUG9zdGVyaW9yIn0= 8.2.1.4 Snippet 4 Q: Verify the hypothesis that there is more dancing at Fun parties than at Boring parties? A: Positive. We reject null hypothesis that there is same amount of dancing at Fun and Boring parties with p &lt; 0.00001 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbm1lYW4ocGFydHlbcGFydHkkUGFydHk9PSdGdW4nLF0kQ2Fsb3JpZXNEYW5jKVxubWVhbihwYXJ0eVtwYXJ0eSRQYXJ0eT09J0JvcmluZycsXSRDYWxvcmllc0RhbmMpXG5cblBlcm11dGF0aW9uVGVzdFNlY29uZDo6UGVybXV0YXRpb24ocGFydHksIFwiUGFydHlcIiwgXCJDYWxvcmllc0RhbmNcIiwxMDAwMCwgXCJGdW5cIiwgXCJCb3JpbmdcIikifQ== 8.2.1.5 Snippet 5 Q: What music is the most popular at Fun parties? A: HipHop The following snippet allows us to find the most popular music, although the code just returns the frequency of music genres distribution. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbnRhYmxlKHBhcnR5W3BhcnR5JFBhcnR5PT0nRnVuJyxdJE11c2ljKSJ9 You are now familiar with the Party dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 8.2.3. 8.2.2 Party Data Quiz Quiz Time 8.2.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbnN1bW1hcnkocGFydHkpIn0= 8.3 When election is truly local - data puzzle Download: Voting1.csv In local elections in some small towns, candidates of three local parties: Royalists, KnowNothings and Anarchists are running for the office of the mayor. DataMaker has generated a survey of thousands of town residents and their political sympathies. Data of course can not be more local, leaving global concerns such as inflation or global warming to national or state office candidates. Here, the electorate cares about issues such as “should we allow leaflowers” (all, only electric, none?), what about CBD stores in town (none, just one, no restrictions), How about liquor (should the town be dry? Or hard liqueurs only). Speed limits? (none, 10mph etc) or even more extreme - the whole town being car-free, streets open only to bicycles and pedestrians? Can we develop the profiles of voters for each of the parties? What does the anarchist electorate care about? Which party is leading among young people who do not want any speed limits in town? Table 8.3: Snippet of Voting Dataset LeafBlowers CBD GasMowers Party LiquerStores SpeedLimit Age 2471 None NoStores NoRestrictions Royalists None NoCars 30 3073 None OneStore ElectircOnly Anarchists HardLiquerOnly NoLimits 20 2162 NoRestrictions NoRestrictions ElectircOnly Royalists NoLimits 25mph 24 3097 ElectricOnly NoStores ElectircOnly Royalists HardLiquerOnly NoLimits 71 3970 ElectricOnly NoStores ElectircOnly Royalists NoLimits NoLimits 87 8.3.1 Practice Snippets 8.3.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbmNvbG5hbWVzKHZvdGUpXG5ucm93KHZvdGUpXG5zdW1tYXJ5KHZvdGUpXG51bmlxdWUodm90ZSRMZWFmQmxvd2VycylcbnVuaXF1ZSh2b3RlJENCRClcbnVuaXF1ZSh2b3RlJEdhc01vd2VycylcbnVuaXF1ZSh2b3RlJFBhcnR5KVxudW5pcXVlKHZvdGUkTGlxdWVyU3RvcmVzKVxudW5pcXVlKHZvdGUkU3BlZWRMaW1pdClcblxudGFibGUodm90ZSRQYXJ0eSlcbnRhYmxlKHZvdGUkU3BlZWRMaW1pdClcbnRhYmxlKHZvdGUkTGVhZkJsb3dlcnMpXG50YWJsZSh2b3RlJEdhc01vd2VycykifQ== 8.3.1.2 Snippet 2 Q: Which party has the oldest constituents? A: Royalists eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhcHBseSh2b3RlJEFnZSwgdm90ZSRQYXJ0eSwgbWVhbikifQ== 8.3.1.3 Snippet 3 Q: How do voters who are against Gas Mowers vote? A: Royalists eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRMZWFmQmxvd2Vycz09J05vbmUnLF0kUGFydHkpIn0= 8.3.1.4 Snippet 4 Q: How do KnowNothings voters vote on speed limits? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRQYXJ0eT09J0tub3dOb3RoaW5ncycsXSRTcGVlZExpbWl0KSJ9 8.3.1.5 Snippet 5 Q: What is the age of the oldest voter for KnowNothings? A: 100 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbm1heCh2b3RlW3ZvdGUkUGFydHk9PSdLbm93Tm90aGluZ3MnLF0kQWdlKSJ9 8.3.1.6 Snippet 6 Q: Which party wins the most votes from supporters of no speed limits, no restrictions on CBD stores and Ban of Leaf Blowers? A: Anarchists eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRTcGVlZExpbWl0ID09J05vTGltaXRzJyZ2b3RlJENCRD09J05vUmVzdHJpY3Rpb25zJyYgdm90ZSRMZWFmQmxvd2Vycz09J05vbmUnLCBdJFBhcnR5KSJ9 8.3.1.7 Snippet 7 Q: Which party wins the most votes of supporters of HardLiquerOnly Liquor stores? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRMaXF1ZXJTdG9yZXM9PSdIYXJkTGlxdWVyT25seScsIF0kUGFydHkpIn0= 8.3.1.8 Snippet 8 Q: What are the odds that a voter older than 65 will vote for Royalists? A: Posterior Odds= 6.44 Likelihood ratio = 6.08 Prior Odds= 1.06 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cblByaW9yPC1ucm93KHZvdGVbdm90ZSRQYXJ0eSA9PSdSb3lhbGlzdHMnLF0pL25yb3codm90ZSlcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3codm90ZVt2b3RlJFBhcnR5PT0nUm95YWxpc3RzJyYgdm90ZSRBZ2U+NjUsXSkvbnJvdyh2b3RlW3ZvdGUkUGFydHk9PSdSb3lhbGlzdHMnLF0pLDIpXG5UcnVlUG9zaXRpdmVcbkZhbHNlUG9zaXRpdmU8LXJvdW5kKG5yb3codm90ZVt2b3RlJFBhcnR5IT0nUm95YWxpc3RzJyYgdm90ZSRBZ2U+NjUsXSkvbnJvdyh2b3RlW3ZvdGUkUGFydHkhPSdSb3lhbGlzdHMnLF0pLDIpXG5GYWxzZVBvc2l0aXZlXG5MaWtlbGlob29kUmF0aW88LXJvdW5kKFRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlLDIpXG5MaWtlbGlob29kUmF0aW9cblBvc3Rlcmlvck9kZHMgPC1MaWtlbGlob29kUmF0aW8gKiBQcmlvck9kZHNcblBvc3Rlcmlvck9kZHNcblBvc3RlcmlvciA8LVBvc3Rlcmlvck9kZHMvKDErUG9zdGVyaW9yT2RkcylcblBvc3RlcmlvciJ9 8.3.1.9 Snippet 9 Q: Verify the hypothesis that the average age of Anarchists voters is higher than the average age of KnowNothings voters? A: Positive. Reject of null hypothesis that average ages of Anarchists and KnowNothings voters are the same eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbm1lYW4odm90ZVt2b3RlJFBhcnR5PT0nQW5hcmNoaXN0cycsXSRBZ2UpIFxubWVhbih2b3RlW3ZvdGUkUGFydHk9PSdLbm93Tm90aGluZ3MnLF0kQWdlKSBcblxuUGVybXV0YXRpb25UZXN0U2Vjb25kOjpQZXJtdXRhdGlvbih2b3RlLCBcIlBhcnR5XCIsIFwiQWdlXCIsMTAwMDAsIFwiQW5hcmNoaXN0c1wiLCBcIktub3dOb3RoaW5nc1wiKSAifQ== 8.3.1.10 Snippet 10 Q: What is the most frequent position of Anarchists on the Speed Limit issue? A: “No limits” is the most frequent position of Anarchists on Speed Limit issue eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRQYXJ0eT09J0FuYXJjaGlzdHMnLF0kU3BlZWRMaW1pdCkifQ== 8.3.1.11 Snippet 11 Q: Which party wins the most votes from supporters of Electric Leaf Blowers? A: Royalists eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRMZWFmQmxvd2Vycz09J0VsZWN0cmljT25seScsXSRQYXJ0eSkifQ== You are now familiar with the Election dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 8.3.3. 8.3.2 Election Data Quiz Quiz Time 8.3.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnN1bW1hcnkodm90ZSkifQ== 8.4 Secrets of good sleep Data Puzzle Download: SleepPrediction2.csv Who wouldn’t want to know the secrets of good sleep? DataMaker has created a data set which may help to find these secrets. We store the number of exercise calories burnt during the day, the amount of wimpy tea a person has drunk (in ounces), hours spent on the computer and the quality of the preceding night’s sleep. Table 8.4: Snippet of Sleep Dataset Sleep ExerciseCal OnComputer WimpyTea RoomTemp Moon LastSleep 1784 Deep 576 3 3Cups 64 Half Shallow 1679 Deep 667 2 2Cups 62 Full Shallow 994 Deep 464 5 2Cups 66 Full Shallow 674 Deep 810 9 ManyCups 63 Full Shallow 1081 Deep 418 2 1Cup 58 Half Shallow 8.4.1 Practice Snippets 8.4.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5jb2xuYW1lcyhzbGVlcClcbm5yb3coc2xlZXApXG5zdW1tYXJ5KHNsZWVwKVxudW5pcXVlKHNsZWVwJFNsZWVwKVxudW5pcXVlKHNsZWVwJFdpbXB5VGVhKVxudW5pcXVlKHNsZWVwJE1vb24pXG51bmlxdWUoc2xlZXAkTGFzdFNsZWVwKVxudGFibGUoc2xlZXAkU2xlZXApXG50YWJsZShzbGVlcCRXaW1weVRlYSlcbnRhYmxlKHNsZWVwJE1vb24pXG50YWJsZShzbGVlcCRMYXN0U2xlZXApIn0= 8.4.1.2 Snippet 2 Q: Is exercising more good for your sleep? A: So and so. You either have Little sleep or deep sleep, much less likely to have shallow sleep eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG50YXBwbHkoc2xlZXAkRXhlcmNpc2VDYWwsIHNsZWVwJFNsZWVwLCBtZWFuKSJ9 8.4.1.3 Snippet 3 Q: What are the odds of Deep sleep when last day’s sleep was Shallow? A: Posterior Odds= 15.27 (probability = 0.93!) Likelihood Ratio = 5.25 Prior Odds = 2.91 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5QcmlvcjwtbnJvdyhzbGVlcFtzbGVlcCRTbGVlcCA9PSdEZWVwJyxdKS9ucm93KHNsZWVwKVxuUHJpb3JcblByaW9yT2Rkczwtcm91bmQoUHJpb3IvKDEtUHJpb3IpLDIpXG5Qcmlvck9kZHNcblRydWVQb3NpdGl2ZTwtcm91bmQobnJvdyhzbGVlcFtzbGVlcCRTbGVlcD09J0RlZXAnJiBzbGVlcCRMYXN0U2xlZXA9PSdTaGFsbG93JyxdKS9ucm93KHNsZWVwW3NsZWVwJFNsZWVwPT0nRGVlcCcsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyh2b3RlW3NsZWVwJFNsZWVwIT0nRGVlcCcmIHNsZWVwJExhc3RTbGVlcD09J1NoYWxsb3cnLF0pL25yb3coc2xlZXBbc2xlZXAkU2xlZXAhPSdEZWVwJyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5Qb3N0ZXJpb3IifQ== 8.4.1.4 Snippet 4 Q: Verify hypothesis that deep sleepers spend on average more time on the computer than Shallow sleepers? A: Negative. Fail to reject null hypotheses that means are the same. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5tZWFuKHNsZWVwW3NsZWVwJFNsZWVwPT0nRGVlcCcsXSRPbkNvbXB1dGVyKVxubWVhbihzbGVlcFtzbGVlcCRTbGVlcD09J1NoYWxsb3cnLF0kT25Db21wdXRlcilcblxuUGVybXV0YXRpb25UZXN0U2Vjb25kOjpQZXJtdXRhdGlvbihzbGVlcCwgXCJTbGVlcFwiLCBcIk9uQ29tcHV0ZXJcIiwxMDAwMCwgXCJEZWVwXCIsIFwiU2hhbGxvd1wiKSJ9 8.4.1.5 Snippet 5 Q: What is the highest Room temperature experienced by a Deep sleeper? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5tYXgoc2xlZXBbc2xlZXAkU2xlZXA9PSdEZWVwJyxdJFJvb21UZW1wKSJ9 You are now familiar with the Sleep dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 8.4.3. 8.4.2 Sleep Data Quiz Quiz Time 8.4.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5zdW1tYXJ5KHNsZWVwKSJ9 8.5 Let’s go to the movies: Data Puzzle Download: Movies2022F-4.csv Using DataMaker we have started with the imdb data set from Kaggle and embedded some patterns in it. The original data set contains data about 12,800+ movies. We have expanded this data set by DataMaker’s opinions. Yes, only DataMaker can have an opinion on each of 12,800 movies! Can you predict which movies does DataMaker love and which movies bore him so much that she quit? What movies DataMaker passionately hates (hmm is DataMaker even passionate about anything at all?). When does DataMaker agree with the imdb score? Can one predict an imdb score on the basis of a combination of DataMaker opinion (sort of super critic) and other attributes? Table 8.5: Snippet of Movies Dataset country content imdb_score Gross Budget genre 3087 USA R 5.35 Low High Drama 4620 USA PG-13 7.88 Medium Medium Comedy 872 USA R 6.06 Medium High History 11859 UK PG-13 7.94 Low Low Action 4513 USA R 7.31 Medium Medium Comedy 8.5.1 Practice Snippets 8.5.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuY29sbmFtZXMobW92aWVzKVxubnJvdyhtb3ZpZXMpXG5zdW1tYXJ5KG1vdmllcylcbnVuaXF1ZShtb3ZpZXMkY29udGVudClcbnVuaXF1ZShtb3ZpZXMkZ2VucmUpXG51bmlxdWUobW92aWVzJEdyb3NzKVxudW5pcXVlKG1vdmllcyRCdWRnZXQpXG5cblxudGFibGUobW92aWVzJGNvbnRlbnQpXG50YWJsZShtb3ZpZXMkZ2VucmUpXG50YWJsZShtb3ZpZXMkR3Jvc3MpXG50YWJsZShtb3ZpZXMkQnVkZ2V0KVxuXG50YXBwbHkobW92aWVzJGltZGJfc2NvcmUsIG1vdmllcyRjb250ZW50LCBtZWFuKVxudGFwcGx5KG1vdmllcyRpbWRiX3Njb3JlLCBtb3ZpZXMkY291bnRyeSwgbWVhbilcbnRhcHBseShtb3ZpZXMkaW1kYl9zY29yZSwgbW92aWVzJEdyb3NzLCBtZWFuKVxudGFwcGx5KG1vdmllcyRpbWRiX3Njb3JlLCBtb3ZpZXMkQnVkZ2V0LCBtZWFuKVxudGFwcGx5KG1vdmllcyRpbWRiX3Njb3JlLCBtb3ZpZXMkZ2VucmUsIG1lYW4pIn0= 8.5.1.2 Snippet 2 Q: What is the mean imdb of low budget comedies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxubWVhbihtb3ZpZXNbbW92aWVzJEJ1ZGdldD09J0xvdycgJiBtb3ZpZXMkZ2VucmU9PSdDb21lZHknLCBdJGltZGJfc2NvcmUpIn0= 8.5.1.3 Snippet 3 Q: What is the lowest imdb score among high budget movies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxubWluKG1vdmllc1ttb3ZpZXMkQnVkZ2V0PT0nSGlnaCcsXSRpbWRiKSJ9 8.5.1.4 Snippet 4 Q: How many low budget movies generated high gross income? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxubnJvdyhtb3ZpZXNbbW92aWVzJEJ1ZGdldD09J0xvdycgJiBtb3ZpZXMkR3Jvc3MgPT0nSGlnaCcsXSkifQ== 8.5.1.5 Snippet 5 Q: What is the least frequent genre among UK movies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxudGFibGUobW92aWVzW21vdmllcyRjb3VudHJ5PT0nVUsnLF0kZ2VucmUsIG1vdmllc1ttb3ZpZXMkY291bnRyeT09J1VLJyxdJGNvdW50cnkpIn0= 8.5.1.6 Snippet 6 Q: Which content rating has the lowest average imdb score? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxudGFwcGx5KG1vdmllcyRpbWRiLCBtb3ZpZXMkY29udGVudCwgbWVhbikifQ== 8.5.1.7 Snippet 7 Q: Movies from which country have the smallest average imdb score? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuTUE8LWFnZ3JlZ2F0ZShtb3ZpZXMkaW1kYl9zY29yZSwgbGlzdChtb3ZpZXMkY291bnRyeSksIG1lYW4pXG5jb2xuYW1lcyhNQSk8LWMoXCJDb3VudHJ5XCIsIFwiTWltZGJcIilcbk1BPC1NQVtvcmRlcigtTUEkTWltZGIpLCBdXG5NQVsxLF0gIn0= 8.5.1.8 Snippet 8 Q: What are the odds that a High Budget Movie will have High Gross Income? A: Prior Odds = 5.59 Likelihood Ratio = 5.08 Prior Odds = 1.11 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuUHJpb3I8LW5yb3cobWFya2V0W21vdmllcyRHcm9zcyA9PSdIaWdoJyxdKS9ucm93KG1vdmllcylcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3cobW92aWVzW21vdmllcyRHcm9zcz09J0hpZ2gnJiBtb3ZpZXMkQnVkZ2V0PT0nSGlnaCcsXSkvbnJvdyhtb3ZpZXNbbW92aWVzJEdyb3NzPT0nSGlnaCcsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb3ZpZXNbbW92aWVzJEdyb3NzIT0nSGlnaCcmIG1vdmllcyRCdWRnZXQ9PSdIaWdoJyxdKS9ucm93KG1vdmllc1ttb3ZpZXMkR3Jvc3MhPSdIaWdoJyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzIn0= You are now familiar with the Movies dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 8.5.3. 8.5.2 Movies Data Quiz Quiz Time 8.5.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuc3VtbWFyeShtb3ZpZXMpIn0= 8.6 When canvas goes wild data puzzle Download: Canvas1.csv You are all familiar with Canvas, right? This is where you look to see your grades for each assignment and exam. This is where you see the scores. However it seems that Canvas went a bit wild and unfair in this data set.One can still fail the class with the score of 82 (sounds familiar, yes, Professor Moody would do it, but Canvas? How can one get a lower grade with a higher score? Yes, Canvas was instructed by someone and your goal is to discover the grading method. How to get an A, how to pass? We know who that someone is… it is DataMaker of course. Table 8.6: Snippet of Canvas Dataset Homeworks Exams Score Grade 483 7 66 12.9 F 1127 100 79 97.9 A 75 92 46 87.4 A 1154 38 16 35.8 F 936 16 96 24.0 F 8.6.1 Practice Snippets 8.6.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxuY29sbmFtZXMoZ3JhZGVzKVxubnJvdyhncmFkZXMpXG5zdW1tYXJ5KGdyYWRlcylcbnVuaXF1ZShncmFkZXMkR3JhZGUpXG50YWJsZShncmFkZXMkR3JhZGUpXG50YXBwbHkoZ3JhZGVzJEhvbWV3b3JrcywgZ3JhZGVzJEdyYWRlLCBtZWFuKVxudGFwcGx5KGdyYWRlcyRFeGFtcywgZ3JhZGVzJEdyYWRlLCBtZWFuKVxudGFwcGx5KGdyYWRlcyRTY29yZSwgZ3JhZGVzJEdyYWRlLCBtZWFuKSJ9 8.6.1.2 Snippet 2 Q: What is the distribution of possible grades when a student’s total scores is over 80? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxudGFibGUoZ3JhZGVzW2dyYWRlcyRTY29yZSA+IDgwLF0kR3JhZGUpIn0= 8.6.1.3 Snippet 3 Q: Previous snippets showed that you can only get an A or an F with a score over 80. How can you get an F? This snippet helps to answer this question. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxudGFibGUoZ3JhZGVzW2dyYWRlcyRTY29yZSA+IDgwICYgZ3JhZGVzJEV4YW1zID40MCxdJEdyYWRlKSJ9 8.6.1.4 Snippet 4 Q: What is the worst exam score of a student with final grade A? A: 25 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxubWluKGdyYWRlc1tncmFkZXMkR3JhZGUgPT0nQScsXSRFeGFtcykifQ== 8.6.1.5 Snippet 5 Q: What are the odds of getting an A with an Exams score above 60? A: Posterior Odds = 0.45 Likelihood Ratio = 4.75 Prior Odds = 0.17 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxuUHJpb3I8LW5yb3coZ3JhZGVzW2dyYWRlcyRHcmFkZSA9PSdBJyxdKS9ucm93KGdyYWRlcylcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3coZ3JhZGVzW2dyYWRlcyRHcmFkZT09J0EnJiBncmFkZXMkRXhhbXM+NjAsXSkvbnJvdyhncmFkZXNbZ3JhZGVzJEdyYWRlPT0nQScsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3N0aXZlPC1yb3VuZChucm93KGdyYWRlc1tncmFkZXMkR3JhZGUhPSdBJyYgZ3JhZGVzJEV4YW1zPDYwLF0pL25yb3coZ3JhZGVzW2dyYWRlcyRHcmFkZXMhPSdBJyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5Qb3N0ZXJpb3IifQ== 8.6.1.6 Snippet 6 Q: Verify Hypothesis that Mean exam score for B students is higher than mean exam score for C students. What is the p-value? A: Negative. We fail to reject the null hypothesis with p=0.23 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxubWVhbihncmFkZXNbZ3JhZGVzJEdyYWRlPT0nQicsXSRFeGFtcylcbm1lYW4oZ3JhZGVzW2dyYWRlcyRHcmFkZT09J0MnLF0kRXhhbXMpXG5cblBlcm11dGF0aW9uVGVzdFNlY29uZDo6UGVybXV0YXRpb24oZ3JhZGVzLCBcIkdyYWRlXCIsIFwiRXhhbXNcIiwxMDAwMCwgXCJDXCIsIFwiQlwiKSAifQ== 8.6.1.7 Snippet 7 Q: What is the chance of getting an A when you score less than 50 on exams? A: 0.093 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxubnJvdyhncmFkZXNbZ3JhZGVzJEdyYWRlID09J0EnICYgZ3JhZGVzJEV4YW1zIDwgNTAsXSkvbnJvdyhncmFkZXNbZ3JhZGVzJEV4YW1zIDwgNTAsXSkifQ== You are now familiar with the Canvas dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 8.6.3. 8.6.2 Canvas Data Quiz Quiz Time 8.6.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxuc3VtbWFyeShncmFkZXMpIn0= 8.7 Very local minimarket data puzzle Download: HomeworkMarket2022.csv What items sell together? A small local minimarket chain (think Wawa at its early days) has a few locations in New Jersey and it sells beer, snacks, sweets. DataMaker provided the data set of several thousand of transactions in the minimarket storing what items were purchased, when they were purchased (weekday or weekend) at which location. Table 8.7: Snippet of Minimarket Dataset Beer Day Location SoftDrinks Sweets Wine Snacks 149 Ale Weekday Princeton None Twix None None 19405 None Weekend Princeton Cola Milky Way Red None 11414 Lager Weekday Metuchen Sprite Milky Way None None 8026 Ale Weekend New Brunswick Cola Snickers None None 17635 Lager Weekday New Brunswick Sprite Snickers Red None 8.7.1 Practice Snippets 8.7.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG5jb2xuYW1lcyhtYXJrZXQpXG5ucm93KG1hcmtldClcbnN1bW1hcnkobWFya2V0KVxudW5pcXVlKG1hcmtldCREYXkpXG51bmlxdWUobWFya2V0JExvY2F0aW9uKVxudW5pcXVlKG1hcmtldCRCZWVyKVxudW5pcXVlKG1hcmtldCRTb2Z0RHJpbmtzKVxudW5pcXVlKG1hcmtldCRTd2VldHMpXG51bmlxdWUobWFya2V0JFdpbmUpXG51bmlxdWUobWFya2V0JFNuYWNrcylcbnRhYmxlKG1hcmtldCREYXkpXG50YWJsZShtYXJrZXQkTG9jYXRpb24pXG50YWJsZShtYXJrZXQkQmVlcilcbnRhYmxlKG1hcmtldCRTb2Z0RHJpbmtzKVxudGFibGUobWFya2V0JFN3ZWV0cylcbnRhYmxlKG1hcmtldCRXaW5lKVxudGFibGUobWFya2V0JFNuYWNrcykifQ== 8.7.1.2 Snippet 2 Q: What are the odds that a customer in New Brunswick buys Lager on a weekend? A: Posterior Odds = 0.53 Likelihood Ratio = 1.08 Prior odds = 0.49 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG5QcmlvcjwtbnJvdyhtYXJrZXRbbWFya2V0JEJlZXIgPT0nTGFnZXInLF0pL25yb3cobWFya2V0KVxuUHJpb3JcblByaW9yT2Rkczwtcm91bmQoUHJpb3IvKDEtUHJpb3IpLDIpXG5Qcmlvck9kZHNcblRydWVQb3NpdGl2ZTwtcm91bmQobnJvdyhtYXJrZXRbbWFya2V0JEJlZXI9PSdMYWdlcicmIG1hcmtldCRMb2NhdGlvbj09J05ldyBCcnVuc3dpY2snICYgbWFya2V0JERheSA9PSdXZWVrZW5kJyxdKS9ucm93KG1hcmtldFttYXJrZXQkQmVlcj09J0xhZ2VyJyxdKSwyKVxuVHJ1ZVBvc2l0aXZlXG5GYWxzZVBvc2l0aXZlPC1yb3VuZChucm93KG1hcmtldFttYXJrZXQkQmVlciE9J0xhZ2VyJyYgbWFya2V0JExvY2F0aW9uPT0nTmV3IEJydW5zd2ljaycgJiBtYXJrZXQkRGF5ID09J1dlZWtlbmQnLF0pL25yb3cobWFya2V0W21hcmtldCRCZWVyIT0nTGFnZXInLF0pLDIpXG5GYWxzZVBvc2l0aXZlXG5MaWtlbGlob29kUmF0aW88LXJvdW5kKFRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlLDIpXG5MaWtlbGlob29kUmF0aW9cblBvc3Rlcmlvck9kZHMgPC1MaWtlbGlob29kUmF0aW8gKiBQcmlvck9kZHNcblBvc3Rlcmlvck9kZHNcblBvc3RlcmlvciA8LVBvc3Rlcmlvck9kZHMvKDErUG9zdGVyaW9yT2RkcylcblBvc3RlcmlvciJ9 8.7.1.3 Snippet 3 Q: What is the most frequent location of Lager purchases? A: Princeton is the most frequent location where Lager is sold eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG50YWJsZShtYXJrZXRbbWFya2V0JEJlZXIgPT0nTGFnZXInLF0kTG9jYXRpb24pIn0= 8.7.1.4 Snippet 4 Q: Is distribution of purchases of snacks among Weekend buyers of Lager in New Brunswick different from base distribution of snacks? A: yes, very different eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG5tYXJrZXQkSU48LSdPdXRfU2xpY2UnXG5tYXJrZXRbbWFya2V0JEJlZXI9PSdMYWdlcicgJiBtYXJrZXQkRGF5PT0nV2Vla2VuZCcgJiAgbWFya2V0JExvY2F0aW9uID09J05ldyBCcnVuc3dpY2snLCBdJElOPC0nSW5fU2xpY2UnXG5kPC10YWJsZShtYXJrZXQkU25hY2tzLG1hcmtldCRJTilcbmNoaXNxLnRlc3QoZCkifQ== You are now familiar with the MiniMarket dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 8.7.3. 8.7.2 MiniMarket Data Quiz Quiz Time 8.7.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG5zdW1tYXJ5KG1hcmtldCkifQ== Now it’s time for two real data sets - the airbnb data set and titanic sinking data set, both from Kaggle. These data sets have been cleaned before, this is why we do not have to spend our time on data wrangling! 8.8 Airbnb data puzzle Download: airbnb.csv The airbnb data set (Kaggle) stores around 30,000 plus data points about airbnb prices in NYC. We have modified the original set a little bit (we can’t stop!) adding the floor where the department is located to the existing attributes such as Room type, neighbourhood_group (boroughs), specific neighborhood and price. Table 8.8: Snippet of Airbnb Dataset id name host_name neighbourhood_group neighbourhood room_type floor price 4941 12642845 Newly renovated! Spacious Park Slope Apt. Quiet St Jada Brooklyn South Slope Entire home/apt 1 195.95387 10197 18387366 Bright &amp; Airy Private Room near L/JMZ trains Leah Brooklyn Bushwick Private room 1 93.48379 26036 35283648 Luxury 3 bedroom home close to LaGuardia 8 guests Joseph Queens East Elmhurst Entire home/apt 5 163.00802 11157 20553159 VERY SPACIOUS COZY ROOM IN HIP EAST VILLAGE AREA Hanna Manhattan East Village Private room 16 265.35986 4659 13995614 Quiet bedroom in a bright loft, Heart of Manhattan Pl Manhattan Midtown Private room 1 150.19908 8.8.1 Practice Snippets 8.8.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5ucm93KGFpcmJuYilcbnN1bW1hcnkoYWlyYm5iKVxudW5pcXVlKGFpcmJuYiRuZWlnaGJvdXJob29kX2dyb3VwKVxudW5pcXVlKGFpcmJuYiRuZWlnaGJvdXJob29kKVxudW5pcXVlKGFpcmJuYiRyb29tX3R5cGUpXG51bmlxdWUoYWlyYm5iJGZsb29yKVxudGFibGUoYWlyYm5iJG5laWdoYm91cmhvb2RfZ3JvdXApXG50YWJsZShhaXJibmIkbmVpZ2hib3VyaG9vZClcbnRhYmxlKGFpcmJuYiRyb29tX3R5cGUpXG50YWJsZShhaXJibmIkZmxvb3IpXG50YXBwbHkoYWlyYm5iJHByaWNlLCBhaXJibmIkZmxvb3IsIG1lYW4pXG50YXBwbHkoYWlyYm5iJHByaWNlLCBhaXJibmIkbmVpZ2hib3VyaG9vZCwgbWVhbilcbnRhcHBseShhaXJibmIkcHJpY2UsIGFpcmJuYiRyb29tX3R5cGUsIG1lYW4pIn0= 8.8.1.2 Snippet 2 Q: What is the price of the cheapest entire home/apt in Tribeca? A: $284 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5taW4oYWlyYm5iW2FpcmJuYiRyb29tX3R5cGU9PSdFbnRpcmUgaG9tZS9hcHQnICZhaXJibmIkbmVpZ2hib3VyaG9vZD09J1RyaWJlY2EnLF0kcHJpY2UpIn0= 8.8.1.3 Snippet 3 Q: What is the lowest price of accommodation above the 10th floor in Manhattan? A: $184 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5taW4oYWlyYm5iW2FpcmJuYiRmbG9vciA+MTAgJmFpcmJuYiRuZWlnaGJvdXJob29kX2dyb3VwPT0nTWFuaGF0dGFuJyxdJHByaWNlKSJ9 8.8.1.4 Snippet 4 Q: What are the odds of finding a place for less than $200 in Tribeca? A: Posterior Odds = 0.09 Prior Odds = 0.86 Likelihood Ratio = 0.11 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5QcmlvcjwtbnJvdyhhaXJibmJbYWlyYm5iJHByaWNlPDIwMCxdKS9ucm93KGFpcmJuYilcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5ucm93KGFpcmJuYlthaXJibmIkcHJpY2U8MjAwJiBhaXJibmIkbmVpZ2hib3VyaG9vZD09J1RyaWJlY2EnLF0pXG5ucm93KGFpcmJuYlthaXJibmIkbmVpZ2hib3VyaG9vZD09J1RyaWJlY2EnLF0pXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3coYWlyYm5iW2FpcmJuYiRwcmljZTwyMDAmIGFpcmJuYiRuZWlnaGJvdXJob29kPT0nVHJpYmVjYScsXSkvbnJvdyhhaXJibmJbYWlyYm5iJHByaWNlPDIwMCxdKSw1KVxuVHJ1ZVBvc2l0aXZlXG5GYWxzZVBvc2l0aXZlPC1yb3VuZChucm93KGFpcmJuYlthaXJibmIkcHJpY2U+MjAwJiBhaXJibmIkbmVpZ2hib3VyaG9vZD09J1RyaWJlY2EnLF0pL25yb3coYWlyYm5iW2FpcmJuYiRwcmljZT4yMDAsXSksNSlcbkZhbHNlUG9zaXRpdmVcbkxpa2VsaWhvb2RSYXRpbzwtcm91bmQoVHJ1ZVBvc2l0aXZlL0ZhbHNlUG9zaXRpdmUsNClcbkxpa2VsaWhvb2RSYXRpb1xuUG9zdGVyaW9yT2RkcyA8LUxpa2VsaWhvb2RSYXRpbyAqIFByaW9yT2Rkc1xuUG9zdGVyaW9yT2Rkc1xuUG9zdGVyaW9yIDwtUG9zdGVyaW9yT2Rkcy8oMStQb3N0ZXJpb3JPZGRzKVxuUG9zdGVyaW9yIn0= 8.8.1.5 Snippet 5 Q: Verify hypothesis that West Village is more expensive than Upper East Side? A: Positive. Null hypothesis is rejected with the p value p &lt; 0.0001 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5QZXJtdXRhdGlvblRlc3RTZWNvbmQ6OlBlcm11dGF0aW9uKGFpcmJuYiwgXCJuZWlnaGJvdXJob29kXCIsIFwicHJpY2VcIiwxMDAwMCwgXCJXZXN0IFZpbGxhZ2VcIiwgXCJVcHBlciBFYXN0IFNpZGVcIikifQ== You are now familiar with the Airbnb dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 8.8.3. 8.8.2 Airbnb Data Quiz Quiz Time 8.8.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5zdW1tYXJ5KGFpcmJuYikifQ== 8.9 Titanic data puzzle Download: Titanic-train.csv The titanic data set (Kaggle) stores records of passengers of Titanic with attributes such as Survived, SibSp (family size), Fare, PClass (type of a cabin), Age etc. Here is a sample of data Table 8.9: Snippet of Titanic Dataset PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 508 508 1 1 Bradley, Mr. George (“George Arthur Brayton”) male NA 0 0 111427 26.5500 S 664 664 0 3 Coleff, Mr. Peju male 36 0 0 349210 7.4958 S 387 387 0 3 Goodwin, Master. Sidney Leonard male 1 5 2 CA 2144 46.9000 S 792 792 0 2 Gaskell, Mr. Alfred male 16 0 0 239865 26.0000 S 641 641 0 3 Jensen, Mr. Hans Peder male 20 0 0 350050 7.8542 S 8.9.1 Practice Snippets 8.9.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1RpdGFuaWMtdHJhaW4uY3N2XCIpXG5cbmNvbG5hbWVzKHRpdGFuaWMpXG5ucm93KHRpdGFuaWMpXG5zdW1tYXJ5KHRpdGFuaWMpXG51bmlxdWUodGl0YW5pYyRQY2xhc3MpXG51bmlxdWUodGl0YW5pYyRTaWJTcClcbnVuaXF1ZSh0aXRhbmljJFNleClcbnVuaXF1ZSh0aXRhbmljJEVtYmFya2VkKVxudW5pcXVlKHRpdGFuaWMkU3Vydml2ZWQpXG50YWJsZSh0aXRhbmljJFBjbGFzcylcbnRhYmxlKHRpdGFuaWMkU2liU3ApXG50YWJsZSh0aXRhbmljJFNleClcbnRhYmxlKHRpdGFuaWMkRW1iYXJrZWQpXG50YWJsZSh0aXRhbmljJFN1cnZpdmVkKSJ9 8.9.1.2 Snippet 2 Q: What are the odds of survival of single males on Titanic? A: Posterior Odds = 0.196 Prior Odds = 0.62 Likelihood Ratio = 0.31 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvVGl0YW5pYy10cmFpbi5jc3ZcIilcblxuUHJpb3I8LW5yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0xLF0pL25yb3codGl0YW5pYylcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0xJiB0aXRhbmljJFNleD09J21hbGUnICZ0aXRhbmljJFNpYlNwPT0wLF0pL25yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0xLF0pLDIpXG5UcnVlUG9zaXRpdmVcbkZhbHNlUG9zaXRpdmU8LXJvdW5kKG5yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0wJiB0aXRhbmljJFNleD09J21hbGUnJnRpdGFuaWMkU2liU3A9PTAsLF0pL25yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0wLF0pLDIpXG5GYWxzZVBvc2l0aXZlXG5MaWtlbGlob29kUmF0aW88LXJvdW5kKFRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlLDQpXG5MaWtlbGlob29kUmF0aW9cblBvc3Rlcmlvck9kZHMgPC1MaWtlbGlob29kUmF0aW8gKiBQcmlvck9kZHNcblBvc3Rlcmlvck9kZHNcblBvc3RlcmlvciA8LVBvc3Rlcmlvck9kZHMvKDErUG9zdGVyaW9yT2RkcylcblBvc3RlcmlvciJ9 8.9.1.3 Snippet 3 Q: Verify hypothesis that survivors paid on average more for the ticker than those who did not survive? A: Positive. Null hypothesis rejected with p &lt; 0.0001 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1RpdGFuaWMtdHJhaW4uY3N2XCIpXG5cblBlcm11dGF0aW9uVGVzdFNlY29uZDo6UGVybXV0YXRpb24odGl0YW5pYywgXCJTdXJ2aXZlZFwiLCBcIkZhcmVcIiwxMDAwMCwgXCIxXCIsIFwiMFwiKSJ9 8.9.1.4 Snippet 4 Q: What is the probability of survival for passengers who paid more than 100 pounds for a ticket? How about those who paid less than 10 pounds? A: 0.73 for passengers who paid more than 100 pounds 0.20 for passengers who paid less than 10 pounds 0.38 for all passengers eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1RpdGFuaWMtdHJhaW4uY3N2XCIpXG5cbm5yb3codGl0YW5pY1t0aXRhbmljJEZhcmUgPjEwMCAmIHRpdGFuaWMkU3Vydml2ZWQgPT0xLF0pL25yb3codGl0YW5pY1t0aXRhbmljJEZhcmUgPjEwMCxdKVxubnJvdyh0aXRhbmljW3RpdGFuaWMkRmFyZSA8MTAgJiB0aXRhbmljJFN1cnZpdmVkID09MSxdKS9ucm93KHRpdGFuaWNbdGl0YW5pYyRGYXJlIDwxMCxdKVxubnJvdyh0aXRhbmljW3RpdGFuaWMkU3Vydml2ZWQgPT0xLF0pL25yb3codGl0YW5pYykifQ== 8.9.1.5 Snippet 5 Q: What was the chance of survival for passengers who traveled at least in a group of 3? A: Just 10%! eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1RpdGFuaWMtdHJhaW4uY3N2XCIpXG5cbnRhYmxlKHRpdGFuaWNbdGl0YW5pYyRTaWJTcD4zLF0kU3Vydml2ZWQpIn0= 8.9.1.6 Snippet 6 Q: Did survival depend on the class of the cabin? A: Positive. Null hypothesis of independence rejected with p-value less than \\(e^{-16}\\) eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvVGl0YW5pYy10cmFpbi5jc3ZcIilcblxuY2hpc3EudGVzdCh0aXRhbmljJFN1cnZpdmVkLCB0aXRhbmljJFBjbGFzcykifQ== You are now familiar with the Titanic dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 8.9.3. 8.9.2 Titanic Data Quiz Quiz Time 8.9.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L2FpcmJuYi5jc3ZcIilcblxuc3VtbWFyeSh0aXRhbmljKSJ9 8.10 Addiotional Reference Prediction - Free Style "],["common_sense.html", "Section: 9 🔖 Common Sense Judgement and Probability 9.1 Introduction 9.2 Additional References", " Section: 9 🔖 Common Sense Judgement and Probability 9.1 Introduction We will step away from coding for a moment. In the class description we promise to address the million dollar question “How not to be fooled by data?”. Let’s dive into this important issue. We have already discussed powerful tools such as hypothesis testing, p-values and Bonferroni correction for multiple hypothesis traps. But even if you never want to write a line of code, you need to know about common traps which you may be fooled by in whatever you do. We need to be informed and educated citizens who can catch the fake inferences and fake discoveries whether we read them in the news or hear politicians falling for the traps. Daniel Kahnemann, the Nobel Prize winner in Economics is the author of the fascinating book “Think fast, think slow” and he identifies pitfalls of human relationships with numbers, frequencies. We discuss Availability, Anchoring, Conjunctive fallacy, Narrative fallacy, Law of small numbers, Reverse to the mean and many other concepts in the attached power points. In the next section we will also discuss Bayesian theorem and Bayesian reasoning (with some coding handy) to finally come back to the paradoxes such as prosecutorial paradox, Simpson paradox and ecological fallacy in section 21. 9.2 Additional References Common Sense Judgement and Probability "],["bayesian_reasoning.html", "Section: 10 🔖 Bayesian Reasoning 10.1 Introduction 10.2 Snippet 1: Covid Odds after positive Home Test. 10.3 Snippet 2: What are the odds that an ‘F’ student is a freshman? 10.4 Snippet 3: What are the odds that a ‘A’ student with the score less than 80 is a psychology major? 10.5 Additinal Reference", " Section: 10 🔖 Bayesian Reasoning 10.1 Introduction Bayesian Reasoning and Bayesian theorem are fundamental instruments to use both in data science as well as in real, everyday life. They are definitely part of data literacy and should be widely taught, especially among future doctors, lawyers and politicians. In this section we will explain why Bayesian reasoning is so important and also teach the most simple and intuitive formulation of Bayesian theorem - the Odds formulation. Bayesian theorem is a calming tool - the chances of bad things happening are lower than expected! This is why Bayes helps pessimists. Take a situation at a doctor’s office when a patient learns that a medical test for some potentially serious condition came positive. The doctor believes the test and the test is almost 100% accurate. Should the patient despair? Not so fast. Bayes theorem allows the patient to ask the doctor some important questions. In bayesian reasoning we distinguish between two concepts = observation and belief. Belief is something unknown, Observation is known. We use observation to modify the odds (probability) of the belief from prior odds (before we learned about observation) and the posterior odds (after we learned about observation). For example a patient taking a covid test is concerned about having covid. But s/he does not know whether they have covid. Thus “having covid” is a belief. Test result is an observation (positive or negative). Given the prior odds of covid (say 1:100), and positive covid test what are the posterior odds of covid? Bayesian theorem tells us how to compute posterior odds from prior odds, given the observation. w Odds formulation of Bayesian theorem states; \\[\\begin{equation} \\text{POSTERIOR ODDS = LIKELIHOOD RATIO * PRIOR ODDS} \\\\ \\textbf{Prior odds} \\text{- odds for the belief before observation (evidence)}\\\\ \\textbf{Likelihood ratio} \\text{- effect of observation, evidence. Can be larger or smaller than 1!!}\\\\ \\textbf{Posterior odds} \\text{- New odds with observation(evidence) taken under consideration.}\\\\ \\end{equation}\\] Let B a belief and O be an observation, then \\[\\begin{equation} \\textbf{Prior odds} - \\frac{P(B)}{P(\\sim B)}\\\\ \\textbf{Likelihood ratio} - \\frac{P(O|B)}{P(O|\\sim B)}\\\\ \\textbf{Posterior odds} - \\frac{P(B|O)}{P(\\sim B|O)} \\end{equation}\\] Let’s discuss the multiplier – likelihood ratio in more detail. It is the red colored part of the bayes Theorem: \\[\\begin{equation} \\frac{P(B|O)}{P(\\sim B|O)} = \\frac{P(O|B)}{P(O|\\sim B)} * \\frac{P(B)}{P(\\sim B)} \\\\ \\text{The red colored ratio is the ratio of true positive and false positive,}\\\\ \\text{P(O|B) – True positive} \\\\ \\text{P(O|$\\sim$ B) – False positive} \\end{equation}\\] True positive is the conditional probability of seeing the observation given that our belief is true. In our medical example it is the probability of testing positive for covid, given that in fact we have covid. False positive, on the other hand, is the probability of observation under condition that the belief is not true. For example in our case that covid test comes positive even when we do not have covid. In real life False positives are often overlooked. And this is the critical question we should ask the doctor or health professional who administers any test. What is the false positive of this test? Since this is what we divide the true positive by. Even if the true positive is 99.9% (almost sure), if the false positive is, say 20% - the likelihood ratio is around 5. In such a situation, a positive test increases the odds of having covid just 5 times. If prior odds of covid are 1:100, the posterior odds of covide after such a positive test are just 5:100, still minimal!. Even if a false positive was 10%, the likelihood ratio of 10, would increase odds of covid 10 fold, to just 1:10 and false positive of 5%, would result in a likelihood ratio of 20 - still leading to higher odds of NOT having covid than having it! This is why false positives are so critical. IBut the main question that Bayes teaches us to ask is what are the prior odds. Since if prior odds are very small (we are testing a really rare condition) then the likelihood ratio would have to be really large to make posterior odds significant. For example if prior odds are one in a million, we need a likelihood ratio of more than half a million to actually make posterior odds better than proverbial fifty - fifty. Hence to main questions we should ask our doctor upon hearing that the test results are positive are: What are the prior odds of the disease? What is the false positive of the test (since we assume that the true positive of the test would usually be close to 100%)? In the following snippets we show how to calculate the posterior odds, while being tested for a disease and then closer to our data puzzles, how to calculate the posterior odds of getting an A in class, when scoring more than 85%.In all these situations we begin with identifying what is belief (the unknown), what is the observation (the known) and we use the snippets by plugging in some assumed values of prior odds, as well as true positives and false positives. 10.2 Snippet 1: Covid Odds after positive Home Test. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjQmVsaWVmID0gXCJIYXZlIENvdmlkXCJcbiNPYnNlcnZhdGlvbiA9IENvdmlkIFRlc3RcbiNIb3cgbXVjaCB0aGUgcHJvYmFiaWxpdHkgb2YgaGF2aW5nIGNvdmlkIGluY3JlYXNlcyB1cG9uIHBvc2l0aXZlIENPVklELXRlc3Q/XG4jV2UgdXNlIHRoZSBvZGRzIGZvcm11bGF0aW9uIG9mIEJheWVzaWFuIFRoZW9yZW1cbiMgd2UgYmVnaW4gd2l0aCBwcmlvciBvZGRzIG9mIGhhdmluZyBDb3ZpZDogIFAoQ292aWQpLygxLVAoQ292aWQpXG5QcmlvckhhdmVDb3ZpZDwtMC4wMVxuUHJpb3JDb3ZpZE9kZHM8LVByaW9ySGF2ZUNvdmlkLygxLVByaW9ySGF2ZUNvdmlkKVxuUHJpb3JDb3ZpZE9kZHNcbiNUcnVlIHBvc2l0aXZlOiAgUHJvYmFiaWxpdHkgb2YgaGF2aW5nIHBvc2l0aXZlIENvdmlkIHRlc3Qgd2hlbiBoYXZpbmcgY292aWQgID0gUChQb3NpdGl2ZUNvdmlkVGVzdHxIYXZlQ292aWQpXG5UcnVlUG9zaXRpdmU8LTAuOTlcbiNGYWxzZSBwb3NpdGl2ZSA9IFByb2JhYmlsaXR5IG9mIGhhdmluZyBwb3NpdGl2ZSBDb3ZpZCB0ZXN0IHdoZW4gbm90IGhhdmluZyBjb3ZpZCA9IFAoUG9zdGl2ZUNvdmlkVGVzdC9Eb05vdEhhdmVDb3ZpZClcbkZhbHNlUG9zaXRpdmU8LTAuMDAxXG5MaWtlbGlob29kUmF0aW88LVRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlXG5Qb3N0ZXJpb3JDb3ZpZE9kZHM8LUxpa2VsaWhvb2RSYXRpbypQcmlvckNvdmlkT2Rkc1xuUG9zdGVyaW9ySGF2ZUNvdmlkPC0gUG9zdGVyaW9yQ292aWRPZGRzLygxK1Bvc3RlcmlvckNvdmlkT2RkcylcblBvc3RlcmlvckhhdmVDb3ZpZCJ9 10.3 Snippet 2: What are the odds that an ‘F’ student is a freshman? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3YnKVxuI0JlbGllZiAtIFN0dWRlbnQgaXMgYSBmcmVzaG1hblxuI09ic2VydmF0aW9uIC0gRmFpbGVkIHRoZSBjbGFzc1xuUHJpb3I8LW5yb3cobW9vZHlbbW9vZHkkU2VuaW9yaXR5ID09J0ZyZXNobWFuJyxdKS9ucm93KG1vb2R5KVxuUHJpb3JcblByaW9yT2Rkczwtcm91bmQoUHJpb3IvKDEtUHJpb3IpLDIpXG5Qcmlvck9kZHNcblRydWVQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRHcmFkZT09J0YnICYgbW9vZHkkU2VuaW9yaXR5PT0nRnJlc2htYW4nLF0pL25yb3coXG4gIG1vb2R5W21vb2R5JFNlbmlvcml0eSA9PSdGcmVzaG1hbicsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRHcmFkZT09J0YnJiBtb29keSRTZW5pb3JpdHkgIT0nRnJlc2htYW4nLF0pL25yb3cobW9vZHlbbW9vZHkkU2VuaW9yaXR5ICE9J0ZyZXNobWFuJyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5yb3VuZChQb3N0ZXJpb3IsMikifQ== 10.4 Snippet 3: What are the odds that a ‘A’ student with the score less than 80 is a psychology major? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjQmVsaWVmIC0gd2hhdCB3ZSBkbyBub3Qga25vdy4gI0lzIGEgc3R1ZGVudCBhIHBzeWNob2xvZ3kgI21ham9yP1xuI09ic2VydmF0aW9uID0gd2hhdCB3ZSBkbyAja25vdy4gVGhleSBnb3QgYW4gQSBhbmQgbGVzcyAjdGhhbiA4MCBpbiBzY29yZVxuXG5tb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3YnKVxuUHJpb3I8LW5yb3cobW9vZHlbbW9vZHkkTWFqb3IgPT0nUHN5Y2hvbG9neScsXSkvbnJvdyhtb29keSlcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3cobW9vZHlbbW9vZHkkU2NvcmUgPDgwICYgbW9vZHkkR3JhZGU9PSdBJyYgbW9vZHkkTWFqb3I9PSdQc3ljaG9sb2d5JyxdKS9ucm93KG1vb2R5W21vb2R5JE1ham9yPT0nUHN5Y2hvbG9neScsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRTY29yZSA8ODAgJiBtb29keSRHcmFkZT09J0EnJiBtb29keSRNYWpvciE9J1BzeWNob2xvZ3knLF0pL25yb3cobW9vZHlbbW9vZHkkTWFqb3IhPSdQc3ljaG9sb2d5JyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5Qb3N0ZXJpb3IifQ== 10.5 Additinal Reference Bayesian Reasoning "],["Free_style.html", "Section: 11 🔖 Free Style: Prediction 11.1 Snippet 1: Example of a simple freestyle prediction model 11.2 Snippet 2: How to build a freestyle (your own code) prediction model? 11.3 Snippet 3: One-step crossvalidation 11.4 General Structure of the Prediction Challenges 11.5 Additional Reference", " Section: 11 🔖 Free Style: Prediction What is a prediction model? Prediction model is a set of rules which, given the values of independent variables (predictors) determine the value of predicted (dependent variable). Here are example of such rules If score &gt; 80 and participation &gt;0.6 then grade =’A’ If score &gt;60 and score &lt;70 and major=’Psychology’ and Ask_questions =’always’ then grade =’B’ If score &lt;50 and score &gt;40 and Doze_off =’always’ then grade = ‘F’ By freestyle prediction we mean building a prediction model without the R library functions such as rpart and other machine learning packages. In freestyle prediction one develops models from scratch, on the basis of plots as well as exploratory queries. Freestyle prediction is important for two reasons: First, building prediction models from scratch allows an aspiring data scientist to “feel the data” - as opposed to often blind direct applications of these library functions, Second, even when one uses the prediction models based on library functions, the best models are often created by combining of several such models. These combinations often arise from skillful subsetting of datasets and applying different models to different subsets. As our prediction challenge competitions indicate, the winning prediction models (the ones with the least error) are predominantly combinations of different models applied to different subsets of the data. Thus, freestyle prediction is almost always a part of the prediction model building. We start with showing an example of a simple freestyle prediction model. 11.1 Snippet 1: Example of a simple freestyle prediction model eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0ZXN0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3ZcIilcblxuc3VtbWFyeSh0ZXN0KVxuXG5teXByZWRpY3Rpb248LXRlc3RcbmRlY2lzaW9uIDwtIHJlcCgnRicsbnJvdyhteXByZWRpY3Rpb24pKVxuZGVjaXNpb25bbXlwcmVkaWN0aW9uJFNjb3JlPjQwXSA8LSAnRCdcbmRlY2lzaW9uW215cHJlZGljdGlvbiRTY29yZT42MF0gPC0gJ0MnXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU2NvcmU+NzBdIDwtICdCJ1xuZGVjaXNpb25bbXlwcmVkaWN0aW9uJFNjb3JlPjgwXSA8LSAnQSdcbm15cHJlZGljdGlvbiRHcmFkZSA8LWRlY2lzaW9uXG5lcnJvciA8LSBtZWFuKHRlc3QkR3JhZGUhPSBteXByZWRpY3Rpb24kR3JhZGUpXG5lcnJvciJ9 11.2 Snippet 2: How to build a freestyle (your own code) prediction model? The key idea behind building freestyle prediction models is to subset data and select the most frequent value of the predicted variable as prediction. Of course we are interested in finding highly discriminative subsets of data with one highly dominant (most frequent value), since such a very frequent value as prediction choice will lead to a small error. But how to find data subsets with such dominant most frequent values? It is a bit of a trial and error process. As we show below in the snippet 2, it is a sequence of one line exploratory queries, which the programmer can rely on. Later, in the next section we show how the rpart() package generates such discriminative subsets of data automatically, though recursive partitioning. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9Nb29keU1hcmNoMjAyMmIuY3N2XCIpXG5cbiMgSG93IGRvIHdlIGJ1aWxkIGEgZnJlZXN0eWxlIHByZWRpY3Rpb24gbW9kZWw/ICBEZWZpbml0ZWx5IHN0YXJ0IHdpdGggcGxvdHMgbGlrZSB0aGUgYm94cGxvdCBmcm9tIHRoZSBzZWN0aW9uIDUgKGRhdGEgZXhwbG9yYXRpb24pLiAgQnV0IHRoZW4gZm9sbG93IHVwIHdpdGggZXhwbG9yYXRvcnkgcXVlcmllcyBhcyBpbiB0aGUgcmVjZW50IHF1aXp6ZXMuIEV4YW1wbGVzIGhlcmUgdXNlIHRhYmxlKCkgIGZ1bmN0b24gYW5kIGxvb2sgZm9yIHNpdHVhdGlvbnMgd2hlbiBvbmUgZ3JhZGUgaXMgYWJzb3V0ZWx5IGRvbWluYW50LiBUaGlzIHdvdWxkIGJlIHlvdXIgcHJlZGljdGlvbi4gVGh1cywgdGhlIGdvYWwgaXMgdG8gc2xpY2UgdGhlIGRhdGEgdXNpbmcgc3Vic2V0dGluZyBpbiBzdWNoIGEgd2F5IHRoYXQgZm9yIGVhY2ggc2xpY2UgeW91IGdldCBhIGNsZWFyIFwid2lubmVyIGdyYWRlXCIuIFRoZW4gY29tYmluZSB0aGVzZSBzdWJzZXQgcnVsZXMgaW50byBkZWNpc2lvbiB2ZWN0b3IgLSBqdXN0IGFzIHdlIGRpZCBpbiBzbmlwcGV0IDE0LjEuXG5cbiMgQmVsb3cgc29tZSBleGFtcGxlcyBvZiBzdWNoIGV4cGxvcmF0b3J5IHF1ZXJpZXMgd2l0aCBjbGVhciBncmFkZSB3aW5uZXJzLlxuXG5zdW1tYXJ5KG1vb2R5KVxudGFibGUobW9vZHkkR3JhZGUpXG50YWJsZShtb29keVttb29keSRTY29yZT44MCxdJEdyYWRlKVxudGFibGUobW9vZHlbbW9vZHkkU2NvcmU+ODAgJiBtb29keSRNYWpvcj09J1BzeWNob2xvZ3knLF0kR3JhZGUpXG50YWJsZShtb29keVttb29keSRTY29yZTw0MCAmIG1vb2R5JE1ham9yPT0nRWNvbm9taWNzJyxdJEdyYWRlKVxudGFibGUobW9vZHlbbW9vZHkkU2NvcmU8NDAgJiBtb29keSRTZW5pb3JpdHk9PSdGcmVzaG1hbicsXSRHcmFkZSkifQ== 11.3 Snippet 3: One-step crossvalidation How do we know if our prediction model is any good? After all, we may easily build a model which is close to perfect on the training data set but performs miserably on the new, testing data. This is a nightmare for every prediction model builder and it is called a Kaggle surprise. Kaggle surprise happens quite often during our prediction competitions when students build models which are overfitting the data and which give them a false feeling of great, low error just to do the opposite on the testing data and yield a miserably high error. To avoid this or at least to protect one against it, cross validation is needed. We illustrate cross-validation in the next snippet. We split training data into the real training data and the testing data, which is the remaining part of our training data set. Thus we use part of the training data as testing data. We do it by randomly splitting our data set. Although we show here just one step of cross-validation, we should do it multiple times. This helps us to observe how our model behaves for different random subsets of training data and helps us to observe inconsistent results (high variance of error) - which is a warning sign of future kaggle surprise. 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 We use selected data puzzles from section 4 in prediction challenges. Given a data puzzle (such as 4.1), we separate it into training data subset and testing data subset. The training data is given to students to build and cross-validate their prediction models. Then we use Kaggle to evaluate their models on the testing subset of the data puzzle. Each prediction challenge is structured as competition and Kaggle ranks students’ models by prediction accuracy. For categorical variables it is the fraction of values which are predicted correctly, for numerical variables it is MSE (mean square error). 11.4 General Structure of the Prediction Challenges The submission will take place on Kaggle which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submissions. The datasets provided for each prediction challenge is as follows: Training Dataset It is used for training and cross-validation purposes in the prediction challenge. This data has all the training attributes along and the values of the attribute wich is predicted (so called, Target attribute). Models for prediction are to be trained using this dataset only. Training data set is the set which is used when you build your prediction model - since this is the only data set which has all values of target attribute. Testing Dataset It is used for applying your prediction model to new data. You do it only when you are finished with building your prediction model. Testing data set consists of all the attributes that were used for training, but it does not contain any values of the target attribute. It is disjoint with the training data set - it contains new data and it is missing the target variable. Submission Dataset After prediction using the “testing” dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g. ID or name,etc) and second the predicted attribute column. Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle. To read the datasets use the read.csv() function and for writing the dataset to the file, use the write.csv() function. Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle. To avoid this, you can add the parameter, row.names = F in the write.csv() function. e.g. write.csv(*dataframe*,*fileaddress*,row.names = F). 11.4.1 Snippet 4: Preparing submission.csv for Kaggle eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEhlcmUgeW91IGp1c3QgbmVlZCB0aGUgdGVzdCB0YWJsZSAod2l0aG91dCBncmFkZXMpIHRvIGFwcGx5IHlvdXIgcHJlZGljdGlvbiBtb2RlbCBhbmQgY2FsY3VsYXRlIHByZWRpY3RlZCBncmFkZXMuIEFuZCBzdWJtaXNzaW9uIGRhdGEgZnJhbWUgdG8gZmlsbCBpdCBpbiB3aXRoIHRoZSBwcmVkaWN0ZWQgI2dyYWRlc1xuXG50ZXN0PC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTTIwMjJ0ZXN0U05vR3JhZGUuY3N2JylcbnN1Ym1pc3Npb248LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMnN1Ym1pc3Npb24uY3N2JylcblxubXlwcmVkaWN0aW9uPC10ZXN0XG4jSGVyZSBpcyB5b3VyIG1vZGVsLiBJIGp1c3Qgc2hvdyBleGFtcGxlIG9mIHRyaXZpYWwgcHJlZGljdGlvbiBtb2RlbFxuZGVjaXNpb24gPC0gcmVwKCdGJyxucm93KG15cHJlZGljdGlvbikpXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU2NvcmU+NDBdIDwtICdEJ1xuZGVjaXNpb25bbXlwcmVkaWN0aW9uJFNjb3JlPjYwXSA8LSAnQydcbmRlY2lzaW9uW215cHJlZGljdGlvbiRTY29yZT43MF0gPC0gJ0InXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU2NvcmU+ODBdIDwtICdBJ1xuI05vdyBtYWtlIHlvdXIgc3VibWlzc2lvbiBmaWxlIC0gaXQgd2lsbCBoYXZlIHRoZSBJRHMgYW5kIG5vdyB0aGUgcHJlZGljdGVkIGdyYWRlc1xuc3VibWlzc2lvbiRHcmFkZTwtZGVjaXNpb25cbnN1Ym1pc3Npb25cbiMgdXNlIHdyaXRlLmNzdihzdWJtaXNzaW9uLCAnc3VibWlzc2lvbi5jc3YnLCByb3cubmFtZXM9RkFMU0UpIHRvIHN0b3JlIHN1Ym1pc3Npb24gYXMgY3N2IGZpbGUgb24geW91ciBtYWNoaW5lIGFuZCBzdWJzZXF1ZW50bHkgc3VibWl0IGl0IG9uIEthZ2dsZSJ9 Data League: https://data101.cs.rutgers.edu/?q=node/155 Kaggle competition: https://www.kaggle.com/competitions/predictive-challenge-2-2022/overview Kaggle submission instructions: https://data101.cs.rutgers.edu/?q=node/150 11.5 Additional Reference Prediction - Free Style "],["Decision_trees.html", "Section: 12 🔖 Predictions with rpart 12.1 Introduction 12.2 Use of Rpart 12.3 Visualize the Decision tree 12.4 Rpart Control 12.5 Cross Validation 12.6 Prediction using rpart. 12.7 Snippet 11: Your Model with rpart 12.8 Snippet 12: Freestyle + rpart: Combining rpart prediction models 12.9 Snippet 13: Submission with rpart 12.10 Additional Reference", " Section: 12 🔖 Predictions with rpart 12.1 Introduction Decision trees are one of the most powerful and popular tools for classification and prediction. The reason decision trees are very popular is that they can generate rules which are easier to understand as compared to other models. They require much less computations for performing modeling and prediction. Both continuous/numerical and categorical variables are handled easily while creating the decision trees. 12.2 Use of Rpart Recursive Partitioning and Regression Tree RPART library is a collection of routines which implements a Decision Tree.The resulting model can be represented as a binary tree. For the purpose of illustration of rpart we will continue to use data puzzle 3.1 set - the Professor Moody data set. The library associated with this RPART is called rpart. Install this library using install.packages(\"rpart\"). Syntax for building the decision tree using rpart(): rpart( formula , method, data, control,...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... method: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use “anova” for regression, “class” for classification, etc. data: here we provide the dataset on which we want to fit the decision tree on. control: here we provide the control parameters for the decision tree. Explained more in detail in the section further in this chapter. For more info on the rpart function visit rpart documentation Lets look at an example on the Moody 2022 dataset. We will use the rpart() function with the following inputs: prediction -&gt; GRADE predictors -&gt; SCORE, DOZES_OFF, TEXTING_IN_CLASS, PARTICIPATION data -&gt; moody dataset method -&gt; “class” for classification. 12.2.1 Snippet 1 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uLlxucnBhcnQoR1JBREUgfiBTQ09SRStET1pFU19PRkYrVEVYVElOR19JTl9DTEFTUytQQVJUSUNJUEFUSU9OLCBkYXRhID0gbW9vZHksbWV0aG9kID0gXCJjbGFzc1wiKSJ9 We can see that the output of the rpart() function is the decision tree with details of, node -&gt; node number split -&gt; split conditions/tests n -&gt; number of records in either branch i.e. subset yval -&gt; output value i.e. the target predicted value. yprob -&gt; probability of obtaining a particular category as the predicted output. Using the output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section 12.6 But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section. 12.3 Visualize the Decision tree To visualize and understand the rpart() tree output in the easiest way possible, we use a library called rpart.plot. The function rpart.plot() of the rpart.plot library is the function used to visualize decision trees. NOTE: The online runnable code block does not support rpart.plot library and functions, thus the output of the following code examples are provided directly. 12.3.1 Snippet 2 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMl9uZXcuY3N2JylcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG5ycGFydChHUkFERSB+IFNDT1JFK0RPWkVTX09GRitURVhUSU5HX0lOX0NMQVNTK1BBUlRJQ0lQQVRJT04sIGRhdGEgPSBtb29keSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgaW1wb3J0IHRoZSBycGFydC5wbG90IGxpYnJhcnkgdG8gdXNlIHRoZSBycGFydC5wbG90KCkgZnVuY3Rpb24uXG4jbGlicmFyeShycGFydC5wbG90KVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQucGxvdCgpIGZ1bmN0aW9uICB0byB2aXN1YWxpemUgdGhlIGRlY2lzaW9uIHRyZWUuXG4jcnBhcnQucGxvdCh0cmVlKSJ9 Output Plot of rpart.plot() function We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about the grade category. the outcome probability of each grade category. the records percentage out of total records. To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides rpart.plot and Plotting with rpart.plot (PDF) NOTE: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment. 12.4 Rpart Control Now let’s look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function. rpart.control( *minsplit*, *minbucket*, *cp*,...) minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -&gt; the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition. minbucket: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -&gt; the minimum number of observation in the terminal/leaf node of the trees must be 500 or above. cp: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by cp, will not be made in the tree. For more information of the other arguments of the rpart.control() function visit rpart.control Let look at few examples. Suppose you want to set the control parameter minsplit=200. 12.4.1 Snippet 3: Minsplit = 200 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIG1pbnNwbGl0PTIwMFxudHJlZSA8LSBycGFydChHUkFERSB+IFNDT1JFK0RPWkVTX09GRitURVhUSU5HX0lOX0NMQVNTK1BBUlRJQ0lQQVRJT04sIGRhdGEgPSBtb29keSwgbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5zcGxpdCA9IDIwMCkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=200 in rpart.control() function 12.4.2 Snippet 4: Minsplit = 100 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIG1pbnNwbGl0PTEwMFxudHJlZSA8LSBycGFydChHUkFERSB+IFNDT1JFK0RPWkVTX09GRitURVhUSU5HX0lOX0NMQVNTK1BBUlRJQ0lQQVRJT04sIGRhdGEgPSBtb29keSwgbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5zcGxpdCA9IDEwMCkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=100 in rpart.control() function We can see from the output of tree$splits and the tree plot, that at each split the total amount of observations are above 200 and 100. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits. Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters. 12.4.3 Snippet 5: Minbucket = 100 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIE1pbmJ1Y2tldD0xMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStET1pFU19PRkYrVEVYVElOR19JTl9DTEFTUytQQVJUSUNJUEFUSU9OLCBkYXRhID0gbW9vZHksIG1ldGhvZCA9IFwiY2xhc3NcIixjb250cm9sPXJwYXJ0LmNvbnRyb2wobWluYnVja2V0ID0gMTAwKSlcblxudHJlZVxuXG4jbGlicmFyeShycGFydC5wbG90KVxuI3JwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=100 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. 12.4.4 Snippet 6: Minbucket = 200 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIE1pbmJ1Y2tldD0yMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStET1pFU19PRkYrVEVYVElOR19JTl9DTEFTUytQQVJUSUNJUEFUSU9OLCBkYXRhID0gbW9vZHksIG1ldGhvZCA9IFwiY2xhc3NcIixjb250cm9sPXJwYXJ0LmNvbnRyb2wobWluYnVja2V0ID0gMjAwKSlcblxudHJlZVxuXG4jbGlicmFyeShycGFydC5wbG90KVxuI3JwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=200 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 200. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. Lets now use the cp parameter and see its effect on the tree. 12.4.5 Snippet 7: cp = 0.05 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIGNwPTAuMlxudHJlZSA8LSBycGFydChHUkFERSB+IC4sIGRhdGEgPSBtb29keSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKGNwID0gMC4wNSkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUpIn0= Output tree plot of after setting cp=0.05 in rpart.control() function 12.4.6 Snippet 8: cp = 0.005 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIGNwPTAuMDA1XG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gLiwgZGF0YSA9IG1vb2R5LG1ldGhvZCA9IFwiY2xhc3NcIixjb250cm9sPXJwYXJ0LmNvbnRyb2woY3AgPSAwLjAwNSkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUpIn0= We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005. 12.5 Cross Validation Overfitting takes place when you have a high accuracy on training dataset, but a low accuracy on the test dataset. But how do you know whether you are overfitting or not? Especially since you cannot determine accuracy on the test dataset? That is where cross-validation comes into play. Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. The partition is defined by split ratio. If split ratio =0.7, 70% of the training dataset will be used for the actual training of your model (rpart or lm), and 30 % will be used for validation (or testing). The accuracy of this validation data is called cross-validation accuracy. To know if you are overfitting or not, compare the training accuracy with the cross-validation accuracy. If your training accuracy is high, and cross-validation accuracy is low, that means you are overfitting. cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*) data: The dataset on which cross validation is to be performed. tree: The decision tree generated using rpart. n_iter: Number of iterations. split_ratio: The splitting ratio of the data into train data and validation data. method: Method of the prediction. “class” for classification. The way the function works is as follows: It randomly partitions your data into training and validation. It then constructs the following two decision trees on training partition: The tree that you pass to the function. The tree is constructed on all attributes as predictors and with no control parameters. -It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees. The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting. We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired. The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant. Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration. Let’s look at the cross_validate() function in action in the example below. We will pass the tree with formula as GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, and control parameter, with minsplit=100. And for cross_validate() function, we will usen_iter=5, and split_raitio=0.7 NOTE: Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use install.packages(&quot;devtools&quot;) devtools::install_github(&quot;devanshagr/CrossValidation&quot;) CrossValidation::cross_validate() 12.5.1 Snippet 9 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 You can see that the cross-validation accuracies for the tree that was passed (accuracy_subset) are fairly high and close to our training accuracy of 84%. This means we are not overfitting. Also observe that accuracy_subset and accuracy_all have the same values, which means that the only relevant attributes are score and participation, and adding more attributes doesn’t make any difference to the tree. Finally, the values in accuracy_subset are reasonably close to each other, which mean low variance. 12.6 Prediction using rpart. Now that we have seen the process to create a decision tree and also plot it, we will like to use the output tree to predict the required attribute. From the moody example, we are trying to predict the grade of students. Lets look at the predict() function to predict the outcomes. predict(*object*,*data*,*type*,...) object: the generated tree from the rpart function. data: the data on which the prediction is to be performed. type: the type of prediction required. One of “vector”, “prob”, “class” or “matrix”. Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset. 12.6.1 Snippet 10 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMl9uZXcuY3N2JylcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gU0NPUkUrRE9aRVNfT0ZGK1RFWFRJTkdfSU5fQ0xBU1MrUEFSVElDSVBBVElPTiwgZGF0YSA9IG1vb2R5ICxtZXRob2QgPSBcImNsYXNzXCIpXG50cmVlXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZCkifQ== 12.7 Snippet 11: Your Model with rpart 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 12.8 Snippet 12: Freestyle + rpart: Combining rpart prediction models eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxuIyBJbXBvcnQgZGF0YXNldFxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5tb2RlbDE8LXJwYXJ0KEdSQURFfi4sIGRhdGE9bW9vZHlbbW9vZHkkU0NPUkU+NTAsXSk7XG5tb2RlbDI8LXJwYXJ0KEdSQURFfi4sIGRhdGE9bW9vZHlbbW9vZHkkU0NPUkU8PTUwLF0pO1xubW9kZWwxXG5tb2RlbDJcbnByZWQxIDwtIHByZWRpY3QobW9kZWwxLCBuZXdkYXRhPW1vb2R5W21vb2R5JFNDT1JFPjUwLF0sIHR5cGU9XCJjbGFzc1wiKVxucHJlZDIgPC0gcHJlZGljdChtb2RlbDIsIG5ld2RhdGE9bW9vZHlbbW9vZHkkU0NPUkU8PTUwLF0sIHR5cGU9XCJjbGFzc1wiKVxubXlwcmVkaWN0aW9uPC1tb29keVxuZGVjaXNpb24gPC0gcmVwKCdGJyxucm93KG15cHJlZGljdGlvbikpXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU0NPUkU+NTBdIDwtIGFzLmNoYXJhY3RlcihwcmVkMSlcbmRlY2lzaW9uW215cHJlZGljdGlvbiRTQ09SRTw9NTBdIDwtYXMuY2hhcmFjdGVyKHByZWQyKVxubXlwcmVkaWN0aW9uJEdSQURFIDwtZGVjaXNpb25cbmVycm9yIDwtIG1lYW4obW9vZHkkR1JBREUhPSBteXByZWRpY3Rpb24kR1JBREUpXG5lcnJvciJ9 12.9 Snippet 13: Submission with rpart eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxudGVzdDwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L00yMDIydGVzdFNOb0dyYWRlLmNzdicpXG5zdWJtaXNzaW9uPC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTTIwMjJzdWJtaXNzaW9uLmNzdicpXG50cmFpbiA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L00yMDIydHJhaW4uY3N2XCIpXG5cbnRyZWUgPC0gcnBhcnQoR3JhZGUgfiBNYWpvcitTY29yZStTZW5pb3JpdHksIGRhdGEgPSB0cmFpbiwgbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5idWNrZXQgPSAyMDApKVxudHJlZVxuXG5wcmVkaWN0aW9uIDwtIHByZWRpY3QodHJlZSwgdGVzdCwgdHlwZT1cImNsYXNzXCIpXG5cbiNOb3cgbWFrZSB5b3VyIHN1Ym1pc3Npb24gZmlsZSAtIGl0IHdpbGwgaGF2ZSB0aGUgSURzIGFuZCBub3cgdGhlIHByZWRpY3RlZCBncmFkZXNcbnN1Ym1pc3Npb24kR3JhZGU8LXByZWRpY3Rpb24gXG5cbiMgdXNlIHdyaXRlLmNzdihzdWJtaXNzaW9uLCAnc3VibWlzc2lvbi5jc3YnLCByb3cubmFtZXM9RkFMU0UpIHRvIHN0b3JlIHN1Ym1pc3Npb24gYXMgY3N2IGZpbGUgb24geW91ciBtYWNoaW5lIGFuZCBzdWJzZXF1ZW50bHkgc3VibWl0IGl0IG9uIEthZ2dsZSJ9 12.10 Additional Reference Prediction with rpart "],["Linear_regression.html", "Section: 13 🔖 Linear Regression 13.1 Introduction 13.2 Linear regression using lm() function 13.3 Calculating the Error using mse() 13.4 Snippet 2: Cross Validate your prediction 13.5 Snippet 3: Submission with lm 13.6 Additional Reference", " Section: 13 🔖 Linear Regression 13.1 Introduction How to build prediction models for numerical variables? So far we have discussed prediction models for categorical target variables. In order to predict numerical variables we often use linear regression. 13.2 Linear regression using lm() function Syntax for building the regression model using the lm() function is as follows: lm(formula, data, ...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the linear regression model is to be trained. For more info on the lm() function visit lm() Lets look at the example on the Moody dataset. Table 13.1: Snippet of Moody Num Dataset Midterm Project FinalExam ClassScore 73 8 70 39.60000 61 100 20 68.20000 58 88 38 67.00000 93 41 46 52.47565 85 52 85 68.50000 97 48 19 49.10000 26 59 22 41.30000 58 62 25 50.10000 53 56 27 46.70000 66 27 17 34.80494 Imagine that we do not know the weights of midterm, project and final exam. However we have the data from the previous semesters. Can we find these weights out? The answer is yes - by using linear regression. 13.2.1 Snippet 1: How much do Midterm, Project and Final Exam count? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keU5VTTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TlVNLmNzdicpXG5zcGxpdDwtMC43Km5yb3cobW9vZHlOVU0pXG5zcGxpdFxubW9vZHlOVU1UcjwtbW9vZHlOVU1bMTpzcGxpdCxdXG5tb29keU5VTVRyXG5tb29keU5VTVRzPC1tb29keU5VTVtzcGxpdDpucm93KG1vb2R5TlVNKSxdXG4jV2UgdXNlIGxpbmVhciByZWdyZXNzaW9uIHRvICNmaW5kIG91dCB0aGUgd2VpZ2h0cyBvZiAjTWlkdGVybSwgUHJvamVjdCBhbmQgRmluYWwgI0V4YW0gaW4gY2FsY3VsYXRpb24gb2YgdGhlICNmaW5hbCBjbGFzcyBzY29yZS4gRWFjaCBvZiAjdGhlbSBhcmUgc2NvcmVkIG91dCBvZiAxMDAgYW5kICN0aGUgZmluYWwgY2xhc3Mgc2NvcmUgaXMgYWxzbyAjc2NvcmVkIG91dCBvZiAxMDAgYXMgd2VpZ2h0ZWQgI3N1bSBvZiBNaWR0ZXJtLCBQcm9qZWN0IGFuZCAjRmluYWwgRXhhbSBzY29yZXMuXG50cmFpbiA8LSBsbShDbGFzc1Njb3Jlfi4sICBkYXRhPW1vb2R5TlVNVHIpXG50cmFpblxucHJlZCA8LSBwcmVkaWN0KHRyYWluLG5ld2RhdGE9bW9vZHlOVU1Ucylcbm1lYW4oKHByZWQgLSBtb29keU5VTVRzJENsYXNzU2NvcmUpXjIpIn0= We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained from the predict function using the trained model and the test data. 13.3 Calculating the Error using mse() As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test won’t prove useful now in our numerical predictions scenario. We don’t want to eyeball every time we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique. To do this we will use the Mean Squared Error(MSE). The MSE is a measure of the quality of an predictor/estimator It is always non-negative Values closer to zero are better. The equation to calculate the MSE is as follows: \\[\\begin{equation} MSE=\\frac{1}{n} \\sum_{i=1}^{n}{(Y_i - \\hat{Y_i})^2} \\\\ \\text{where $n$ is the number of data points, $Y_i$ are the observed value}\\\\ \\text{and $\\hat{Y_i}$ are the predicted values} \\end{equation}\\] To implement this, we will use the mse() function present in the Metrics Package, so remember to install the Metrics package and use library(Metrics) in the code for local use. The syntax for mse() function is very simple: mse(actual,predicted) actual: vector of the actual values of the attribute we want to predict. predicted: vector of the predicted values obtained using our model. 13.4 Snippet 2: Cross Validate your prediction eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxudHJhaW4gPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9Nb29keU5VTS5jc3ZcIilcbiNzY3JhbWJsZSB0aGUgdHJhaW4gZnJhbWVcbnY8LXNhbXBsZSgxOm5yb3codHJhaW4pKVxudlsxOjVdXG50cmFpblNjcmFtYmxlZDwtdHJhaW5bdiwgXVxuXG4jb25lIHN0ZXAgY3Jvc3N2YWxpZGF0aW9uXG5uIDwtIDEwMFxudHJhaW5TYW1wbGU8LXRyYWluU2NyYW1ibGVkW25yb3codHJhaW5TY3JhbWJsZWQpLW46bnJvdyh0cmFpblNjcmFtYmxlZCksIF1cbnRlc3RTYW1wbGUgPC0gdHJhaW5TY3JhbWJsZWRbMTpuLF1cblxubG0udHJlZSA8LSBsbShDbGFzc1Njb3Jlfi4sICBkYXRhPXRyYWluU2FtcGxlKVxubG0udHJlZVxuXG5wcmVkIDwtIHByZWRpY3QobG0udHJlZSxuZXdkYXRhPXRlc3RTYW1wbGUpXG5wcmVkXG5cbm1zZSh0ZXN0U2FtcGxlJENsYXNzU2NvcmUscHJlZCkifQ== We can see that, The summary of the lm model gives us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained from the predict function using the trained model and the test data. In comparison to the previous model we are using the cross validation technique to check if we have more accurate predictions, thus increasing the overall accuracy of the model. 13.5 Snippet 3: Submission with lm eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxudGVzdDwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TlVNX3Rlc3QuY3N2JylcbnN1Ym1pc3Npb248LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMnN1Ym1pc3Npb24uY3N2JylcbnRyYWluIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW9vZHlOVU0uY3N2XCIpXG5cbnRyZWUgPC0gbG0oQ2xhc3NTY29yZX4uLCAgZGF0YT10cmFpbilcbnRyZWVcblxucHJlZGljdGlvbiA8LSBwcmVkaWN0KHRyZWUsIG5ld2RhdGE9dGVzdClcblxuI05vdyBtYWtlIHlvdXIgc3VibWlzc2lvbiBmaWxlIC0gaXQgd2lsbCBoYXZlIHRoZSBJRHMgYW5kIG5vdyB0aGUgcHJlZGljdGVkIGdyYWRlc1xuc3VibWlzc2lvbiRHcmFkZTwtcHJlZGljdGlvbiBcblxuIyB1c2Ugd3JpdGUuY3N2KHN1Ym1pc3Npb24sICdzdWJtaXNzaW9uLmNzdicsIHJvdy5uYW1lcz1GQUxTRSkgdG8gc3RvcmUgc3VibWlzc2lvbiBhcyBjc3YgZmlsZSBvbiB5b3VyIG1hY2hpbmUgYW5kIHN1YnNlcXVlbnRseSBzdWJtaXQgaXQgb24gS2FnZ2xlIn0= 13.6 Additional Reference Linear Regression "],["Prediction_loop.html", "Section: 14 🔖 Machine Learning-Prediction Loop 14.1 Introduction 14.2 Additional Reference", " Section: 14 🔖 Machine Learning-Prediction Loop 14.1 Introduction Believe it or not you are ready now to use pretty much any machine learning package from the extensive R library. In other words you can drive any car without knowing how the engine works. This you can find out by taking more advanced classes in Machine learning from computer science, statistics or machine learning departments. As per CRAN there are around 8,341 packages that are currently available. Apart from CRAN, there are other repositories which contribute multiple packages. The simple straightforward syntax to install any of these machine learning packages is: install.packages (“MLPackage”). Install Packages(‘MLPackage’) Library(MLPackage) MlPackage&lt;-MLPackage(Formula, data=YOUR_TRAINING,…) Predict(MLPackage, newdata=YOUR_TESTING…) Error &lt;- …… MLPackage can be rpart, Random Forests, naive Bayes, LDA, SVM, Neural Network and many others. 14.2 Additional Reference Prediction Loop "],["Mysteries_of_Aggregated_data.html", "Section: 15 🔖 How can data fool us? 15.1 Introduction 15.2 Additional References", " Section: 15 🔖 How can data fool us? 15.1 Introduction How not to be fooled by data? In the description of this class we promised that data 101 will teach you this. Have we? I hope so. Please use our question roulette to test yourself. In this section we discuss Simpson Paradox, Prosecutorial and Ecological Fallacies. Before we proceed with the paradoxes let us summarize what we have learned so far: Beware of randomness (Hypothesis testing, p-values, Multiple hypothesis testing) Be an optimist - use Bayesian reasoning. Remember about prior odds first! Beware of extreme results - Apply law of small numbers Remember we process information following availability - we assign high frequency falsely to events which are just talked about very often Narrative fallacy - do not find false patterns - “Stocks went down due to concerns about rising cost of living” Here we will discuss the Simpson paradox as well as Prosecutorial and Ecological fallacies. Let us start with the Simpson paradox. Here is a simple example of two basketball players, Aaron and Barry The Table 16.1 shows Aaron’s and Barry’s free throw point averages (FTP) over the 2021 and 2022 season respectively. Clearly for both seasons Barry beats Aaron in terms of FTP, in 2021 by 90% to 80% and in 2022 by 70% to 65%. Can Aaron still beat Barry over both seasons - that is get a higher FTP over the sum of two seasons, 2021+2022? First answer which comes to mind is absolutely not. How can Aaaron beat Barry over 2021+2022 when Barry beats him in each of the two seasons? Table 16.1 season/player Aaron Barry 2021 season 80% 90% 2022 season 65% 70% But table 16.2 explains that it is quite possible that Aaron can beat Barry over 2021+ 2022. Indeed, since we do not know the absolute number of attempts at free throws, we can easily pick any number of attempts for each of them in any of the two seasons. Indeed - here is the proof that Aaron can still beat Barry. If Barry made 100 attempts in 2021 and 20 attempts in 2022, while Barry made only 20 attempts in 2021 and 100 attempts in 2022, Aaron’s overall FRP for both 2021+2022 will be higher than Barry’s. And this is Simposon’s paradox. Table 16.2 season/player Aaron Barry 2021 season 80% out of 100 90% out of 20 2022 season 65% out of 20 70% out of 100 Indeed Aaron’s FTP over 2021+2022 is \\[\\begin{equation} \\frac{80+13}{120} = \\frac{93}{120} \\end{equation}\\] which is larger than Barry’s \\[\\begin{equation} \\frac{18+20}{120} = \\frac{88}{120} \\end{equation}\\] More generally, trends in subsets of data may reverse themselves after aggregation. In fact we can have any number of seasons and have Barry beat Aaron in FTP in each and every season and Aaron still wins with better FTP over all seasons. This is again simply because we do not know how many attempts each player made each season. This applies to many real world situations such as graduate admissions for example (the famous Berkeley admission bias case). There women may have a higher chance to be admitted than men in each single academic department and nevertheless, men beat women in overall acceptance ratio. This is again hard to comprehend at first but it is due to the fact that the absolute number of female and male applicants may be different for each department. Is such reversal always possible? Let’s look at the table below: Table 16.3 season/player Aaron Barry 2021 season 65% 90% 2022 season 60% 70% In this case the Simpson paradox is not possible. Why? Because Aaron’s highest FTP (65% in 2021 season is lower than Barry’s lowest FTP in 2022). You can easily see that no matter what the absolute numbers of attempts in each season, Aaron can never beat Barry for 2021+2022. Thus the Simpson paradox was possible in this simple case only because Aaron’s highest FTP was higher than Barry’s lowest FTP. One also has to be careful with the Simpson paradox and not apply it to situations when both groups /individuals have the same absolute number of “attempts”. For example, the Simpson paradox is not possible for students and their individual scores on homework’s and exams. If Barry scores higher than Aaron on each homework and on each exam then Barry will always have a higher score overall than Aaron. There is no Simpsonian trend reversal. Every homework and every exam counts the same for all students. This is as if players always made the same number of free throw attempts. 15.1.1 Ecological Paradox Ecological paradox is kind of the reverse of the Simpson paradox. Let’s assume that we consider net worth for each member of groups A and B. Even if average net worth of group A is higher than average net worth of group B, it may be possible that random individual member of the group B has higher net worth than random individual member of the group A. Thus the order of aggregates may be reversed when we look at the level of individuals. For example as table 16.4 illustrates, the average net worth of Group A dominates the average net worth of Group B due to the presence of one wealthy individual. However for 90% of pairs of individuals, group B members are more wealthy than Group A members. Table 16.3 Group A Group B $10,000,000 $210,000 $100,000 $290,000 $120,000 $220,000 $80,000 $210,000 $60,000 $270,000 $160,000 $210,000 $110,000 $240,000 $100,000 $210,000 $200,000 $240,000 For example, it is well known that Democrats win the richest states, while (until recently), the richest individuals vote republican. How is it possible? Explanation is simple. Everyone’s vote counts the same and there are few very rich people. Very rich people may contribute more to the average wealth of the state (due to their extreme wealth), but there are just very few of them. Do not be fooled by aggregates! Let’s assume that a Democrat wins 70% of the vote and a Republican wins 30% of the vote in some state. Is it possible that, nevertheless, the republican candidate wins all 19 counties out of 20 in the state? This actually happens a lot when the population is heavily concentrated in a heavily populated urban county which has the vast majority of voters living there. 15.2 Additional References How can data fool us? "],["22.html", "Section: 16 Boundless Analytics - Pre-discovery Tool 16.1 Introduction 16.2 Minimarket Data Set description 16.3 Demo of Boundless Analytics 16.4 The Boundless Analytics web application 16.5 Snippet 1: Chi square hunt", " Section: 16 Boundless Analytics - Pre-discovery Tool 16.1 Introduction In this section we demonstrate application of Boundless Analytics - the tool developed by Tomasz Imielinski and his team at Rutgers (and supported by NSF subcontract of Center of Science of Information at Purdue University). Boundless Analytics calculates all significant bar graphs from the data set and allows to find data subsets (slices) which deviate the most from the whole data set in regard to frequency distribution of an attribute. Boundless performs an otherwise very tedious task of looking at all combinations of attribute value pairs to identify the “significant ones” - saving enormous amounts of work in preliminary exploration of data. We are using here the Minimarket data puzzle 3.8 describing customer transactions in the small chain of minimarkets in NJ. Data 101 students used Boundless Analytics to discover the most interesting subsets of this data set 16.2 Minimarket Data Set description Zoom recording 16.3 Demo of Boundless Analytics Zoom Recording 16.4 The Boundless Analytics web application Boundless Analytics Interface: http://209.97.156.178:8082/ (it is a soft login abc/abc will do) Objective: Nominate the most interesting subset of the Minimarket2022 data set Seems open ended, no? what is the “most interesting”? Chi-square value is a good measure. We explain it below. By swiping through possible plots (using Next), one can identify good candidates for the “interesting data subsets”) These are plots where red and blue bars differ the most. In other words we want to reject the null hypothesis of independence of red and blue distributions over the data slice and the complement of the data slice. The higher the chi-square is, the strongest is our rejection of independence of red and blue distributions. Therefore this task can be seen as chi-square hunt for the highest chi-square value (use the snippet 17.1 code after plugging in definition of a slice and the anchor attribute) 16.5 Snippet 1: Chi square hunt eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFNheSwgdGhlIEJvdW5kbGVzcyBhbmFseXRpY3MgcHJvdmlkZXMgdXMgd2l0aCB0aGUgc2xpY2U6ICBCZWVyID09J0xhZ2VyJyAmICBEYXkgPT0nV2Vla2VuZCcgYW5kIFNuYWNrcyA9J0NyYWNrZXJzJyBhbmQgYW5jaG9yIGF0dHJpYnV0ZSBpcyBMb2NhdGlvbi4gIFlvdSBjYW4gY2FsY3VsYXRlIENoaXNxIGZvciB0aGlzIHNsaWNlIGFuZCB0aGUgTG9jYXRpb24gYXR0cmlidXRlIHRvIHRlc3QgaWYgZGlzdHJpYnV0aW9uIG9mIGxvY2F0aW9ucyBpcyBhZmZlY3RlZCBpZiB3ZSBsaW1pdCBvdXJzZWx2ZXMgb25seSB0byB0cmFuc2FjdGlvbnMgc2VsbGluZyBMYWdlciBhbmQgQ3JhY2tlcnMgb24gV2Vla2VuZHM/ICBcblxuIyBUaGUgbW9zdCBpbnRlcmVzdGluZyBzbGljZS1hbmNob3IgYXR0cmlidXRlIGNvbWJpbmF0aW9ucyBhcmUgdGhlIG9uZXMgd2l0aCB0aGUgbGFyZ2VzdCBjaGlzcSB0ZXN0IGFuZCBsb3dlc3QgcC12YWx1ZS4gTmV2ZXJ0aGVsZXNzIGRvIG5vdCBmb3JnZXQgYWJvdXQgbXVsdGlwbGUgaHlwb3RoZXNpcyBjb3JyZWN0aW9uIC0gc2luY2Ugd2UgY2FuIG9uIGNoaS1zcXVhcmUgaHVudCBoZXJlIVxuXG5NaW5pbWFya2V0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L0hvbWV3b3JrTWFya2V0MjAyMi5jc3ZcIilcblxuTWluaW1hcmtldCRJTjwtJ091dF9TbGljZSdcbk1pbmltYXJrZXRbTWluaW1hcmtldCRCZWVyPT0nTGFnZXInICYgTWluaW1hcmtldCREYXk9PSdXZWVrZW5kJyAmICBNaW5pbWFya2V0JFNuYWNrcyA9PSdDcmFja2VycycsIF0kSU48LSdJbl9TbGljZSdcbmQ8LXRhYmxlKE1pbmltYXJrZXQkTG9jYXRpb24sIE1pbmltYXJrZXQkSU4pXG5jaGlzcS50ZXN0KGQpIn0= ATTACHED - the data set (same as on the Boundless Analytics interface) HomeworkMarket2022-2.csv RESULTS: Here are two out of 250+ submissions. The one with the highest chi-square of 600.15 is the slice showing weekend buyers of lager in New brunswick but disproportionately more snacks (in particular Crackers). This was identified by nearly 20 students. Here is another find by Eva Zhang showing disproportionately frequent sales of Coca Cola on Weekdays in Princeton for transactions which purchased Popcorn. The chi-square value of this find is 205.31, with df=3. "],["best_work_2022.html", "Section: 17 🔖 Best Works of 2022 17.1 DataBlog 17.2 Boundless Analytics", " Section: 17 🔖 Best Works of 2022 17.1 DataBlog Ella Walmsley 17.2 Boundless Analytics Anastasiya Chuchkova Shreya Tiwari George Basta Paul Kotys Selin Altimparmak "],["Leaderboard.html", "Section: 18 Data League Leaderboard", " Section: 18 Data League Leaderboard Table 18.1: Leaderboard 2022 Rank Participant.Name 1 Jeevanandan Ramasamy 2 George Basta 3 Joyce Huang 4 Jiaxu Hu 5 Dhiren Patel 6 Chicheng Shao 7 Cheyenne Pourkay 8 Christopher Nguyen 9 Aaron Mok 10 Ethan Matta Honourable Mentions: Upsham Naik, Joshua B. Sze, Kirtan Patel, Maria Xu, Devam Patel, Eva Zhang, Toshanraju Vysyaraju, Maanas Pimplikar, Jared Chiou, Nitya Narayanan, Shrish Vellore, Yousra Belgaid, Mitali Shroff, Michael Jucan, Jackie Hong, Arvin Sung, Eric Xuan, Eva Allred, Leah Ranavat, Nami Jain, Gautam Agarwal, Aditya Patil "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
