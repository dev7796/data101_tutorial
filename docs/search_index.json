[["intro.html", "Section: 1 Introduction", " Section: 1 Introduction The objective of this textbook is to provide you with the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. In other words, to introduce you to data science. We call it an active textbook, since students can interact with the book by running and modifying snippets of R code. Students can also test themselves using Query and Code Roulette - on questions and simple coding tasks. Thus, an active textbook interacts with its reader, helps to run code and also asks students questions and gives them simple coding tasks. Concepts which we discuss in the book are widely covered on the web with countless youtube video tutorials. We briefly introduce the basic concepts here as well, but our focus is on problem solving and simple coding. In other words, honing the skills to do something with the data, not just talk about it. To learn a concept you have to know how to code it. To put it bluntly, no coding, no learning! Active textbook is data-centric. We stress that prior to data analysis and exploration, you have to know your data. Paraphrasing the famous saying that real estate is about “location, location and location”, data science is about data, data and data. Using numerous data sets we guide the student through the process of getting acquainted with their data. We call these data sets - data puzzles, since each of them is synthetically created and has hidden patterns embedded in the process of data generation. For each of our data puzzles we show the process of getting familiar with the data beginning from simple scripts called queries and proceeding through as-hoc hypothesis testing as well as Bayesian reasoning. As we said, each data puzzle hides interesting and non-trivial patterns which are to be discovered. This process of discovery makes data science similar to the work of a detective. Discoveries range from grading methods of Professor Moody, factors influencing quality of a party, voter profiles in local town elections or quality of sleep determinants. Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in a statistically convincing manner, perhaps in a colorful and visually appealing way. Finally, you will be able to apply some basic machine learning methods to build, train and test prediction models. All of this will be accomplished in a succinct and crisp way using a small subset of R instructions. We assume no prior programming background. We will teach you as little R as necessary to achieve the goals of this book: explore the data, visualize it, verify hypotheses and build prediction models. Thus, you will be able to do a good chunk of work which data scientists do. We will accomplish this goal through active snippets of executable code. These are examples of R code (around 100 executable snippets of code) embedded in the textbook itself. More importantly, you will be able to modify the code and execute the modified code without need to install any application on your machine. This will allow you to understand the code in the book through the “what if” exploratory process. Thus, every code snippet is just an invitation to endless modifications. This is why we call this textbook - active. Another unique aspect of this textbook is its reliance on data puzzles. These are synthetic data sets with embedded patterns and rules generated by our tool called DataMaker. We will present our data puzzles (dynamic list, may vary from year to year) in section 8 following introduction to plots, then we will proceed to freestyle data exploration. This will allow us to learn more about our data, form the leads, and finally state our hypotheses. We will follow up by an elementary introduction to hypothesis testing through a permutation test. We will learn how to calculate p-values and how to use them to defend our findings against the randomness trap. This will be particularly important in case of multiple hypotheses when one has to be particularly careful to avoid “false positives” . We will introduce Bayesian reasoning and learn how to compute posterior odds of a belief given an observation. All these important concepts will be introduced via executable snippets of code and “what if” practicing. We will then enter the key section of the book - the data puzzles section. In the data puzzle section, for each data set we will go through the process of getting to know the data and using the concepts learned so far by executing code tailored to each of the data puzzles. In the second part of the book we discuss prediction models. We focus on decision trees (rpart() - recursive partitioning) and linear regression. But we also show how to use other machine learning methods from the rich R-library. We go over cross-validation and show how to build prediction models which combine multiple machine learning models. We stress the importance of knowing your data first, instead of just blind application of machine learning packages. Humans in the loop is very important and prior data exploration and visualization leads to improved quality prediction models. Students can practice prediction model building on especially prediction snippets to make themselves prepared for Kaggle based prediction challenge competition which takes the last months of the data 101 class. The last leaderboard of 2022 challenge is presented here LeaderBoard . We will use as few R functions as possible to achieve our goals. In fact we will demonstrate how using less than ten R functions is sufficient for us. In the appendix, we show many more useful commands of R which eventually you would have to use. However, our goal in this short textbook, is to present the shortest path to data analysis which will let you import the data, plot it, make some analysis yourself and use R-libraries to build machine learning models. In this textbook and in this class we do not teach how to clean the data (data wrangling) and how to deal with a wide variety of data types. We also do not address complex data transformations such as multi-frame operations like merge function. We also do not explain how different machine learning methods work, we only show you how to use them. It is similar to teaching one how to drive a car without knowing how a car engine works. Sections 2.5 and 2.6 provide the lists of all concepts which we cover in our active textbook and all R functions which are needed. Notice how small the set of R functions is. It is important for programming novices to start small and also see how far this small set of functions can get you. Our question roulette allows self-testing on nearly 100 questions relevant to the material. Each question is answered, but students are encouraged first to answer questions themselves and only then follow it with checking the correct answer. The code roulette, on the other hand, consists of around 100 of simple common data science coding tasks. "],["Setting_up_R.html", "Section: 2 Setting Up R 2.1 Create New Project 2.2 How to upload a data set? 2.3 Saving your work 2.4 General R References 2.5 Textbook Concepts 2.6 R functions used in this class", " Section: 2 Setting Up R Important Instructions Installation of R is required before installing RStudio “R” is a programming language, and, “RStudio” is an Integrated Development Environment (IDE) which provides you a platform to code in R. How to download and install R &amp; RStudio? Downloading and installing R. For Windows Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cran.r-project.org/bin/windows/base/ Click on the link at top left where it says “Download R 4.0.3 for windows” or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. For MAC Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cloud.r-project.org/bin/macosx/ Under “Latest release”, click on “R-4.0.3.pkg” or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. Downloading and installing RStudio. For Windows Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named “All Installers”. Click on the download link beside “Windows 10/8/7” to download the windows version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. For MAC Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named “All Installers”. Click on the link beside “macOS 10.13+” to start your download the MAC version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. 2.1 Create New Project After installing R studio successfully the first step is to create a project R studio. Step 1: Go to File -&gt; New Project New Project Step 2: Select New Directory New Directory Step 3: Select New Project New Project Step 4: Give your preferred directory name like “Data101_Assignmnets” Directory Name Step 5: Click on Create Project and finally the R studio should look like Rstudio 2.2 How to upload a data set? To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon. There are two options while accessing the dataset from your local machine: To avoid giving long directory paths for accessing the dataset, one should use the command getwd() to get the current working directory and store the dataset in the same directory. Getwd To access the dataset stored in the same directory one can use the following: read.csv(“MOODY_DATA.csv”). Store the moody dataset in the same directory One can also store the dataset at a different location and can access it using the following command: (Suppose the dataset is stored inside the folder Data101_Tutorials on the desktop) - For Windows Users. - Example: read.csv(&quot;C:/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv&quot;) - For MAC Users. - Example: read.csv(&quot;/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv&quot;) Note: The directory path given here is the current working directory hosted on Github where the dataset has been stored. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFJlYWQgaW4gdGhlIGRhdGFcbmRmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIilcblxuIyBQcmludCBvdXQgYGRmYFxuaGVhZChkZikifQ== 2.3 Saving your work To save your work go to File -&gt; Save. It will ask you to give a name for your .R file and then click on Save. Save After making modifications to your saved file, you will need to save the file again. If the name of the file on the top is in Red Color indicates that the file have unsaved changes. Unsaved File Go to File -&gt; Save to save your .R file again. After saving the file the color of the file name i.e. HW1.R will again change back to black. Saved File Note: You can create multiple files inside the same project such as for your each homework assignments 2.4 General R References https://www.w3schools.com/r/ https://cran.r-project.org/doc/contrib/Short-refcard.pdf https://www.amazon.com/Statistics-Engineers-Scientists-William-Navidi/dp/0073376337/ref=pd_lpo_3?pd_rd_i=0073376337&amp;psc=1 https://data101.cs.rutgers.edu/laboratory/ 2.5 Textbook Concepts Hypothesis testing: 6 Difference of means hypothesis testing: 6 Null Hypothesis: 6 Alternative Hypothesis: 6 z-value: 6 critical value: 6 significance level: 6 p-value: 6 Bonferroni correction: 8 Chi square test: 7 Independence: 7 Multiple Hypothesis testing: 8 False Discovery Proportion: 8 Contingency Matrix: 7 Bayesian Reasoning: 9 Prior odds: 9 Posterior odds: 9 Likelihood ratio: 9 False positive: 9 True positive: 9 Crossvalidation: 13.5 Decision trees: 13 Linear regression: 14 Recursive partitioning: 14 MSE: 14 Prediction accuracy: 14 Training: 14 Testing: 14 2.6 R functions used in this class Elementary instructions: c() 3.1, mean() 3.4.1, nrow() 3.5.1, rep(), sd() 3.4.5, cut() ?? Plots: plot() 4.1, barplot() 4.2, boxplot() 4.3 mosaicplot() 4.4 Data Transformations: subset() 3.5, tapply() 3.6, table() 3.3, aggregate() Library functions: chisq.test() 7, pnorm() 6.2, Permutation() 6.2, rpart() 13, predict() 13.6, lm() 14.2, crossvalidation() 13.5 Parameters of rpart: minsplit 13.4, minbucket 13.4, cp 13.4 "],["tapply_subsetting.html", "Section: 3 🔖 Basic R Intructions 3.1 Vector 3.2 Data Frames 3.3 Table 3.4 Basic Functions 3.5 Subset 3.6 tapply", " Section: 3 🔖 Basic R Intructions In this section we introduce the absolutely basic R instructions, call it R101, which will be sufficient for the entire data 101 class. This is a very small subset of the entire R. The good news is that using this very small subset of R we can accomplish all coding objectives for data 101! The set we present below is a mix of simple arithmetic aggregate functions such as mean() 3.4.1, max() 3.4.3, sum() , basic data structures such as vectors and data frames and finally, two core functions defined for data frames: subset() 3.5, tapply() 3.6 and table() 3.3 function defined on vectors. 3.1 Vector A vector is simply a list of items that are of the same type. 3.1.1 Snippet 1 Lets look at example of creating a vector: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgMyB2ZWN0b3JzIHdpdGggdGl0bGUsIGF1dGhvciBhbmQgeWVhci5cbmNvbG9yIDwtIGMoJ1JlZCcsJ0JsdWUnLCdZZWxsb3cnLCdHcmVlbicpXG5cbiNMZXRzIGxvb2sgYXQgaG93IHRoZSBjcmVhdGVkIHZlY3RvcnMgbG9vay5cbmNvbG9yIn0= 3.1.2 Snippet 2 Create a vector with numerical values in a sequence, use the : operator: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgYSB2ZWN0b3JzIHdpdGggbnVtZXJpY2FsIHNlcXVlbmNlLlxueWVhciA8LSAyMDE4OjIwMjJcblxuI0xldHMgbG9vayBhdCBob3cgdGhlIGNyZWF0ZWQgdmVjdG9ycyBsb29rLlxueWVhciJ9 3.2 Data Frames Data Frames are data displayed in a format as a table. 3.2.1 Snippet 1 Data frames will serve as containers of imported data - typically data provided in csv format, like the moody data set above. Snippet 4.21 shows how to populate a data frame using read.csv() instruction. Notice that the moody data frame which is the container for the imported data set will automatically inherit attribute names (columns) of the underlying data set. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQgaW50byB0aGUgbW9vZHkgdmFyaWFibGVcbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMi5jc3ZcIilcblxuIyBOb3cgbGV0cyB2aWV3IHRoZSBkYXRhZnJhbWUgbW9vZHkgd2l0aCBqdXN0IDUtNiB0dXBsZXNcbmhlYWQobW9vZHkpIn0= 3.2.2 Snippet 2 Get the summary of the dataframe: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQgaW50byB0aGUgbW9vZHkgdmFyaWFibGVcbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMi5jc3ZcIilcblxuIyBVc2UgdGhlIHN1bW1hcnkoKSBmdW5jdGlvbiB0byBzdW1tYXJpemUgdGhlIGRhdGEgZnJvbSBhIERhdGEgRnJhbWU6XG5zdW1tYXJ5KG1vb2R5KSJ9 3.2.3 Snippet 3 We can select subsets of columns and subsets of rows for a data frame using the following the notation data[rows, columns]: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQgaW50byB0aGUgbW9vZHkgdmFyaWFibGVcbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMi5jc3ZcIilcblxuIyBSZXR1cm4gcm93IDFcbm1vb2R5WzEsIF1cblxuIyBSZXR1cm4gY29sdW1uIDVcbm1vb2R5WywgNV1cblxuIyBSb3dzIDE6NSBhbmQgY29sdW1uIDJcbm1vb2R5WzE6NSwgMl1cblxuIyBHaXZlIG1lIHJvd3MgMS0zIGFuZCBjb2x1bW5zIDIgYW5kIDQgb2YgbW9vZHlcbm1vb2R5WzE6MywgYygyOjQpXSJ9 3.3 Table 3.3.1 Snippet 1 The below examples show how to use this function: eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpICN3ZWIgbG9hZFxuXG4jbGV0cyBtYWtlIGEgdGFibGUgZm9yIHRoZSBncmFkZXMgb2Ygc3R1ZGVudHMgYW5kIGNvdW50cyBvZiBzdHVkZW50cyBmb3IgZWFjaCBHcmFkZS4gXG5ncmFkZXMgPC0gdGFibGUobW9vZHkkR3JhZGUpXG5cbiNKb2ludCBkaXN0cmlidXRpb24gb2YgZ3JhZGUgYW5kIG1ham9yXG50YWJsZShtb29keSRHcmFkZSwgbW9vZHkkTWFqb3IpIn0= 3.4 Basic Functions Table 3.1: Snippet of moody Dataset Major Score Seniority GPA Grade 905 Statistics 14 Junior 4 F 247 Statistics 10 Senior 2 F 869 CS 98 Junior 1 A 50 CS 36 Senior 1 F 698 Economics 95 Junior 4 A 3.4.1 mean() mean() function is used to find the average of values in a numerical vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIG1lYW4gb2Ygc2NvcmUgY29sdW1uLlxubWVhbihtb29keSRTY29yZSkifQ== 3.4.2 length() length() function is used to get the number of elements in any vector eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIGxlbmd0aCBvZiB0aGUgZ3JhZGUgY29sdW1uIFxubGVuZ3RoKG1vb2R5JEdyYWRlKSJ9 3.4.3 max() max() function is used to get the maximum value in a numerical vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG5cbiNsZXRzIGxvb2sgYXQgdGhlIG1heGltdW0gdmFsdWUgb2YgdGhlIHNjb3JlIGluIHRoZSBzY29yZSBjb2x1bW5cbm1heChtb29keSRTY29yZSkifQ== 3.4.4 min() min() function is used to get the minimum value in a numerical vector eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIG1pbmltdW0gdmFsdWUgb2Ygc2NvcmUgaW4gdGhlIHNjb3JlIGNvbHVtbi5cbm1pbihtb29keSRTY29yZSkifQ== 3.4.5 sd() sd() function is used to find the standard deviation of numerical vector eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIHN0YW5kYXJkIGRldmlhdGlvbiBvZiBzY29yZSBjb2x1bW5cbnNkKG1vb2R5JFNjb3JlKSJ9 Now we are ready to introduce basic data transformation techniques such as slicing and dicing. Slicing, otherwise known as subsetting, allows the selection of data frame subsets. These subsets are defined by boolean conditions built from Attribute op value pairs where op is one of the arithmetic operators such as =, !=, &lt; etc. For example (Score &gt;70)&amp; (Grade ==’A’) refers to a subset of a data frame describing students who scored more than 70 points and got an A. Dicing refers to eliminating some of the attributes from a data frame - it is vertical slicing - which results in a more “narrow” frame. Finally we can also expand our data frame with new, so called derived, attributes. This is a very useful operation in data analysis since it allows so-called “feature engineering”. These new user-defined features can lead to totally new insights into the data. 3.5 Subset The following snippets demonstrate two ways of subsetting a data frame: first through explicit function subset() and second through the native sub-data frame notation df[ ]. 3.5.1 Snippet 1 Subsetting data frame via subset() function eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG4jU3Vic2V0IG9mIHJvd3Ncbm1vb2R5X3BzeWNob2xvZ3k8LXN1YnNldChtb29keSwgTWFqb3I9PSAnUHN5Y2hvbG9neScpXG5ucm93KG1vb2R5KVxubnJvdyhtb29keV9wc3ljaG9sb2d5KSJ9 3.5.2 Snippet 2 Example of subset function eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG5cbiNBbHRlcm5hdGUgd2F5IHRvIHN1YnNldC5cbm1vb2R5W21vb2R5JE1ham9yPT1cIlBzeWNob2xvZ3lcIiwgXVxubW9vZHlbbW9vZHkkTWFqb3IhPVwiUHN5Y2hvbG9neVwiLCBdXG5tb29keVttb29keSRTY29yZSA+ODAsIF1cbm1vb2R5W21vb2R5JFNjb3JlID44MCAmIG1vb2R5JEdyYWRlID09ICdCJywgXSJ9 3.5.3 Snippet 3 Subsetting columns eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG5jb2xuYW1lcyhtb29keSlcbiNzdWJzZXQgb2YgY29sdW1uc1xubW9vZHkzPC1zdWJzZXQobW9vZHksIHNlbGVjdCA9IC1jKDEpKVxubmNvbChtb29keTMpXG4jIFlvdSBjYW4gc2VlIHRoZSBudW1iZXIgb2YgY29sdW1ucyBoYXMgYmVlbiByZWR1Y2VkIGJ5IDEsIGR1ZSB0byBzdWItc2V0dGluZyB3aXRob3V0IGNvbHVtbiAxXG5uY29sKG1vb2R5MykifQ== 3.5.4 Snippet 4 Sub-setting rows and columns eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG4jU3Vic2V0IG9mIFJvd3MgYW5kIENvbHVtbnNcbm1vb2R5MTwtc3Vic2V0KG1vb2R5LCBzZWxlY3QgPSBjKDI6NCksIE1ham9yPT1cIlBzeWNob2xvZ3lcIilcbmNvbG5hbWVzKG1vb2R5MSlcbiNOb3RpY2UgdGhhdCBvbmx5IDMgY29sdW1ucyBhcmUgcmVtYWluaW5nXG5kaW0obW9vZHkxKSJ9 One of the most important R instructions is tapply. It allows parallel execution of an aggregate function for different values of a categorical variable. 3.6 tapply tapply() has four arguments: the data frame (df), numerical attribute of df, categorical attribute of df and aggregate function (mean, max, min etc). Syntax of df is as follows: tapply(df$numerical attribute, df$categorical attribute, aggregate function) tapply() first slices data frame df by different values of a categorical attribute and then computes an aggregate (mean, median, min, max, etc..) of a numerical attribute to each slice. 3.6.1 Snippet 1 Example of tapply followed by plot eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG5cbiMgVG8gc2VlIGRpc3RyaWJ1dGlvbnMgb2YgbWVhbiBzY29yZXMgZm9yIGRpZmZlcmVudCBncmFkZXMgb25seSBmb3Igc3R1ZGVudHMgd2hvIHdlcmUgXHUyMDFjYWx3YXlzXHUyMDFkIG9uIHNtYXJ0cGhvbmVzIGluIGNsYXNzLiBcbnRhcHBseShtb29keVttb29keSRTZW5pb3JpdHkgPT0gXCJKdW5pb3JcIixdJFNjb3JlLCBtb29keVttb29keSRTZW5pb3JpdHk9PSBcIkp1bmlvclwiLF0kR3JhZGUsIG1lYW4pIn0= 3.6.2 Snippet 2 Combining table and subset eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjIuY3N2XCIpXG5cbiNEaXN0cmlidXRpb24gb2YgZ3JhZGVzIGZvciBqdW5pb3JzXG50YWJsZShtb29keVttb29keSRTZW5pb3JpdHkgPT0gJ0p1bmlvcicsXSRHcmFkZSlcbiNEaXN0cmlidXRpb24gb2YgZ3JhZGVzIGZvciBzZW5pb3JzIHdobyBtYWpvciBpbiBFY29ub21pY3NcbnRhYmxlKG1vb2R5W21vb2R5JFNlbmlvcml0eSA9PSAnU2VuaW9yJyAmIG1vb2R5JE1ham9yID09ICdFY29ub21pY3MnLCAsXSRHcmFkZSkifQ== "],["plots.html", "Section: 4 🔖 Plots 4.1 Scatter Plot 4.2 Bar Plot 4.3 Box Plot 4.4 Mosaic Plot 4.5 Additional References", " Section: 4 🔖 Plots When you import your data to R studio one of the first things you do is plot. Data visualization is a key components of data analysis. Before we talk about plots, we introduce some very basis data structures in R: vectors, data frames and tables. These are introduced below in the form of code snippets that you can run and modify. Then we are ready to plot! We will introduce several basic plots such as scatter plot, bar plot, boxplot and mosaic plot. How do we know which plot to apply? It depends on whether the variables to be plotted are categorical or numerical. Below we show a simple table which can serve as a guide which plot to use depending on types of variables to be plotted. NUM x NUM scatter plot CAT x CAT mosaic plot CAT x NUM box plot NUM box plot, histogram CAT bargraph 4.1 Scatter Plot Scatter Plot are used to plot two numerical variables. Hence it is used when both the labels are numerical values. Lets look at example of scatter plot using Moody. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExldCdzIGxvb2sgYXQgYSAyIGF0dHJpYnV0ZSBzY2F0dGVyIHBsb3QuXG4jIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbnBsb3QobW9vZHkkcGFydGljaXBhdGlvbixtb29keSRzY29yZSx5bGFiPVwic2NvcmVcIix4bGFiPVwicGFydGljaXBhdGlvblwiLG1haW49XCIgUGFydGljaXBhdGlvbiB2cyBTY29yZVwiLGNvbD1cInJlZFwiKSJ9 4.2 Bar Plot A bar plot are used to plot a categorical variable. This rectangle height is proportional to the value of the variable in the vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbmNvbG9yczwtIGMoJ3JlZCcsJ2JsdWUnLCdjeWFuJywneWVsbG93JywnZ3JlZW4nKSAjIEFzc2lnbmluZyBkaWZmZXJlbnQgY29sb3JzIHRvIGJhcnNcblxuI2xldHMgbWFrZSBhIHRhYmxlIGZvciB0aGUgZ3JhZGVzIG9mIHN0dWRlbnRzIGFuZCBjb3VudHMgb2Ygc3R1ZGVudHMgZm9yIGVhY2ggR3JhZGUuIFxuXG50PC10YWJsZShtb29keSRncmFkZSlcblxuI29uY2Ugd2UgaGF2ZSB0aGUgdGFibGUgbGV0cyBjcmVhdGUgYSBiYXJwbG90IGZvciBpdC5cblxuYmFycGxvdCh0LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJOdW1iZXIgb2YgU3R1ZGVudHNcIixjb2w9Y29sb3JzLCBcbiAgICAgICAgbWFpbj1cIkJhcnBsb3QgZm9yIHN0dWRlbnQgZ3JhZGUgZGlzdHJpYnV0aW9uXCIsYm9yZGVyPVwiYmxhY2tcIikifQ== 4.3 Box Plot A boxplot is used to display a numerical variable. A boxplot shows the distribution of data in a dataset. A boxplot shows the following things: Minimum Maximum Median First quartile Third quartile Outliers eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbmNvbG9yczwtIGMoJ3JlZCcsJ2JsdWUnLCdjeWFuJywneWVsbG93JywnZ3JlZW4nKSAjIEFzc2lnbmluZyBkaWZmZXJlbnQgY29sb3JzIHRvIGJhcnNcblxuI1N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCB0aGUgZGlzdHJpYnV0aW9uIG9mIHN0dWRlbnRzIHNjb3JlIHBlciBHcmFkZS4gV2UgdXNlIGJveCBwbG90IGZvciBnZXR0aW5nIHRoYXQuIFxuYm94cGxvdChzY29yZX5ncmFkZSxkYXRhPW1vb2R5LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJTY29yZVwiLCBtYWluPVwiQm94cGxvdCBvZiBncmFkZSB2cyBzY29yZVwiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIilcblxuIyB0aGUgY2lyY2xlcyByZXByZXNlbnQgb3V0bGllcnMuIn0= 4.4 Mosaic Plot Mosaic plot is used to visualize two categorical variables. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjBiLmNzdlwiKSAjd2ViIGxvYWRcbmNvbG9yczwtIGMoJ3JlZCcsJ2JsdWUnLCdjeWFuJywneWVsbG93JywnZ3JlZW4nKSAjIEFzc2lnbmluZyBkaWZmZXJlbnQgY29sb3JzIHRvIGJhcnNcblxuI3N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCBudW1iZXJzIG9mIHN0dWRlbnRzIHdpdGggYSBwYXJ0aWN1bGFyIGdyYWRlIGJhc2VkIG9uIHRoZWlyIHRleHRpbmcgaGFiaXRzLiBVc2UgTW9zaWFjLXBsb3QuXG5cbm1vc2FpY3Bsb3QobW9vZHkkZ3JhZGV+bW9vZHkkdGV4dGluZyx4bGFiID0gJ0dyYWRlJyx5bGFiID0gJ1RleHRpbmcgaGFiaXQnLCBtYWluID0gXCJNb3NpYWMgb2YgZ3JhZGUgdnMgdGV4aW5nIGhhYml0IGluIGNsYXNzXCIsY29sPWNvbG9ycyxib3JkZXI9XCJibGFja1wiKSJ9 4.5 Additional References Plots https://www.datamentor.io/r-programming/plot-function/ "],["Derived_attributes.html", "Section: 5 🔖 Data Transformation with Derived Attributes 5.1 Making new categorical attributes 5.2 Making categorical attribute from numerical attribute using function Cut() 5.3 Making new numerical attribute from numerical attributes 5.4 More complex example of defining derived attributes 5.5 Additional references", " Section: 5 🔖 Data Transformation with Derived Attributes R allows creating new data frame attributes (columns) “on the fly”. These are new vectors, which are often defined as functions of existing attributes. Hence, the name - derived attributes. Derived attributes will play an important role in data exploration as well as in building prediction models. Very often, derived attributes allow discovery of important patterns in data. Similarly, derived attributes may be more predictive than original attributes in the imported data sets. The term feature engineering is often used in machine learning to describe creation of derived attributes. 5.1 Making new categorical attributes Here we define a new attribute PF (Pass/Fail) to “Pass”. Students who got A, B or C, passed. Students who received F, failed. We are grouping values of Grade into two categories of a new categorical attribute PF. The line 5 replaces “Pass” by “Fail” for students who received F. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKSAjd2ViIGxvYWRcblxuIyBDdXQgRXhhbXBsZSB1c2luZyBicmVha3MgLSBDdXR0aW5nIGRhdGEgdXNpbmcgZGVmaW5lZCB2ZWN0b3IuIFxubW9vZHkkUEY8LSdQYXNzJ1xubW9vZHlbbW9vZHkkR1JBREU9PSdGJyxdJFBGPC0nRmFpbCdcbm1vb2R5In0= 5.2 Making categorical attribute from numerical attribute using function Cut() cut() function divides the range of x into intervals. Provides ability to label intervals as well. It plays important role in defining derived attributes from attributes which are numerical. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKSAjd2ViIGxvYWRcblxuIyBDdXQgRXhhbXBsZSB1c2luZyBicmVha3MgLSBDdXR0aW5nIGRhdGEgdXNpbmcgZGVmaW5lZCB2ZWN0b3IuIFxuc2NvcmUxIDwtIGN1dChtb29keSRTQ09SRSxicmVha3M9YygwLDUwLDEwMCksbGFiZWxzPWMoXCJGXCIsXCJQXCIpKVxudGFibGUoc2NvcmUxKSJ9 5.3 Making new numerical attribute from numerical attributes Suppose we would like to combine score and participation into one combined score. We can define a new numerical attribute from SCORE and PARTICIPATION . We can see that the moody data frame will be expanded by the additional attribute. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKSAjd2ViIGxvYWRcblxubW9vZHkkQ29tYmluZWQgPC0gKG1vb2R5JFNDT1JFICArIDEwKiBtb29keSRQQVJUSUNJUEFUSU9OKVxubW9vZHlbMToxMCxdIn0= 5.4 More complex example of defining derived attributes The way we define combined score attributes rewards students even for poor participation. Their combined score is always higher than their score in class even if their participation was quite low. It would make more sense to define combined score by either penalizing for poor performance or rewarding good performance. The next snippet illustrates defining such a new numerical attribute, $adjustedScore of a student in the Moody data frame. adjustedScore penalizes low participation or rewards for good participation. Score is adjusted by the value of participation attribute in the following way: If participation is larger than 0.5 - a bonus proportional to participation * 10 is added to the score. If participation is smaller than 0.5, a penalty of 1-participation) * 10 is subtracted from the score. In this way, for someone with very small participation, the 10 point penalty will be imposed (10 points subtracted from the score). Conversely, someone with perfect participation (1.0) will receive a 10 point bonus. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKSAjd2ViIGxvYWRcblxubW9vZHkkY29uZGl0aW9uYWwgPC0wXG5tb29keVttb29keSRwYXJ0aWNpcGF0aW9uPDAuNTAsIF0kY29uZGl0aW9uYWwgPC0gbW9vZHlbbW9vZHkkcGFydGljaXBhdGlvbjwwLjUwLCBdJHNjb3JlIC0xMCooMS1tb29keVttb29keSRwYXJ0aWNpcGF0aW9uPDAuNTAsIF0kcGFydGljaXBhdGlvbilcblxubW9vZHlbbW9vZHkkcGFydGljaXBhdGlvbj49MC41MCwgXSRjb25kaXRpb25hbCA8LSBtb29keVttb29keSRwYXJ0aWNpcGF0aW9uPj0wLjUwLCBdJHNjb3JlICsxMCptb29keVttb29keSRwYXJ0aWNpcGF0aW9uPj0wLjUwLCBdJHBhcnRpY2lwYXRpb25cblxuI0xldHMgc2VlIGEgc2FtcGxlIG9mIG5ldyByb3dzIG9mIG1vb2R5IHdpdGggdGhlIG5ldyBkZXJpdmVkIGF0dHJpYnV0ZVxubW9vZHlbMToxMCxdIn0= We are now able to transform our data by slicing and dicing rows and columns, using subset function (or sub-data frame), we can also add new attributes as shown above. Data Transformation is critical not just in data exploration and plotting but foremost in building high quality prediction models as we will show later. 5.5 Additional references Data Transformation "],["hypothesis_testing.html", "Section: 6 🔖 Hypothesis Testing 6.1 Introduction 6.2 Snippet 1: Permutation test 6.3 Snippet 2: z-test 6.4 Snippet 3: Make your own data and see how p-value changes 6.5 Additional References", " Section: 6 🔖 Hypothesis Testing 6.1 Introduction Randomness is the biggest enemy of data scientists. How to distinguish what’s real from what’s random? This is the goal of hypothesis testing. We will introduce hypothesis testing through the permutation test. To illustrate the permutation test, let us start with a simple example of a dataset storing information about traffic in Lincoln and Holland tunnels. Table 6.1: Snippet of Traffic Dataset TUNNEL DAY VOLUME_PER_MINUTE 2694 Lincoln weekend 77.0 263 Holland weekday 46.5 992 Holland weekday 80.5 2275 Lincoln weekday 70.0 1348 Holland weekend 71.5 315 Holland weekday 77.5 1866 Lincoln weekday 93.0 1315 Holland weekend 67.5 1044 Holland weekend 69.5 2724 Lincoln weekend 69.0 We observe that Lincoln traffic is higher than Holland tunnel traffic by calculating average traffic volume per minute for each of the tunnels using the provided data. We conclude with 68.54 mean volume per minute for Lincoln and 67.71 mean volume per minute for the Holland tunnel. This seems to indicate that Lincoln traffic is higher than Holland traffic. But is it? Or is it just random deviation? Perhaps if we took more measurements, the trend would be reversed? This is where the permutation test comes handy. First, let us talk about the null hypothesis and the alternative hypothesis. Null hypothesis for the Lincoln-Holland tunnel observation is that, not surprisingly, there is no difference in traffic between the two tunnels. The alternative hypothesis states that Lincoln tunnel is more busy than Holland tunnel. Does observed data (observed traffic difference) provide us with enough evidence to reject null hypothesis and in fact support the alternative hypothesis? To answer this question we need to decide whether the observed result is reasonably likely to come up randomly under the condition that NULL hypothesis holds. How likely it is that observed difference (D=0.83) comes randomly? Permutation test helps us to estimate the chance that D=0.83 will come up randomly under the condition that traffic in Holland and Lincoln tunnels is equal. In each permutation of the permutation test we randomly scramble the traffic table once. Permutation test is run many times, typically 10,000, even 100,000 times, and each permutation simulates a random process by simply reassigning the traffic volume values randomly between tunnels. The numbers of traffic measurements in Holland and Lincoln tunnels respectively remain the same. Existing values are scrambled though - breaking any relationship between volume numbers and tunnel names. Each permutation is like rolling a dice. How often will this random process produce the result which is at least as extreme as D=0.83 that we have observed? The less often it happens the more likely it is that what we have observed is NOT random. For example, if we can get our observed result only 3 times in 1000 rolls of a dice (permutation test) it means that with probability of 99.7% our observed result cannot be random. Permutation test provides a palpable experience of randomness. Just roll the dice many times and see how often you can get the observed result or more. If you can get D&gt;0.83 relatively often (above what is called significance level usually it is at least 5% of the time), then you cannot reject the null hypothesis. In other words the conclusion that your observation appeared RANDOMLY. Otherwise, we can conclude that observation was not random - and we reject the null hypothesis. Notice, that every time we run the permutation test function we may get slightly different p-values. This is because permutations are random. The more times we run a permutation test, the closer it will approximate the “real” p-value. Snippet 6.1 shows permutation test results for the Traffic data set. Another test which is often used for difference of means hypothesis testing is the z-test. It is described very well in the attached link to the Khan Academy lecture. Here we run z-test function in one of the following snippets. 6.2 Snippet 1: Permutation test The following snippet 6.2 shows the code for hypothesis test of difference of means. Is the mean traffic (VOLUME_PER_MINUTE) in the Holland tunnel bigger than mean traffic (VOLUME_PER_MINUTE) in the Lincoln? Do this in your R studio, since we cannot install our package in data camp service we are using to run the code snippets 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 6.3 Snippet 2: z-test Null Hypothesis - Traffic in Holland tunnel is the same as traffic in Lincoln tunnel. Alternative Hypothesis - Traffic in the Holland Tunnel is larger than traffic in the Lincoln tunnel. In the snippet 6.3 we end up calculating the p-value which leads to rejection of Null hypothesis (good news for data scientist, bad for the sceptic). Indeed, p-value is less than the significance level of 5%. This means, that under null hypothesis it is extremely unlikely (less than 5% chance) to see the result which is at least as big as the observed difference of means. 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 6.4 Snippet 3: Make your own data and see how p-value changes For students familiar with basic descriptive statistics (mean, standard deviation)we build a synthetic data set ourselves and see how difference of means and difference of standard deviations affects the p-value. We will build our two distributions ourselves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. Build the data frame with two attributes: Cat and Val, using rnorm() function. Our null hypothesis is that Group A and Group B have identical mean(Val). The alternative hypothesis is that the mean(Val) for Group B is higher than mean(Val) for Group A. We will change the mean and standard deviation of the data distributions for Group A and Group B and see how these changes affect the p-value. We will first use a permutation test and a single-step permutation test (just to illustrate what happens each single step when we run a permutation test). Then we finish off with the z-test. 6.4.1 Permuation test Exercise - How p-value is affected by difference of means and standard deviations We will build our two distributions ourseleves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use permutation test (can be z-test as well) to test difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. Build the data frame with two attributes: Cat and Val, using rnorm() function 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 6.4.2 One permutation at a time eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0cmFmZmljPC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvVHJhZmZpYzIwMjIuY3N2JylcblxucmFuTnVtIDwtIHNhbXBsZSgxOm5yb3codHJhZmZpYyksbnJvdyh0cmFmZmljKSlcbnJhbk51bVsxOjVdXG5cblZPTFVNRV9QRVJfTUlOVVRFPC10cmFmZmljJFZPTFVNRV9QRVJfTUlOVVRFW3Jhbk51bV1cblRVTk5FTDwtdHJhZmZpYyRUVU5ORUxcblxuUGVybXV0ZWRfdHJhZmZpYzwtZGF0YS5mcmFtZShUVU5ORUwsIFZPTFVNRV9QRVJfTUlOVVRFKVxuXG5tZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywgXSRWT0xVTUVfUEVSX01JTlVURSkgLW1lYW4odHJhZmZpY1t0cmFmZmljJFRVTk5FTD09J0hvbGxhbmQnLCBdJFZPTFVNRV9QRVJfTUlOVVRFKVxuXG5tZWFuKFBlcm11dGVkX3RyYWZmaWNbUGVybXV0ZWRfdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywgXSRWT0xVTUVfUEVSX01JTlVURSktbWVhbihQZXJtdXRlZF90cmFmZmljW1Blcm11dGVkX3RyYWZmaWMkVFVOTkVMPT0nSG9sbGFuZCcsIF0kVk9MVU1FX1BFUl9NSU5VVEUpIn0= 6.4.3 z-test How p-value is affected by difference of means and standard deviations. We will build two distributions ourselves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. You can do it by changing values of mean and standard deviation in the rnorm() function. Clearly the further apart the mean values are - the lower the p-value. But how do standard deviations affect the p-value? See for yourself. Build the data frame with two attributes: Cat and Val, using rnorm() function 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 6.5 Additional References Hypothesis Testing Permutation Test Khan Academy Video https://www.khanacademy.org/math/statistics-probability/significance-tests-confidence-intervals-two-samples/comparing-two-means/v/hypothesis-test-for-difference-of-means https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/p-value/ http://www.z-table.com/ https://www.statisticshowto.com/probability-and-statistics/z-score/ https://sixsigmastudyguide.com/z-scores-z-table-z-transformations/ "],["Chisquare.html", "Section: 7 🔖 Test of Independence 7.1 Introduction 7.2 Snippet 1 7.3 Snippet 2 7.4 Snippet 3 7.5 Snippet 4 7.6 Additional Reference", " Section: 7 🔖 Test of Independence 7.1 Introduction We would like to test if a student’s final grade in Professor Moody’s class depends on the student’s major. The null hypothesis in this case is the hypothesis of independence. Independence means that the distribution of final grades is the same no matter what the major is. The alternative hypothesis is that the distribution of final grades changes from major to major, rejecting the null hypothesis of independence. Notice that we do not specify how the grades depend on students’ majors. Do CS students get better grades than Psychology majors? Do Economics majors get lower grades than Statistics majors? We do not care about this. We are only testing here whether there is a relationship between Major and the final grade distribution. We will describe the permutation test which scrambles our data randomly in such a way that any relationship between grades and major (if it ever existed) is broken. We will run a permutation test a large number of times - possibly tens of thousands of times. Then we will determine how likely it is to randomly obtain the observed result. But what is the observed result? It is a bit more complex than the difference of means which were observing the difference of means hypothesis testing. The observed result in our case is calculated by so called chi-square statistic which is calculated on the contingency table, table(moody$Grade, moody$Major) To explain it, let us first define two tables: The observed contingency table and the expected contingency table. It is calculated by table() function. OBSERVED CONTINGENCY TABLE Grade/Major CS Economics Psychology Statistics A 46 54 69 42 B 46 12 2 35 C 51 33 30 34 D 41 37 29 34 F 108 99 99 99 The second table we need is called the expected table. This is the hypothetical table of the relationship between grade and major, assuming grade and major are completely independent. Such a table would be the result of the same distribution of grades for each of the majors. Notice that we 1000 students in the data set the expected table (i.e. table which have grades completely independent from majors) would have the same distribution of grades for each major that over all students - which is shown by the TOTAL column. EXPECTED CONTINGENCY TABLE Grade/Major CS Economics Psychology Statistics A 61 50 48 51 B 28 22 22 23 C 43 35 34 36 D 41 33 32 34 F 118 96 93 99 TOTAL 292 236 229 244 1000 We kept fractions - although these are number of students - therefore would have to be rounded up to integers The chi-square formula calculates the “distance” between the observed contingency table and the expected contingency table. \\[\\begin{equation} \\sum \\frac{(O_i - E_i)^2}{E_i}\\\\ \\text{where:}\\\\ \\text{O = observed values}\\\\ \\text{E = expected values}\\\\ \\end{equation}\\] For the two tables above the, \\[\\begin{equation} \\sum \\frac{(O_i - E_i)^2}{E_i} = 60.03\\\\ \\end{equation}\\] To evaluate how far off is this observed result assuming that Grades are independent from Major, we run a permutation test which scrambles Grades and Majors randomly and every time computes the chi-square formula with the new observed table (the expected table is always the same). Then, we see how many times out of, say 10,000 iterations of permutation test we obtain a result larger than the observed result of 60.03? This is the p-value. Permutation test for independence hypothesis gives us again a better feeling about the impact of randomness and whether the observed chi-square result for “similarity” of grade distributions for different majors can be obtained randomly. In the following snippet we run the chisq test which is based on the so-called chi square distribution. Here we simply show you a function which can calculate p-value, just like the z-test function does. The explanation of the chi-square test is provided in attached link to the excellent Khan Academy video. Permutation tests in both cases of difference of means and independence hypotheses give a better intuitive sense of how we answer the question - can the observed result be obtained randomly? Notice that the independence test is looking globally at two vectors and whether one depends on another. If we wanted to be more specific and know if psychology majors are more likely to get an A than CS majors, we can frame this as a difference of means hypothesis. Testing this hypothesis will be using the difference of means of frequencies of A’s among CS majors and psychology majors. This could be done again by permutation test in section 8 or the z-test. 7.2 Snippet 1 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJFeHBlY3RlZCA8LW1hdHJpeChjKDIwMCw0MjAsMTgwLCA0MCwxMjAsNDApLCBucm93PTMsIG5jb2w9Milcbk9ic2VydmVkPC1tYXRyaXgoYygyMDAsNDIwLDE4MCwzNSwxMjAsNDUpLCBucm93PTMsIG5jb2w9MilcbkV4cGVjdGVkXG5PYnNlcnZlZFxuY2hpc3EudGVzdChPYnNlcnZlZCkifQ== 7.3 Snippet 2 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZHBseXIpXG5vcHRpb25zKHdhcm49LTEpXG5jaGlfdGVzdCA8LSBmdW5jdGlvbihkYXRhLGNvbDEsY29sMixpdGVyKSB7XG4gIFxuICBkZiA8LSBkYXRhLmZyYW1lKGRhdGEpXG4gIHZhbHM8LSB1bmlxdWUoZGZbW2NvbDJdXSlcbiAgbm9fcm93cyA8LSBucm93KGRmKVxuICBkdCA8LSB0YWJsZShkZltbY29sMV1dLCBkZltbY29sMl1dKVxuICByZXMgPC0gY2hpc3EudGVzdChkdClcbiAgcmVhbF9hbnMgPC0gcmVzJHN0YXRpc3RpY1xuICBwX3ZhbHVlIDwtIHJlcyRwLnZhbHVlXG4gIGFuc192ZWMgPC0gdmVjdG9yKClcbiAgZm9yICh4IGluIDE6aXRlcil7XG5cbiAgICBuZXdfZGF0YSA8LSBzYW1wbGUoeD12YWxzLHNpemU9bm9fcm93cyxyZXBsYWNlID0gVFJVRSlcblxuICAgIGR0X25ldyA8LSB0YWJsZShkZltbY29sMV1dLCBuZXdfZGF0YSlcblxuICAgIHJlc19uZXcgPC0gY2hpc3EudGVzdChkdF9uZXcpXG5cbiAgICBhbnNfdmVjIDwtIGFwcGVuZChhbnNfdmVjLHJlc19uZXckc3RhdGlzdGljKVxuICB9XG4gIGhpc3QoYW5zX3ZlYyxtYWluPVwiUGVybXV0YXRpb24gVGVzdCBmb3IgQ2hpLVNxdWFyZVwiLHhsYWI9XCJDaGktU3F1YXJlIFZhbHVlc1wiLGJyZWFrcyA9IDEwMClcbiAgcHJpbnQocmVhbF9hbnMpXG4gIGFibGluZSh2PXJlYWxfYW5zLGNvbD1cImJsdWVcIixsd2Q9MilcbiAgcmV0dXJuIChwX3ZhbHVlKVxufSIsInNhbXBsZSI6ImQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW9vZHlNYXJjaDIwMjJiLmNzdlwiKVxuaGVhZChkKVxuXG5jaGlfdGVzdChkLFwiTWFqb3JcIixcIkdyYWRlXCIsMTAwMCkifQ== 7.4 Snippet 3 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxubW9vZHkkSU48LSdPdXRfU2xpY2UnXG5tb29keVttb29keSRET1pFU19PRkY9PSduZXZlcicgJiBtb29keSRURVhUSU5HX0lOX0NMQVNTPT0nYWx3YXlzJywgXSRJTjwtJ0luX1NsaWNlJ1xuZDwtdGFibGUobW9vZHkkR1JBREUsIG1vb2R5JElOKVxuZFxuY2hpc3EudGVzdChkKSJ9 7.5 Snippet 4 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcbmRhdGE8LXRhYmxlKG1vdmllcyRjb250ZW50LCBtb3ZpZXMkZ2VucmUpXG5jaGlzcS50ZXN0KGRhdGEpIn0= 7.6 Additional Reference Chi Square Khan Academy Video https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-goodness-fit/v/chi-square-statistic "],["Multiple_Hypothesis.html", "Section: 8 🔖 Multiple Hypothesis Testing 8.1 Introduction 8.2 Additional References", " Section: 8 🔖 Multiple Hypothesis Testing 8.1 Introduction We often consider multiple possible hypotheses in our search for discovery to find one with lowest possible p-value. Consciously or subconsciously we are engaging, what is often called, p-value hunting. We have to be very careful! We may “discover” what is simply random even if we correctly calculate p-value and compare it with the significance level. It is very important to learn about multiple hypothesis traps very early in the process of learning data science. For example assume that we are looking for associations between sales of individual items in a supermarket. Does bread sell with butter? Does coffee sell with spring water? There is an exponential number of possible combinations (N choose 2 to be exact, where N is the number of items). For each such pair we perform hypothesis testing. If one test is performed at the 5% significance level and the corresponding null hypothesis is true, there is only a 5% chance of incorrectly rejecting the null hypothesis. However, if 100 tests are each conducted at the 5% significance level and all corresponding null hypotheses are true, the expected number of incorrect rejections (also known as false positives or Type I errors) is 5. If the tests are statistically independent from each other, the probability of at least one incorrect rejection is approximately 99.4%. Thus, we will almost surely find one false positive! In other words we will be fooled by data. Bonferroni correction is a method to counteract the multiple hypothesis (often called multiple comparison problem. Make it harder to reject null hypotheses by dividing the significance level by number of hypotheses. The Bonferroni correction compensates for that increase by testing each individual hypothesis at a significance level of α / m where m is the number of hypotheses. For example, if a trial is testing me = 20 hypotheses with a desired α = 0.05, then the Bonferroni correction would test each individual hypothesis at \\[\\begin{equation} \\alpha = \\frac{0.05} {20} = 0.0025 \\end{equation}\\] Thus, there is a very simple remedy for multiple hypothesis traps. Just divide the significance level by the number of (potential) hypotheses tested. This will make it harder, often much much harder to reject the null hypothesis and yell Eureka! Critics say that in fact Bonferroni correction is too conservative and too “pro-null” and tough on alternative hypotheses to be acceptable. The unwanted side effect of Bonferroni correction is that we may fail to reject the null hypothesis too often. Bonferroni correction makes discovery sometimes too hard, making data scientists too conservative and accepting null hypothesis when they should be rejected. It may also be the case that even Bonferroni correction will not protect us, as we will show in our example below. But at least we will be much less likely to make fools of ourselves coming with false discoveries leading potentially to very wrong business decisions. There are other less conservative methods of correcting for multiple hypotheses - such as the Benjamini-Hochberg method described in the attached slides. We illustrate the p-value hunt in snippet 8.1 below. It is based on synthetic data set showing summer temperatures in New Jersey townships. Table below describes the data set based on hypothetical temperature readings in various municipalities of New Jersey over summer. Is one city experiencing higher average temperatures than another? Can we find such a pair of cities? This is the ultimate p-value hunt. Let’s compare townshiships pair by pair, until we find a pair with sufficiently large differences of mean temperatures and sufficiently low p-value. Careful! You may come up with false discovery if you do not correct for multiple hypotheses! The 8.1.1 shows several permutation tests for different pairs of townships and difference of means of temperatures hypothesis test. Two of four pairs show p–values less than customary significance level of 5%. Should we then reject the null hypothesis and conclude that indeed Ocean Grove is warmer than New Brunswick and that New Brunswick is warmer than Holmdel? Indeed, both pairs result in p-values significantly lower than 5%. If we incorrectly disregard the number of hypotheses considered, we may come to wrong conclusions supporting these two alternative hypotheses. But there are around 20 townships in the Temp data set. Thus there are around 200 possible hypotheses (200 pairs of townships) which we may consider in our p-value hunt. If we apply Bonferroni correction for N=200, the significance level will be 200 times lower, instead of 5%, it will be 0.025%. None of the two hypotheses (Ocean Grove vs New Brunswick and New Brunswick vs Holmdel) meets the new significance level. Indeed in both cases p-values are significantly larger than 0.025%. Thus, for none of the four pairs we can reject null hypotheses. Now we can disclose that we have created our Temp data set completely randomly - assigning random temperatures between 50 and 100 degrees to each township. Thus, without Bonferroni coefficient we would be fooled by data, not once, but twice in our four tests. We would find a trend when it does not exist - it is simply random deviation. It turns out however that even Bonferroni correction is not sufficient to protect us against incorrectly rejecting null hypothesis. Indeed, for Red Bank and Holmdel, we conclude that Red Band is warmer than Holmdel with p-value of 0.01%! (see the last permutation test in the snippet 1). This p-value falls even below significance level adjusted with Bonferroni correction (0.025%). It only shows that dealing with multiple hypotheses is a risky adventure. We may end up being fooled by data even when we apply Bonferroni correction. But at least we are less likely to fall into the trap of multiple hypotheses when we apply Bonferroni Correction. Download: Tempratures.csv Table 8.1: Snippet of Temperature Dataset Township Temprature 1194 Highland Park 82 1270 Trenton 80 359 Highland Park 67 1180 Asbury Park 71 692 Atlantic City 85 294 Highland Park 68 1991 Asbury Park 98 1594 Atlantic City 100 1274 Passaic 70 1144 Red Bank 66 The Temp data set assigned random temperatures between 50 and 100 degrees to around 20 townships in New Jersey. Without Bonferroni correction we would have to incorrectly reject the null hypothesis in the last three permutation tests (3rd, 4th and 5th) tests. We will still reject null hypothesis (and have false positive discovery) in the last, 5th case. Indeed, the 5th p-value is 0.0002 which is less than the significance level after Bonferroni correction (0.00025). 8.1.1 Snippet 1 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 8.2 Additional References Multiple Hypothesis Testing https://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/ "],["bayesian_reasoning.html", "Section: 9 🔖 Bayesian Reasoning 9.1 Introduction 9.2 Snippet 1: Covid Odds after positive Home Test. 9.3 Snippet 2: What are the odds that an ‘F’ student is a freshman? 9.4 Snippet 3: What are the odds that a ‘A’ student with the score less than 80 is a psychology major? 9.5 Additinal Reference", " Section: 9 🔖 Bayesian Reasoning 9.1 Introduction Bayesian Reasoning and Bayesian theorem are fundamental instruments to use both in data science as well as in real, everyday life. They are definitely part of data literacy and should be widely taught, especially among future doctors, lawyers and politicians. In this section we will explain why Bayesian reasoning is so important and also teach the most simple and intuitive formulation of Bayesian theorem - the Odds formulation. Bayesian theorem is a calming tool - the chances of bad things happening are lower than expected! This is why Bayes helps pessimists. Take a situation at a doctor’s office when a patient learns that a medical test for some potentially serious condition came positive. The doctor believes the test and the test is almost 100% accurate. Should the patient despair? Not so fast. Bayes theorem allows the patient to ask the doctor some important questions. In bayesian reasoning we distinguish between two concepts = observation and belief. Belief is something unknown, Observation is known. We use observation to modify the odds (probability) of the belief from prior odds (before we learned about observation) and the posterior odds (after we learned about observation). For example a patient taking a covid test is concerned about having covid. But s/he does not know whether they have covid. Thus “having covid” is a belief. Test result is an observation (positive or negative). Given the prior odds of covid (say 1:100), and positive covid test what are the posterior odds of covid? Bayesian theorem tells us how to compute posterior odds from prior odds, given the observation. w Odds formulation of Bayesian theorem states; \\[\\begin{equation} \\text{POSTERIOR ODDS = LIKELIHOOD RATIO * PRIOR ODDS} \\\\ \\textbf{Prior odds} \\text{- odds for the belief before observation (evidence)}\\\\ \\textbf{Likelihood ratio} \\text{- effect of observation, evidence. Can be larger or smaller than 1!!}\\\\ \\textbf{Posterior odds} \\text{- New odds with observation(evidence) taken under consideration.}\\\\ \\end{equation}\\] Let B a belief and O be an observation, then \\[\\begin{equation} \\textbf{Prior odds} - \\frac{P(B)}{P(\\sim B)}\\\\ \\textbf{Likelihood ratio} - \\frac{P(O|B)}{P(O|\\sim B)}\\\\ \\textbf{Posterior odds} - \\frac{P(B|O)}{P(\\sim B|O)} \\end{equation}\\] Let’s discuss the multiplier – likelihood ratio in more detail. It is the red colored part of the bayes Theorem: \\[\\begin{equation} \\frac{P(B|O)}{P(\\sim B|O)} = \\frac{P(O|B)}{P(O|\\sim B)} * \\frac{P(B)}{P(\\sim B)} \\\\ \\text{The red colored ratio is the ratio of true positive and false positive,}\\\\ \\text{P(O|B) – True positive} \\\\ \\text{P(O|$\\sim$ B) – False positive} \\end{equation}\\] True positive is the conditional probability of seeing the observation given that our belief is true. In our medical example it is the probability of testing positive for covid, given that in fact we have covid. False positive, on the other hand, is the probability of observation under condition that the belief is not true. For example in our case that covid test comes positive even when we do not have covid. In real life False positives are often overlooked. And this is the critical question we should ask the doctor or health professional who administers any test. What is the false positive of this test? Since this is what we divide the true positive by. Even if the true positive is 99.9% (almost sure), if the false positive is, say 20% - the likelihood ratio is around 5. In such a situation, a positive test increases the odds of having covid just 5 times. If prior odds of covid are 1:100, the posterior odds of covide after such a positive test are just 5:100, still minimal!. Even if a false positive was 10%, the likelihood ratio of 10, would increase odds of covid 10 fold, to just 1:10 and false positive of 5%, would result in a likelihood ratio of 20 - still leading to higher odds of NOT having covid than having it! This is why false positives are so critical. IBut the main question that Bayes teaches us to ask is what are the prior odds. Since if prior odds are very small (we are testing a really rare condition) then the likelihood ratio would have to be really large to make posterior odds significant. For example if prior odds are one in a million, we need a likelihood ratio of more than half a million to actually make posterior odds better than proverbial fifty - fifty. Hence to main questions we should ask our doctor upon hearing that the test results are positive are: What are the prior odds of the disease? What is the false positive of the test (since we assume that the true positive of the test would usually be close to 100%)? In the following snippets we show how to calculate the posterior odds, while being tested for a disease and then closer to our data puzzles, how to calculate the posterior odds of getting an A in class, when scoring more than 85%.In all these situations we begin with identifying what is belief (the unknown), what is the observation (the known) and we use the snippets by plugging in some assumed values of prior odds, as well as true positives and false positives. 9.2 Snippet 1: Covid Odds after positive Home Test. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjQmVsaWVmID0gXCJIYXZlIENvdmlkXCJcbiNPYnNlcnZhdGlvbiA9IENvdmlkIFRlc3RcbiNIb3cgbXVjaCB0aGUgcHJvYmFiaWxpdHkgb2YgaGF2aW5nIGNvdmlkIGluY3JlYXNlcyB1cG9uIHBvc2l0aXZlIENPVklELXRlc3Q/XG4jV2UgdXNlIHRoZSBvZGRzIGZvcm11bGF0aW9uIG9mIEJheWVzaWFuIFRoZW9yZW1cbiMgd2UgYmVnaW4gd2l0aCBwcmlvciBvZGRzIG9mIGhhdmluZyBDb3ZpZDogIFAoQ292aWQpLygxLVAoQ292aWQpXG5QcmlvckhhdmVDb3ZpZDwtMC4wMVxuUHJpb3JDb3ZpZE9kZHM8LVByaW9ySGF2ZUNvdmlkLygxLVByaW9ySGF2ZUNvdmlkKVxuUHJpb3JDb3ZpZE9kZHNcbiNUcnVlIHBvc2l0aXZlOiAgUHJvYmFiaWxpdHkgb2YgaGF2aW5nIHBvc2l0aXZlIENvdmlkIHRlc3Qgd2hlbiBoYXZpbmcgY292aWQgID0gUChQb3NpdGl2ZUNvdmlkVGVzdHxIYXZlQ292aWQpXG5UcnVlUG9zaXRpdmU8LTAuOTlcbiNGYWxzZSBwb3NpdGl2ZSA9IFByb2JhYmlsaXR5IG9mIGhhdmluZyBwb3NpdGl2ZSBDb3ZpZCB0ZXN0IHdoZW4gbm90IGhhdmluZyBjb3ZpZCA9IFAoUG9zdGl2ZUNvdmlkVGVzdC9Eb05vdEhhdmVDb3ZpZClcbkZhbHNlUG9zaXRpdmU8LTAuMDAxXG5MaWtlbGlob29kUmF0aW88LVRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlXG5Qb3N0ZXJpb3JDb3ZpZE9kZHM8LUxpa2VsaWhvb2RSYXRpbypQcmlvckNvdmlkT2Rkc1xuUG9zdGVyaW9ySGF2ZUNvdmlkPC0gUG9zdGVyaW9yQ292aWRPZGRzLygxK1Bvc3RlcmlvckNvdmlkT2RkcylcblBvc3RlcmlvckhhdmVDb3ZpZCJ9 9.3 Snippet 2: What are the odds that an ‘F’ student is a freshman? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3YnKVxuI0JlbGllZiAtIFN0dWRlbnQgaXMgYSBmcmVzaG1hblxuI09ic2VydmF0aW9uIC0gRmFpbGVkIHRoZSBjbGFzc1xuUHJpb3I8LW5yb3cobW9vZHlbbW9vZHkkU2VuaW9yaXR5ID09J0ZyZXNobWFuJyxdKS9ucm93KG1vb2R5KVxuUHJpb3JcblByaW9yT2Rkczwtcm91bmQoUHJpb3IvKDEtUHJpb3IpLDIpXG5Qcmlvck9kZHNcblRydWVQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRHcmFkZT09J0YnICYgbW9vZHkkU2VuaW9yaXR5PT0nRnJlc2htYW4nLF0pL25yb3coXG4gIG1vb2R5W21vb2R5JFNlbmlvcml0eSA9PSdGcmVzaG1hbicsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRHcmFkZT09J0YnJiBtb29keSRTZW5pb3JpdHkgIT0nRnJlc2htYW4nLF0pL25yb3cobW9vZHlbbW9vZHkkU2VuaW9yaXR5ICE9J0ZyZXNobWFuJyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5yb3VuZChQb3N0ZXJpb3IsMikifQ== 9.4 Snippet 3: What are the odds that a ‘A’ student with the score less than 80 is a psychology major? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjQmVsaWVmIC0gd2hhdCB3ZSBkbyBub3Qga25vdy4gI0lzIGEgc3R1ZGVudCBhIHBzeWNob2xvZ3kgI21ham9yP1xuI09ic2VydmF0aW9uID0gd2hhdCB3ZSBkbyAja25vdy4gVGhleSBnb3QgYW4gQSBhbmQgbGVzcyAjdGhhbiA4MCBpbiBzY29yZVxuXG5tb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3YnKVxuUHJpb3I8LW5yb3cobW9vZHlbbW9vZHkkTWFqb3IgPT0nUHN5Y2hvbG9neScsXSkvbnJvdyhtb29keSlcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3cobW9vZHlbbW9vZHkkU2NvcmUgPDgwICYgbW9vZHkkR3JhZGU9PSdBJyYgbW9vZHkkTWFqb3I9PSdQc3ljaG9sb2d5JyxdKS9ucm93KG1vb2R5W21vb2R5JE1ham9yPT0nUHN5Y2hvbG9neScsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRTY29yZSA8ODAgJiBtb29keSRHcmFkZT09J0EnJiBtb29keSRNYWpvciE9J1BzeWNob2xvZ3knLF0pL25yb3cobW9vZHlbbW9vZHkkTWFqb3IhPSdQc3ljaG9sb2d5JyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5Qb3N0ZXJpb3IifQ== 9.5 Additinal Reference Bayesian Reasoning "],["FreeStyle.html", "Section: 10 🔖 Data puzzles 10.1 Strange grading methods of Professor Moody Data Puzzle 10.2 How to predict a good party? Data puzzle 10.3 When election is truly local - data puzzle 10.4 Secrets of good sleep Data Puzzle 10.5 Let’s go to the movies: Data Puzzle 10.6 When canvas goes wild data puzzle 10.7 Very local minimarket data puzzle 10.8 Airbnb data puzzle 10.9 Titanic data puzzle 10.10 Addiotional Reference", " Section: 10 🔖 Data puzzles Data Puzzles are synthetically generated datasets with some embedded patterns. Patterns have various forms from relationships between attributes to rules of the form “if condition then value” between specific attribute-value pairs. These patterns are stochastic and embedded in datasets using DataMaker - our Data Puzzle Generation Tool. We use data puzzles extensively in the class assignments. These range from data exploration and plotting through hypothesis testing to prediction and machine learning. After the assignment is completed we reveal the data secrets - the patterns which were embedded by DataMaker. Students do not have to find exactly the embedded patterns, often they find related patterns which makes the “game” even more fun. In the following we provide the list of data puzzles along with the underlying data sets. Using DataMaker we change the patterns and even data sets from academic year to academic year.. We can also provide data puzzles of different levels of difficulty from the one star (easy) to five star (most difficult) ones. 10.1 Strange grading methods of Professor Moody Data Puzzle Download: moody2022_new.csv How to get a good grade in Professor Moody’s class? Professor Moody does not give final grades just on the basis of your total score alone. Our data shows that two students with the same total score may get widely varying final grades. Can you believe that you can even fail his class with a score as high as 82%? This is outrageous, isn’t it? DataMaker has generated thousands of tuples which in addition to the total score and final grade also store bizarre information about student behaviors in the class - do they often doze off? Does a student text a lot? Does s/he ask a lot of questions? Does it help if you ask a lot of questions? Does it hurt if you doze off a lot? Comment: There is a series of Professor Moody’s puzzles which we have used over the years. We have used different attributes including student’s major, , seniority, class participation etc. Table 10.1: Snippet of Moody Dataset SCORE GRADE DOZES_OFF TEXTING_IN_CLASS PARTICIPATION 21.33 F never never 0.29 71.57 C always rarely 0.11 90.11 A always never 0.26 31.52 D sometimes rarely 0.03 95.94 A always rarely 0.21 10.1.1 Practice Snippets 10.1.1.1 Snippet 1: Get familiar with the data set eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5jb2xuYW1lcyhtb29keSlcbnN1bW1hcnkobW9vZHkpXG51bmlxdWUobW9vZHkkR1JBREUpXG5ucm93KG1vb2R5KVxudW5pcXVlKG1vb2R5JERPWkVTX09GRilcbnVuaXF1ZShtb29keSRURVhUSU5HX0lOX0NMQVNTKVxudGFibGUobW9vZHkkR1JBREUpXG50YWJsZShtb29keSRET1pFU19PRkYpXG50YWJsZShtb29keSRURVhUSU5HX0lOX0NMQVNTKVxudGFwcGx5KG1vb2R5JFNDT1JFLCBtb29keSRHUkFERSwgbWVhbilcbnRhcHBseShtb29keSRQQVJUSUNJUEFUSU9OLCBtb29keSRHUkFERSwgbWVhbilcbnRhcHBseShtb29keSRTQ09SRSwgbW9vZHkkRE9aRVNfT0ZGLCBtZWFuKVxudGFwcGx5KG1vb2R5JFBBUlRJQ0lQQVRJT04sIG1vb2R5JERPWkVTX09GRiwgbWVhbilcbnRhcHBseShtb29keSRTQ09SRSwgbW9vZHkkVEVYVElOR19JTl9DTEFTUywgbWVhbilcbnRhcHBseShtb29keSRQQVJUSUNJUEFUSU9OLCBtb29keSRURVhUSU5HX0lOX0NMQVNTLCBtZWFuKSJ9 10.1.1.2 Snippet 2 Q: Did you know that students who never doze off in class have more than twice as many A’s than students who sometimes doze off? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG50YWJsZShtb29keSRET1pFU19PRkYsIG1vb2R5JEdSQURFKSJ9 10.1.1.3 Snippet 3 Q: Did you know that the students who scored over 85 and still received a B almost always dozed off all the time during class? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5tb29keVsobW9vZHkkU0NPUkU+ODUpICYgKG1vb2R5JEdSQURFPT0nQicpLCBdJERPWkVTX09GRiJ9 10.1.1.4 Snippet 4 Q: What gives a higher chance of failing, texting all the time or always dozing off during class? A: Always texting in class! Almost 40% chance of failing! eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG4jUHJvYmFiaWxpdHkgb2YgZmFpbGluZyB0aGUgY2xhc3Mgd2hlbiBkb3ppbmcgb2ZmIGFsbCB0aGUgdGltZVxubnJvdyhtb29keVttb29keSRET1pFU19PRkY9PSdhbHdheXMnICYgbW9vZHkkR1JBREU9PSdGJyxdKS9ucm93KG1vb2R5W21vb2R5JERPWkVTX09GRj09J2Fsd2F5cycsXSlcbiNQcm9iYWJpbGl0eSBvZiBmYWlsaW5nIHRoZSBjbGFzcyB3aGVuICBhbHdheXMgdGV4dGluZyBpbiBjbGFzcyBcbm5yb3cobW9vZHlbbW9vZHkkVEVYVElOR19JTl9DTEFTUz09J2Fsd2F5cycgJiBtb29keSRHUkFERT09J0YnLF0pL25yb3cobW9vZHlbbW9vZHkkVEVYVElOR19JTl9DTEFTUz09J2Fsd2F5cycsXSkifQ== 10.1.1.5 Snippet 5 Q: What grade did a student who scores 39.57 and always dozed off received? A: D eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5tb29keVttb29keSRTQ09SRT09JzM5LjU3JyZtb29keSRET1pFU19PRkY9PSdhbHdheXMnLF0kR1JBREUifQ== 10.1.1.6 Snippet 6 Q: What are posterior odds that an A student never dozes off? A: Posterior Odds = 2.66 Likelihood Ratio = 4 Prior Odds =0.56 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5QcmlvcjwtbnJvdyhtb29keVttb29keSRET1pFU19PRkYgPT0nbmV2ZXInLF0pL25yb3cobW9vZHkpXG5QcmlvclxuUHJpb3JPZGRzPC1yb3VuZChQcmlvci8oMS1QcmlvciksMilcblByaW9yT2Rkc1xuVHJ1ZVBvc2l0aXZlPC1yb3VuZChucm93KG1vb2R5W21vb2R5JEdSQURFPT0nQScmIG1vb2R5JERPWkVTX09GRj09J25ldmVyJyxdKS9ucm93KG1vb2R5W21vb2R5JERPWkVTX09GRj09J2Fsd2F5cycsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb29keVttb29keSRHUkFERT09J0EnJiBtb29keSRET1pFU19PRkYhPSduZXZlcicsXSkvbnJvdyhtb29keVttb29keSRET1pFU19PRkYhPSdhbHdheXMnLF0pLDIpXG5GYWxzZVBvc2l0aXZlXG5MaWtlbGlob29kUmF0aW88LXJvdW5kKFRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlLDIpXG5MaWtlbGlob29kUmF0aW9cblBvc3Rlcmlvck9kZHMgPC1MaWtlbGlob29kUmF0aW8gKiBQcmlvck9kZHNcblBvc3Rlcmlvck9kZHNcblBvc3RlcmlvciA8LVBvc3Rlcmlvck9kZHMvKDErUG9zdGVyaW9yT2RkcylcblBvc3RlcmlvciJ9 10.1.1.7 Snippet 7 Q: Verify the hypothesis that C students have higher mean participation than F students? What is the p-value? A: Negative. Fail to reject null hypothesis that mean participations of C and F students are the same with p=0.11 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiIjaW5zdGFsbC5wYWNrYWdlcyhcImRldnRvb2xzXCIpXG4jZGV2dG9vbHM6Omluc3RhbGxfZ2l0aHViKFwiZGV2YW5zaGFnci9QZXJtdXRhdGlvblRlc3RTZWNvbmRcIilcblxuI1Blcm11dGF0aW9uVGVzdFNlY29uZDo6UGVybXV0YXRpb24oZCwgXCJDYXRcIiwgXCJWYWxcIiwxMDAwMCwgXCJHcm91cEFcIiwgXCJHcm91cEJcIikgXG5cbm1vb2R5PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMl9uZXcuY3N2XCIpXG5cblBlcm11dGF0aW9uVGVzdFNlY29uZDo6UGVybXV0YXRpb24obW9vZHksIFwiR1JBREVcIiwgXCJQQVJUSUNJUEFUSU9OXCIsMTAwMDAsIFwiQ1wiLCBcIkZcIikifQ== 10.1.1.8 Snippet 8 Q: What is the mean score of students who always doze off in class and what is the most frequent grade that they received? A: The mean score is 50.26 and the most frequent grade is D. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5tZWFuKG1vb2R5W21vb2R5JERPWkVTX09GRj09J2Fsd2F5cycsXSRTQ09SRSlcbnRhYmxlKG1vb2R5W21vb2R5JERPWkVTX09GRj09J2Fsd2F5cycsXSRHUkFERSkifQ== Great job!! You have made it this far. You are now familiar with the moody dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 10.1.3. 10.1.2 Moody Data Quiz Quiz Time 10.1.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdlwiKVxuXG5zdW1tYXJ5KG1vb2R5KSJ9 10.2 How to predict a good party? Data puzzle Download: Partyb.csv DataMaker has generated data about thousands of parties, some fun parties, others which were OK or simply boring. Your goal is to discover secrets of a fun party. Is it music? Dancing? Does the host matter? Or who was present at a party? Maybe who was NOT present at the party? All this data is stored in this data puzzle. Table 10.2: Snippet of Party Dataset Party Music Host WasThere WasNotThere CaloriesDanc 3250 Boring Techno Alex Janusz Angela 396 1405 Boring HipHop Alex Janusz Manny 174 4790 Fun None Janek Qiong Manny 267 1430 Boring Rock Alex Janusz Angela 103 2225 Boring Rock Xi Janusz Joe 213 10.2.1 Practice Snippets 10.2.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbmNvbG5hbWVzKHBhcnR5KVxubnJvdyhwYXJ0eSlcbnN1bW1hcnkocGFydHkpXG51bmlxdWUocGFydHkkUGFydHkpXG51bmlxdWUocGFydHkkTXVzaWMpXG51bmlxdWUocGFydHkkSG9zdClcbnVuaXF1ZShwYXJ0eSRXYXNUaGVyZSlcbnVuaXF1ZShwYXJ0eSRXYXNOb3RUaGVyZSlcbnRhYmxlKHBhcnR5JFBhcnR5KVxudGFibGUocGFydHkkTXVzaWMpXG50YWJsZShwYXJ0eSRIb3N0KVxudGFibGUocGFydHkkV2FzVGhlcmUpXG50YWJsZShwYXJ0eSRXYXNOb3RUaGVyZSlcbmNvbG5hbWVzKHBhcnR5KVxudGFwcGx5KHBhcnR5JENhbG9yaWVzRGFuYywgcGFydHkkUGFydHksIG1lYW4pXG50YXBwbHkocGFydHkkQ2Fsb3JpZXNEYW5jLCBwYXJ0eSRNdXNpYywgbWVhbilcbnRhcHBseShwYXJ0eSRDYWxvcmllc0RhbmMsIHBhcnR5JEhvc3QsIG1lYW4pXG50YXBwbHkocGFydHkkQ2Fsb3JpZXNEYW5jLCBwYXJ0eSRXYXNUaGVyZSwgbWVhbilcbnRhcHBseShwYXJ0eSRDYWxvcmllc0RhbmMsIHBhcnR5JFdhc05vdFRoZXJlLCBtZWFuKSJ9 10.2.1.2 Snippet 2 Q: Did you know that a party is often boring when Angela is not there? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbnRhYmxlKHBhcnR5W3BhcnR5JFdhc05vdFRoZXJlPT0nQW5nZWxhJyxdJFBhcnR5KSJ9 10.2.1.3 Snippet 3 Q: What are the odds of the party being fun when Vladimir is not there? A: PosteriorOdds = 2.91 LikelihoodRatio = 3.83 Prior Odds = 0.76 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cblByaW9yPC1ucm93KHBhcnR5W3BhcnR5JFBhcnR5ID09J0Z1bicsXSkvbnJvdyhwYXJ0eSlcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3cocGFydHlbcGFydHkkUGFydHk9PSdGdW4nJiBwYXJ0eSRXYXNOb3RUaGVyZT09J1ZsYWRpbWlyJyxdKS9ucm93KHBhcnR5W3BhcnR5JFBhcnR5PT0nRnVuJyxdKSwyKVxuVHJ1ZVBvc2l0aXZlXG5GYWxzZVBvc2l0aXZlPC1yb3VuZChucm93KHBhcnR5W3BhcnR5JFBhcnR5IT0nRnVuJyYgcGFydHkkV2FzTm90VGhlcmU9PSdWbGFkaW1pcicsXSkvbnJvdyhwYXJ0eVtwYXJ0eSRQYXJ0eSE9J0Z1bicsXSksMilcbkZhbHNlUG9zaXRpdmVcbkxpa2VsaWhvb2RSYXRpbzwtcm91bmQoVHJ1ZVBvc2l0aXZlL0ZhbHNlUG9zaXRpdmUsMilcbkxpa2VsaWhvb2RSYXRpb1xuUG9zdGVyaW9yT2RkcyA8LUxpa2VsaWhvb2RSYXRpbyAqIFByaW9yT2Rkc1xuUG9zdGVyaW9yT2Rkc1xuUG9zdGVyaW9yIDwtUG9zdGVyaW9yT2Rkcy8oMStQb3N0ZXJpb3JPZGRzKVxuUG9zdGVyaW9yIn0= 10.2.1.4 Snippet 4 Q: Verify the hypothesis that there is more dancing at Fun parties than at Boring parties? A: Positive. We reject null hypothesis that there is same amount of dancing at Fun and Boring parties with p &lt; 0.00001 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbm1lYW4ocGFydHlbcGFydHkkUGFydHk9PSdGdW4nLF0kQ2Fsb3JpZXNEYW5jKVxubWVhbihwYXJ0eVtwYXJ0eSRQYXJ0eT09J0JvcmluZycsXSRDYWxvcmllc0RhbmMpXG5cblBlcm11dGF0aW9uVGVzdFNlY29uZDo6UGVybXV0YXRpb24ocGFydHksIFwiUGFydHlcIiwgXCJDYWxvcmllc0RhbmNcIiwxMDAwMCwgXCJGdW5cIiwgXCJCb3JpbmdcIikifQ== 10.2.1.5 Snippet 5 Q: What music is the most popular at Fun parties? A: HipHop The following snippet allows us to find the most popular music, although the code just returns the frequency of music genres distribution. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbnRhYmxlKHBhcnR5W3BhcnR5JFBhcnR5PT0nRnVuJyxdJE11c2ljKSJ9 You are now familiar with the Party dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 10.2.3. 10.2.2 Party Data Quiz Quiz Time 10.2.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJwYXJ0eTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9QYXJ0eWIuY3N2XCIpXG5cbnN1bW1hcnkocGFydHkpIn0= 10.3 When election is truly local - data puzzle Download: Voting1.csv In local elections in some small towns, candidates of three local parties: Royalists, KnowNothings and Anarchists are running for the office of the mayor. DataMaker has generated a survey of thousands of town residents and their political sympathies. Data of course can not be more local, leaving global concerns such as inflation or global warming to national or state office candidates. Here, the electorate cares about issues such as “should we allow leaflowers” (all, only electric, none?), what about CBD stores in town (none, just one, no restrictions), How about liquor (should the town be dry? Or hard liqueurs only). Speed limits? (none, 10mph etc) or even more extreme - the whole town being car-free, streets open only to bicycles and pedestrians? Can we develop the profiles of voters for each of the parties? What does the anarchist electorate care about? Which party is leading among young people who do not want any speed limits in town? Table 10.3: Snippet of Voting Dataset LeafBlowers CBD GasMowers Party LiquerStores SpeedLimit Age 4513 NoRestrictions NoStores ElectircOnly KnowNothings HardLiquerOnly 25mph 44 1100 NoRestrictions NoRestrictions None KnowNothings HardLiquerOnly 10mph 52 991 NoRestrictions NoStores NoRestrictions KnowNothings HardLiquerOnly 10mph 25 1071 None NoStores ElectircOnly Royalists None NoCars 61 4626 None OneStore None Royalists None NoCars 89 10.3.1 Practice Snippets 10.3.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbmNvbG5hbWVzKHZvdGUpXG5ucm93KHZvdGUpXG5zdW1tYXJ5KHZvdGUpXG51bmlxdWUodm90ZSRMZWFmQmxvd2VycylcbnVuaXF1ZSh2b3RlJENCRClcbnVuaXF1ZSh2b3RlJEdhc01vd2VycylcbnVuaXF1ZSh2b3RlJFBhcnR5KVxudW5pcXVlKHZvdGUkTGlxdWVyU3RvcmVzKVxudW5pcXVlKHZvdGUkU3BlZWRMaW1pdClcblxudGFibGUodm90ZSRQYXJ0eSlcbnRhYmxlKHZvdGUkU3BlZWRMaW1pdClcbnRhYmxlKHZvdGUkTGVhZkJsb3dlcnMpXG50YWJsZSh2b3RlJEdhc01vd2VycykifQ== 10.3.1.2 Snippet 2 Q: Which party has the oldest constituents? A: Royalists eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhcHBseSh2b3RlJEFnZSwgdm90ZSRQYXJ0eSwgbWVhbikifQ== 10.3.1.3 Snippet 3 Q: How do voters who are against Gas Mowers vote? A: Royalists eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRMZWFmQmxvd2Vycz09J05vbmUnLF0kUGFydHkpIn0= 10.3.1.4 Snippet 4 Q: How do KnowNothings voters vote on speed limits? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRQYXJ0eT09J0tub3dOb3RoaW5ncycsXSRTcGVlZExpbWl0KSJ9 10.3.1.5 Snippet 5 Q: What is the age of the oldest voter for KnowNothings? A: 100 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbm1heCh2b3RlW3ZvdGUkUGFydHk9PSdLbm93Tm90aGluZ3MnLF0kQWdlKSJ9 10.3.1.6 Snippet 6 Q: Which party wins the most votes from supporters of no speed limits, no restrictions on CBD stores and Ban of Leaf Blowers? A: Anarchists eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRTcGVlZExpbWl0ID09J05vTGltaXRzJyZ2b3RlJENCRD09J05vUmVzdHJpY3Rpb25zJyYgdm90ZSRMZWFmQmxvd2Vycz09J05vbmUnLCBdJFBhcnR5KSJ9 10.3.1.7 Snippet 7 Q: Which party wins the most votes of supporters of HardLiquerOnly Liquor stores? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRMaXF1ZXJTdG9yZXM9PSdIYXJkTGlxdWVyT25seScsIF0kUGFydHkpIn0= 10.3.1.8 Snippet 8 Q: What are the odds that a voter older than 65 will vote for Royalists? A: Posterior Odds= 6.44 Likelihood ratio = 6.08 Prior Odds= 1.06 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cblByaW9yPC1ucm93KHZvdGVbdm90ZSRQYXJ0eSA9PSdSb3lhbGlzdHMnLF0pL25yb3codm90ZSlcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3codm90ZVt2b3RlJFBhcnR5PT0nUm95YWxpc3RzJyYgdm90ZSRBZ2U+NjUsXSkvbnJvdyh2b3RlW3ZvdGUkUGFydHk9PSdSb3lhbGlzdHMnLF0pLDIpXG5UcnVlUG9zaXRpdmVcbkZhbHNlUG9zaXRpdmU8LXJvdW5kKG5yb3codm90ZVt2b3RlJFBhcnR5IT0nUm95YWxpc3RzJyYgdm90ZSRBZ2U+NjUsXSkvbnJvdyh2b3RlW3ZvdGUkUGFydHkhPSdSb3lhbGlzdHMnLF0pLDIpXG5GYWxzZVBvc2l0aXZlXG5MaWtlbGlob29kUmF0aW88LXJvdW5kKFRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlLDIpXG5MaWtlbGlob29kUmF0aW9cblBvc3Rlcmlvck9kZHMgPC1MaWtlbGlob29kUmF0aW8gKiBQcmlvck9kZHNcblBvc3Rlcmlvck9kZHNcblBvc3RlcmlvciA8LVBvc3Rlcmlvck9kZHMvKDErUG9zdGVyaW9yT2RkcylcblBvc3RlcmlvciJ9 10.3.1.9 Snippet 9 Q: Verify the hypothesis that the average age of Anarchists voters is higher than the average age of KnowNothings voters? A: Positive. Reject of null hypothesis that average ages of Anarchists and KnowNothings voters are the same eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbm1lYW4odm90ZVt2b3RlJFBhcnR5PT0nQW5hcmNoaXN0cycsXSRBZ2UpIFxubWVhbih2b3RlW3ZvdGUkUGFydHk9PSdLbm93Tm90aGluZ3MnLF0kQWdlKSBcblxuUGVybXV0YXRpb24odm90ZSwgXCJQYXJ0eVwiLCBcIkFnZVwiLDEwMDAwLCBcIkFuYXJjaGlzdHNcIiwgXCJLbm93Tm90aGluZ3NcIikgIn0= 10.3.1.10 Snippet 10 Q: What is the most frequent position of Anarchists on the Speed Limit issue? A: “No limits” is the most frequent position of Anarchists on Speed Limit issue eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRQYXJ0eT09J0FuYXJjaGlzdHMnLF0kU3BlZWRMaW1pdCkifQ== 10.3.1.11 Snippet 11 Q: Which party wins the most votes from supporters of Electric Leaf Blowers? A: Royalists eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnRhYmxlKHZvdGVbdm90ZSRMZWFmQmxvd2Vycz09J0VsZWN0cmljT25seScsXSRQYXJ0eSkifQ== You are now familiar with the Election dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 10.3.3. 10.3.2 Election Data Quiz Quiz Time 10.3.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2b3RlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1ZvdGluZzEuY3N2XCIpXG5cbnN1bW1hcnkodm90ZSkifQ== 10.4 Secrets of good sleep Data Puzzle Download: SleepPrediction2.csv Who wouldn’t want to know the secrets of good sleep? DataMaker has created a data set which may help to find these secrets. We store the number of exercise calories burnt during the day, the amount of wimpy tea a person has drunk (in ounces), hours spent on the computer and the quality of the preceding night’s sleep. Table 10.4: Snippet of Sleep Dataset Sleep ExerciseCal OnComputer WimpyTea RoomTemp Moon LastSleep 114 Little 748 9 2Cups 66 Full Deep 1593 Shallow 75 8 ManyCups 62 Dark Deep 1049 Deep 838 10 2Cups 68 Full Shallow 1922 Shallow 30 2 1Cup 60 Half Shallow 216 Little 520 5 2Cups 67 Full Deep 10.4.1 Practice Snippets 10.4.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5jb2xuYW1lcyhzbGVlcClcbm5yb3coc2xlZXApXG5zdW1tYXJ5KHNsZWVwKVxudW5pcXVlKHNsZWVwJFNsZWVwKVxudW5pcXVlKHNsZWVwJFdpbXB5VGVhKVxudW5pcXVlKHNsZWVwJE1vb24pXG51bmlxdWUoc2xlZXAkTGFzdFNsZWVwKVxudGFibGUoc2xlZXAkU2xlZXApXG50YWJsZShzbGVlcCRXaW1weVRlYSlcbnRhYmxlKHNsZWVwJE1vb24pXG50YWJsZShzbGVlcCRMYXN0U2xlZXApIn0= 10.4.1.2 Snippet 2 Q: Is exercising more good for your sleep? A: So and so. You either have Little sleep or deep sleep, much less likely to have shallow sleep eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG50YXBwbHkoc2xlZXAkRXhlcmNpc2VDYWwsIHNsZWVwJFNsZWVwLCBtZWFuKSJ9 10.4.1.3 Snippet 3 Q: What are the odds of Deep sleep when last day’s sleep was Shallow? A: Posterior Odds= 15.27 (probability = 0.93!) Likelihood Ratio = 5.25 Prior Odds = 2.91 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5QcmlvcjwtbnJvdyhzbGVlcFtzbGVlcCRTbGVlcCA9PSdEZWVwJyxdKS9ucm93KHNsZWVwKVxuUHJpb3JcblByaW9yT2Rkczwtcm91bmQoUHJpb3IvKDEtUHJpb3IpLDIpXG5Qcmlvck9kZHNcblRydWVQb3NpdGl2ZTwtcm91bmQobnJvdyhzbGVlcFtzbGVlcCRTbGVlcD09J0RlZXAnJiBzbGVlcCRMYXN0U2xlZXA9PSdTaGFsbG93JyxdKS9ucm93KHNsZWVwW3NsZWVwJFNsZWVwPT0nRGVlcCcsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyh2b3RlW3NsZWVwJFNsZWVwIT0nRGVlcCcmIHNsZWVwJExhc3RTbGVlcD09J1NoYWxsb3cnLF0pL25yb3coc2xlZXBbc2xlZXAkU2xlZXAhPSdEZWVwJyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5Qb3N0ZXJpb3IifQ== 10.4.1.4 Snippet 4 Q: Verify hypothesis that deep sleepers spend on average more time on the computer than Shallow sleepers? A: Negative. Fail to reject null hypotheses that means are the same. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5tZWFuKHNsZWVwW3NsZWVwJFNsZWVwPT0nRGVlcCcsXSRPbkNvbXB1dGVyKVxubWVhbihzbGVlcFtzbGVlcCRTbGVlcD09J1NoYWxsb3cnLF0kT25Db21wdXRlcilcblxuUGVybXV0YXRpb24oc2xlZXAsIFwiU2xlZXBcIiwgXCJPbkNvbXB1dGVyXCIsMTAwMDAsIFwiRGVlcFwiLCBcIlNoYWxsb3dcIikifQ== 10.4.1.5 Snippet 5 Q: What is the highest Room temperature experienced by a Deep sleeper? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5tYXgoc2xlZXBbc2xlZXAkU2xlZXA9PSdEZWVwJyxdJFJvb21UZW1wKSJ9 You are now familiar with the Sleep dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 10.4.3. 10.4.2 Sleep Data Quiz Quiz Time 10.4.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJzbGVlcDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9TbGVlcFByZWRpY3Rpb24yLmNzdlwiKVxuXG5zdW1tYXJ5KHNsZWVwKSJ9 10.5 Let’s go to the movies: Data Puzzle Download: Movies2022F-4.csv Using DataMaker we have started with the imdb data set from Kaggle and embedded some patterns in it. The original data set contains data about 12,800+ movies. We have expanded this data set by DataMaker’s opinions. Yes, only DataMaker can have an opinion on each of 12,800 movies! Can you predict which movies does DataMaker love and which movies bore him so much that she quit? What movies DataMaker passionately hates (hmm is DataMaker even passionate about anything at all?). When does DataMaker agree with the imdb score? Can one predict an imdb score on the basis of a combination of DataMaker opinion (sort of super critic) and other attributes? Table 10.5: Snippet of Movies Dataset country content imdb_score Gross Budget genre 6131 USA PG 5.17 Medium Medium Comedy 352 USA PG-13 8.24 High High Family 9308 UK PG-13 5.58 Medium Medium Action 12152 Japan PG 6.19 Medium High Action 12231 USA PG-13 7.00 Low Low History 10.5.1 Practice Snippets 10.5.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuY29sbmFtZXMobW92aWVzKVxubnJvdyhtb3ZpZXMpXG5zdW1tYXJ5KG1vdmllcylcbnVuaXF1ZShtb3ZpZXMkY29udGVudClcbnVuaXF1ZShtb3ZpZXMkZ2VucmUpXG51bmlxdWUobW92aWVzJEdyb3NzKVxudW5pcXVlKG1vdmllcyRCdWRnZXQpXG5cblxudGFibGUobW92aWVzJGNvbnRlbnQpXG50YWJsZShtb3ZpZXMkZ2VucmUpXG50YWJsZShtb3ZpZXMkR3Jvc3MpXG50YWJsZShtb3ZpZXMkQnVkZ2V0KVxuXG50YXBwbHkobW92aWVzJGltZGJfc2NvcmUsIG1vdmllcyRjb250ZW50LCBtZWFuKVxudGFwcGx5KG1vdmllcyRpbWRiX3Njb3JlLCBtb3ZpZXMkY291bnRyeSwgbWVhbilcbnRhcHBseShtb3ZpZXMkaW1kYl9zY29yZSwgbW92aWVzJEdyb3NzLCBtZWFuKVxudGFwcGx5KG1vdmllcyRpbWRiX3Njb3JlLCBtb3ZpZXMkQnVkZ2V0LCBtZWFuKVxudGFwcGx5KG1vdmllcyRpbWRiX3Njb3JlLCBtb3ZpZXMkZ2VucmUsIG1lYW4pIn0= 10.5.1.2 Snippet 2 Q: What is the mean imdb of low budget comedies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxubWVhbihtb3ZpZXNbbW92aWVzJEJ1ZGdldD09J0xvdycgJiBtb3ZpZXMkZ2VucmU9PSdDb21lZHknLCBdJGltZGJfc2NvcmUpIn0= 10.5.1.3 Snippet 3 Q: What is the lowest imdb score among high budget movies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxubWluKG1vdmllc1ttb3ZpZXMkQnVkZ2V0PT0nSGlnaCcsXSRpbWRiKSJ9 10.5.1.4 Snippet 4 Q: How many low budget movies generated high gross income? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxubnJvdyhtb3ZpZXNbbW92aWVzJEJ1ZGdldD09J0xvdycgJiBtb3ZpZXMkR3Jvc3MgPT0nSGlnaCcsXSkifQ== 10.5.1.5 Snippet 5 Q: What is the least frequent genre among UK movies? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxudGFibGUobW92aWVzW21vdmllcyRjb3VudHJ5PT0nVUsnLF0kZ2VucmUsIG1vdmllc1ttb3ZpZXMkY291bnRyeT09J1VLJyxdJGNvdW50cnkpIn0= 10.5.1.6 Snippet 6 Q: Which content rating has the lowest average imdb score? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxudGFwcGx5KG1vdmllcyRpbWRiLCBtb3ZpZXMkY29udGVudCwgbWVhbikifQ== 10.5.1.7 Snippet 7 Q: Movies from which country have the smallest average imdb score? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuTUE8LWFnZ3JlZ2F0ZShtb3ZpZXMkaW1kYl9zY29yZSwgbGlzdChtb3ZpZXMkY291bnRyeSksIG1lYW4pXG5jb2xuYW1lcyhNQSk8LWMoXCJDb3VudHJ5XCIsIFwiTWltZGJcIilcbk1BPC1NQVtvcmRlcigtTUEkTWltZGIpLCBdXG5NQVsxLF0gIn0= 10.5.1.8 Snippet 8 Q: What are the odds that a High Budget Movie will have High Gross Income? A: Prior Odds = 5.59 Likelihood Ratio = 5.08 Prior Odds = 1.11 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuUHJpb3I8LW5yb3cobWFya2V0W21vdmllcyRHcm9zcyA9PSdIaWdoJyxdKS9ucm93KG1vdmllcylcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3cobW92aWVzW21vdmllcyRHcm9zcz09J0hpZ2gnJiBtb3ZpZXMkQnVkZ2V0PT0nSGlnaCcsXSkvbnJvdyhtb3ZpZXNbbW92aWVzJEdyb3NzPT0nSGlnaCcsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3NpdGl2ZTwtcm91bmQobnJvdyhtb3ZpZXNbbW92aWVzJEdyb3NzIT0nSGlnaCcmIG1vdmllcyRCdWRnZXQ9PSdIaWdoJyxdKS9ucm93KG1vdmllc1ttb3ZpZXMkR3Jvc3MhPSdIaWdoJyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzIn0= You are now familiar with the Movies dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 10.5.3. 10.5.2 Movies Data Quiz Quiz Time 10.5.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb3ZpZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW92aWVzMjAyMkYtNC5jc3ZcIilcblxuc3VtbWFyeShtb3ZpZXMpIn0= 10.6 When canvas goes wild data puzzle Download: Canvas1.csv You are all familiar with Canvas, right? This is where you look to see your grades for each assignment and exam. This is where you see the scores. However it seems that Canvas went a bit wild and unfair in this data set.One can still fail the class with the score of 82 (sounds familiar, yes, Professor Moody would do it, but Canvas? How can one get a lower grade with a higher score? Yes, Canvas was instructed by someone and your goal is to discover the grading method. How to get an A, how to pass? We know who that someone is… it is DataMaker of course. Table 10.6: Snippet of Canvas Dataset Homeworks Exams Score Grade 1117 35 30 34.5 F 1039 11 95 19.4 F 524 92 77 90.5 A 872 46 95 50.9 C 822 15 60 19.5 F 10.6.1 Practice Snippets 10.6.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxuY29sbmFtZXMoZ3JhZGVzKVxubnJvdyhncmFkZXMpXG5zdW1tYXJ5KGdyYWRlcylcbnVuaXF1ZShncmFkZXMkR3JhZGUpXG50YWJsZShncmFkZXMkR3JhZGUpXG50YXBwbHkoZ3JhZGVzJEhvbWV3b3JrcywgZ3JhZGVzJEdyYWRlLCBtZWFuKVxudGFwcGx5KGdyYWRlcyRFeGFtcywgZ3JhZGVzJEdyYWRlLCBtZWFuKVxudGFwcGx5KGdyYWRlcyRTY29yZSwgZ3JhZGVzJEdyYWRlLCBtZWFuKSJ9 10.6.1.2 Snippet 2 Q: What is the distribution of possible grades when a student’s total scores is over 80? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxudGFibGUoZ3JhZGVzW2dyYWRlcyRTY29yZSA+IDgwLF0kR3JhZGUpIn0= 10.6.1.3 Snippet 3 Q: Previous snippets showed that you can only get an A or an F with a score over 80. How can you get an F? This snippet helps to answer this question. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxudGFibGUoZ3JhZGVzW2dyYWRlcyRTY29yZSA+IDgwICYgZ3JhZGVzJEV4YW1zID40MCxdJEdyYWRlKSJ9 10.6.1.4 Snippet 4 Q: What is the worst exam score of a student with final grade A? A: 25 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxubWluKGdyYWRlc1tncmFkZXMkR3JhZGUgPT0nQScsXSRFeGFtcykifQ== 10.6.1.5 Snippet 5 Q: What are the odds of getting an A with an Exams score above 60? A: Posterior Odds = 0.45 Likelihood Ratio = 4.75 Prior Odds = 0.17 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxuUHJpb3I8LW5yb3coZ3JhZGVzW2dyYWRlcyRHcmFkZSA9PSdBJyxdKS9ucm93KGdyYWRlcylcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3coZ3JhZGVzW2dyYWRlcyRHcmFkZT09J0EnJiBncmFkZXMkRXhhbXM+NjAsXSkvbnJvdyhncmFkZXNbZ3JhZGVzJEdyYWRlPT0nQScsXSksMilcblRydWVQb3NpdGl2ZVxuRmFsc2VQb3N0aXZlPC1yb3VuZChucm93KGdyYWRlc1tncmFkZXMkR3JhZGUhPSdBJyYgZ3JhZGVzJEV4YW1zPDYwLF0pL25yb3coZ3JhZGVzW2dyYWRlcyRHcmFkZXMhPSdBJyxdKSwyKVxuRmFsc2VQb3NpdGl2ZVxuTGlrZWxpaG9vZFJhdGlvPC1yb3VuZChUcnVlUG9zaXRpdmUvRmFsc2VQb3NpdGl2ZSwyKVxuTGlrZWxpaG9vZFJhdGlvXG5Qb3N0ZXJpb3JPZGRzIDwtTGlrZWxpaG9vZFJhdGlvICogUHJpb3JPZGRzXG5Qb3N0ZXJpb3JPZGRzXG5Qb3N0ZXJpb3IgPC1Qb3N0ZXJpb3JPZGRzLygxK1Bvc3Rlcmlvck9kZHMpXG5Qb3N0ZXJpb3IifQ== 10.6.1.6 Snippet 6 Q: Verify Hypothesis that Mean exam score for B students is higher than mean exam score for C students. What is the p-value? A: Negative. We fail to reject the null hypothesis with p=0.23 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxubWVhbihncmFkZXNbZ3JhZGVzJEdyYWRlPT0nQicsXSRFeGFtcylcbm1lYW4oZ3JhZGVzW2dyYWRlcyRHcmFkZT09J0MnLF0kRXhhbXMpXG5cblBlcm11dGF0aW9uKGdyYWRlcywgXCJHcmFkZVwiLCBcIkV4YW1zXCIsMTAwMDAsIFwiQ1wiLCBcIkJcIikgIn0= 10.6.1.7 Snippet 7 Q: What is the chance of getting an A when you score less than 50 on exams? A: 0.093 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxubnJvdyhncmFkZXNbZ3JhZGVzJEdyYWRlID09J0EnICYgZ3JhZGVzJEV4YW1zIDwgNTAsXSkvbnJvdyhncmFkZXNbZ3JhZGVzJEV4YW1zIDwgNTAsXSkifQ== You are now familiar with the Canvas dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 10.6.3. 10.6.2 Canvas Data Quiz Quiz Time 10.6.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJncmFkZXM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvQ2FudmFzMS5jc3ZcIilcblxuc3VtbWFyeShncmFkZXMpIn0= 10.7 Very local minimarket data puzzle Download: HomeworkMarket2022.csv What items sell together? A small local minimarket chain (think Wawa at its early days) has a few locations in New Jersey and it sells beer, snacks, sweets. DataMaker provided the data set of several thousand of transactions in the minimarket storing what items were purchased, when they were purchased (weekday or weekend) at which location. Table 10.7: Snippet of Minimarket Dataset Beer Day Location SoftDrinks Sweets Wine Snacks 6627 Ale Weekday Edison None Twix Red None 18507 None Weekday Princeton Cola Snickers White None 15490 None Weekday Princeton Sprite Milky Way White None 6353 Ale Weekday Princeton Cola Milky Way Red None 10591 Ale Weekday Metuchen None Twix Red Crackers 10.7.1 Practice Snippets 10.7.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG5jb2xuYW1lcyhtYXJrZXQpXG5ucm93KG1hcmtldClcbnN1bW1hcnkobWFya2V0KVxudW5pcXVlKG1hcmtldCREYXkpXG51bmlxdWUobWFya2V0JExvY2F0aW9uKVxudW5pcXVlKG1hcmtldCRCZWVyKVxudW5pcXVlKG1hcmtldCRTb2Z0RHJpbmtzKVxudW5pcXVlKG1hcmtldCRTd2VldHMpXG51bmlxdWUobWFya2V0JFdpbmUpXG51bmlxdWUobWFya2V0JFNuYWNrcylcbnRhYmxlKG1hcmtldCREYXkpXG50YWJsZShtYXJrZXQkTG9jYXRpb24pXG50YWJsZShtYXJrZXQkQmVlcilcbnRhYmxlKG1hcmtldCRTb2Z0RHJpbmtzKVxudGFibGUobWFya2V0JFN3ZWV0cylcbnRhYmxlKG1hcmtldCRXaW5lKVxudGFibGUobWFya2V0JFNuYWNrcykifQ== 10.7.1.2 Snippet 2 Q: What are the odds that a customer in New Brunswick buys Lager on a weekend? A: Posterior Odds = 0.53 Likelihood Ratio = 1.08 Prior odds = 0.49 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG5QcmlvcjwtbnJvdyhtYXJrZXRbbWFya2V0JEJlZXIgPT0nTGFnZXInLF0pL25yb3cobWFya2V0KVxuUHJpb3JcblByaW9yT2Rkczwtcm91bmQoUHJpb3IvKDEtUHJpb3IpLDIpXG5Qcmlvck9kZHNcblRydWVQb3NpdGl2ZTwtcm91bmQobnJvdyhtYXJrZXRbbWFya2V0JEJlZXI9PSdMYWdlcicmIG1hcmtldCRMb2NhdGlvbj09J05ldyBCcnVuc3dpY2snICYgbWFya2V0JERheSA9PSdXZWVrZW5kJyxdKS9ucm93KG1hcmtldFttYXJrZXQkQmVlcj09J0xhZ2VyJyxdKSwyKVxuVHJ1ZVBvc2l0aXZlXG5GYWxzZVBvc2l0aXZlPC1yb3VuZChucm93KG1hcmtldFttYXJrZXQkQmVlciE9J0xhZ2VyJyYgbWFya2V0JExvY2F0aW9uPT0nTmV3IEJydW5zd2ljaycgJiBtYXJrZXQkRGF5ID09J1dlZWtlbmQnLF0pL25yb3cobWFya2V0W21hcmtldCRCZWVyIT0nTGFnZXInLF0pLDIpXG5GYWxzZVBvc2l0aXZlXG5MaWtlbGlob29kUmF0aW88LXJvdW5kKFRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlLDIpXG5MaWtlbGlob29kUmF0aW9cblBvc3Rlcmlvck9kZHMgPC1MaWtlbGlob29kUmF0aW8gKiBQcmlvck9kZHNcblBvc3Rlcmlvck9kZHNcblBvc3RlcmlvciA8LVBvc3Rlcmlvck9kZHMvKDErUG9zdGVyaW9yT2RkcylcblBvc3RlcmlvciJ9 10.7.1.3 Snippet 3 Q: What is the most frequent location of Lager purchases? A: Princeton is the most frequent location where Lager is sold eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG50YWJsZShtYXJrZXRbbWFya2V0JEJlZXIgPT0nTGFnZXInLF0kTG9jYXRpb24pIn0= 10.7.1.4 Snippet 4 Q: Is distribution of purchases of snacks among Weekend buyers of Lager in New Brunswick different from base distribution of snacks? A: yes, very different eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG5tYXJrZXQkSU48LSdPdXRfU2xpY2UnXG5tYXJrZXRbbWFya2V0JEJlZXI9PSdMYWdlcicgJiBtYXJrZXQkRGF5PT0nV2Vla2VuZCcgJiAgbWFya2V0JExvY2F0aW9uID09J05ldyBCcnVuc3dpY2snLCBdJElOPC0nSW5fU2xpY2UnXG5kPC10YWJsZShtYXJrZXQkU25hY2tzLG1hcmtldCRJTilcbmNoaXNxLnRlc3QoZCkifQ== You are now familiar with the MiniMarket dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 10.7.3. 10.7.2 MiniMarket Data Quiz Quiz Time 10.7.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtYXJrZXQ8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvSG9tZXdvcmtNYXJrZXQyMDIyLmNzdlwiKVxuXG5zdW1tYXJ5KG1hcmtldCkifQ== Now it’s time for two real data sets - the airbnb data set and titanic sinking data set, both from Kaggle. These data sets have been cleaned before, this is why we do not have to spend our time on data wrangling! 10.8 Airbnb data puzzle Download: airbnb.csv The airbnb data set (Kaggle) stores around 30,000 plus data points about airbnb prices in NYC. We have modified the original set a little bit (we can’t stop!) adding the floor where the department is located to the existing attributes such as Room type, neighbourhood_group (boroughs), specific neighborhood and price. Table 10.8: Snippet of Airbnb Dataset id name host_name neighbourhood_group neighbourhood room_type floor price 8853 12408850 1 BR in 2 BR Apt (Upper East Side) Jeffrey Paul Manhattan East Harlem Private room 16 251.4872 34535 7202612 Queen Bed, Quiet Room, Prime Williamsburg! Wes Brooklyn Williamsburg Private room 1 100.4465 26028 25293702 Cozy (☆) Room In Upper Manhattan (♥) Mamadou Manhattan Harlem Private room 16 245.6305 7864 4338177 Sleeps 4! Prime Chelsea~large 1BR Host Manhattan Chelsea Entire home/apt 16 536.4529 14521 2230762 Harlem/Morningside, charm and quiet Karen Manhattan Harlem Private room 1 265.8137 10.8.1 Practice Snippets 10.8.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5ucm93KGFpcmJuYilcbnN1bW1hcnkoYWlyYm5iKVxudW5pcXVlKGFpcmJuYiRuZWlnaGJvdXJob29kX2dyb3VwKVxudW5pcXVlKGFpcmJuYiRuZWlnaGJvdXJob29kKVxudW5pcXVlKGFpcmJuYiRyb29tX3R5cGUpXG51bmlxdWUoYWlyYm5iJGZsb29yKVxudGFibGUoYWlyYm5iJG5laWdoYm91cmhvb2RfZ3JvdXApXG50YWJsZShhaXJibmIkbmVpZ2hib3VyaG9vZClcbnRhYmxlKGFpcmJuYiRyb29tX3R5cGUpXG50YWJsZShhaXJibmIkZmxvb3IpXG50YXBwbHkoYWlyYm5iJHByaWNlLCBhaXJibmIkZmxvb3IsIG1lYW4pXG50YXBwbHkoYWlyYm5iJHByaWNlLCBhaXJibmIkbmVpZ2hib3VyaG9vZCwgbWVhbilcbnRhcHBseShhaXJibmIkcHJpY2UsIGFpcmJuYiRyb29tX3R5cGUsIG1lYW4pIn0= 10.8.1.2 Snippet 2 Q: What is the price of the cheapest entire home/apt in Tribeca? A: $284 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5taW4oYWlyYm5iW2FpcmJuYiRyb29tX3R5cGU9PSdFbnRpcmUgaG9tZS9hcHQnICZhaXJibmIkbmVpZ2hib3VyaG9vZD09J1RyaWJlY2EnLF0kcHJpY2UpIn0= 10.8.1.3 Snippet 3 Q: What is the lowest price of accommodation above the 10th floor in Manhattan? A: $184 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5taW4oYWlyYm5iW2FpcmJuYiRmbG9vciA+MTAgJmFpcmJuYiRuZWlnaGJvdXJob29kX2dyb3VwPT0nTWFuaGF0dGFuJyxdJHByaWNlKSJ9 10.8.1.4 Snippet 4 Q: What are the odds of finding a place for less than $200 in Tribeca? A: Posterior Odds = 0.09 Prior Odds = 0.86 Likelihood Ratio = 0.11 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5QcmlvcjwtbnJvdyhhaXJibmJbYWlyYm5iJHByaWNlPDIwMCxdKS9ucm93KGFpcmJuYilcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5ucm93KGFpcmJuYlthaXJibmIkcHJpY2U8MjAwJiBhaXJibmIkbmVpZ2hib3VyaG9vZD09J1RyaWJlY2EnLF0pXG5ucm93KGFpcmJuYlthaXJibmIkbmVpZ2hib3VyaG9vZD09J1RyaWJlY2EnLF0pXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3coYWlyYm5iW2FpcmJuYiRwcmljZTwyMDAmIGFpcmJuYiRuZWlnaGJvdXJob29kPT0nVHJpYmVjYScsXSkvbnJvdyhhaXJibmJbYWlyYm5iJHByaWNlPDIwMCxdKSw1KVxuVHJ1ZVBvc2l0aXZlXG5GYWxzZVBvc2l0aXZlPC1yb3VuZChucm93KGFpcmJuYlthaXJibmIkcHJpY2U+MjAwJiBhaXJibmIkbmVpZ2hib3VyaG9vZD09J1RyaWJlY2EnLF0pL25yb3coYWlyYm5iW2FpcmJuYiRwcmljZT4yMDAsXSksNSlcbkZhbHNlUG9zaXRpdmVcbkxpa2VsaWhvb2RSYXRpbzwtcm91bmQoVHJ1ZVBvc2l0aXZlL0ZhbHNlUG9zaXRpdmUsNClcbkxpa2VsaWhvb2RSYXRpb1xuUG9zdGVyaW9yT2RkcyA8LUxpa2VsaWhvb2RSYXRpbyAqIFByaW9yT2Rkc1xuUG9zdGVyaW9yT2Rkc1xuUG9zdGVyaW9yIDwtUG9zdGVyaW9yT2Rkcy8oMStQb3N0ZXJpb3JPZGRzKVxuUG9zdGVyaW9yIn0= 10.8.1.5 Snippet 5 Q: Verify hypothesis that West Village is more expensive than Upper East Side? A: Positive. Null hypothesis is rejected with the p value p &lt; 0.0001 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5QZXJtdXRhdGlvbihhaXJibmIsIFwibmVpZ2hib3VyaG9vZFwiLCBcInByaWNlXCIsMTAwMDAsIFwiV2VzdCBWaWxsYWdlXCIsIFwiVXBwZXIgRWFzdCBTaWRlXCIpIn0= You are now familiar with the Airbnb dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 10.8.3. 10.8.2 Airbnb Data Quiz Quiz Time 10.8.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvYWlyYm5iLmNzdlwiKVxuXG5zdW1tYXJ5KGFpcmJuYikifQ== 10.9 Titanic data puzzle Download: Titanic-train.csv The titanic data set (Kaggle) stores records of passengers of Titanic with attributes such as Survived, SibSp (family size), Fare, PClass (type of a cabin), Age etc. Here is a sample of data Table 10.9: Snippet of Titanic Dataset PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 193 193 1 3 Andersen-Jensen, Miss. Carla Christine Nielsine female 19 1 0 350046 7.8542 S 118 118 0 2 Turpin, Mr. William John Robert male 29 1 0 11668 21.0000 S 664 664 0 3 Coleff, Mr. Peju male 36 0 0 349210 7.4958 S 86 86 1 3 Backstrom, Mrs. Karl Alfred (Maria Mathilda Gustafsson) female 33 3 0 3101278 15.8500 S 351 351 0 3 Odahl, Mr. Nils Martin male 23 0 0 7267 9.2250 S 10.9.1 Practice Snippets 10.9.1.1 Snippet 1: Get to know your data eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1RpdGFuaWMtdHJhaW4uY3N2XCIpXG5cbmNvbG5hbWVzKHRpdGFuaWMpXG5ucm93KHRpdGFuaWMpXG5zdW1tYXJ5KHRpdGFuaWMpXG51bmlxdWUodGl0YW5pYyRQY2xhc3MpXG51bmlxdWUodGl0YW5pYyRTaWJTcClcbnVuaXF1ZSh0aXRhbmljJFNleClcbnVuaXF1ZSh0aXRhbmljJEVtYmFya2VkKVxudW5pcXVlKHRpdGFuaWMkU3Vydml2ZWQpXG50YWJsZSh0aXRhbmljJFBjbGFzcylcbnRhYmxlKHRpdGFuaWMkU2liU3ApXG50YWJsZSh0aXRhbmljJFNleClcbnRhYmxlKHRpdGFuaWMkRW1iYXJrZWQpXG50YWJsZSh0aXRhbmljJFN1cnZpdmVkKSJ9 10.9.1.2 Snippet 2 Q: What are the odds of survival of single males on Titanic? A: Posterior Odds = 0.196 Prior Odds = 0.62 Likelihood Ratio = 0.31 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvVGl0YW5pYy10cmFpbi5jc3ZcIilcblxuUHJpb3I8LW5yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0xLF0pL25yb3codGl0YW5pYylcblByaW9yXG5Qcmlvck9kZHM8LXJvdW5kKFByaW9yLygxLVByaW9yKSwyKVxuUHJpb3JPZGRzXG5UcnVlUG9zaXRpdmU8LXJvdW5kKG5yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0xJiB0aXRhbmljJFNleD09J21hbGUnICZ0aXRhbmljJFNpYlNwPT0wLF0pL25yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0xLF0pLDIpXG5UcnVlUG9zaXRpdmVcbkZhbHNlUG9zaXRpdmU8LXJvdW5kKG5yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0wJiB0aXRhbmljJFNleD09J21hbGUnJnRpdGFuaWMkU2liU3A9PTAsLF0pL25yb3codGl0YW5pY1t0aXRhbmljJFN1cnZpdmVkPT0wLF0pLDIpXG5GYWxzZVBvc2l0aXZlXG5MaWtlbGlob29kUmF0aW88LXJvdW5kKFRydWVQb3NpdGl2ZS9GYWxzZVBvc2l0aXZlLDQpXG5MaWtlbGlob29kUmF0aW9cblBvc3Rlcmlvck9kZHMgPC1MaWtlbGlob29kUmF0aW8gKiBQcmlvck9kZHNcblBvc3Rlcmlvck9kZHNcblBvc3RlcmlvciA8LVBvc3Rlcmlvck9kZHMvKDErUG9zdGVyaW9yT2RkcylcblBvc3RlcmlvciJ9 10.9.1.3 Snippet 3 Q: Verify hypothesis that survivors paid on average more for the ticker than those who did not survive? A: Positive. Null hypothesis rejected with p &lt; 0.0001 eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1RpdGFuaWMtdHJhaW4uY3N2XCIpXG5cblBlcm11dGF0aW9uKHRpdGFuaWMsIFwiU3Vydml2ZWRcIiwgXCJGYXJlXCIsMTAwMDAsIFwiMVwiLCBcIjBcIikifQ== 10.9.1.4 Snippet 4 Q: What is the probability of survival for passengers who paid more than 100 pounds for a ticket? How about those who paid less than 10 pounds? A: 0.73 for passengers who paid more than 100 pounds 0.20 for passengers who paid less than 10 pounds 0.38 for all passengers eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1RpdGFuaWMtdHJhaW4uY3N2XCIpXG5cbm5yb3codGl0YW5pY1t0aXRhbmljJEZhcmUgPjEwMCAmIHRpdGFuaWMkU3Vydml2ZWQgPT0xLF0pL25yb3codGl0YW5pY1t0aXRhbmljJEZhcmUgPjEwMCxdKVxubnJvdyh0aXRhbmljW3RpdGFuaWMkRmFyZSA8MTAgJiB0aXRhbmljJFN1cnZpdmVkID09MSxdKS9ucm93KHRpdGFuaWNbdGl0YW5pYyRGYXJlIDwxMCxdKVxubnJvdyh0aXRhbmljW3RpdGFuaWMkU3Vydml2ZWQgPT0xLF0pL25yb3codGl0YW5pYykifQ== 10.9.1.5 Snippet 5 Q: What was the chance of survival for passengers who traveled at least in a group of 3? A: Just 10%! eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L1RpdGFuaWMtdHJhaW4uY3N2XCIpXG5cbnRhYmxlKHRpdGFuaWNbdGl0YW5pYyRTaWJTcD4zLF0kU3Vydml2ZWQpIn0= 10.9.1.6 Snippet 6 Q: Did survival depend on the class of the cabin? A: Positive. Null hypothesis of independence rejected with p-value less than \\(e^{-16}\\) eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhaXJibmI8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvVGl0YW5pYy10cmFpbi5jc3ZcIilcblxuY2hpc3EudGVzdCh0aXRhbmljJFN1cnZpdmVkLCB0aXRhbmljJFBjbGFzcykifQ== You are now familiar with the Titanic dataset and it’s time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet 10.9.3. 10.9.2 Titanic Data Quiz Quiz Time 10.9.3 Check yourself eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0aXRhbmljPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L2FpcmJuYi5jc3ZcIilcblxuc3VtbWFyeSh0aXRhbmljKSJ9 10.10 Addiotional Reference Prediction - Free Style "],["common_sense.html", "Section: 11 🔖 Common Sense Judgement and Probability 11.1 Introduction 11.2 Additional References", " Section: 11 🔖 Common Sense Judgement and Probability 11.1 Introduction We will step away from coding for a moment. In the class description we promise to address the million dollar question “How not to be fooled by data?”. Let’s dive into this important issue. We have already discussed powerful tools such as hypothesis testing, p-values and Bonferroni correction for multiple hypothesis traps. But even if you never want to write a line of code, you need to know about common traps which you may be fooled by in whatever you do. We need to be informed and educated citizens who can catch the fake inferences and fake discoveries whether we read them in the news or hear politicians falling for the traps. Daniel Kahnemann, the Nobel Prize winner in Economics is the author of the fascinating book “Think fast, think slow” and he identifies pitfalls of human relationships with numbers, frequencies. We discuss Availability, Anchoring, Conjunctive fallacy, Narrative fallacy, Law of small numbers, Reverse to the mean and many other concepts in the attached power points. In the next section we will also discuss Bayesian theorem and Bayesian reasoning (with some coding handy) to finally come back to the paradoxes such as prosecutorial paradox, Simpson paradox and ecological fallacy in section 21. 11.2 Additional References Common Sense Judgement and Probability "],["Free_style.html", "Section: 12 🔖 Free Style: Prediction 12.1 Snippet 1: Example of a simple freestyle prediction model 12.2 Snippet 2: How to build a freestyle (your own code) prediction model? 12.3 Snippet 3: One-step crossvalidation 12.4 General Structure of the Prediction Challenges 12.5 Additional Reference", " Section: 12 🔖 Free Style: Prediction What is a prediction model? Prediction model is a set of rules which, given the values of independent variables (predictors) determine the value of predicted (dependent variable). Here are example of such rules If score &gt; 80 and participation &gt;0.6 then grade =’A’ If score &gt;60 and score &lt;70 and major=’Psychology’ and Ask_questions =’always’ then grade =’B’ If score &lt;50 and score &gt;40 and Doze_off =’always’ then grade = ‘F’ By freestyle prediction we mean building a prediction model without the R library functions such as rpart and other machine learning packages. In freestyle prediction one develops models from scratch, on the basis of plots as well as exploratory queries. Freestyle prediction is important for two reasons: First, building prediction models from scratch allows an aspiring data scientist to “feel the data” - as opposed to often blind direct applications of these library functions, Second, even when one uses the prediction models based on library functions, the best models are often created by combining of several such models. These combinations often arise from skillful subsetting of datasets and applying different models to different subsets. As our prediction challenge competitions indicate, the winning prediction models (the ones with the least error) are predominantly combinations of different models applied to different subsets of the data. Thus, freestyle prediction is almost always a part of the prediction model building. We start with showing an example of a simple freestyle prediction model. 12.1 Snippet 1: Example of a simple freestyle prediction model eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ0ZXN0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TWFyY2gyMDIyYi5jc3ZcIilcblxuc3VtbWFyeSh0ZXN0KVxuXG5teXByZWRpY3Rpb248LXRlc3RcbmRlY2lzaW9uIDwtIHJlcCgnRicsbnJvdyhteXByZWRpY3Rpb24pKVxuZGVjaXNpb25bbXlwcmVkaWN0aW9uJFNjb3JlPjQwXSA8LSAnRCdcbmRlY2lzaW9uW215cHJlZGljdGlvbiRTY29yZT42MF0gPC0gJ0MnXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU2NvcmU+NzBdIDwtICdCJ1xuZGVjaXNpb25bbXlwcmVkaWN0aW9uJFNjb3JlPjgwXSA8LSAnQSdcbm15cHJlZGljdGlvbiRHcmFkZSA8LWRlY2lzaW9uXG5lcnJvciA8LSBtZWFuKHRlc3QkR3JhZGUhPSBteXByZWRpY3Rpb24kR3JhZGUpXG5lcnJvciJ9 12.2 Snippet 2: How to build a freestyle (your own code) prediction model? The key idea behind building freestyle prediction models is to subset data and select the most frequent value of the predicted variable as prediction. Of course we are interested in finding highly discriminative subsets of data with one highly dominant (most frequent value), since such a very frequent value as prediction choice will lead to a small error. But how to find data subsets with such dominant most frequent values? It is a bit of a trial and error process. As we show below in the snippet 2, it is a sequence of one line exploratory queries, which the programmer can rely on. Later, in the next section we show how the rpart() package generates such discriminative subsets of data automatically, though recursive partitioning. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9Nb29keU1hcmNoMjAyMmIuY3N2XCIpXG5cbiMgSG93IGRvIHdlIGJ1aWxkIGEgZnJlZXN0eWxlIHByZWRpY3Rpb24gbW9kZWw/ICBEZWZpbml0ZWx5IHN0YXJ0IHdpdGggcGxvdHMgbGlrZSB0aGUgYm94cGxvdCBmcm9tIHRoZSBzZWN0aW9uIDUgKGRhdGEgZXhwbG9yYXRpb24pLiAgQnV0IHRoZW4gZm9sbG93IHVwIHdpdGggZXhwbG9yYXRvcnkgcXVlcmllcyBhcyBpbiB0aGUgcmVjZW50IHF1aXp6ZXMuIEV4YW1wbGVzIGhlcmUgdXNlIHRhYmxlKCkgIGZ1bmN0b24gYW5kIGxvb2sgZm9yIHNpdHVhdGlvbnMgd2hlbiBvbmUgZ3JhZGUgaXMgYWJzb3V0ZWx5IGRvbWluYW50LiBUaGlzIHdvdWxkIGJlIHlvdXIgcHJlZGljdGlvbi4gVGh1cywgdGhlIGdvYWwgaXMgdG8gc2xpY2UgdGhlIGRhdGEgdXNpbmcgc3Vic2V0dGluZyBpbiBzdWNoIGEgd2F5IHRoYXQgZm9yIGVhY2ggc2xpY2UgeW91IGdldCBhIGNsZWFyIFwid2lubmVyIGdyYWRlXCIuIFRoZW4gY29tYmluZSB0aGVzZSBzdWJzZXQgcnVsZXMgaW50byBkZWNpc2lvbiB2ZWN0b3IgLSBqdXN0IGFzIHdlIGRpZCBpbiBzbmlwcGV0IDE0LjEuXG5cbiMgQmVsb3cgc29tZSBleGFtcGxlcyBvZiBzdWNoIGV4cGxvcmF0b3J5IHF1ZXJpZXMgd2l0aCBjbGVhciBncmFkZSB3aW5uZXJzLlxuXG5zdW1tYXJ5KG1vb2R5KVxudGFibGUobW9vZHkkR3JhZGUpXG50YWJsZShtb29keVttb29keSRTY29yZT44MCxdJEdyYWRlKVxudGFibGUobW9vZHlbbW9vZHkkU2NvcmU+ODAgJiBtb29keSRNYWpvcj09J1BzeWNob2xvZ3knLF0kR3JhZGUpXG50YWJsZShtb29keVttb29keSRTY29yZTw0MCAmIG1vb2R5JE1ham9yPT0nRWNvbm9taWNzJyxdJEdyYWRlKVxudGFibGUobW9vZHlbbW9vZHkkU2NvcmU8NDAgJiBtb29keSRTZW5pb3JpdHk9PSdGcmVzaG1hbicsXSRHcmFkZSkifQ== 12.3 Snippet 3: One-step crossvalidation How do we know if our prediction model is any good? After all, we may easily build a model which is close to perfect on the training data set but performs miserably on the new, testing data. This is a nightmare for every prediction model builder and it is called a Kaggle surprise. Kaggle surprise happens quite often during our prediction competitions when students build models which are overfitting the data and which give them a false feeling of great, low error just to do the opposite on the testing data and yield a miserably high error. To avoid this or at least to protect one against it, cross validation is needed. We illustrate cross-validation in the next snippet. We split training data into the real training data and the testing data, which is the remaining part of our training data set. Thus we use part of the training data as testing data. We do it by randomly splitting our data set. Although we show here just one step of cross-validation, we should do it multiple times. This helps us to observe how our model behaves for different random subsets of training data and helps us to observe inconsistent results (high variance of error) - which is a warning sign of future kaggle surprise. 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 We use selected data puzzles from section 4 in prediction challenges. Given a data puzzle (such as 4.1), we separate it into training data subset and testing data subset. The training data is given to students to build and cross-validate their prediction models. Then we use Kaggle to evaluate their models on the testing subset of the data puzzle. Each prediction challenge is structured as competition and Kaggle ranks students’ models by prediction accuracy. For categorical variables it is the fraction of values which are predicted correctly, for numerical variables it is MSE (mean square error). 12.4 General Structure of the Prediction Challenges The submission will take place on Kaggle which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submissions. The datasets provided for each prediction challenge is as follows: Training Dataset It is used for training and cross-validation purposes in the prediction challenge. This data has all the training attributes along and the values of the attribute wich is predicted (so called, Target attribute). Models for prediction are to be trained using this dataset only. Training data set is the set which is used when you build your prediction model - since this is the only data set which has all values of target attribute. Testing Dataset It is used for applying your prediction model to new data. You do it only when you are finished with building your prediction model. Testing data set consists of all the attributes that were used for training, but it does not contain any values of the target attribute. It is disjoint with the training data set - it contains new data and it is missing the target variable. Submission Dataset After prediction using the “testing” dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g. ID or name,etc) and second the predicted attribute column. Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle. To read the datasets use the read.csv() function and for writing the dataset to the file, use the write.csv() function. Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle. To avoid this, you can add the parameter, row.names = F in the write.csv() function. e.g. write.csv(*dataframe*,*fileaddress*,row.names = F). 12.4.1 Snippet 4: Preparing submission.csv for Kaggle eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEhlcmUgeW91IGp1c3QgbmVlZCB0aGUgdGVzdCB0YWJsZSAod2l0aG91dCBncmFkZXMpIHRvIGFwcGx5IHlvdXIgcHJlZGljdGlvbiBtb2RlbCBhbmQgY2FsY3VsYXRlIHByZWRpY3RlZCBncmFkZXMuIEFuZCBzdWJtaXNzaW9uIGRhdGEgZnJhbWUgdG8gZmlsbCBpdCBpbiB3aXRoIHRoZSBwcmVkaWN0ZWQgI2dyYWRlc1xuXG50ZXN0PC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTTIwMjJ0ZXN0U05vR3JhZGUuY3N2JylcbnN1Ym1pc3Npb248LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMnN1Ym1pc3Npb24uY3N2JylcblxubXlwcmVkaWN0aW9uPC10ZXN0XG4jSGVyZSBpcyB5b3VyIG1vZGVsLiBJIGp1c3Qgc2hvdyBleGFtcGxlIG9mIHRyaXZpYWwgcHJlZGljdGlvbiBtb2RlbFxuZGVjaXNpb24gPC0gcmVwKCdGJyxucm93KG15cHJlZGljdGlvbikpXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU2NvcmU+NDBdIDwtICdEJ1xuZGVjaXNpb25bbXlwcmVkaWN0aW9uJFNjb3JlPjYwXSA8LSAnQydcbmRlY2lzaW9uW215cHJlZGljdGlvbiRTY29yZT43MF0gPC0gJ0InXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU2NvcmU+ODBdIDwtICdBJ1xuI05vdyBtYWtlIHlvdXIgc3VibWlzc2lvbiBmaWxlIC0gaXQgd2lsbCBoYXZlIHRoZSBJRHMgYW5kIG5vdyB0aGUgcHJlZGljdGVkIGdyYWRlc1xuc3VibWlzc2lvbiRHcmFkZTwtZGVjaXNpb25cbnN1Ym1pc3Npb25cbiMgdXNlIHdyaXRlLmNzdihzdWJtaXNzaW9uLCAnc3VibWlzc2lvbi5jc3YnLCByb3cubmFtZXM9RkFMU0UpIHRvIHN0b3JlIHN1Ym1pc3Npb24gYXMgY3N2IGZpbGUgb24geW91ciBtYWNoaW5lIGFuZCBzdWJzZXF1ZW50bHkgc3VibWl0IGl0IG9uIEthZ2dsZSJ9 Data League: https://data101.cs.rutgers.edu/?q=node/155 Kaggle competition: https://www.kaggle.com/competitions/predictive-challenge-2-2022/overview Kaggle submission instructions: https://data101.cs.rutgers.edu/?q=node/150 12.5 Additional Reference Prediction - Free Style "],["Decision_trees.html", "Section: 13 🔖 Predictions with rpart 13.1 Introduction 13.2 Use of Rpart 13.3 Visualize the Decision tree 13.4 Rpart Control 13.5 Cross Validation 13.6 Prediction using rpart. 13.7 Snippet 11: Your Model with rpart 13.8 Snippet 12: Freestyle + rpart: Combining rpart prediction models 13.9 Snippet 13: Submission with rpart 13.10 Additional Reference", " Section: 13 🔖 Predictions with rpart 13.1 Introduction Decision trees are one of the most powerful and popular tools for classification and prediction. The reason decision trees are very popular is that they can generate rules which are easier to understand as compared to other models. They require much less computations for performing modeling and prediction. Both continuous/numerical and categorical variables are handled easily while creating the decision trees. 13.2 Use of Rpart Recursive Partitioning and Regression Tree RPART library is a collection of routines which implements a Decision Tree.The resulting model can be represented as a binary tree. For the purpose of illustration of rpart we will continue to use data puzzle 3.1 set - the Professor Moody data set. The library associated with this RPART is called rpart. Install this library using install.packages(\"rpart\"). Syntax for building the decision tree using rpart(): rpart( formula , method, data, control,...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... method: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use “anova” for regression, “class” for classification, etc. data: here we provide the dataset on which we want to fit the decision tree on. control: here we provide the control parameters for the decision tree. Explained more in detail in the section further in this chapter. For more info on the rpart function visit rpart documentation Lets look at an example on the Moody 2022 dataset. We will use the rpart() function with the following inputs: prediction -&gt; GRADE predictors -&gt; SCORE, DOZES_OFF, TEXTING_IN_CLASS, PARTICIPATION data -&gt; moody dataset method -&gt; “class” for classification. 13.2.1 Snippet 1 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uLlxucnBhcnQoR1JBREUgfiBTQ09SRStET1pFU19PRkYrVEVYVElOR19JTl9DTEFTUytQQVJUSUNJUEFUSU9OLCBkYXRhID0gbW9vZHksbWV0aG9kID0gXCJjbGFzc1wiKSJ9 We can see that the output of the rpart() function is the decision tree with details of, node -&gt; node number split -&gt; split conditions/tests n -&gt; number of records in either branch i.e. subset yval -&gt; output value i.e. the target predicted value. yprob -&gt; probability of obtaining a particular category as the predicted output. Using the output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section 13.6 But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section. 13.3 Visualize the Decision tree To visualize and understand the rpart() tree output in the easiest way possible, we use a library called rpart.plot. The function rpart.plot() of the rpart.plot library is the function used to visualize decision trees. NOTE: The online runnable code block does not support rpart.plot library and functions, thus the output of the following code examples are provided directly. 13.3.1 Snippet 2 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMl9uZXcuY3N2JylcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG5ycGFydChHUkFERSB+IFNDT1JFK0RPWkVTX09GRitURVhUSU5HX0lOX0NMQVNTK1BBUlRJQ0lQQVRJT04sIGRhdGEgPSBtb29keSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgaW1wb3J0IHRoZSBycGFydC5wbG90IGxpYnJhcnkgdG8gdXNlIHRoZSBycGFydC5wbG90KCkgZnVuY3Rpb24uXG4jbGlicmFyeShycGFydC5wbG90KVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQucGxvdCgpIGZ1bmN0aW9uICB0byB2aXN1YWxpemUgdGhlIGRlY2lzaW9uIHRyZWUuXG4jcnBhcnQucGxvdCh0cmVlKSJ9 Output Plot of rpart.plot() function We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about the grade category. the outcome probability of each grade category. the records percentage out of total records. To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides rpart.plot and Plotting with rpart.plot (PDF) NOTE: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment. 13.4 Rpart Control Now let’s look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function. rpart.control( *minsplit*, *minbucket*, *cp*,...) minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -&gt; the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition. minbucket: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -&gt; the minimum number of observation in the terminal/leaf node of the trees must be 500 or above. cp: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by cp, will not be made in the tree. For more information of the other arguments of the rpart.control() function visit rpart.control Let look at few examples. Suppose you want to set the control parameter minsplit=200. 13.4.1 Snippet 3: Minsplit = 200 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIG1pbnNwbGl0PTIwMFxudHJlZSA8LSBycGFydChHUkFERSB+IFNDT1JFK0RPWkVTX09GRitURVhUSU5HX0lOX0NMQVNTK1BBUlRJQ0lQQVRJT04sIGRhdGEgPSBtb29keSwgbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5zcGxpdCA9IDIwMCkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=200 in rpart.control() function 13.4.2 Snippet 4: Minsplit = 100 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIG1pbnNwbGl0PTEwMFxudHJlZSA8LSBycGFydChHUkFERSB+IFNDT1JFK0RPWkVTX09GRitURVhUSU5HX0lOX0NMQVNTK1BBUlRJQ0lQQVRJT04sIGRhdGEgPSBtb29keSwgbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5zcGxpdCA9IDEwMCkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=100 in rpart.control() function We can see from the output of tree$splits and the tree plot, that at each split the total amount of observations are above 200 and 100. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits. Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters. 13.4.3 Snippet 5: Minbucket = 100 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIE1pbmJ1Y2tldD0xMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStET1pFU19PRkYrVEVYVElOR19JTl9DTEFTUytQQVJUSUNJUEFUSU9OLCBkYXRhID0gbW9vZHksIG1ldGhvZCA9IFwiY2xhc3NcIixjb250cm9sPXJwYXJ0LmNvbnRyb2wobWluYnVja2V0ID0gMTAwKSlcblxudHJlZVxuXG4jbGlicmFyeShycGFydC5wbG90KVxuI3JwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=100 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. 13.4.4 Snippet 6: Minbucket = 200 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIE1pbmJ1Y2tldD0yMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStET1pFU19PRkYrVEVYVElOR19JTl9DTEFTUytQQVJUSUNJUEFUSU9OLCBkYXRhID0gbW9vZHksIG1ldGhvZCA9IFwiY2xhc3NcIixjb250cm9sPXJwYXJ0LmNvbnRyb2wobWluYnVja2V0ID0gMjAwKSlcblxudHJlZVxuXG4jbGlicmFyeShycGFydC5wbG90KVxuI3JwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=200 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 200. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. Lets now use the cp parameter and see its effect on the tree. 13.4.5 Snippet 7: cp = 0.05 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIGNwPTAuMlxudHJlZSA8LSBycGFydChHUkFERSB+IC4sIGRhdGEgPSBtb29keSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKGNwID0gMC4wNSkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUpIn0= Output tree plot of after setting cp=0.05 in rpart.control() function 13.4.6 Snippet 8: cp = 0.005 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5cbiMgVXNlIG9mIHRoZSBycGFydCgpIGZ1bmN0aW9uIHdpdGggdGhlIGNvbnRyb2wgcGFyYW1ldGVyIGNwPTAuMDA1XG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gLiwgZGF0YSA9IG1vb2R5LG1ldGhvZCA9IFwiY2xhc3NcIixjb250cm9sPXJwYXJ0LmNvbnRyb2woY3AgPSAwLjAwNSkpXG5cbnRyZWVcblxuI2xpYnJhcnkocnBhcnQucGxvdClcbiNycGFydC5wbG90KHRyZWUpIn0= We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005. 13.5 Cross Validation Overfitting takes place when you have a high accuracy on training dataset, but a low accuracy on the test dataset. But how do you know whether you are overfitting or not? Especially since you cannot determine accuracy on the test dataset? That is where cross-validation comes into play. Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. The partition is defined by split ratio. If split ratio =0.7, 70% of the training dataset will be used for the actual training of your model (rpart or lm), and 30 % will be used for validation (or testing). The accuracy of this validation data is called cross-validation accuracy. To know if you are overfitting or not, compare the training accuracy with the cross-validation accuracy. If your training accuracy is high, and cross-validation accuracy is low, that means you are overfitting. cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*) data: The dataset on which cross validation is to be performed. tree: The decision tree generated using rpart. n_iter: Number of iterations. split_ratio: The splitting ratio of the data into train data and validation data. method: Method of the prediction. “class” for classification. The way the function works is as follows: It randomly partitions your data into training and validation. It then constructs the following two decision trees on training partition: The tree that you pass to the function. The tree is constructed on all attributes as predictors and with no control parameters. -It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees. The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting. We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired. The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant. Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration. Let’s look at the cross_validate() function in action in the example below. We will pass the tree with formula as GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, and control parameter, with minsplit=100. And for cross_validate() function, we will usen_iter=5, and split_raitio=0.7 NOTE: Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use install.packages(&quot;devtools&quot;) devtools::install_github(&quot;devanshagr/CrossValidation&quot;) CrossValidation::cross_validate() 13.5.1 Snippet 9 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 You can see that the cross-validation accuracies for the tree that was passed (accuracy_subset) are fairly high and close to our training accuracy of 84%. This means we are not overfitting. Also observe that accuracy_subset and accuracy_all have the same values, which means that the only relevant attributes are score and participation, and adding more attributes doesn’t make any difference to the tree. Finally, the values in accuracy_subset are reasonably close to each other, which mean low variance. 13.6 Prediction using rpart. Now that we have seen the process to create a decision tree and also plot it, we will like to use the output tree to predict the required attribute. From the moody example, we are trying to predict the grade of students. Lets look at the predict() function to predict the outcomes. predict(*object*,*data*,*type*,...) object: the generated tree from the rpart function. data: the data on which the prediction is to be performed. type: the type of prediction required. One of “vector”, “prob”, “class” or “matrix”. Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset. 13.6.1 Snippet 10 eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMl9uZXcuY3N2JylcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gU0NPUkUrRE9aRVNfT0ZGK1RFWFRJTkdfSU5fQ0xBU1MrUEFSVElDSVBBVElPTiwgZGF0YSA9IG1vb2R5ICxtZXRob2QgPSBcImNsYXNzXCIpXG50cmVlXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZCkifQ== 13.7 Snippet 11: Your Model with rpart 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 13.8 Snippet 12: Freestyle + rpart: Combining rpart prediction models eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxuIyBJbXBvcnQgZGF0YXNldFxubW9vZHk8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9tb29keTIwMjJfbmV3LmNzdicpXG5tb2RlbDE8LXJwYXJ0KEdSQURFfi4sIGRhdGE9bW9vZHlbbW9vZHkkU0NPUkU+NTAsXSk7XG5tb2RlbDI8LXJwYXJ0KEdSQURFfi4sIGRhdGE9bW9vZHlbbW9vZHkkU0NPUkU8PTUwLF0pO1xubW9kZWwxXG5tb2RlbDJcbnByZWQxIDwtIHByZWRpY3QobW9kZWwxLCBuZXdkYXRhPW1vb2R5W21vb2R5JFNDT1JFPjUwLF0sIHR5cGU9XCJjbGFzc1wiKVxucHJlZDIgPC0gcHJlZGljdChtb2RlbDIsIG5ld2RhdGE9bW9vZHlbbW9vZHkkU0NPUkU8PTUwLF0sIHR5cGU9XCJjbGFzc1wiKVxubXlwcmVkaWN0aW9uPC1tb29keVxuZGVjaXNpb24gPC0gcmVwKCdGJyxucm93KG15cHJlZGljdGlvbikpXG5kZWNpc2lvbltteXByZWRpY3Rpb24kU0NPUkU+NTBdIDwtIGFzLmNoYXJhY3RlcihwcmVkMSlcbmRlY2lzaW9uW215cHJlZGljdGlvbiRTQ09SRTw9NTBdIDwtYXMuY2hhcmFjdGVyKHByZWQyKVxubXlwcmVkaWN0aW9uJEdSQURFIDwtZGVjaXNpb25cbmVycm9yIDwtIG1lYW4obW9vZHkkR1JBREUhPSBteXByZWRpY3Rpb24kR1JBREUpXG5lcnJvciJ9 13.9 Snippet 13: Submission with rpart eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxudGVzdDwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L00yMDIydGVzdFNOb0dyYWRlLmNzdicpXG5zdWJtaXNzaW9uPC1yZWFkLmNzdignaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTTIwMjJzdWJtaXNzaW9uLmNzdicpXG50cmFpbiA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L00yMDIydHJhaW4uY3N2XCIpXG5cbnRyZWUgPC0gcnBhcnQoR3JhZGUgfiBNYWpvcitTY29yZStTZW5pb3JpdHksIGRhdGEgPSB0cmFpbiwgbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5idWNrZXQgPSAyMDApKVxudHJlZVxuXG5wcmVkaWN0aW9uIDwtIHByZWRpY3QodHJlZSwgdGVzdCwgdHlwZT1cImNsYXNzXCIpXG5cbiNOb3cgbWFrZSB5b3VyIHN1Ym1pc3Npb24gZmlsZSAtIGl0IHdpbGwgaGF2ZSB0aGUgSURzIGFuZCBub3cgdGhlIHByZWRpY3RlZCBncmFkZXNcbnN1Ym1pc3Npb24kR3JhZGU8LXByZWRpY3Rpb24gXG5cbiMgdXNlIHdyaXRlLmNzdihzdWJtaXNzaW9uLCAnc3VibWlzc2lvbi5jc3YnLCByb3cubmFtZXM9RkFMU0UpIHRvIHN0b3JlIHN1Ym1pc3Npb24gYXMgY3N2IGZpbGUgb24geW91ciBtYWNoaW5lIGFuZCBzdWJzZXF1ZW50bHkgc3VibWl0IGl0IG9uIEthZ2dsZSJ9 13.10 Additional Reference Prediction with rpart "],["Linear_regression.html", "Section: 14 🔖 Linear Regression 14.1 Introduction 14.2 Linear regression using lm() function 14.3 Calculating the Error using mse() 14.4 Snippet 2: Cross Validate your prediction 14.5 Snippet 3: Submission with lm 14.6 Additional Reference", " Section: 14 🔖 Linear Regression 14.1 Introduction How to build prediction models for numerical variables? So far we have discussed prediction models for categorical target variables. In order to predict numerical variables we often use linear regression. 14.2 Linear regression using lm() function Syntax for building the regression model using the lm() function is as follows: lm(formula, data, ...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the linear regression model is to be trained. For more info on the lm() function visit lm() Lets look at the example on the Moody dataset. Table 14.1: Snippet of Moody Num Dataset Midterm Project FinalExam ClassScore 73 8 70 39.60000 61 100 20 68.20000 58 88 38 67.00000 93 41 46 52.47565 85 52 85 68.50000 97 48 19 49.10000 26 59 22 41.30000 58 62 25 50.10000 53 56 27 46.70000 66 27 17 34.80494 Imagine that we do not know the weights of midterm, project and final exam. However we have the data from the previous semesters. Can we find these weights out? The answer is yes - by using linear regression. 14.2.1 Snippet 1: How much do Midterm, Project and Final Exam count? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keU5VTTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TlVNLmNzdicpXG5zcGxpdDwtMC43Km5yb3cobW9vZHlOVU0pXG5zcGxpdFxubW9vZHlOVU1UcjwtbW9vZHlOVU1bMTpzcGxpdCxdXG5tb29keU5VTVRyXG5tb29keU5VTVRzPC1tb29keU5VTVtzcGxpdDpucm93KG1vb2R5TlVNKSxdXG4jV2UgdXNlIGxpbmVhciByZWdyZXNzaW9uIHRvICNmaW5kIG91dCB0aGUgd2VpZ2h0cyBvZiAjTWlkdGVybSwgUHJvamVjdCBhbmQgRmluYWwgI0V4YW0gaW4gY2FsY3VsYXRpb24gb2YgdGhlICNmaW5hbCBjbGFzcyBzY29yZS4gRWFjaCBvZiAjdGhlbSBhcmUgc2NvcmVkIG91dCBvZiAxMDAgYW5kICN0aGUgZmluYWwgY2xhc3Mgc2NvcmUgaXMgYWxzbyAjc2NvcmVkIG91dCBvZiAxMDAgYXMgd2VpZ2h0ZWQgI3N1bSBvZiBNaWR0ZXJtLCBQcm9qZWN0IGFuZCAjRmluYWwgRXhhbSBzY29yZXMuXG50cmFpbiA8LSBsbShDbGFzc1Njb3Jlfi4sICBkYXRhPW1vb2R5TlVNVHIpXG50cmFpblxucHJlZCA8LSBwcmVkaWN0KHRyYWluLG5ld2RhdGE9bW9vZHlOVU1Ucylcbm1lYW4oKHByZWQgLSBtb29keU5VTVRzJENsYXNzU2NvcmUpXjIpIn0= We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained from the predict function using the trained model and the test data. 14.3 Calculating the Error using mse() As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test won’t prove useful now in our numerical predictions scenario. We don’t want to eyeball every time we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique. To do this we will use the Mean Squared Error(MSE). The MSE is a measure of the quality of an predictor/estimator It is always non-negative Values closer to zero are better. The equation to calculate the MSE is as follows: \\[\\begin{equation} MSE=\\frac{1}{n} \\sum_{i=1}^{n}{(Y_i - \\hat{Y_i})^2} \\\\ \\text{where $n$ is the number of data points, $Y_i$ are the observed value}\\\\ \\text{and $\\hat{Y_i}$ are the predicted values} \\end{equation}\\] To implement this, we will use the mse() function present in the Metrics Package, so remember to install the Metrics package and use library(Metrics) in the code for local use. The syntax for mse() function is very simple: mse(actual,predicted) actual: vector of the actual values of the attribute we want to predict. predicted: vector of the predicted values obtained using our model. 14.4 Snippet 2: Cross Validate your prediction eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxudHJhaW4gPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9Nb29keU5VTS5jc3ZcIilcbiNzY3JhbWJsZSB0aGUgdHJhaW4gZnJhbWVcbnY8LXNhbXBsZSgxOm5yb3codHJhaW4pKVxudlsxOjVdXG50cmFpblNjcmFtYmxlZDwtdHJhaW5bdiwgXVxuXG4jb25lIHN0ZXAgY3Jvc3N2YWxpZGF0aW9uXG5uIDwtIDEwMFxudHJhaW5TYW1wbGU8LXRyYWluU2NyYW1ibGVkW25yb3codHJhaW5TY3JhbWJsZWQpLW46bnJvdyh0cmFpblNjcmFtYmxlZCksIF1cbnRlc3RTYW1wbGUgPC0gdHJhaW5TY3JhbWJsZWRbMTpuLF1cblxubG0udHJlZSA8LSBsbShDbGFzc1Njb3Jlfi4sICBkYXRhPXRyYWluU2FtcGxlKVxubG0udHJlZVxuXG5wcmVkIDwtIHByZWRpY3QobG0udHJlZSxuZXdkYXRhPXRlc3RTYW1wbGUpXG5wcmVkXG5cbm1zZSh0ZXN0U2FtcGxlJENsYXNzU2NvcmUscHJlZCkifQ== We can see that, The summary of the lm model gives us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained from the predict function using the trained model and the test data. In comparison to the previous model we are using the cross validation technique to check if we have more accurate predictions, thus increasing the overall accuracy of the model. 14.5 Snippet 3: Submission with lm eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxudGVzdDwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L01vb2R5TlVNX3Rlc3QuY3N2JylcbnN1Ym1pc3Npb248LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGV2Nzc5Ni9kYXRhMTAxX3R1dG9yaWFsL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMnN1Ym1pc3Npb24uY3N2JylcbnRyYWluIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2Rldjc3OTYvZGF0YTEwMV90dXRvcmlhbC9tYWluL2ZpbGVzL2RhdGFzZXQvTW9vZHlOVU0uY3N2XCIpXG5cbnRyZWUgPC0gbG0oQ2xhc3NTY29yZX4uLCAgZGF0YT10cmFpbilcbnRyZWVcblxucHJlZGljdGlvbiA8LSBwcmVkaWN0KHRyZWUsIG5ld2RhdGE9dGVzdClcblxuI05vdyBtYWtlIHlvdXIgc3VibWlzc2lvbiBmaWxlIC0gaXQgd2lsbCBoYXZlIHRoZSBJRHMgYW5kIG5vdyB0aGUgcHJlZGljdGVkIGdyYWRlc1xuc3VibWlzc2lvbiRHcmFkZTwtcHJlZGljdGlvbiBcblxuIyB1c2Ugd3JpdGUuY3N2KHN1Ym1pc3Npb24sICdzdWJtaXNzaW9uLmNzdicsIHJvdy5uYW1lcz1GQUxTRSkgdG8gc3RvcmUgc3VibWlzc2lvbiBhcyBjc3YgZmlsZSBvbiB5b3VyIG1hY2hpbmUgYW5kIHN1YnNlcXVlbnRseSBzdWJtaXQgaXQgb24gS2FnZ2xlIn0= 14.6 Additional Reference Linear Regression "],["Prediction_loop.html", "Section: 15 🔖 Machine Learning-Prediction Loop 15.1 Introduction 15.2 Additional Reference", " Section: 15 🔖 Machine Learning-Prediction Loop 15.1 Introduction Believe it or not you are ready now to use pretty much any machine learning package from the extensive R library. In other words you can drive any car without knowing how the engine works. This you can find out by taking more advanced classes in Machine learning from computer science, statistics or machine learning departments. As per CRAN there are around 8,341 packages that are currently available. Apart from CRAN, there are other repositories which contribute multiple packages. The simple straightforward syntax to install any of these machine learning packages is: install.packages (“MLPackage”). Install Packages(‘MLPackage’) Library(MLPackage) MlPackage&lt;-MLPackage(Formula, data=YOUR_TRAINING,…) Predict(MLPackage, newdata=YOUR_TESTING…) Error &lt;- …… MLPackage can be rpart, Random Forests, naive Bayes, LDA, SVM, Neural Network and many others. 15.2 Additional Reference Prediction Loop "],["Mysteries_of_Aggregated_data.html", "Section: 16 🔖 How can data fool us? 16.1 Introduction 16.2 Additional References", " Section: 16 🔖 How can data fool us? 16.1 Introduction How not to be fooled by data? In the description of this class we promised that data 101 will teach you this. Have we? I hope so. Please use our question roulette to test yourself. In this section we discuss Simpson Paradox, Prosecutorial and Ecological Fallacies. Before we proceed with the paradoxes let us summarize what we have learned so far: Beware of randomness (Hypothesis testing, p-values, Multiple hypothesis testing) Be an optimist - use Bayesian reasoning. Remember about prior odds first! Beware of extreme results - Apply law of small numbers Remember we process information following availability - we assign high frequency falsely to events which are just talked about very often Narrative fallacy - do not find false patterns - “Stocks went down due to concerns about rising cost of living” Here we will discuss the Simpson paradox as well as Prosecutorial and Ecological fallacies. Let us start with the Simpson paradox. Here is a simple example of two basketball players, Aaron and Barry The Table 16.1 shows Aaron’s and Barry’s free throw point averages (FTP) over the 2021 and 2022 season respectively. Clearly for both seasons Barry beats Aaron in terms of FTP, in 2021 by 90% to 80% and in 2022 by 70% to 65%. Can Aaron still beat Barry over both seasons - that is get a higher FTP over the sum of two seasons, 2021+2022? First answer which comes to mind is absolutely not. How can Aaaron beat Barry over 2021+2022 when Barry beats him in each of the two seasons? Table 16.1 season/player Aaron Barry 2021 season 80% 90% 2022 season 65% 70% But table 16.2 explains that it is quite possible that Aaron can beat Barry over 2021+ 2022. Indeed, since we do not know the absolute number of attempts at free throws, we can easily pick any number of attempts for each of them in any of the two seasons. Indeed - here is the proof that Aaron can still beat Barry. If Barry made 100 attempts in 2021 and 20 attempts in 2022, while Barry made only 20 attempts in 2021 and 100 attempts in 2022, Aaron’s overall FRP for both 2021+2022 will be higher than Barry’s. And this is Simposon’s paradox. Table 16.2 season/player Aaron Barry 2021 season 80% out of 100 90% out of 20 2022 season 65% out of 20 70% out of 100 Indeed Aaron’s FTP over 2021+2022 is \\[\\begin{equation} \\frac{80+13}{120} = \\frac{93}{120} \\end{equation}\\] which is larger than Barry’s \\[\\begin{equation} \\frac{18+20}{120} = \\frac{88}{120} \\end{equation}\\] More generally, trends in subsets of data may reverse themselves after aggregation. In fact we can have any number of seasons and have Barry beat Aaron in FTP in each and every season and Aaron still wins with better FTP over all seasons. This is again simply because we do not know how many attempts each player made each season. This applies to many real world situations such as graduate admissions for example (the famous Berkeley admission bias case). There women may have a higher chance to be admitted than men in each single academic department and nevertheless, men beat women in overall acceptance ratio. This is again hard to comprehend at first but it is due to the fact that the absolute number of female and male applicants may be different for each department. Is such reversal always possible? Let’s look at the table below: Table 16.3 season/player Aaron Barry 2021 season 65% 90% 2022 season 60% 70% In this case the Simpson paradox is not possible. Why? Because Aaron’s highest FTP (65% in 2021 season is lower than Barry’s lowest FTP in 2022). You can easily see that no matter what the absolute numbers of attempts in each season, Aaron can never beat Barry for 2021+2022. Thus the Simpson paradox was possible in this simple case only because Aaron’s highest FTP was higher than Barry’s lowest FTP. One also has to be careful with the Simpson paradox and not apply it to situations when both groups /individuals have the same absolute number of “attempts”. For example, the Simpson paradox is not possible for students and their individual scores on homework’s and exams. If Barry scores higher than Aaron on each homework and on each exam then Barry will always have a higher score overall than Aaron. There is no Simpsonian trend reversal. Every homework and every exam counts the same for all students. This is as if players always made the same number of free throw attempts. 16.1.1 Ecological Paradox Ecological paradox is kind of the reverse of the Simpson paradox. Let’s assume that we consider net worth for each member of groups A and B. Even if average net worth of group A is higher than average net worth of group B, it may be possible that random individual member of the group B has higher net worth than random individual member of the group A. Thus the order of aggregates may be reversed when we look at the level of individuals. For example as table 16.4 illustrates, the average net worth of Group A dominates the average net worth of Group B due to the presence of one wealthy individual. However for 90% of pairs of individuals, group B members are more wealthy than Group A members. Table 16.3 Group A Group B $10,000,000 $210,000 $100,000 $290,000 $120,000 $220,000 $80,000 $210,000 $60,000 $270,000 $160,000 $210,000 $110,000 $240,000 $100,000 $210,000 $200,000 $240,000 For example, it is well known that Democrats win the richest states, while (until recently), the richest individuals vote republican. How is it possible? Explanation is simple. Everyone’s vote counts the same and there are few very rich people. Very rich people may contribute more to the average wealth of the state (due to their extreme wealth), but there are just very few of them. Do not be fooled by aggregates! Let’s assume that a Democrat wins 70% of the vote and a Republican wins 30% of the vote in some state. Is it possible that, nevertheless, the republican candidate wins all 19 counties out of 20 in the state? This actually happens a lot when the population is heavily concentrated in a heavily populated urban county which has the vast majority of voters living there. 16.2 Additional References How can data fool us? "],["22.html", "Section: 17 Boundless Analytics - Pre-discovery Tool 17.1 Introduction 17.2 Minimarket Data Set description 17.3 Demo of Boundless Analytics 17.4 The Boundless Analytics web application 17.5 Snippet 1: Chi square hunt", " Section: 17 Boundless Analytics - Pre-discovery Tool 17.1 Introduction In this section we demonstrate application of Boundless Analytics - the tool developed by Tomasz Imielinski and his team at Rutgers (and supported by NSF subcontract of Center of Science of Information at Purdue University). Boundless Analytics calculates all significant bar graphs from the data set and allows to find data subsets (slices) which deviate the most from the whole data set in regard to frequency distribution of an attribute. Boundless performs an otherwise very tedious task of looking at all combinations of attribute value pairs to identify the “significant ones” - saving enormous amounts of work in preliminary exploration of data. We are using here the Minimarket data puzzle 10.7 describing customer transactions in the small chain of minimarkets in NJ. Data 101 students used Boundless Analytics to discover the most interesting subsets of this data set 17.2 Minimarket Data Set description Zoom recording 17.3 Demo of Boundless Analytics Zoom Recording 17.4 The Boundless Analytics web application Boundless Analytics Interface: http://209.97.156.178:8082/ (it is a soft login abc/abc will do) Objective: Nominate the most interesting subset of the Minimarket2022 data set Seems open ended, no? what is the “most interesting”? Chi-square value is a good measure. We explain it below. By swiping through possible plots (using Next), one can identify good candidates for the “interesting data subsets”) These are plots where red and blue bars differ the most. In other words we want to reject the null hypothesis of independence of red and blue distributions over the data slice and the complement of the data slice. The higher the chi-square is, the strongest is our rejection of independence of red and blue distributions. Therefore this task can be seen as chi-square hunt for the highest chi-square value (use the snippet 17.1 code after plugging in definition of a slice and the anchor attribute) 17.5 Snippet 1: Chi square hunt eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFNheSwgdGhlIEJvdW5kbGVzcyBhbmFseXRpY3MgcHJvdmlkZXMgdXMgd2l0aCB0aGUgc2xpY2U6ICBCZWVyID09J0xhZ2VyJyAmICBEYXkgPT0nV2Vla2VuZCcgYW5kIFNuYWNrcyA9J0NyYWNrZXJzJyBhbmQgYW5jaG9yIGF0dHJpYnV0ZSBpcyBMb2NhdGlvbi4gIFlvdSBjYW4gY2FsY3VsYXRlIENoaXNxIGZvciB0aGlzIHNsaWNlIGFuZCB0aGUgTG9jYXRpb24gYXR0cmlidXRlIHRvIHRlc3QgaWYgZGlzdHJpYnV0aW9uIG9mIGxvY2F0aW9ucyBpcyBhZmZlY3RlZCBpZiB3ZSBsaW1pdCBvdXJzZWx2ZXMgb25seSB0byB0cmFuc2FjdGlvbnMgc2VsbGluZyBMYWdlciBhbmQgQ3JhY2tlcnMgb24gV2Vla2VuZHM/ICBcblxuIyBUaGUgbW9zdCBpbnRlcmVzdGluZyBzbGljZS1hbmNob3IgYXR0cmlidXRlIGNvbWJpbmF0aW9ucyBhcmUgdGhlIG9uZXMgd2l0aCB0aGUgbGFyZ2VzdCBjaGlzcSB0ZXN0IGFuZCBsb3dlc3QgcC12YWx1ZS4gTmV2ZXJ0aGVsZXNzIGRvIG5vdCBmb3JnZXQgYWJvdXQgbXVsdGlwbGUgaHlwb3RoZXNpcyBjb3JyZWN0aW9uIC0gc2luY2Ugd2UgY2FuIG9uIGNoaS1zcXVhcmUgaHVudCBoZXJlIVxuXG5NaW5pbWFya2V0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZXY3Nzk2L2RhdGExMDFfdHV0b3JpYWwvbWFpbi9maWxlcy9kYXRhc2V0L0hvbWV3b3JrTWFya2V0MjAyMi5jc3ZcIilcblxuTWluaW1hcmtldCRJTjwtJ091dF9TbGljZSdcbk1pbmltYXJrZXRbTWluaW1hcmtldCRCZWVyPT0nTGFnZXInICYgTWluaW1hcmtldCREYXk9PSdXZWVrZW5kJyAmICBNaW5pbWFya2V0JFNuYWNrcyA9PSdDcmFja2VycycsIF0kSU48LSdJbl9TbGljZSdcbmQ8LXRhYmxlKE1pbmltYXJrZXQkTG9jYXRpb24sIE1pbmltYXJrZXQkSU4pXG5jaGlzcS50ZXN0KGQpIn0= ATTACHED - the data set (same as on the Boundless Analytics interface) HomeworkMarket2022-2.csv RESULTS: Here are two out of 250+ submissions. The one with the highest chi-square of 600.15 is the slice showing weekend buyers of lager in New brunswick but disproportionately more snacks (in particular Crackers). This was identified by nearly 20 students. Here is another find by Eva Zhang showing disproportionately frequent sales of Coca Cola on Weekdays in Princeton for transactions which purchased Popcorn. The chi-square value of this find is 205.31, with df=3. "],["best_work_2022.html", "Section: 18 🔖 Best Works of 2022 18.1 DataBlog 18.2 Boundless Analytics", " Section: 18 🔖 Best Works of 2022 18.1 DataBlog Ella Walmsley 18.2 Boundless Analytics Anastasiya Chuchkova Shreya Tiwari George Basta Paul Kotys Selin Altimparmak "],["Leaderboard.html", "Section: 19 Data League Leaderboard", " Section: 19 Data League Leaderboard Table 19.1: Leaderboard 2022 Rank Participant.Name 1 Jeevanandan Ramasamy 2 George Basta 3 Joyce Huang 4 Jiaxu Hu 5 Dhiren Patel 6 Chicheng Shao 7 Cheyenne Pourkay 8 Christopher Nguyen 9 Aaron Mok 10 Ethan Matta Honourable Mentions: Upsham Naik, Joshua B. Sze, Kirtan Patel, Maria Xu, Devam Patel, Eva Zhang, Toshanraju Vysyaraju, Maanas Pimplikar, Jared Chiou, Nitya Narayanan, Shrish Vellore, Yousra Belgaid, Mitali Shroff, Michael Jucan, Jackie Hong, Arvin Sung, Eric Xuan, Eva Allred, Leah Ranavat, Nami Jain, Gautam Agarwal, Aditya Patil "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
