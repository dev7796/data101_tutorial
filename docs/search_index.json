[["index.html", "DATA 101 Shortest Textbook How To Accomplish More With Less ", " DATA 101 Shortest Textbook How To Accomplish More With Less Tomasz Imielinski 2021-06-12 This is a textbook based on the DATA 101 course thought by Prof. Tomasz Imielinski at Computer Science Department at Rutgers University, New Brunswick. Data 101 is an introductory course for beginners from any field of study, interested in the field of Data Science. Acknowledgment: Contribution of Deep Lokhande to this draft is gratefully acknowledged. This books pages are created using Rmarkdown from Rstudio (similar to jupyter notebook) and the book is compiled using bookdown package. The most important aspect of this interactive book are the interactive code chunks for running code, which are powered by a minimal version of Datacamps learning interface called Datacamp Light. For more info visit Datacamp Light Note1: This book is undergoing constant updates following along with the course thought in Spring 2021. Topics under progress are marked with a \" * \". Note2: This book uses Datacamp Light for supporting runnable code chunks. In case the code chunks do not connect to run-time session, please copy the code and run in RStudio. Also, please report if facing this issue to the instructor via email. "],["intro.html", "Chapter 1 Introduction 1.1 Setting Up R", " Chapter 1 Introduction The objective of this textbook is to provide you the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in statistically convincing manner, perhaps in colorful and visually appealing way. Questions which you will have to anticipate and you will have to answer are - How do you know that your findings are not random? - And fundamental of all questions: - So what? Even the most impressing looking results may come up randomly. And you will be asked this question along with the question what was your p-value and how did you compute it And even if you convince your audience that your results are not random, you will have to be ready to explain why should you audience care about the results you reported. In other words, is there any actionable value in your results? Or they are just simply interesting, good to know, but no one really needs to care much about them otherwise? Hopefully it is the former not the latter. In the following sections we will address these questions and go through the process of data exploration, validation, and presentation. We will start with making plots, follow with free style data exploration  which allows us to form the leads, that is hypotheses. Then we will follow with simple statistical tests which will allow us to validate these hypothesis and defend our findings against randomness claims. - We will learn how to calculate p-values and how to use them to defend our findings. We will use as few R commands as possible and reach our goal in shortest possible path. In fact we will demonstrate how using just 7 R commands we can perform quite sophisticated data exploration. 1.1 Setting Up R Important Instructions Installation of R is required before installing RStudio R is a programming language, and, RStudio is an Integrated Development Environment (IDE) which provides you a platform to code in R. How to download and install R &amp; RStudio? Downloading and installing R. For Windows Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cran.r-project.org/bin/windows/base/ Click on the link at top left where it says Download R 4.0.3 for windows or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. For MAC Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cloud.r-project.org/bin/macosx/ Under Latest release, click on R-4.0.3.pkg or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. Downloading and installing RStudio. For Windows Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named All Installers. Click on the download link beside Windows 10/8/7 to download the windows version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. For MAC Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named All Installers. Click on the link beside macOS 10.13+ to start your download the MAC version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. How to upload a data set? To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon. Let us look at the example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFJlYWQgaW4gdGhlIGRhdGFcbmRmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMGIuY3N2XCIpXG5cbiMgUHJpbnQgb3V0IGBkZmBcbmhlYWQoZGYpIn0= "],["dataexp.html", "Chapter 2 Data Exploration 2.1 Plots 2.2 Free Style data exploration with just seven R commands \" R.7 \"", " Chapter 2 Data Exploration 2.1 Plots Table 2.1: Snippet of Moody Dataset score grade texting questions participation 26.89 F never never 0.41 71.57 B always rarely 0.00 90.11 A always never 0.27 31.52 D sometimes rarely 0.68 95.94 A always rarely 0.09 45.72 D always rarely 0.19 90.82 A always always 0.25 75.52 B sometimes never 0.28 52.31 C never never 0.67 39.57 D always always 0.40 2.1.1 Scatter Plot Scatter Plot are used to plot points on the Cartesian plane (X-Y Plane) Hence it is used when both the labels are numerical values. Lets look at example of scatter plot using Moody. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExldCdzIGxvb2sgYXQgYSAyIGF0dHJpYnV0ZSBzY2F0dGVyIHBsb3QuXG4jIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5wbG90KG1vb2R5JHBhcnRpY2lwYXRpb24sbW9vZHkkc2NvcmUseWxhYj1cInNjb3JlXCIseGxhYj1cInBhcnRpY2lwYXRpb25cIixtYWluPVwiIFBhcnRpY2lwYXRpb24gdnMgU2NvcmVcIixjb2w9XCJyZWRcIikifQ== 2.1.2 Bar Plot A bar plot is used to plot rectangular bars proportional to the values present in a numerical vector. This rectangle height is proportional to the value of the variable in the vector. Barplots are also used to graphically represent the distribution of a categorical variable, after converting the categorical vector into a table(i.e. frequency distribution table) In a bar plot, you can also give different colors to each bar. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cbiNsZXRzIG1ha2UgYSB0YWJsZSBmb3IgdGhlIGdyYWRlcyBvZiBzdHVkZW50cyBhbmQgY291bnRzIG9mIHN0dWRlbnRzIGZvciBlYWNoIEdyYWRlLiBcblxudDwtdGFibGUobW9vZHkkZ3JhZGUpXG5cbiNvbmNlIHdlIGhhdmUgdGhlIHRhYmxlIGxldHMgY3JlYXRlIGEgYmFycGxvdCBmb3IgaXQuXG5cbmJhcnBsb3QodCx4bGFiPVwiR3JhZGVcIix5bGFiPVwiTnVtYmVyIG9mIFN0dWRlbnRzXCIsY29sPWNvbG9ycyxcbm1haW49XCJCYXJwbG90IGZvciBzdHVkZW50IGdyYWRlIGRpc3RyaWJ1dGlvblwiLGJvcmRlcj1cImJsYWNrXCIpIn0= 2.1.3 Box Plot A boxplot shows the distribution of data in a dataset. A boxplot shows the following things: Minimum Maximum Median First quartile Third quartile Outliers You can create a single boxplot using just a vector or a multiple boxplot using a formula. When you write a formula, you should use the Tilde (~) operator. This column name on the left side of this operator goes on the y axis and the column name on the right side of this operator goes on the x axis. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cblxuI1N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCB0aGUgZGlzdHJpYnV0aW9uIG9mIHN0dWRlbnRzIHNjb3JlIHBlciBHcmFkZS4gV2UgdXNlIGJveCBwbG90IGZvciBnZXR0aW5nIHRoYXQuIFxuYm94cGxvdChzY29yZX5ncmFkZSxkYXRhPW1vb2R5LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJTY29yZVwiLCBtYWluPVwiQm94cGxvdCBvZiBncmFkZSB2cyBzY29yZVwiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIilcblxuIyB0aGUgY2lyY2xlcyByZXByZXNlbnQgb3V0bGllcnMuIn0= 2.1.4 Mosiac Plot Mosaic plot is a graphical method for visualizing data from two or more qualitative variables. The length of the rectangles in the mosaic plot represents the frequency of that particular value. The width and length of the mosaic plot can be used to interpret the frequencies of the elements. -For example, if you want to plot the number of individuals per letter grade using a smartphone, you want to look at mosiac plot. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cbiNzdXBwb3NlIHlvdSB3YW50IHRvIGZpbmQgbnVtYmVycyBvZiBzdHVkZW50cyB3aXRoIGEgcGFydGljdWxhciBncmFkZSBiYXNlZCBvbiB0aGVpciB0ZXh0aW5nIGhhYml0cy4gVXNlIE1vc2lhYy1wbG90LlxuXG5tb3NhaWNwbG90KG1vb2R5JGdyYWRlfm1vb2R5JHRleHRpbmcseGxhYiA9ICdHcmFkZScseWxhYiA9ICdUZXh0aW5nIGhhYml0JywgbWFpbiA9IFwiTW9zaWFjIG9mIGdyYWRlIHZzIHRleGluZyBoYWJpdCBpbiBjbGFzc1wiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIikifQ== 2.2 Free Style data exploration with just seven R commands \" R.7 \" Now as you know to make simple, but quite colorful, basic graphs it is time to go one step further and use the plots to explore your data. This is the subject of the process known as data exploration. We will begin with what we call, the free style data exploration. We call it free style, since we are not going to use any sophisticated libraries, but rather just seven basic of commands of R. We call this set of commands - R.7. And with these seven commands of R.7 we will be able to do quite a bit of data exploration. We will be able to slice and dice data any way we want to. No statistics yet, and no more sophisticated R programs.These will come later. For now, we are just feeling the data with four plots and three R instructions: subset(), table() and tapply() Through this section we will use our usual initial example, of synthetically generate data describing mysterious methods of grading used by an eccentric Professor Moody. We have been using different versions of this data puzzle over the 6 years of teaching data 101. Data is different but narrative is always the same: 2.2.0.1 Professor Moody Puzzle Professor Moody has been teaching statistics 101 class for many years. His teaching evaluations went considerably south with the chief complaint: he DOES NOT seem to assign grades fairly. Students compared their scores among themselves and found quite a bit of discrepancies! But their complaints went nowhere since Professor promptly disappeared after posting the final grades and scores. A new brave TA, managed to get hold of the carefully maintained grading table (spanning multiple years) of professor Moody by .messing a bit with Moodys computer.well, lets not explain the details because he would get in trouble. What he found out was a remarkably structured account of how professor Moody assigns his grades. Looks like Professor Moody is in fact very alert in class. He is aware of what students do, detecting texting during class and remembering exactly who asked many questions in class. He also keeps the mysterious participation index which is a numerical score from 0 to 1. This is probably related to questions asked and answered by students as well as their general attentiveness in class. Remarkable but a little creepy, isnt it? What is the best advice the new TA, can give future students how to get a good grade in Professor Moodys class? What factors influence the grade besides the score? Back your recommendation up with plots and evidence from the attached data. The Moody data set is defined here by the following attributes Table 2.2: Snippet of Moody Dataset score grade texting questions participation 26.89 F never never 0.41 71.57 B always rarely 0.00 90.11 A always never 0.27 31.52 D sometimes rarely 0.68 95.94 A always rarely 0.09 45.72 D always rarely 0.19 90.82 A always always 0.25 75.52 B sometimes never 0.28 52.31 C never never 0.67 39.57 D always always 0.40 Moody[score, grade, participation, questions, texting] Score and grade are self explanatory. Participation is supposedly measuring students participation in class. We do not know whether higher participation would necessarily be positive, since Professor Moodys mood changes from year to year and he may be annoyed by students who are too active and bother him too much. Who knows? We have to find out. Attribute questions has several values always, frequently, sometimes and never. So does the attribute texting. In our data set there are students who are are always texting and who never ask any questions. Oh, yes, and some students participation index is almost zero. Guess what grade are they getting? F, you probably guess. Well, but about their score.It should matter probably. At least to some degree! Grading rules, which Professor Moody applies each year are different. The objective of our free style data exploration is to find some leads/hypotheses which would help us direct students what they should do to get a good grade in his class. This is a good illustration of what data exploration is and can achieve. It is just an example, but one can of course easily see that, things we discuss here, applies to any data sets. Data exploration can be viewed as an indefinite loop: REPEAT{ Plot,one or many plots. Transform Data. } UNTIL GRATIFICATION Put yourself in the position of a student in Moodys class. What does s/he want to know? What should I do in order to pass his class aside from getting the best score possible? Ask many questions? Do not text? Come to class as often as you can? Presumably improving participation index? What is the approach? First you need kick the tires, make some plots, feel the data and perhaps rule out the obvious. In case of Professor Moody data it may mean the following: - Test if straightforward mapping of scores into grades work in Professor Moodys class. Admittedly it is a long shot. We expect more from professor Moody than just merely following the scoring intervals with A above, say 85, B between 70 and 85 etc! But we need to establish that it is not the case quickly. Since it would be embarrassing to miss the obvious and simplest recommendation. Just score as high as you can. Otherwise no need to come to class, and in class you can text as much as you want to and ask no questions. Does not matter what else you do. You may never ask any questions, always text in class or simplynever even show up. All it matters is score! There is just one plot which can quickly establish whether this simple rule works. And it is the boxplot. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5ib3hwbG90KG1vb2R5JHNjb3JlIH4gbW9vZHkkZ3JhZGUsIG1haW4gPSAnRGlzdHJpYnV0aW9uIG9mIFNjb3JlcyBieSBHcmFkZScsIHlsYWIgPSdTY29yZSAob3V0IG9mIDEwMCknLCB4bGFiID0gJ0xldHRlciBHcmFkZScsY29sPWMoJ3JlZCcsJ2JsdWUnLCdncmVlbicsJ2N5YW4nLCd5ZWxsb3cnKSkifQ== As illustrated by the boxplot, there are significant overlaps between successive grades. This disproves that there is a deterministic function between score and grade. At least it is not always a function. If your score falls in certain gray areas you may get either one of two grades (A or B, B or C, C or D, D or F). And we do not know what is this additional decider in such case when score falls into this gray area. Here is how we can check which factors may impact the grade. One way of doing this analysis is to make barplots for all possible slices of Moody data frame by a given categorical variable For example,we want to know if asking questions matters for the grade? This can be validated by comparing barplots of grade distribution for different values of attribute questions.You can either do it by applying the mosaic plot which allows for two-dimensional representation of data and allows to create multicolored table for grade x questions to eyeball if values of attribute questions matter for values of attribute grade. To dig deeper into the relationship of categorical variables questions and texting with grade we will use sequence of bar plots over subsets of the Moody data frame. Then we will follow with the mosaic plots. The following slices represent subsets of the Moody data frame for each of the values of the attribute questions The command \\(\\color{violet}{\\text{table}}\\) (one of the 7 commands) will provide us grade distribution for each of these slices. And the barplot, will visualize this table. Lets look at the example of the above process for students who always ask question. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhYmxlKG1vb2R5W21vb2R5JHF1ZXN0aW9ucz09J2Fsd2F5cycsXSRncmFkZSksbWFpbiA9ICdGcmVxdWVuY3kgb2Ygc3R1ZGVudHMgYnkgR3JhZGUgd2hvIFwiYWx3YXlzXCIgYXNrIHF1ZXN0aW9ucycsIHlsYWIgPSdGcmVxdWVuY3knLCB4bGFiID0gJyBHcmFkZScsY29sPWMoJ3JlZCcsJ2JsdWUnLCdncmVlbicsJ2N5YW4nLCd5ZWxsb3cnKSlcblxuI05vdGljZSB0aGF0IHlvdSBjYW4gbW9kaWZ5IHRoZXNlIGJhcnBsb3RzIGdyYXBocyBhbmQgcmVwbGFjZSB0aGUgdmFsdWUgb2YgbW9vZHkkcXVlc3Rpb25zIGF0dHJpYnV0ZXMgZnJvbSBcImFsd2F5c1wiIHRvIFwic29tZXRpbWVzXCIgb3IgXCJuZXZlclwiIGFuZCBzZWUgaW1wYWN0IHRoZXNlIG5ldyBzbGljZXMgaGF2ZSBvbiB0aGUgZ3JhZGUgZGlzdHJpYnV0aW9uLiBKdXN0IGNoYW5nZSB0byBjb2RlIGFib3ZlIGFuZCBydW4gaXQuIFlvdSBjYW4gYWxzbyBjaGFuZ2UgdGhlIG1vb2R5JHF1ZXN0aW9ucyBhdHRyaWJ1dGUgYW5kIHJlcGxhY2UgaXQgd2l0aCB0aGUgbW9vZHkkdGV4dGluZyBhdHRyaWJ1dGUgYW5kIGl0cyBkaWZmZXJlbnQgdmFsdWVzLiBUaHVzIHlvdSBjYW4gcnVuIDYgZGlmZmVyZW50IGJhcnBsb3RzIHVzaW5nIHRoZSBjb2RlIGFib3ZlIGFuZCBzZWUgaG93IEdyYWRlIGRpc3RyaWJ1dGlvbiBpcyBhZmZlY3RlZCBmb3IgZWFjaCBvZiB0aGVzZSA2IGNhc2VzLiJ9 We can also run two mosaic plots of GRADE vs questions or texting respectively - and be able to asses the same - do these attributes matter for the grade? In the following command we can combine attribute grade with anyone of the behavioral attributes eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5tb3NhaWNwbG90KG1vb2R5JGdyYWRlfm1vb2R5JHRleHRpbmcseGxhYiA9ICdHcmFkZScseWxhYiA9ICdUZXh0aW5nIGhhYml0JywgbWFpbiA9IFwiTW9zaWFjIG9mIGdyYWRlIHZzIHRleGluZyBoYWJpdCBpbiBjbGFzc1wiLGNvbD1jKCdyZWQnLCdibHVlJywnZ3JlZW4nLCdjeWFuJywneWVsbG93JyksYm9yZGVyPVwiYmxhY2tcIikifQ== This can be concluded by comparing different columns and rows of the mosaic table. If grade distribution is similar for different values of behavioral attributes, this would indicate that these attributes do not matter in the establishing the grade. On the other hand we may catch professor Moody and find out that for some value of some attribute, grade distribution is significantly affected. This was the case several years ago when students sitting in the first row, got a grade bump up, even if they get similar scores to students sitting in the back row. In that case one of the extra attributes included the row where students were sitting during class. We can see that asking many questions (frequently and always) really matters for the grade, there is more As and more Bs for these slices than in general. But this may have nothing to do with Professor Moody rewarding students with the bonus for asking questions. It may be simply the case that such students are more involved and study harder (or are more interested in the topic) and simply get higher scores. We need to dig deeper and see which of the two is the case. Moodys just gives his personal bonus to students who ask a lot of questions or no such bonus is given  such students simply score higher. We can accomplish this using again one of the seven R commands  the tapply. ## always never rarely ## 51.08277 56.32474 53.69217 will return average score for each of the values of the attribute moody$questions. If this values are more or less uniform than it will informally (not statistically yet, for this we have to wait for the next sections) show that questions matter in professor moody grading method and are not just correlated with students score. Take a look at eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhcHBseShtb29keSRzY29yZSwgbW9vZHkkcXVlc3Rpb25zLCBtZWFuKSwgeGxhYiA9ICdxdWVzdGlvbiBjYXRlZ29yaWVzJyx5bGFiID0gJ1Njb3JlIEF2ZXJhZ2UnLCBtYWluID0gXCJNZWFuIFNjb3JlIHZzIFF1ZXN0aW9ucyBBc2tlZCB1c2luZyB0YXBwbHkoKVwiLGNvbD1jKCdyZWQnLCdibHVlJywnZ3JlZW4nLCdjeWFuJywneWVsbG93JyksYm9yZGVyPVwiYmxhY2tcIikifQ== What is the conclusion? Does asking questions often imply higher score? Or it does not affect the score but affects the grade through the grading rules of Professor Moody. Similarly, does asking questions often imply higher score? Or it does not affect the score but affects the grade through the grading rules of Professor Moody. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhcHBseShtb29keSRzY29yZSwgbW9vZHkkdGV4dGluZywgbWVhbiksIHhsYWIgPSAndGV4dGluZyBjYXRlZ29yaWVzJyx5bGFiID0gJ1Njb3JlIEF2ZXJhZ2UnLCBtYWluID0gXCJNZWFuIFNjb3JlIHZzIFRleHRpbmcgdXNpbmcgdGFwcGx5KClcIixjb2w9YygncmVkJywnYmx1ZScsJ2dyZWVuJywnY3lhbicsJ3llbGxvdycpLGJvcmRlcj1cImJsYWNrXCIpIn0= shows that mean scores are the same across different values of the textingattribute. Same is true for the mean scores of questions attribute. Neither do these two attributes texing and questions seem to have impact on the grade. Therefore it seems that the behavioral attributes: questions and texting do not seem to have an impact on the grade. Lets examine participation attribute. This is the only culprit left - to explain different grades in the grey intervals of score. We define intervals of score as clear, if there is only one grade associated with scores from such interval. The remaining intervals are defined as grey - scores where grade can be either A or B, B or C, C or D and D or F respectively. Then we can examine how participation influences grades in these gray areas of score. Our hypothesis is that higher participation would probably offer better odds for higher grade. We can run the following command for different values of q. Let q be a threshold of participation which we want to test. May be if participation is higher than q, higher grade (from the two possible grades in the gray area of score) is given, while if participation is lower than q, it works against a student, who then gets lower grade? Lets check how the grade distribution changes for different values of q from the lower values of q to higher values of q. We can just change q directly in the code below, and see results immediately. Run the following command for different values of q. We will only show it for the grey interval between A and B. The same process can be repeated for other gray areas between B and C, C and D and D and F. In fact one can modify the code below by just replacing grade A and B with B and C respectively as well as replacing the variables LowestA with Lowest B and Highest B with Highest C respectively. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNTaW1wbGUgUiBjb21tYW5kcyB0byBnZXQgaW50ZXJ2YWwgZm9yIGVhY2ggZ3JhZGUuXG5Mb3dlc3RBPC1taW4obW9vZHlbbW9vZHkkZ3JhZGU9PSdBJywgXSRzY29yZSlcbkhpZ2hlc3RCPC1tYXgobW9vZHlbbW9vZHkkZ3JhZGU9PSdCJywgXSRzY29yZSkgXG4jVGhpcyBnaXZlcyB1cyBpbnRlcnZhbCA8SGlnaGVzdEIsIExvd2VzdEE+XG5wcmludChjKExvd2VzdEEsSGlnaGVzdEIpKVxuXG5xPTAuNSAjIFBsZWFzZSBFZGl0IHRoaXMgXCIgcSBcIiB2YWx1ZSBhbmQgc2VlIHRoZSBjaGFuZ2VzIGFzIG1lbnRpb25lZCBhYm92ZS5cbnRhYmxlKG1vb2R5W21vb2R5JHNjb3JlPkxvd2VzdEEgJiBtb29keSRzY29yZTxIaWdoZXN0QiYgbW9vZHkkcGFydGljaXBhdGlvbiA+IHEsXSRncmFkZSlcblxuI05vdGUgdGhlIHNhbWUgcHJvY2VzcyBjYW4gYmUgcmVwZWF0ZWQgd2l0aCBvdGhlciBhZGphY2VudCBncmFkZXMuIEV4OiA8QixDPiBldGMuIn0= Please verify that for higher values of 0&lt;q&lt;1, the table operation show higher percentages of better grades. Is there a critical value of q which clearly separates, say As from Bs? It seems to be q=0.6 - but it is not a clear cut deterministic. Rather, higher value of participation threshold, q increases probability (frequency) of getting As. We come to conclusion that participation matter in grey area of score, in having higher chance for better grade, if participation is higher. Thus, just in case (since no one can predict if they will end up in border line score) it is better to earn high participation index  by (probably) coming to class more often and participating in discussions, and answering professor Moodys questions. Simply put come to as many classes as you can. But while in class, do not worry about texting or asking questions. These two attributes do not seem to matter. Now we can reveal how data was generated? What was the real rule embedded in the data. Now it is time to reveal how we generated our data? This what we embedded in our data generation method: participation increases the chances of higher grade in the gray areas of score (border areas). We have indeed defined border areas in score value. In this border areas of score, participation plays a role. Student whoses score falls into the grey area may get one of two grades, A or B, B or C, C or D and D or F, depending on the score.For example score of 72 may result in A or B. It is more likely to be A if students participation is high (higher the better the odds of getting A). If students participation is low, it is much more likely to result in lower grade, for the score of 72, it would be B. Therefore we have discovered more or less the rule which guided generation of the Moodys data set and provided students with actionable intelligence how to increase chances of getting higher grade. Relationship between participation, score and grade In the process of slicing, dicing and plotting the data we would also discover other interesting relationships still using just 7 commands. Does higher participation mean higher score, in general? Meaning that coming to class is positively correlated with higher score? We can run scatter plot eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5wbG90KG1vb2R5JHBhcnRpY2lwYXRpb24sIG1vb2R5JHNjb3JlKSJ9 To our surprise it looks like the higher the participation, the lower the score! The distinct linear patterns in scatter graph seem to be sloping down with participation. We are tempted to infer that participation is bad for score, that somehow Moodys lectures have negative impact on the score - hence do not carry pedagogical value. Such conclusion is typical confusion between correlation and causation. What is true in our simulated data set - that students had some prior background in the subject matter of Professor Moody just do not show up in class that often. They already know the material. Students who do show up are the ones who are not confident in their knowledge of the subject matter - in general weak As and below. Then of course lower grade students (Ds and Fs) just simply do not apply themselves that much - are not invested into the class and just show up in class even less. Thus, the explanation probably has more to do with the students attribute about the class than with the pedagogical value of Professor Moodys lectures. We can change values of parameters q and s and examine in more detail the relationships between scores and participation. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNJbnRlcmVzaW5nIGFuYWx5c2lzIGhhcyB0byBkbyB3aXRoIGRpZ2dpbmcgZGVlcGVyIGludG8gcmVsYXRpb25zaGlwIGJldHdlZW4gcGFydGljcGF0aW9uIGFuZCBzY29yZS4gV2hhdCBhcmUgdGhlIHNjb3JlcyBvZiBzdHVkZW50cyB3aG8gcGFydGljaXBhdGUgbGVzcyB0aGFuIHNvbWUgdmFsdWUgcSA8MS4gV2hhdCBhcmUgdGhlIHBhcnRpY2lwYXRpb24gdmFsdWVzIG9mIHN0dWRlbmV0cyB3aG8gc2NvcmUgbGVzcyB0aGFuIHM/ICAgQnkgY2hhbmdpbmcgdGhlIHZhbHVlcyBvZiBxIGFuZCBzIGluIHRoZSBjb2RlIHlvdSBjYW4gZ2FpbiBtb3JlIGluc2lnaHQgaW50byByZWxhdGlvbnNoaXAgYmV0d2VlbiBwYXJ0aXBhdGlvbiBhbmQgc2NvcmUuXG5cbiMgQ2hhbmdlIHRoZSB2YWx1ZXMgb2YgXCJxXCIgYW5kIFwic1wiIGJlbG93LlxucTwtMC4xXG5zPC03MFxubWVhbihtb29keVttb29keSRwYXJ0aWNpcGF0aW9uIDxxLF0kc2NvcmUpXG5tZWFuKG1vb2R5W21vb2R5JHNjb3JlIDxzLF0kcGFydGljaXBhdGlvbikifQ== Exploring Behaviors of Students in professor Moodys class. One may even drop the grade entirely from the picture and simply inquire about behavioral characteristics of Professor Moodys students. We already know what is the distribution of each type of behavior eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG50YWJsZShtb29keSRxdWVzdGlvbnMpO1xudGFibGUobW9vZHkkdGV4dGluZykifQ== But lets ask for associations between behaviors Do students who ask a lot of questions also spend little time texting? Do students who participate more, generally texting less? These questions have nothing to do with students performance. But can all be answered using simple R.7 commands. Same data may serve different purposes. We started with predicting what behaviors help getting higher grade in professor Moodys class. But we can imagine a different study  which is addressing student behavior in professor Moodys class. Yet another study could address the impact of behavioral attributes on students scores (not grades). All these analysis can be done using or free style exploration and R.7. What we discover, and lets be very clear about it is not yet guaranteed to be statistically valid. For this we need statistical evaluation, The p-values, the z-tests etc. Later we will also find statistical functions which can greatly help in data exploration. Free style exploration role is to generate leads known otherwise as conjectures or hypotheses. Our professor Moodys data puzzle has been traditional the first data puzzle we ask students to solve in data 101 class. Here are some examples of conclusions reached on different instances of professor Moodys data set in the past semesters. Notice that attributes of Moodys data set in past may have been different (like sleeping in class) Here is the set of recommendations from former student who cracked that years professor Moodys puzzle (or did she?) Judging by plots and means calculated earlier, there are several factors, besides score, that affect students grades:  Sleeping in class increases grade  Texting in class decreases grades a little  Being active(participating) in class all the time significantly increases the grade, BUT:  Being active(participating) in class just occasionally decreases the grade even more, than not participating at all.  Being active only occasionally significantly decreases the grades.  Texting does not significantly affect grades . So for students in order to succeed in professor Moodys class, my advice will be(besides getting high score):  VERY IMPORTANT: Participate all the time., or do not participate at all!!!  Sleep in class(especially if you do not participate anyway)  While texting might bring down your grade a little bit, the difference is very small "],["stateval.html", "Chapter 3 Simple Statistical Evaluation 3.1 Z-test 3.2 Permutation Test 3.3 Multiple Hypothesis - Bonferroni Correction.", " Chapter 3 Simple Statistical Evaluation The biggest enemy of your findings is randomness. In order to convince your audience that you have found something you need to address the question how do you know your result is simply sheer luck, it is random? This is where you need statistical tests for use in hypothesis testing. 3.0.0.1 Two Important Formulas: Mean \\[\\begin{equation} \\bar{X}=\\frac{\\sum{X}}{N} \\ \\text{where, X is set of numbers and N is size of set.} \\end{equation}\\] Standard Deviation \\[\\begin{equation} \\sigma = \\sqrt{\\frac{\\sum{(X - \\mu)^2}}{N}}\\\\ \\text{where, X is set of numbers, $\\mu$ is average of set of numbers, }\\\\ \\text{ N is size of the set, $\\sigma$ is standard deviation} \\end{equation}\\] 3.1 Z-test A z-test is any statistical test used in hypothesis testing with an underlying normal distribution. In other words, when the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution, z-test can be used. Outcome of the z-test is the z-score which is a numerical measure to test the mean of a distribution. z-score is measured in terms of standard deviation from mean. 3.1.1 Steps for hypothesis testing using Z-test. Running a Z-test requires 5 steps: State the null hypothesis and the alternate hypothesis Select a null hypothesis and an alternate hypothesis which will be tested using the z-test. Choose an Alpha \\(\\alpha\\) level. Usually this is selected to be small, such that the area under the normal distribution curve is accumulated most in the range between the alpha level. Thus mostly in statistical testing, \\(\\alpha = 0.05\\) is selected. Calculate the z-test statistic. The z-test statistic is calculated using the z-score formula. \\[\\begin{equation} z = \\frac{x-\\mu}{\\sigma}\\text{ where, $z$ = z-score, $x$ = raw score, $\\mu$ = mean and $\\sigma$ = standard deviation } \\end{equation}\\] Calculate the p-value using the z-score Once we have the z-score we want to calculate the p-value from it. To do this, there are 2 ways, First use the z-table available online at z-table.com Second, use the pnorm() function in R to find the p-value. Compare the p-value with \\(\\alpha\\) After getting the p-value from step 4, compare it with the \\(\\alpha\\) level we selected in step 2. This decides if we can reject the null hypothesis or not. If the p-value obtained is lower than \\(\\alpha\\), then we can reject the null hypothesis. If the p-value is more than \\(\\alpha\\), we fail to reject the null hypothesis due to lack of significant evidence. Some important relation between one-sided and two sided test while using hypothesis testing is as follows: First, estimate the expected value \\(\\mu\\) of T(statistic) under the null hypothesis, and obtain an estimate \\(\\sigma\\) of the standard deviation of T. Second, determine the properties of T : one tailed or two tailed. For Null hypothesis H0: \\(\\mu \\geq \\mu_0\\) vs alternative hypothesis H1: \\(\\mu &lt; \\mu_0\\) , it is upper/right-tailed (one tailed). For Null hypothesis H0:\\(\\mu \\leq \\mu_0\\) vs alternative hypothesis H1: \\(\\mu &gt; \\mu_0\\) , it is lower/left-tailed (one tailed). For Null hypothesis H0: \\(\\mu = \\mu_0\\) vs alternative hypothesis H1: \\(\\mu \\neq \\mu_0\\) , it is two-tailed. Once you calculate the pnorm() in step 4, depending on the properties of two as described above, use pnorm(-Z) for right tailed tests, use 2*pnorm(-Z) for two tailed test, and use pnorm(Z) for left tailed tests. Note: (Here Z = z-score). Also the method mentioned above works similar to that studied in class/recitations, but is simple to understand, and does not require subtracting the pnorm() output from 1. 3.1.2 Z-test Example 1 (Right Sided) Now lets look at an example to use this z-test for hypothesis testing. We will study the example to statistically find the relation of the traffic volume per minute between two tunnels, namely Holland and Lincoln . Table 3.1: Snippet of Traffic Dataset TUNNEL DAY VOLUME_PER_MINUTE 832 Holland weekday 37 1290 Holland weekend 61 166 Holland weekday 50 1713 Lincoln weekday 77 1058 Holland weekend 57 2278 Lincoln weekday 61 234 Holland weekday 41 988 Holland weekday 49 139 Holland weekday 58 675 Holland weekday 42 Thus stating out Null Hypothesis and Alternate Hypothesis. Null Hypothesis H0: Traffic in Lincoln is same as Traffic in Holland tunnel. Alternate Hypothesis H1: Traffic in Lincoln is higher than traffic in Holland tunnel. Once we have stated our hypothesis, lets see the z-test in practice. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YXNldFxuVFJBRkZJQzwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9UUkFGRklDLmNzdicpXG5zdW1tYXJ5KFRSQUZGSUMpICNnaXZlcyB1cyB0aGUgZGF0YSBzdGF0aXN0aWNzXG5cbiNkYXRhIGNsZWFuIGFuZCBzdWJzZXRcbmxpbmNvbG4uZGF0YSA8LSBzdWJzZXQoVFJBRkZJQywgVFJBRkZJQyRUVU5ORUwgPT0gXCJMaW5jb2xuXCIpXG5ob2xsYW5kLmRhdGEgPC0gc3Vic2V0KFRSQUZGSUMsIFRSQUZGSUMkVFVOTkVMID09IFwiSG9sbGFuZFwiKVxuXG4jIHRyYWZmaWMgYXQgbGluY29sblxuIyBUaGlzIHZhcmlhYmxlIGlzIGEgY29sdW1uIG9mIDE0MDEgcm93cy5cbmxpbmNvbG4udHJhZmZpYyA8LSBsaW5jb2xuLmRhdGEkVk9MVU1FX1BFUl9NSU5VVEVcblxuIyB0cmFmZmljIGF0IGhvbGxhbmRcbiMgVGhpcyB2YXJpYWJsZSBpcyBhIGNvbHVtbiBvZiAxNDAxIHJvd3MuXG5ob2xsYW5kLnRyYWZmaWMgPC0gaG9sbGFuZC5kYXRhJFZPTFVNRV9QRVJfTUlOVVRFXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuIyBUaGUgZmluYWwgdmFsdWUgaXMgdGhlIHN0YW5kYXJkIGRldmlhdGlvbiwgaW4gVm9sdW1lIHBlciBtaW51dGUuXG5zZC5saW5jb2xuIDwtIHNkKGxpbmNvbG4udHJhZmZpYylcbnNkLmhvbGxhbmQgPC0gc2QoaG9sbGFuZC50cmFmZmljKVxuXG4jIG1lYW5zIG9mIHR3byBzYW1wbGVzXG5tZWFuLmxpbmNvbG4gPC0gbWVhbihsaW5jb2xuLnRyYWZmaWMpXG5tZWFuLmhvbGxhbmQgPC0gbWVhbihob2xsYW5kLnRyYWZmaWMpXG5cbiMgbGVuZ3RoIG9mIGxpbmNvbG4gYW5kIGhvbGxhbmRcbmxlbl9saW5jb2xuIDwtIGxlbmd0aChsaW5jb2xuLnRyYWZmaWMpXG5sZW5faG9sbGFuZCA8LSBsZW5ndGgoaG9sbGFuZC50cmFmZmljKVxuXG4jIHN0YW5kYXJkIGRldmlhdGlvbiBvZiB0cmFmZmljXG5zZC5saW4uaG9sIDwtIHNxcnQoc2QubGluY29sbl4yL2xlbl9saW5jb2xuICsgc2QuaG9sbGFuZF4yL2xlbl9ob2xsYW5kKVxuXG4jIHogc2NvcmVcbnpldGEgPC0gKG1lYW4ubGluY29sbiAtIG1lYW4uaG9sbGFuZCkvc2QubGluLmhvbFxuemV0YVxuXG4jIGdldCBwXG5wID0gcG5vcm0oLXpldGEpXG5wXG5cbiMgcGxvdCB0aGUgemV0YSB2YWx1ZSBvbiB0aGUgbm9ybWFsIGRpc3RyaWJ1dGlvbiBjdXJ2ZS5cbnBsb3QoeD1zZXEoZnJvbSA9IC0yNSwgdG89IDI1LCBieT0wLjEpLHk9ZG5vcm0oc2VxKGZyb20gPSAtMjUsIHRvPSAyNSwgIGJ5PTAuMSksbWVhbj0wKSx0eXBlPSdsJyx4bGFiID0gJ21lYW4gZGlmZmVyZW5jZScsICB5bGFiPSdwb3NzaWJpbGl0eScpXG5hYmxpbmUodj16ZXRhLCBjb2w9J3JlZCcpIn0= We can see that form the P-Value obtained is near to 0, which is less than 0.05. Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that traffic in Lincoln is higher than traffic Holland. 3.1.3 Z-test Example 2 (Left Sided) Now lets look at another example to use this z-test for hypothesis testing. We will study the example to statistically find the relation between capital gains of people with two Zodiac Signs , namely Aquarius and Libra. Table 3.2: Snippet of Zodiac Dataset AGE STATUS EDUCATION YEARS PROFESSION CAPITALGAINS CAPITALLOSS NATIVE ZODIAK 10906 34 Private 10th 6 Craft-repair 0 0 United-States Capricorn 24608 52 Self-emp-inc Some-college 10 Sales 0 0 United-States Taurus 11463 34 Private Bachelors 13 Prof-specialty 0 0 United-States Taurus 24993 53 Private HS-grad 9 Craft-repair 0 0 United-States Sagittarius 19318 44 Private Bachelors 13 Craft-repair 0 0 United-States Leo 2714 24 Private Some-college 10 Exec-managerial 0 0 United-States Leo 8041 30 Private Some-college 10 Craft-repair 249330 0 United-States Taurus 11112 34 State-gov Bachelors 13 Prof-specialty 0 11276 United-States Leo 2455 24 Private Bachelors 13 Sales 0 0 United-States Cancer 7293 30 Private HS-grad 9 Machine-op-inspct 5013 0 United-States Aquarius Now stating out Null Hypothesis and Alternate Hypothesis. Null Hypothesis H0: Capital Gains of people with Aquarius is same as people with Libra zodiac sign. Alternate Hypothesis H1: Capital Gains of people with Aquarius is lower than as people with Libra zodiac sign. Once we have stated our hypothesis, lets see the z-test in practice. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YXNldFxuWm9kaWFjRGF0YTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9ab2RpYWNDaGFsbGVuZ2UuY3N2JylcbnN1bW1hcnkoWm9kaWFjRGF0YSkgI2dpdmVzIHVzIHRoZSBkYXRhIHN0YXRpc3RpY3NcblxuI2RhdGEgY2xlYW4gYW5kIHN1YnNldFxuQXF1YXJpdXMuZGF0YSA8LSBzdWJzZXQoWm9kaWFjRGF0YSwgWm9kaWFjRGF0YSRaT0RJQUsgPT0gXCJBcXVhcml1c1wiKVxuTGlicmEuZGF0YSA8LSBzdWJzZXQoWm9kaWFjRGF0YSwgWm9kaWFjRGF0YSRaT0RJQUsgPT0gXCJMaWJyYVwiKVxuXG4jIFpvZGlhYyBBcXVhcml1c1xuQXF1YXJpdXMuWm9kaWFjIDwtIEFxdWFyaXVzLmRhdGEkQ0FQSVRBTEdBSU5TXG5cbiMgWm9kaWFjICBMaWJyYVxuTGlicmEuWm9kaWFjIDwtIExpYnJhLmRhdGEkQ0FQSVRBTEdBSU5TXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuc2QuQXF1YXJpdXMgPC0gc2QoQXF1YXJpdXMuWm9kaWFjKVxuc2QuTGlicmEgPC0gc2QoTGlicmEuWm9kaWFjKVxuXG4jIG1lYW5zIG9mIHR3byBzYW1wbGVzXG5tZWFuLkFxdWFyaXVzIDwtIG1lYW4oQXF1YXJpdXMuWm9kaWFjKVxubWVhbi5MaWJyYSA8LSBtZWFuKExpYnJhLlpvZGlhYylcblxuIyBsZW5ndGggb2YgQXF1YXJpdXMgYW5kIExpYnJhXG5sZW5fQXF1YXJpdXMgPC0gbGVuZ3RoKEFxdWFyaXVzLlpvZGlhYylcbmxlbl9MaWJyYSA8LSBsZW5ndGgoTGlicmEuWm9kaWFjKVxuXG4jIHN0YW5kYXJkIGRldmlhdGlvblxuc2QuYXF1LmxpYiA8LSBzcXJ0KHNkLkFxdWFyaXVzXjIvbGVuX0FxdWFyaXVzICsgc2QuTGlicmFeMi9sZW5fTGlicmEpXG5cbiMgeiBzY29yZVxuemV0YSA8LSAobWVhbi5BcXVhcml1cyAtIG1lYW4uTGlicmEpL3NkLmFxdS5saWJcbnpldGFcblxuIyBnZXQgcFxucCA9IHBub3JtKHpldGEpXG5wXG5cbiMgcGxvdCB0aGUgemV0YSB2YWx1ZSBvbiB0aGUgbm9ybWFsIGRpc3RyaWJ1dGlvbiBjdXJ2ZS5cbnBsb3QoeD1zZXEoZnJvbSA9IC0yNSwgdG89IDI1LCBieT0wLjEpLHk9ZG5vcm0oc2VxKGZyb20gPSAtMjUsIHRvPSAyNSwgIGJ5PTAuMSksbWVhbj0wKSx0eXBlPSdsJyx4bGFiID0gJ21lYW4gZGlmZmVyZW5jZScsICB5bGFiPSdwb3NzaWJpbGl0eScpXG5hYmxpbmUodj16ZXRhLCBjb2w9J3JlZCcpIn0= We can see that form the P-Value obtained is less than 0.05. Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that Capital Gains of people with Aquarius is lower than as people with Libra zodiac sign. 3.1.4 Z-test Example 3 (Two Tailed) We will study the example to statistically find the relation between capital gains of people with two Countries, namely US and Columbia. Now stating out Null Hypothesis and Alternate Hypothesis. Null Hypothesis H0: Capital Gains of people of United States is same as people of Colombia. Alternate Hypothesis H1: Capital Gains of people of United States is not equal to that of the people of Colombia. Once we have stated our hypothesis, lets see the z-test in practice. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YXNldFxuWm9kaWFjRGF0YTwtcmVhZC5jc3YoJ2h0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9ab2RpYWNDaGFsbGVuZ2UuY3N2JylcbnN1bW1hcnkoWm9kaWFjRGF0YSkgI2dpdmVzIHVzIHRoZSBkYXRhIHN0YXRpc3RpY3NcblxuI2RhdGEgY2xlYW4gYW5kIHN1YnNldFxuVVMuZGF0YSA8LSBzdWJzZXQoWm9kaWFjRGF0YSwgWm9kaWFjRGF0YSROQVRJVkUgPT0gXCJVbml0ZWQtU3RhdGVzXCIpXG5Db2x1bWJpYS5kYXRhIDwtIHN1YnNldChab2RpYWNEYXRhLCBab2RpYWNEYXRhJE5BVElWRSA9PSBcIkNvbHVtYmlhXCIpXG5cbiMgQ291bnRyeSBVU1xuVVMuY291bnRyeSA8LSBVUy5kYXRhJENBUElUQUxHQUlOU1xuXG4jIENvdW50cnkgIENvbHVtYmlhXG5Db2x1bWJpYS5jb3VudHJ5IDwtIENvbHVtYmlhLmRhdGEkQ0FQSVRBTEdBSU5TXG5cbiMgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIHR3byBzYW1wbGVzLlxuc2QuVVMgPC0gc2QoVVMuY291bnRyeSlcbnNkLkNvbHVtYmlhIDwtIHNkKENvbHVtYmlhLmNvdW50cnkpXG5cbiMgbWVhbnMgb2YgdHdvIHNhbXBsZXNcbm1lYW4uVVMgPC0gbWVhbihVUy5jb3VudHJ5KVxubWVhbi5Db2x1bWJpYSA8LSBtZWFuKENvbHVtYmlhLmNvdW50cnkpXG5cbiMgbGVuZ3RoIG9mIFVTIGFuZCBDb2x1bWJpYVxubGVuX1VTIDwtIGxlbmd0aChVUy5jb3VudHJ5KVxubGVuX0NvbHVtYmlhIDwtIGxlbmd0aChDb2x1bWJpYS5jb3VudHJ5KVxuXG4jIHN0YW5kYXJkIGRldmlhdGlvblxuc2QudXMuY29sIDwtIHNxcnQoc2QuVVNeMi9sZW5fVVMgKyBzZC5Db2x1bWJpYV4yL2xlbl9Db2x1bWJpYSlcblxuIyB6IHNjb3JlXG56ZXRhIDwtIChtZWFuLlVTIC0gbWVhbi5Db2x1bWJpYSkvc2QudXMuY29sXG56ZXRhXG5cbiMgZ2V0IHBcbnAgPSAyKnBub3JtKC16ZXRhKVxucFxuXG4jIHBsb3QgdGhlIHpldGEgdmFsdWUgb24gdGhlIG5vcm1hbCBkaXN0cmlidXRpb24gY3VydmUuXG5wbG90KHg9c2VxKGZyb20gPSAtMjUsIHRvPSAyNSwgYnk9MC4xKSx5PWRub3JtKHNlcShmcm9tID0gLTI1LCB0bz0gMjUsICBieT0wLjEpLG1lYW49MCksdHlwZT0nbCcseGxhYiA9ICdtZWFuIGRpZmZlcmVuY2UnLCAgeWxhYj0ncG9zc2liaWxpdHknKVxuYWJsaW5lKHY9emV0YSwgY29sPSdyZWQnKSJ9 We can see that form the P-Value obtained is less than 0.05. Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that Capital Gains of people of United States is not equal to that of the people of Colombia. 3.2 Permutation Test Permutation test allows us to observe randomness directly, with naked eye, without the lenses of statistical tests such as z-tests etc. We shuffle data randomly like a deck of cards. There may be many such shuffles - 10,000, 100,000 etc. The goal is always to see how often we can obtained the observed difference of means (since we are testing either one sided or two sided hypothesis), by purely random shuffles of our data. These permutations (shuffles) destroy all relationships which may pre-exist in our data. We are hoping to show that our observed difference of means can be obtained very rarely in completely random fashion. Then we experimentally show that our result is unlikely to randomly occur under null hypothesis. Then we can reject the null hypothesis. The less often our result appear in the histogram of permutation test results, the better the news for our alternative hypothesis. What is surprising to many newcomers, is that permutation test will give different p-values (not dramatically different, but still different) in each run of permutation test. This is the case because permutation test in random itself. It is not like z-test which will give the same result when run again for the same hypothesis and same data set. Also p-value computed by permutation test will be, in general, different than p-value computed by z-test. Not very different but different. Again, it is the case because permutation test provides only approximation of p-value. Great advantage of permutation test is that it is universal and robust. One can test different relationships between two variables than just difference of means. For example we can use permutation test to validate whether traffic in Lincoln tunnel is more than twice the traffic in Holland tunnel or even provide different weights for different days of the week. 3.2.1 Permutation Test One Step Permutation test in one step is the most direct way to see randomness close by. One step permutation function shows one single data shuffle. By shuffling the data one destroys associations which exist between values of the data frame. This make data frame random. You can execute the one step permutation multiple times. This will show how data frame varies and how does it affect the observed difference of means. Apply one step permutation function first, multiple times before you move to the proper Permutation test function. One of the parameters of the Permutation test function specifies the number of shuffles which will be preformed. This could be a very large number, 10,000 or even 100,000. The purpose of making so many random permutations is to test how often observed difference of means can arise in just random data. The more often this takes place, the more likely you observation is just random. To reject the null hypothesis you need to show that the observed difference of means will come very infrequently in permutation test. Less than 5% of the time, to be exact. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKSIsInNhbXBsZSI6InN1bW1hcnkodHJhZmZpYylcbkQ8LSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdIb2xsYW5kJywzXSkgLSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywzXSlcbm51bGxfdHVubmVsIDwtIHJlcChcIkhvbGxhbmRcIiwyODAxKSAjIENyZWF0ZSAyODAxIGNvcGllcyBvZiBIb2xsYW5kIFxubnVsbF90dW5uZWxbc2FtcGxlKDI4MDEsMTQwMCldIDwtIFwiTGluY29sblwiICMgUmVwbGFjZSBSQU5ET01MWSAxNDAwIGNvcGllcyB3aXRoIExpbmNvbG5cbm51bGwgPC0gZGF0YS5mcmFtZShudWxsX3R1bm5lbCx0cmFmZmljWywzXSlcbm5hbWVzKG51bGwpIDwtIGMoXCJUVU5ORUxcIixcIlZPTFVNRV9QRVJfTUlOVVRFXCIpXG5zdW1tYXJ5KG51bGwpXG5ob2xsYW5kX251bGwgPC0gbnVsbFtudWxsJFRVTk5FTCA9PSBcIkhvbGxhbmRcIiwyXVxubGluY29sbl9udWxsIDwtIG51bGxbbnVsbCRUVU5ORUwgPT0gXCJMaW5jb2xuXCIsMl1cbm1lYW4oaG9sbGFuZF9udWxsKVxubWVhbihsaW5jb2xuX251bGwpXG5EX251bGwgPC0gbWVhbihsaW5jb2xuX251bGwpIC0gbWVhbihob2xsYW5kX251bGwpXG5jYXQoXCJUaGUgbWVhbiBkaWZmZXJlbmNlIG9mIHBlcm11dGF0aW9uIG9uZSBzdGVwIGRhdGE6IFwiLCBEX251bGwsXCJcXG5cIikjIENhbGN1bGF0ZSB0aGUgZGlmZmVyZW5jZSBiZXR3ZWVuIHRoZSBtZWFuIG9mIHRoZSByYW5kb20gZGF0YS5cbmNhdChcIlRoZSBtZWFuIGRpZmZlcmVuY2Ugb2Ygb3JpZ2luYWwgZGF0YTogXCIsIEQpICMgRGlmZmVyZW5jZSBvZiBtZWFuIHZhbHVlIG9mIG9yaWdpbmFsIGRhdGEuIn0= 3.2.2 Permutation Function The permutation function is used to run multiple iteration of the one-step permutation studied above, to get a complete relational understanding between the components involved in any hypothesis. Here you can run the example of running the permutation test on the Traffic.csv dataset, on volume of traffic in Holland and Lincoln Tunnel. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKVxuUGVybXV0YXRpb24gPC0gZnVuY3Rpb24oZGYxLGMxLGMyLG4sdzEsdzIpe1xuICBkZiA8LSBhcy5kYXRhLmZyYW1lKGRmMSlcbiAgRF9udWxsPC1jKClcbiAgVjE8LWRmWyxjMV1cbiAgVjI8LWRmWyxjMl1cbiAgc3ViLnZhbHVlMSA8LSBkZltkZlssIGMxXSA9PSB3MSwgYzJdXG4gIHN1Yi52YWx1ZTIgPC0gZGZbZGZbLCBjMV0gPT0gdzIsIGMyXVxuICBEIDwtICBhYnMobWVhbihzdWIudmFsdWUyLCBuYS5ybT1UUlVFKSAtIG1lYW4oc3ViLnZhbHVlMSwgbmEucm09VFJVRSkpXG4gIG09bGVuZ3RoKFYxKVxuICBsPWxlbmd0aChWMVtWMT09dzJdKVxuICBmb3IoamogaW4gMTpuKXtcbiAgICBudWxsIDwtIHJlcCh3MSxsZW5ndGgoVjEpKVxuICAgIG51bGxbc2FtcGxlKG0sbCldIDwtIHcyXG4gICAgbmYgPC0gZGF0YS5mcmFtZShLZXk9bnVsbCwgVmFsdWU9VjIpXG4gICAgbmFtZXMobmYpIDwtIGMoXCJLZXlcIixcIlZhbHVlXCIpXG4gICAgdzFfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzEsMl1cbiAgICB3Ml9udWxsIDwtIG5mW25mJEtleSA9PSB3MiwyXVxuICAgIERfbnVsbCA8LSBjKERfbnVsbCxtZWFuKHcyX251bGwsIG5hLnJtPVRSVUUpIC0gbWVhbih3MV9udWxsLCBuYS5ybT1UUlVFKSlcbiAgfVxuICBteWhpc3Q8LWhpc3QoRF9udWxsLCBwcm9iPVRSVUUpXG4gIG11bHRpcGxpZXIgPC0gbXloaXN0JGNvdW50cyAvIG15aGlzdCRkZW5zaXR5XG4gIG15ZGVuc2l0eSA8LSBkZW5zaXR5KERfbnVsbCwgYWRqdXN0PTIpXG4gIG15ZGVuc2l0eSR5IDwtIG15ZGVuc2l0eSR5ICogbXVsdGlwbGllclsxXVxuICBwbG90KG15aGlzdClcbiAgbGluZXMobXlkZW5zaXR5LCBjb2w9J2JsdWUnKVxuICBhYmxpbmUodj1ELCBjb2w9J3JlZCcpXG4gIE08LW1lYW4oRF9udWxsPkQpXG4gIHJldHVybihNKVxufSIsInNhbXBsZSI6IlBlcm11dGF0aW9uKHRyYWZmaWMsIFwiVFVOTkVMXCIsIFwiVk9MVU1FX1BFUl9NSU5VVEVcIiwxMDAwLFwiSG9sbGFuZFwiLCBcIkxpbmNvbG5cIikifQ== Note: You can find the permutation function code here: Permutation() NOTE: The red line in the output plots of the permutation test function is not the p-value, but it is just the difference of the value of means of the two categories under test. 3.2.3 Exercise - How p-value is affected by difference of means and standard deviations Here, you can generate your own data by changing parameters of the rnorm() function. See how changing the mean and sd in rnorm distributions affects the p-value! Again you can do it directly in the code and observe the results immediately. It is very revealing.. Think of Val1, and Val2 as traffic volumes in Holland and Lincoln tunnels respectively. The larger the difference between the means of rnorm() function the smaller the p-value - since it is less and less likely that observed difference of means would come frequently, due to random shuffles of permutation function. Now keep the same means and change the variances. See how changing the variances in rnorm() will affect the p-value and try to explain the effect that standard deviations have on the p-value. In general, the higher the standard deviation, the more widely data is centered around the mean. Thus even for the same two means, and two different value of deviations, we can see larger value of deviation to lead to higher p-value. Since we are less certain of the role of the mean if standard deviation is higher. Therefore, the chance of randomly obtaining the observed result, is higher. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IlBlcm11dGF0aW9uIDwtIGZ1bmN0aW9uKGRmMSxjMSxjMixuLHcxLHcyKXtcbiAgZGYgPC0gYXMuZGF0YS5mcmFtZShkZjEpXG4gIERfbnVsbDwtYygpXG4gIFYxPC1kZlssYzFdXG4gIFYyPC1kZlssYzJdXG4gIHN1Yi52YWx1ZTEgPC0gZGZbZGZbLCBjMV0gPT0gdzEsIGMyXVxuICBzdWIudmFsdWUyIDwtIGRmW2RmWywgYzFdID09IHcyLCBjMl1cbiAgRCA8LSAgYWJzKG1lYW4oc3ViLnZhbHVlMiwgbmEucm09VFJVRSkgLSBtZWFuKHN1Yi52YWx1ZTEsIG5hLnJtPVRSVUUpKVxuICBtPWxlbmd0aChWMSlcbiAgbD1sZW5ndGgoVjFbVjE9PXcyXSlcbiAgZm9yKGpqIGluIDE6bil7XG4gICAgbnVsbCA8LSByZXAodzEsbGVuZ3RoKFYxKSlcbiAgICBudWxsW3NhbXBsZShtLGwpXSA8LSB3MlxuICAgIG5mIDwtIGRhdGEuZnJhbWUoS2V5PW51bGwsIFZhbHVlPVYyKVxuICAgIG5hbWVzKG5mKSA8LSBjKFwiS2V5XCIsXCJWYWx1ZVwiKVxuICAgIHcxX251bGwgPC0gbmZbbmYkS2V5ID09IHcxLDJdXG4gICAgdzJfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzIsMl1cbiAgICBEX251bGwgPC0gYyhEX251bGwsbWVhbih3Ml9udWxsLCBuYS5ybT1UUlVFKSAtIG1lYW4odzFfbnVsbCwgbmEucm09VFJVRSkpXG4gIH1cbiAgbXloaXN0PC1oaXN0KERfbnVsbCwgcHJvYj1UUlVFKVxuICBtdWx0aXBsaWVyIDwtIG15aGlzdCRjb3VudHMgLyBteWhpc3QkZGVuc2l0eVxuICBteWRlbnNpdHkgPC0gZGVuc2l0eShEX251bGwsIGFkanVzdD0yKVxuICBteWRlbnNpdHkkeSA8LSBteWRlbnNpdHkkeSAqIG11bHRpcGxpZXJbMV1cbiAgcGxvdChteWhpc3QpXG4gIGxpbmVzKG15ZGVuc2l0eSwgY29sPSdibHVlJylcbiAgYWJsaW5lKHY9RCwgY29sPSdyZWQnKVxuICBNPC1tZWFuKERfbnVsbD5EKVxuICByZXR1cm4oTSlcbn0iLCJzYW1wbGUiOiJOLmggPC0gMTAgI051bWJlciBvZiB0dXBsZXMgZm9yIEhvbGxhbmQgVHVubmVsXG5OLmwgPC0gMTAgI051bWJlciBvZiB0dXBsZXMgZm9yIExpbmNvbG4gVHVubmVsXG5cbkNhdDE8LXJlcChcIkdyb3VwQVwiLE4uaCkgICMgZm9yIGV4YW1wbGUgR3JvdXBBIGNhbiBiZSBIb2xsYW5kIFR1bm5lbFxuQ2F0MjwtcmVwKFwiR3JvdXBCXCIsTi5sKSAgIyBmb3IgZXhhbXBsZSBHcm91cCBCIHdpbGwgYmUgTGluY29sbiBUdW5uZWxcblxuQ2F0MVxuQ2F0MlxuXG4jVGhlIHJlcCBjb21tYW5kIHdpbGwgcmVwZWF0LCB0aGUgdmFyaWFibGVzIHdpbGwgYmUgb2YgdHlwZSBjaGFyYWN0ZXIgYW5kIHdpbGwgY29udGFpbiAxMCB2YWx1ZXMgZWFjaC5cblxuQ2F0PC1jKENhdDEsQ2F0MikgIyBBIHZhcmlhYmxlIHdpdGggZmlyc3QgMTAgdmFsdWVzIEdyb3VwQSBhbmQgbmV4dCAxMCB2YWx1ZXMgR3JvdXBCXG5DYXRcblxuI1RyeSBjaGFuZ2luZyBtZWFuIGFuZCBzZCB2YWx1ZXMuIFdoZW4geW91IHJ1biB0aGlzIHlvdSB3aWxsIHNlZSB0aGF0IHRoZSBkaWZmZXJlbmNlIGlzIHNvbWV0aW1lcyBuZWdhdGl2ZSAjb3Igc29tZXRpbWVzIHBvc2l0aXZlLlxuXG5WYWwxPC1ybm9ybShOLmgsbWVhbj0yNSwgc2Q9MTApICNzYXksIHRyYWZmaWMgdm9sdW1lIGluIEhvbGxhbmQgVCBhcyBub3JtYWwgZGlzdHJpYnV0aW9uIHdpdGggbWVhbiBhbmQgc2RcblZhbDI8LXJub3JtKE4ubCxtZWFuPTMwLCBzZD0xMCkgI3NheSwgdHJhZmZpYyB2b2x1bWUgaW4gTGluY29sbiBUIGFzIG5vcm1hbCBkaXN0cmlidXRpb24gd2l0aCBtZWFuIGFuZCBzZFxuXG5WYWw8LWMoVmFsMSxWYWwyKSAjQSB2YXJpYWJsZSB3aXRoIDIwIHJvd3MsIHdpdGggZmlyc3QgMTAgcm93cyBjb250YWluaW5nIDEwIHJhbmRvbSBub3JtYWwgdmFsdWVzIG9mIFZhbDEgI2FuZCB0aGUgbmV4dCAxMCB2YWx1ZXMgb2YgVmFsMlxuXG5WYWxcblxuZDwtZGF0YS5mcmFtZShDYXQsVmFsKVxuXG5PYnNlcnZlZF9EaWZmZXJlbmNlPC1tZWFuKGRbZCRDYXQ9PSdHcm91cEEnLDJdKS1tZWFuKGRbZCRDYXQ9PSdHcm91cEInLDJdKVxuXG4jVGhpcyB3aWxsIGNhbGN1bGF0ZSB0aGUgbWVhbiBvZiB0aGUgc2Vjb25kIGNvbHVtbiAoaGF2aW5nIDEwIHJhbmRvbSB2YWx1ZXMgZm9yIGVhY2ggZ3JvdXApLCBhbmQgdGhlIG1lYW4gb2YgZ3JvdXBCIHZhbHVlcyBpcyBzdWJ0cmFjdGVkIGZyb20gdGhlIG1lYW4gb2YgZ3JvdXBBIHZhbHVlcywgd2hpY2ggd2lsbCBnaXZlIHlvdSB0aGUgdmFsdWUgb2YgdGhlIGRpZmZlcmVuY2Ugb2YgdGhlIG1lYW4uXG5PYnNlcnZlZF9EaWZmZXJlbmNlXG5cblxuUGVybXV0YXRpb24oZCwgXCJDYXRcIiwgXCJWYWxcIiwxMDAwMCwgXCJHcm91cEFcIiwgXCJHcm91cEJcIilcblxuI1RoZSBQZXJtdXRhdGlvbiBmdW5jdGlvbiByZXR1cm5zIHRoZSBhYnNvbHV0ZSB2YWx1ZSBvZiB0aGUgZGlmZmVyZW5jZS4gU28gdGhlIHJlZCBsaW5lIGlzIHRoZSBhYnNvbHV0ZSB2YWx1ZSBvZiB0aGUgb2JzZXJ2ZWQgZGlmZmVyZW5jZS4gWW91IHdpbGwgc2VlIGEgaGlzdG9ncmFtIGhhdmluZyBhIG5vcm1hbCBkaXN0cmlidXRpb24gd2l0aCBhIHJlZCBzaG93aW5nIHRoZSBvYnNlcnZlZCBkaWZmZXJlbmNlLiJ9 3.3 Multiple Hypothesis - Bonferroni Correction. While dealing with the dataset with several number of dimensions, it is possible to get a lot of amazing and interesting insights and conclusions from it. But, unfortunately, sometimes a lot of the data included in case of such large dataset, might be junk. We can make multiple assumptions from such data. But, while doing so, we may consider some useless data/patterns that might hamper our results and lead to the pitfall of believing in hypotheses, that are not actually true. This is common when performing multiple hypothesis testing. Multiple hypothesis testing refers to any instance that involves the simultaneous testing of more than one hypothesis. Lets consider the example of Traffic dataset. We have given two tunnels Holland and Lincoln, but what if we were given all the tunnels in the US? We can make a lot of hypotheses in that case. And for each set of hypothesis, would you still consider the value of  as 0.05 as the cut-off for P-value? It may seem to be a good idea to just go and check the p-value for any set of hypotheses with the cut-off value of \\(\\alpha\\) as 0.05. But this might not give you the correct answer always. If you have 100 different hypotheses to consider in the data, then the probability of getting at least one significant result with \\(\\alpha = 0.05\\) will be, \\[P(\\text{at least one significant result}) = 1- (1-0.05)^{100}  0.99\\] This means that if we consider 0.05 as our cut-off value, then the probability of getting at least one significant result will be about 99%, which leads to overfitting of data and it clearly doesnt give us proper idea about our hypothesis. Methods for dealing with multiple testing frequently call for adjusting \\(\\alpha\\) in some way, so that the probability of observing at least one significant result due to chance remains below your desired significance level. One such method for adjusting \\(\\alpha\\) is BONFERRONI CORRECTION! The Bonferroni correction sets the significance cut-off at \\(\\alpha / N\\) where N is the number of possible hypotheses. For example, in the example above, with 100 tests and \\(\\alpha = 0.05\\), youd only reject a null hypothesis if the p-value is less than \\(\\alpha/N = 0.05/100 = 0.0005\\) Thus, the value of \\(\\alpha\\) after Bonferroni correction would be \\(0.0005\\). Again, lets calculate the probability of observing at least one significant result when using the correction just described: \\[P(\\text{at least one significant result}) = 1  P(\\text{no significant results}) \\\\ = 1  (1  0.0005)^{100}  0.048\\] This gives us 4.8% probability of getting at least one significant result. As we can see this value of probability using Bonferroni correction is much better than the 99% which we saw before when we did not use correction for performing multiple hypothesis testing. But there are some downfall of using Bonferroni correction too. (Although for the scope of this course Bonferroni Correction works fine.) The Bonferroni correction tends to be a bit too conservative. Also, we benefit here from assuming that all tests are independent of each other. In practical applications, that is often not the case. Depending on the correlation structure of the tests, the Bonferroni correction could beextremely conservative, leading to a high rate of false negatives. 3.3.1 Examples for Multiple hypothesis testing. Lets consider the Happiness dataset as an example. Table 3.3: Snippet of Happiness Dataset IDN AGE COUNTRY GENDER IMMIGRANT INCOME HAPPINESS 3308 72704 37 Kazakhstan Female 0 88201 7.42 4600 76777 56 Trinidad and Tobago Male 1 93598 4.95 1282 31315 53 Sweden Male 1 49034 7.83 5122 84009 20 Colombia Male 0 100414 8.26 2433 42920 33 Iraq Female 0 57674 4.36 4308 13675 54 Zambia Male 1 28754 3.28 4071 51005 43 Thailand Male 0 67206 5.47 3927 38442 30 Morocco Male 0 53284 4.33 6382 20492 50 Turkey Female 0 35729 2.51 287 55146 63 Suriname Female 0 71402 5.67 There are 156 unique countries in the dataset. This can be checked using the unique() function  unique(indiv_happiness$country) Since there are 156 distinct countries, we have \\({{n}\\choose{2}} = {156\\choose2}=(156 * 155)/2 = 12090\\) different hypotheses. Lets call this value N. Using this N, the P-value cutoff after Bonferroni correction will be, \\( = 0.05 / 12090  4.13 *10^{-6}\\) 3.3.1.1 Example 1 Lets calculate the P-value for the following hypotheses from the dataset. Our hypothesis: People from Canada are happier than people from Iceland. Null hypothesis: There is no difference in happiness levels of people from Canada and people from Iceland. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgZGF0YXNldFxuaGFwcGluZXNzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L0hBUFBJTkVTUzIwMTcuY3N2XCIsIHN0cmluZ3NBc0ZhY3RvcnMgPSBUKSAjd2ViIGxvYWRcblxuIyBUd28gc3Vic2V0cyBvZiBDYW5hZGEgYW5kIEljZWxhbmQgXG5oYXBwaW5lc3MuY2FuYWRhIDwtIHN1YnNldChoYXBwaW5lc3MkSEFQUElORVNTLCBoYXBwaW5lc3MkQ09VTlRSWSA9PVwiQ2FuYWRhXCIpXG5oYXBwaW5lc3MuaWNlbGFuZCA8LSBzdWJzZXQoaGFwcGluZXNzJEhBUFBJTkVTUywgaGFwcGluZXNzJENPVU5UUlkgPT0gXCJJY2VsYW5kXCIpXG5cbiMgTWVhbiBvZiBzdWJzZXRzLlxubWVhbi5jYW5hZGEgPC0gbWVhbihoYXBwaW5lc3MuY2FuYWRhKVxubWVhbi5pY2VsYW5kIDwtIG1lYW4oaGFwcGluZXNzLmljZWxhbmQpXG5cbm1lYW4uY2FuYWRhXG5tZWFuLmljZWxhbmRcblxuIyBMZW5ndGggb2Ygc3Vic2V0c1xubGVuLmNhbmFkYSA8LSBsZW5ndGgoaGFwcGluZXNzLmNhbmFkYSlcbmxlbi5pY2VsYW5kIDwtIGxlbmd0aChoYXBwaW5lc3MuaWNlbGFuZClcblxuIyBTdGFuZGFyZCBEZXZpYXRpb24gb2YgU3Vic2V0cy5cbnNkLmNhbmFkYSA8LSBzZChoYXBwaW5lc3MuY2FuYWRhKVxuc2QuaWNlbGFuZCA8LSBzZChoYXBwaW5lc3MuaWNlbGFuZClcblxuIyBDYWxjdWxhdGluZyBaLXNjb3JlIFxuemV0YSA8LSAobWVhbi5jYW5hZGEgLSBtZWFuLmljZWxhbmQpLyBzcXJ0KChzZC5jYW5hZGFeMikvbGVuLmNhbmFkYSArIChzZC5pY2VsYW5kXjIpL2xlbi5pY2VsYW5kKVxuemV0YVxuXG4jIENhbGN1bGF0ZSBwLXZhbHVlIGZyb20gWi1zY29yZVxucF92YWx1ZSA8LSBwbm9ybSgtemV0YSlcbnBfdmFsdWUifQ== In this case, after applying Bonferroni Correction we get the value of \\( = 0.05/12090  4.14 * 10^{-06}\\) Here, we get the p-value of 0.25 which is much higher than the value of our . Based on this we fail reject our null hypothesis. 3.3.1.2 Example 2 Lets consider the following hypotheses from the dataset. Our hypothesis: People from Italy are happier than people from Afghanistan. Null hypothesis: There is no difference in happiness levels of people from Italy and people from Afghanistan. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgZGF0YXNldFxuaGFwcGluZXNzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L0hBUFBJTkVTUzIwMTcuY3N2XCIsIHN0cmluZ3NBc0ZhY3RvcnMgPSBUKSAjd2ViIGxvYWRcblxuIyBUd28gc3Vic2V0cyBvZiBJdGFseSBhbmQgQWZnaGFuaXN0YW4gXG5oYXBwaW5lc3MuaXRhbHkgPC0gc3Vic2V0KGhhcHBpbmVzcyRIQVBQSU5FU1MsIGhhcHBpbmVzcyRDT1VOVFJZID09XCJJdGFseVwiKVxuaGFwcGluZXNzLmFmZ2hhbmlzdGFuIDwtIHN1YnNldChoYXBwaW5lc3MkSEFQUElORVNTLCBoYXBwaW5lc3MkQ09VTlRSWSA9PSBcIkFmZ2hhbmlzdGFuXCIpXG5cbiMgTWVhbiBvZiBzdWJzZXRzLlxubWVhbi5pdGFseSA8LSBtZWFuKGhhcHBpbmVzcy5pdGFseSlcbm1lYW4uYWZnaGFuaXN0YW4gPC0gbWVhbihoYXBwaW5lc3MuYWZnaGFuaXN0YW4pXG5cbm1lYW4uaXRhbHlcbm1lYW4uYWZnaGFuaXN0YW5cblxuIyBMZW5ndGggb2Ygc3Vic2V0c1xubGVuLml0YWx5IDwtIGxlbmd0aChoYXBwaW5lc3MuaXRhbHkpXG5sZW4uYWZnaGFuaXN0YW4gPC0gbGVuZ3RoKGhhcHBpbmVzcy5hZmdoYW5pc3RhbilcblxuIyBTdGFuZGFyZCBEZXZpYXRpb24gb2YgU3Vic2V0cy5cbnNkLml0YWx5IDwtIHNkKGhhcHBpbmVzcy5pdGFseSlcbnNkLmFmZ2hhbmlzdGFuIDwtIHNkKGhhcHBpbmVzcy5hZmdoYW5pc3RhbilcblxuIyBDYWxjdWxhdGluZyBaLXNjb3JlIFxuemV0YSA8LSAobWVhbi5pdGFseSAtIG1lYW4uYWZnaGFuaXN0YW4pLyBzcXJ0KChzZC5pdGFseV4yKS9sZW4uaXRhbHkgKyAoc2QuYWZnaGFuaXN0YW5eMikvbGVuLmFmZ2hhbmlzdGFuKVxuemV0YVxuXG4jIENhbGN1bGF0ZSBwLXZhbHVlIGZyb20gWi1zY29yZVxucF92YWx1ZSA8LSBwbm9ybSgtemV0YSlcbnBfdmFsdWUifQ== In this case, after applying Bonferroni Correction we get the value of \\( = 0.05/12090  4.14 * 10^{-06}\\) Here, we get the p-value of 0.00364 which is lower than the value of default p-value cutoff \\( = 0.05\\), but this obtained p-value is higher than our Bonferroni correction cutoff. So, based on the results, we fail to reject our null hypothesis even though the obtained p-value is less than 0.05. EOC "],["revision.html", "Chapter 4 Revision of R commands 4.1 c() &amp; data.frame() &amp; class() 4.2 summary(), mean(),length(), max(),min(), sd(),nrow(), ncol(), dim() 4.3 Table 4.4 Subset 4.5 tapply 4.6 Cut 4.7 What would R say?", " Chapter 4 Revision of R commands In this chapter we are going to recap at some basic and useful functions we have used in R. The examples we use here will be helpful in revising few of the functions we studied and would give an baseline of function we would need in the future while programming in R. List of commands: c() data.frame() subset() table() tapply() cut() summary(), mean(),length(), max(),min(), sd(),nrow(), ncol() class() And also all the plot commands present in section 2.1 4.1 c() &amp; data.frame() &amp; class() c() The c() function is used for combining arguments. The default behavior of the c() method is to combine its arguments to form a vector. All arguments are coerced (forcibly converted) to a common type which is the type of the returned value. For example,the non-character values are coerced to character type if one of the elements is a character. the hierarchy followed is NULL &lt; raw &lt; logical &lt; integer &lt; double &lt; complex &lt; character &lt; list &lt; expression. dataframe() A data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. Following are the characteristics of a data frame. The column names should be non-empty. The row names should be unique. The data stored in a data frame can be of numeric, factor or character type. Each column should contain same number of data items. class() The class() function has multiple uses, but for here, it is used to check the type of object. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgMyB2ZWN0b3JzIHdpdGggdGl0bGUsIGF1dGhvciBhbmQgeWVhci5cbnRpdGxlIDwtIGMoJ0RhdGEgU21hcnQnLCdPcmllbnRhbGlzbScsJ0ZhbHNlIEltcHJlc3Npb25zJywnTWFraW5nIFNvZnR3YXJlJylcbmF1dGhvciA8LSBjKCdGb3JlbWFuLCBKb2huJywnU2FpZCwgRWR3YXJkJywnQXJjaGVyLCBKZWZmZXJ5JywnT3JhbSwgQW5keScpXG55ZWFyIDwtIGMoMjAxMCwyMDExLDIwMTIsMTk5OClcblxuI0xldHMgbG9vayBhdCBob3cgdGhlIGNyZWF0ZWQgdmVjdG9ycyBsb29rLlxudGl0bGVcbmF1dGhvclxueWVhclxuXG4jIEFsc28gbGV0cyBsb29rIGF0IHRoZWlyIHR5cGVzIHVzaW5nIHRoZSBjbGFzcyBmdW5jdGlvbi5cbmNsYXNzKHRpdGxlKVxuY2xhc3MoYXV0aG9yKVxuY2xhc3MoeWVhcilcblxuXG4jIE5vdyBsZXRzIGNyZWF0ZSBhIGRhdGFmcmFtZSB1c2luZyB0aGUgYWJvdmUgY29sdW1uIHZlY3RvcnMuXG5cbmRmIDwtIGRhdGEuZnJhbWUodGl0bGUsIGF1dGhvciwgeWVhcilcbmRmICMgTGV0cyBsb29rIGF0IGhvdyB0aGUgZGF0YWZyYW1lIGxvb2tzLiJ9 4.2 summary(), mean(),length(), max(),min(), sd(),nrow(), ncol(), dim() The functions in this section are very simple yet are always useful to get more information from data. summary() function computes summary statistics of data. mean() function is used to find the average of the data. sd() fucntion is used to find the standard deviation of the data. length() function is used to get or set the length of data. max() function is used to get the maximum valued element in the data. min() function is used to get the minimum valued element in the data. nrow() function is used to find the number/count of the rows present in data. ncol() function is used to find the number/count of the columns present in data. dim() function is used to find the dimensions of the data. Lets look at example of all these functions. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBMZXRzIGxvb2sgYXQgdGhlIHN1bW1hcnlcbnN1bW1hcnkobW9vZHkpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIG51bWJlciBvZiByb3dzIGluIHRoZSBkYXRhc2V0LlxubnJvdyhtb29keSlcblxuI0xldHMgbG9vayBhdCB0aGUgbnVtYmVyIG9mIGNvbHVtbnMgaW4gdGhlIGRhdGFzZXQuXG5uY29sKG1vb2R5KVxuXG4jTGV0cyBsb29rIGF0IHRoZSBkaW1lbnNpb25zIGkuZS4gYm90aCBudW1iZXJzIG9mIHJvd3MgYW5kIGNvbHVtbnMgb2YgdGhlIGRhdGEgdXNpbmcganVzdCBvbmUgY29tbWFuZFxuZGltKG1vb2R5KVxuXG4jTGV0cyBsb29rIGF0IHRoZSBtZWFuIG9mIHNjb3JlIGNvbHVtbi5cbm1lYW4obW9vZHkkc2NvcmUpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIHN0YW5kYXJkIGRldmlhdGlvbiBvZiBzY29yZSBjb2x1bW5cbnNkKG1vb2R5JHNjb3JlKVxuXG4jTGV0cyBsb29rIGF0IHRoZSBsZW5ndGggb2YgdGhlIGdyYWRlIGNvbHVtbiBcbmxlbmd0aChtb29keSRncmFkZSlcblxuI0xldHMgbG9vayBhdCB0aGUgbWluaW11bSB2YWx1ZSBvZiBzY29yZSBpbiB0aGUgc2NvcmUgY29sdW1uLlxubWluKG1vb2R5JHNjb3JlKVxuXG4jbGV0cyBsb29rIGF0IHRoZSBtYXhpbXVtIHZhbHVlIG9mIHRoZSBzY29yZSBpbiB0aGUgc2NvcmUgY29sdW1uXG5tYXgobW9vZHkkc2NvcmUpIn0= 4.3 Table table() function in R Language is used to create a categorical representation of data with variable name and the frequency in the form of a table. More use of table() is when you use multiple categorical columns. For example, well see the count of grade vs asks_questions in example 2. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG50YWJsZWV4MTwtIHRhYmxlKG1vb2R5JEdSQURFKSAjVXNlIG9mIHRhYmxlICBmdW5jdGlvbiBvbiB0aGUgbmV3IGNvbHVtbi5cbnRhYmxlZXgxXG5iYXJwbG90KHRhYmxlZXgxLGNvbCA9YyhcInJlZFwiLFwicHVycGxlXCIsXCJjeWFuXCIsXCJ5ZWxsb3dcIixcImdyZWVuXCIpLHhsYWIgPSBcIkxhYmVsc1wiLCB5bGFiID0gXCJGcmVxdWVuY3lcIixtYWluID0gXCJ0YWJsZSgpIGV4YW1wbGUgMVwiKSAjcGxvdC5cblxuXG50YWJsZWV4MjwtdGFibGUobW9vZHkkR1JBREUsbW9vZHkkQVNLU19RVUVTVElPTlMpXG50YWJsZWV4MlxubW9zYWljcGxvdCh0YWJsZWV4Mixjb2wgPWMoXCJyZWRcIixcInB1cnBsZVwiLFwiY3lhblwiLFwieWVsbG93XCIsXCJncmVlblwiKSxtYWluID0gXCJ0YWJsZSgpIGV4YW1wbGUgMlwiKSJ9 4.3.1 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFibGUobW9vZHlbbW9vZHkkQVNLU19RVUVTVElPTlMhPSdhbHdheXMnLF0kR1JBREUpXG4jV2hhdCB3aWxsIFIgc2F5P1xuXG4jIEEuIGVycm9yXG4jIEIuIGRpc3RyaWJ1dGlvbiBvZiBncmFkZXMgZm9yIHN0dWRlbnRzIHdobyBhbHdheXMgYXNrIHF1ZXN0aW9uc1xuIyBDLiBkaXN0cmlidXRpb24gb2YgZ3JhZGVzIGZvciBzdHVkZW50cyB3aG8gZG8gbm90IGFsd2F5cyBhc2sgcXVlc3Rpb25zICJ9 4.3.2 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFibGUobW9vZHlbbW9vZHkkQVNLU19RVUVTVElPTlM9PWxpc3QoJ2Fsd2F5cycsJ25ldmVyJyksXSRHUkFERSlcbiNXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gZXJyb3IuXG4jIEIuIGRpc3RyaWJ1dGlvbiBvZiBncmFkZXMgZm9yIHN0dWRlbnRzIHdobyBhbHdheXMgb3IgbmV2ZXIgYXNrIHF1ZXN0aW9ucy4gIFxuIyBDLiBkaXN0cmlidXRpb24gb2YgZ3JhZGVzIGZvciBzdHVkZW50cyB3aG8gZG8gbm90IGFzayBxdWVzdGlvbnMgYWx3YXlzIG9yIG5ldmVyLiAifQ== 4.4 Subset subset() function in R programming is used to create a subset of vectors, matrices or data frames based on the conditions provided in the parameters. NOTE: To create a subset, not only can you use the subset() function, but also: You can use [ ] operator. Ex: dataFrameName[columnName] Even $ operator is a subset operator. Ex: dataFrameName$columnName Also, subsetting in R (commonly called subscripting) is done with square brackets. When subscripting a data frame there will be two places inside the square brackets separated by a comma. The first part inside the square brackets corresponds to rows. The second part corresponds to columns. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG4jU3Vic2V0IG9mIHJvd3Ncbm1vb2R5X25ldmVyX3NtYXJ0cGhvbmU8LXN1YnNldChtb29keSxPTl9TTUFSVFBIT05FPT1cIm5ldmVyXCIpXG5ucm93KG1vb2R5KVxubnJvdyhtb29keV9uZXZlcl9zbWFydHBob25lKVxudGFibGUobW9vZHlfbmV2ZXJfc21hcnRwaG9uZSRPTl9TTUFSVFBIT05FKSAjIFlvdSBjYW4gc2VlIG9ubHkgc3R1ZGVudCBuZXZlciBvbiBzbWFydHBob25lIGFyZSBpbiB0aGUgc3Vic2V0LlxuXG4jQWx0ZXJuYXRlIHdheSB0byBzdWJzZXQuXG5tb29keV9uZXZlcl9zbWFydHBob25lX2FsdDwtbW9vZHlbbW9vZHkkT05fU01BUlRQSE9ORT09XCJuZXZlclwiLCBdXG50YWJsZShtb29keV9uZXZlcl9zbWFydHBob25lX2FsdCRPTl9TTUFSVFBIT05FKSAjIFlvdSBjYW4gc2VlIGEgc2ltaWxhciB0YWJsZSBhcyBhYm92ZS5cblxuXG4jc3Vic2V0IG9mIGNvbHVtbnNcbm1vb2R5X2V4Y2VwdDg8LXN1YnNldChtb29keSwgc2VsZWN0ID0gLWMoOCkpXG5uY29sKG1vb2R5KVxubmNvbChtb29keV9leGNlcHQ4KSAjIFlvdSBjYW4gc2VlIHRoZSBudW1iZXIgb2YgY29sdW1ucyBoYXMgYmVlbiByZWR1Y2VkIGJ5IDEsIGR1ZSB0byBzdWJzZXR0aW5nIHdpdGhvdXQgY29sdW1uIDhcblxuI1N1YnNldCBvZiBSb3dzIGFuZCBDb2x1bW5zXG5tb29keV9leGNlcHQ4X25ldmVyPC1zdWJzZXQobW9vZHksIHNlbGVjdCA9IC1jKDgpLCBPTl9TTUFSVFBIT05FID09IFwibmV2ZXJcIilcbnRhYmxlKG1vb2R5X2V4Y2VwdDhfbmV2ZXIkT05fU01BUlRQSE9ORSlcbmRpbShtb29keSlcbmRpbShtb29keV9leGNlcHQ4X25ldmVyKSMgWW91IGNhbiBzZWUgb25seSBzdHVkZW50IG5ldmVyIG9uIHNtYXJ0cGhvbmVzIHdpdGhvdXQgY29sdW1uIDggZGF0YSBhcmUgcHJlc2VudCBpbiB0aGUgc3Vic2V0LiJ9 4.4.1 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxubW9vZHlbbW9vZHkkU0NPUkU+PTkwLDNdXG4jIFdoYXQgd2lsbCBSIHNheT9cblxuXG4jIEEuIEdldCBzdWJzZXQgb2YgYWxsIGNvbHVtbnMgd2hpY2ggY29udGFpbnMgc3R1ZGVudHMgd2hvIHNjb3JlZCBtb3JlIHRoYW4gZXF1YWwgdG8gOTBcbiMgQi4gZXJyb3JcbiMgQy4gZ2V0IGFsbCBzY29yZSB2YWx1ZXMgd2hpY2ggYXJlIG1vcmUgdGhhbiBlcXVhbCB0byA5MFxuIyBELiBnZXQgc3Vic2V0IG9mIG9ubHkgdGhlIGdyYWRlcyBvZiBzdHVkZW50cyB3aXRoIHNjb3JlIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA5MCJ9 4.4.2 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxubW9vZHlbbW9vZHkkU0NPUkU+PTgwLjAgJiBtb29keSRHUkFERSA9PSdCJyxdIFxuIyBXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gc3Vic2V0IG9mIG1vb2R5IGRhdGEgZnJhbWUgd2hvIGdvdCBCIGdyYWRlLlxuIyBCLiBlcnJvci5cbiMgQy4gc3Vic2V0IG9mIG1vb2R5IGRhdGEgZnJhbWUgd2l0aCBzY29yZSBncmVhdGVyIHRoYW4gODAuXG4jIEQuIHN1YnNldCBvZiBtb29keSBkYXRhIGZyYW1lIHdpdGggc2NvcmUgbW9yZSB0aGFuIDgwIGFuZCBnb3QgQiBncmFkZS4ifQ== 4.5 tapply tapply() function in R Language is used to apply a function over a subset of vectors given by a combination of factors This is a very versatile function, as well see from the use case. Note : There are different aggregate functions that can be used. For example, Mean, Median, Variance, Sum etc. We can also factor it on multiple attributes. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG4jIFRvIGFwcGx5IHRhcHBseSgpIG9uIFNDT1JFIGZhY3RvcmVkIG9uIE9OX1NNQVJUUEhPTkVcblxubW9vZHlfc2NvcmVhdmc8LXRhcHBseShtb29keSRTQ09SRSxtb29keSRPTl9TTUFSVFBIT05FLG1lYW4pXG5tb29keV9zY29yZWF2ZyAjIFdlIGNhbiBzZWUgaXQgY2FsY3VsYXRlZCBtZWFuIHZhbHVlIG9mIHRoZSBzY29yZSBieSBzdHVkZW50cyB3aXRoIHJlc3BlY3QgdG8gdGhlaXIgdXNlIG9mIHBob25lIGluIGNsYXNzLlxuXG5iYXJwbG90KG1vb2R5X3Njb3JlYXZnLGNvbCA9IFwiY3lhblwiLHhsYWIgPSBcIkxhYmVsc1wiLCB5bGFiID0gXCJtZWFuX3ZhbFwiLG1haW4gPSBcInRhcHBseSgpIGV4YW1wbGUgMVwiLGxhcyA9IDIsIGNleC5uYW1lcyA9IDAuNzUpI3Bsb3RcblxuI0xldHMgZmFjdG9yIHRoZSBncmFkZXMgb24gb25fc21hcnRwaG9uZSBhcyB3ZWxsIGFzIGdyYWRlIGNhdGVnb3J5LlxuXG5tb29keS5zY29yZWF2ZzJkPC10YXBwbHkobW9vZHkkR1JBREUsbGlzdChtb29keSRPTl9TTUFSVFBIT05FLG1vb2R5JEdSQURFKSxsZW5ndGgpXG5tb29keS5zY29yZWF2ZzJkW2lzLm5hKG1vb2R5LnNjb3JlYXZnMmQpXTwtMFxubW9vZHkuc2NvcmVhdmcyZCMgV2UgY2FuIHNlZSBpdCBjYWxjdWxhdGVkIGNvdW50IG9mIHRoZSBncmFkZSBvZiBzdHVkZW50IHdpdGggcmVzcGVjdCB0byB0aGVpciBpbi1jbGFzcyBzbWFydHBob25lIHVzYWdlICBhbmQgZ3JhZGUgY2F0ZWdvcnkuXG5iYXJwbG90KG1vb2R5LnNjb3JlYXZnMmQsY29sPWMoXCJyZWRcIixcImN5YW5cIixcIm9yYW5nZVwiLFwiYmx1ZVwiKSxtYWluID0gXCJ0YXBwbHkoKSBleGFtcGxlIDJcIixiZXNpZGUgPSBUUlVFLGxlZ2VuZD1yb3duYW1lcyhtb29keS5zY29yZWF2ZzJkKSkifQ== 4.5.1 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFwcGx5KG1vb2R5LCBHUkFERSwgU0NPUkUsIG1pbilcbiMgV2hhdCB3aWxsIFIgc2F5P1xuXG4jIEEuIG1pbmltdW0gc2NvcmUgZm9yIGVhY2ggZ3JhZGVcbiMgQi4gbWluaW11bSBncmFkZSBmb3IgZWFjaCBzY29yZVxuIyBDLiBtaW5pbXVtIGdyYWRlIG9ubHkgXG4jIEQuIEVycm9yLiJ9 4.5.2 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFwcGx5KG1vb2R5JEFTS19RVUVTVElPTlMsIG1vb2R5JEdSQURFLCBtZWFuKVxuIyBXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gbWVhbiBncmFkZSBmb3IgZWFjaCB2YWx1ZXMgb2YgYXNrX3F1ZXN0aW9uIGF0dHJpYnV0ZVxuIyBCLiBtZWFuIHZhbHVlIG9mIGFza19xdWVzdGlvbnMgYXR0cmlidXRlIGZvciBlYWNoIGdyYWRlXG4jIEMuIG1lYW4gY2F0ZWdvcnkgb2YgYXNrX3F1ZXN0aW9ucyBvbmx5IFxuIyBELiBlcnJvci4ifQ== 4.6 Cut cut() function in R Language is used to divide a numeric vector into different ranges eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG4jIFdlIGFjY2VzcyB0aGUgU2NvcmUgY29sdW1uIGZyb20gbW9vZHkgZGF0YXNldC5cbnNjb3JlMCA8LSBjdXQobW9vZHkkU0NPUkUsMTApXG50YWJsZShzY29yZTApICNsZXRzIGNoZWNrIHRoZSBkaXN0cmlidXRpb24gb2YgcGVvcGxlIGluIGVhY2ggcGFydGl0aW9uLlxuXG4jIEN1dCBFeGFtcGxlIHVzaW5nIGJyZWFrcyAtIEN1dHRpbmcgZGF0YSB1c2luZyBkZWZpbmVkIHZlY3Rvci4gXG5zY29yZTEgPC0gY3V0KG1vb2R5JFNDT1JFLGJyZWFrcz1jKDAsNTAsMTAwKSxsYWJlbHM9YyhcIkZcIixcIlBcIikpXG50YWJsZShzY29yZTEpIn0= 4.6.1 QuestionWhat would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuY3V0KG1vb2R5JFNDT1JFLCBicmVha3M9YygwLDI1LDcwLDEwMCksbGFiZWxzPWMoXCJsb3dcIiwgXCJtZWRpdW1cIiwgXCJoaWdoXCIpKVxuI1doYXQgd291bGQgUiBzYXk/XG5cbiMgQS4gNSBpbnRlcnZhbHMgb2YgYXR0cmlidXRlIHNjb3JlXG4jIEIuIDMgaW50ZXJ2YWxzICgwLDI1KSAoMjUsNzApICg3NSwxMDApXG4jIEMuIDMgY2F0ZWdvcmljYWwgdmFsdWVzIFwibG93XCIsIFwibWVkaXVtXCIgYW5kIFwiaGlnaFwiIGZvciBkaWZmZXJlbnQgc2NvcmUgaW50ZXJ2YWxzXG4jIEQuIDMgc2VwYXJhdGUgZGF0YXNldHMgd2l0aCBzaW1pbGFyIHNjb3JlIHZhbHVlcyJ9 4.6.2 QuestionWhat would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxub3V0cHV0PC1jdXQobW9vZHkkU0NPUkUsIDUpXG5zdW1tYXJ5KG91dHB1dClcbiNXaGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEuIDUgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBzY29yZSBvZiB1bmVxdWFsIGNvdW50IG9mIGVsZW1lbnRzXG4jIEIuIDUgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBzY29yZSBvZiBlcXVhbCBjb3VudCBvZiBlbGVtZW50c1xuIyBDLiA1IGNhdGVnb3JpY2FsIHZhbHVlcyBmb3IgZGlmZmVyZW50IHNjb3JlIGludGVydmFsc1xuIyBELiA1IHNlcGFyYXRlIGRhdGFzZXQgd2l0aCBzaW1pbGFyIHNjb3JlIHZhbHVlcyJ9 4.6.3 QuestionWhat would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxub3V0cHV0PC1jdXQobW9vZHkkQVNLU19RVUVTVElPTlMsIDIpXG5zdW1tYXJ5KG91dHB1dClcbiNXaGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEuIDIgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBhc2tfcXVlc3Rpb25zIG9mIHVuZXF1YWwgY291bnQgb2YgZWxlbWVudHMgaW4gZWFjaCBpbnRlcnZhbFxuIyBCLiAyIGludGVydmFscyBvZiBhdHRyaWJ1dGUgYXNrX3F1ZXN0aW9ucyBvZiBlcXVhbCBjb3VudCBvZiBlbGVtZW50cyBpbiBlYWNoIGludGVydmFsXG4jIEMuIDIgY2F0ZWdvcmljYWwgdmFsdWVzIGZvciBkaWZmZXJlbnQgYXNrX3F1ZXN0aW9ucyBpbnRlcnZhbHNcbiMgRC4gRXJyb3IuIn0= 4.6.4 A complex example eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuXG5tb29keSRjb25kaXRpb25hbCA8LTBcbm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb248MC41MCwgXSRjb25kaXRpb25hbCA8LSBtb29keVttb29keSRwYXJ0aWNpcGF0aW9uPDAuNTAsIF0kc2NvcmUgLTEwKm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb248MC41MCwgXSRwYXJ0aWNpcGF0aW9uXG5tb29keVttb29keSRwYXJ0aWNpcGF0aW9uPj0wLjUwLCBdJGNvbmRpdGlvbmFsIDwtIG1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb24+PTAuNTAsIF0kc2NvcmUgKzEwKm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb24+PTAuNTAsIF0kcGFydGljaXBhdGlvblxuXG5zdW1tYXJ5KG1vb2R5JGNvbmRpdGlvbmFsKVxuXG5ib3hwbG90KG1vb2R5JGNvbmRpdGlvbmFsLGNvbCA9IGMoXCJyZWRcIiksbWFpbj1cIkNvbXBsZXggRXhhbXBsZVwiKSJ9 4.7 What would R say? In this section we will look at few examples based on the question What do you think would R say? All the questions are based on what we have studied in the sections above. INSTRUCTIONS: Do not run the following examples directly, first ask yourself and note down, what do you think would R say? Only then run them. This is the only way to learn simple commands - and have them memorized so you can write code without having to check every single command. 4.7.1 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ3ZWF0aGVyID1kYXRhLmZyYW1lKERheT1jKCd3ZWVrZGF5JywgJ3dlZWtlbmQnKSwgQ29uZGl0aW9ucyA9Yygnc3VubnknLCdyYWlueScsJ2Nsb3VkeScsICdzbm93JywgJ3N0b3JtJywnaWNlJykpXG5kaW0od2VhdGhlcilcbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpIDYgNlxuIyBCKSAyIDZcbiMgQykgNiAyXG4jIEQpIEVycm9yIn0= 4.7.2 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ3ZWF0aGVyID1kYXRhLmZyYW1lKERheT1jKCd3ZWVrZGF5JywgJ3dlZWtlbmQnKSwgQ29uZGl0aW9ucyA9Yygnc3VubnknLCdyYWlueScsJ2Nsb3VkeScsICdzbm93JywgJ3N0b3JtJywnaWNlJykpXG53ZWF0aGVyJHRlbXBlcmF0dXJlID1jKDgwLCA3MCwgNjUsIDQwLCAzMCwyNSlcbndlYXRoZXJbd2VhdGhlciR0ZW1wZXJhdHVyZSA+IDQwLF1cbmRpbSh3ZWF0aGVyKVxuI3doYXQgd291bGQgUiBzYXk/XG5cbiMgQSkgNiAzXG4jIEIpIHN1YnNldCBvZiB0aGUgZGF0YWZyYW1lIHdpdGggdGVtcGVyYXR1cmUgPiA0MC5cbiMgQykgQm90aCBBIGFuZCBCXG4jIEQpIEVycm9yIn0= 4.7.3 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KTtcbkdSQURFPWMoJ0MnLCAnRicsICdBJylcbk9OX1NNQVJUUEhPTkU9YygnYWx3YXlzJywgJ25ldmVyJywgJ3NvbWV0aW1lcycpXG5GSU5BTEVYQU09YygxMiw1LDIwKVxuTT1kYXRhLmZyYW1lKFNDT1JFLCBHUkFERSwgT05fU01BUlRQSE9ORSwgRklOQUxFWEFNKVxuc3Vic2V0KE0sIEdSQURFPT0nRicpXG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSBTdWJzZXQgb2YgZGF0YWZyYW1lIGJhc2VkIG9uIEdyYWRlIGVxdWFsIHRvIEZcbiMgQikgU3Vic2V0IG9mIHRoZSBkYXRhZnJhbWUgYmFzZWQgb24gR3JhZGUgbm90IGVxdWFsIHRvIEZcbiMgQykgdGhlIGNvbXBsZXRlIGRhdGFmcmFtZVxuIyBEKSBFcnJvciJ9 4.7.4 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KTtcbkdSQURFPWMoJ0MnLCAnRicsICdBJylcbk9OX1NNQVJUUEhPTkU9YygnYWx3YXlzJywgJ25ldmVyJywgJ3NvbWV0aW1lcycpXG5GSU5BTEVYQU09YygxMiw1LDIwKVxuTT1kYXRhLmZyYW1lKFNDT1JFLCBHUkFERSwgT05fU01BUlRQSE9ORSwgRklOQUxFWEFNKVxuTVtGSU5BTEVYQU0gPiA1LF1cbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpIFN1YnNldCBvZiBkYXRhZnJhbWUgd2l0aCBmaW5hbGV4YW0gdmFsdWVzIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA2XG4jIEIpIFN1YnNldCBvZiBkYXRhZnJhbWUgd2l0aCBmaW5hbGV4YW0gdmFsdWVzIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA1XG4jIEMpIFN1YnNldCBvZiBkYXRhZnJhbWUgd2l0aCBmaW5hbGV4YW0gdmFsdWVzIGxlc3MgdGhhbiA1LiJ9 4.7.5 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KVxuR1JBREU9YygnQycsICdGJywgJ0EnKVxuT05fU01BUlRQSE9ORT1jKCdhbHdheXMnLCAnbmV2ZXInLCAnc29tZXRpbWVzJylcbkZJTkFMRVhBTT1jKDEyLDUsMjApXG5NPWRhdGEuZnJhbWUoU0NPUkUsIEdSQURFLCBPTl9TTUFSVFBIT05FLCBGSU5BTEVYQU0pXG5NJFFVRVNUSU9OUz0nbm9uZSdcbk1bLDVdXG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSBPdXRwdXQgdGhlIGNvbnRlbnQgb2YgYWxsIHRoZSBjb2x1bW5zXG4jIEIpIE91dHB1dCB3b3JkIFwibm9uZVwiIGZvciAzIHRpbWVzIFxuIyBDKSBPdXRwdXQgd29yZCBcIm5vbmVcIiBmb3IgNSB0aW1lc1xuIyBEKSBFcnJvciJ9 4.7.6 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KVxuR1JBREU9YygnQycsICdGJywgJ0EnKVxuT05fU01BUlRQSE9ORT1jKCdhbHdheXMnLCAnbmV2ZXInLCAnc29tZXRpbWVzJylcbkZJTkFMRVhBTT1jKDEyLDUsMjApXG5NPWRhdGEuZnJhbWUoU0NPUkUsIEdSQURFLCBPTl9TTUFSVFBIT05FLCBGSU5BTEVYQU0pXG50YWJsZShNJFNDT1JFPjE1LCBNJEdSQURFKVxuI3doYXQgd291bGQgUiBzYXk/XG5cbiMgQSkgT3V0cHV0IHRoZSB0YWJsZSBvZiBjb3VudCBvZiBTY29yZSBncmVhdGVyIHRoYW4gMTUgdnMgR3JhZGVcbiMgQikgT3V0cHV0IHRoZSB0YWJsZSBvZiBjb3VudCBvZiBzY29yZSBncmVhdGVyIHRoYW4gMTUgb25seVxuIyBDKSBPdXRwdXQgdGhlIHRhYmxlIG9mIGNvdW50IG9mIGdyYWRlcyBvbmx5XG4jIEQpIE91dHB1dCB0aGUgdGFibGUgb2YgZ3JhZGUgZGlzdHJpYnV0aW9uIHZzIGFsbCBzY29yZS4ifQ== 4.7.7 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ1PC1jKDE6MTApXG53IDwtYygxLC0xLDMpXG51W3c+MF1cbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpICAxICAzICA0ICA2ICA3ICA5IDEwIFxuIyBCKSAgMSAgMiAgMyAgNCAgNSAgNiAgNyAgOCAgOSAgMTBcbiMgQykgIDEgIDMgIDEgIDMgIDEgIDMgIDEgIDMgIDEgIDMgXG4jIEQpIEVycm9yIn0= 4.7.8 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2IDwtIGMoLTIsMCwyLC01KVxudlt2PjBdXG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSAyXG4jIEIpIDAgMlxuIyBDKSBGQUxTRSBGQUxTRSAgVFJVRSBGQUxTRVxuIyBEKSBFcnJvciJ9 4.7.9 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJjKFwiYVwiLDEsVClcbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpIE5hTiAgMSAgTmFOXG4jIEIpIFwiYVwiICAxICBUXG4jIEMpIFwiYVwiICBcIjFcIiAgXCJUUlVFXCJcbiMgRCkgXCJhXCIgIFwiMVwiICBcIlRcIiJ9 4.7.10 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ4PC0xOjRcbnk8LTI6OVxueCt5XG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSAzICA1ICA3ICA5ICA3ICA5IDExIDEzXG4jIEIpIDEgIDIgIDMgIDQgIDIgIDMgIDQgIDUgIDYgIDcgIDggIDlcbiMgQykgMyAgNSAgNyAgOVxuIyBEKSBFcnJvciJ9 4.7.11 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2MTwtIGMoMSwyLDMsNClcbnYyPC0gYygxLDIsMyw0KVxudjM8LSBjKDEsMiwzLDQpXG5cbmRmPC1kYXRhLmZyYW1lKHYxLHYyLHYzKVxuIzFcbmRmW2RmPjJdICBcblxuIyBBLiB2YWx1ZXMgb2YgdjEgdmFyaWFibGUgd2hpY2ggYXJlIGxhcmdlciB0aGFuIDIgXG5cbiMgQi4gdmFsdWVzIG9mIHYxLCB2MiBhbmQgdjMgd2hpY2ggYXJlIGxhcmdlciB0aGFuIDIuIFxuXG4jIEMuIGVycm9yICJ9 4.7.12 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2MTwtIGMoMSwyLDMsNClcbnYyPC0gYygxLDIsMyw0KVxudjM8LSBjKDEsMiwzLDQpXG5cbmRmPC1kYXRhLmZyYW1lKHYxLHYyLHYzKVxuXG5kZiR2MSA+IDIgXG5cbiMgQS4gZXJyb3IgXG5cbiMgQi4gdmFsdWVzIG9mIHYxIHZhcmlhYmxlIHdoaWNoIGFyZSBsYXJnZXIgdGhhbiAyLlxuXG4jIEMuIFRSVUUgd2hlcmUgdGhlIHZhbHVlIGlzIGdyZWF0ZXIgdGhhbiAyIGFuZCBGYWxzZSB3aGVyZSB0aGUgdmFsdWUgaXMgbGVzcyB0aGFuIDIuICJ9 4.7.13 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2MTwtIGMoMSwyLDMsNClcbnYyPC0gYygxLDIsMyw0KVxudjM8LSBjKDEsMiwzLDQpXG5cbmRmPC1kYXRhLmZyYW1lKHYxLHYyLHYzKVxuXG52MT4yXG5cbiMgQS4gVFJVRSB3aGVyZSB0aGUgdmFsdWUgaXMgZ3JlYXRlciB0aGFuIDIgYW5kIEZhbHNlIHdoZXJlIHRoZSB2YWx1ZSBpcyBsZXNzIHRoYW4gMi5cblxuIyBCLiB2YWx1ZXMgb2YgdjEgdmFyaWFibGUgd2hpY2ggYXJlIGxhcmdlciB0aGFuIDJcblxuIyBDLiBlcnJvciJ9 "],["datatransformation.html", "Chapter 5 Data Frames &amp; Transformation. 5.1 Create Column 5.2 Factor Function: factor() 5.3 Coercing Values in data frames 5.4 Merging Two Relational Data Frames. 5.5 Slicing and Dicing. 5.6 Group By 5.7 Handling Date and Time in dataframes.", " Chapter 5 Data Frames &amp; Transformation. Now we have to introduce the core data structure of R  the data frame and show we can expand it with extra attributes. Defining new attributes can very often be critical in data exploration and help to find patterns and relationships which otherwise would not be visible. For example, may be participation matters but only to Pass/Fail grades? In other words students who Pass (A or B or C) always have participation above a certain threshold? Perhaps students who always text never pass the class? And students who always ask questions never fail? Such rules can only be discovered if we define a new Pass/Fail attribute, additional to grade attribute. Similarly intervals of participation or score may discover important relationships which would not emerge with just numerical values of such attributes. May be High scores correlate with High participation? To establish it one would have first to define categorical attributes with named intervals of their numerical counterparts. 5.1 Create Column Lets put a column I have created using score. Suppose I am given a new column \" pf \" with same number of rows as that of the dataframe with the categories (P , F). eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiMgcGYgY29sdW1uIGhhcyAyIGNhdGVnb3J5IGFuZCBkaXZpZGVzIG9uIHRoZSBiYXNpcyBvZiBzY29yZS5cbnBmIDwtIGN1dChtb29keSRzY29yZSxicmVha3M9YygwLDUwLDEwMCksbGFiZWxzPWMoXCJGXCIsXCJQXCIpKVxuIyBsZW5ndGgocGYpICMgTnVtYmVyIG9mIHJvd3MgaW4gbmV3IGNvbHVtbi5cbiMgbnJvdyhtb29keSkgIyBOdW1iZXIgb2YgUm93cyBpbiBkYXRhZnJhbWVcblxuIyBUbyBhZGQgdGhpcyBuZXcgY29sdW1uIHBmIGluIGRhdGFmcmFtZSBtb29keS5cbm5hbWVzKG1vb2R5KSAjIEluaXRpYWxseSBkYXRhZnJhbWUgaGFzIDUgY29sdW1uc1xubW9vZHkkcGFzc2ZhaWwgPC0gcGYgI1B1dCBzeW50YXggZGF0YUZyYW1lTmFtZSRjb2x1bW5IZWFkZXJOYW1lIDwtIG5ld0NvbHVtblxubmFtZXMobW9vZHkpICMgTm93IGRhdGFmcmFtZSBoYXMgNiBjb2x1bW5zIn0= eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNXaGF0IGhhcHBlbnMgd2hlbiB5b3UgaGF2ZSBjb2x1bW4gc2l6ZSBtaXNtYXRjaC5cbmJhZGNvbCA8LSBjKDE6MTApXG5sZW5ndGgoYmFkY29sKVxuXG5tb29keSRiYWRjb2wgPC0gYmFkY29sICNUaHJvd3MgQ29tcGF0aWJpbGl0eSBlcnJvci4gIn0= 5.2 Factor Function: factor() Factors are the data objects which are used to categorize the data and store it as levels. They can store both strings and numbers. They are useful in the columns which have a limited number of unique values. Like Male,Female\" and True, False etc. Factor data objects are useful in data analysis for statistical modeling. The factor function is used to encode a vector as a factor. Lets look at first example, checking if a data object is of factor type using the function is.factor(x) eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIENyZWF0ZSBhIHZlY3RvciBhcyBpbnB1dC5cbmdlbmRlciA8LSBjKFwibWFsZVwiLFwibWFsZVwiLFwiZmVtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwibWFsZVwiKVxuXG5nZW5kZXJcblxuI0NoZWNrIGlmIGRhdGEgb2JqZWN0IGlzIGZhY3Rvci5cbmlzLmZhY3RvcihnZW5kZXIpIn0= Now lets convert the above vector to a factor data object. To do this we will use the function factor(x). eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIENyZWF0ZSBhIHZlY3RvciBhcyBpbnB1dC5cbmdlbmRlciA8LSBjKFwibWFsZVwiLFwibWFsZVwiLFwiZmVtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwibWFsZVwiKVxuXG4jIEFwcGx5IHRoZSBmYWN0b3IgZnVuY3Rpb24uXG5mYWN0b3JfZ2VuZGVyIDwtIGZhY3RvcihnZW5kZXIpXG5cbmZhY3Rvcl9nZW5kZXJcbmlzLmZhY3RvcihmYWN0b3JfZ2VuZGVyKSJ9 Notice that for the factor data objects, the attribute Levels is also created. This is an extremely important feature of the factor data object. Lets look at how the factor data object looks when included in a dataframe. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIENyZWF0ZSB0aGUgdmVjdG9ycyBmb3IgZGF0YSBmcmFtZS5cbmhlaWdodCA8LSBjKDEzMiwxNTEsMTYyLDEzOSwxNjYsMTQ3LDEyMilcbndlaWdodCA8LSBjKDQ4LDQ5LDY2LDUzLDY3LDUyLDQwKVxuZ2VuZGVyX25vdF9mYWN0b3IgPC0gYyhcIm1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwiZmVtYWxlXCIsXCJtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIilcblxuIyBDT252ZXJ0IHRoZSBnZW5kZXJfbm90X2ZhY3RvciB2ZWN0b3IgdG8gYSBmYWN0b3IgZGF0YSBvYmplY3QuXG5nZW5kZXIgPC0gZmFjdG9yKGdlbmRlcl9ub3RfZmFjdG9yKVxuXG4jIENyZWF0ZSB0aGUgZGF0YSBmcmFtZS5cbmlucHV0X2RhdGEgPC0gZGF0YS5mcmFtZShoZWlnaHQsd2VpZ2h0LGdlbmRlcilcbnByaW50KGlucHV0X2RhdGEpXG5cbiMgVGVzdCBpZiB0aGUgZ2VuZGVyIGNvbHVtbiBpcyBhIGZhY3Rvci5cbnByaW50KGlzLmZhY3RvcihpbnB1dF9kYXRhJGdlbmRlcikpXG5cbiMgUHJpbnQgdGhlIGdlbmRlciBjb2x1bW4gc28gc2VlIHRoZSBsZXZlbHMuXG5wcmludChpbnB1dF9kYXRhJGdlbmRlcikifQ== Note: Sometimes depending on your version of R and packages, you might find that while inserting categorical vector into the data frame using the data.frame() function, without converting the categorical vector to factor, it automatically gets converted into a factor column. But to avoid confusion, it is a better technique to convert the categorical vector into factor using factor() function and then insert it in the data frame. Lets look at an example where the use of factor data object turns out to be useful. We have a categorical vector that we want to coerce as numeric for use in some model/application. Lets look at what happens when we just have a categorical vector, and we try to coerce it to numeric vector. We see that the outcome of the as.numeric() function on a normal categorical vector is coercion to NA of all elements. But when we convert the same categorical vector to factor, then after coercion to numeric type, we get a numeric vector of elements corresponding the the index of the labels of the factor data object. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJnZW5kZXJfbm90X2ZhY3RvciA8LSBjKFwibWFsZVwiLFwibWFsZVwiLFwiZmVtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwibWFsZVwiKVxuZ2VuZGVyX25vdF9mYWN0b3JcblxuIyBDb2VyY2UgaW50byBudW1lcmljIHZlY3RvciB3aXRob3V0IGNvbnZlcnRpbmcgdG8gZmFjdG9yXG5hcy5udW1lcmljKGdlbmRlcl9ub3RfZmFjdG9yKVxuXG5cbiMgQ09udmVydCB0aGUgZ2VuZGVyX25vdF9mYWN0b3IgdmVjdG9yIHRvIGEgZmFjdG9yIGRhdGEgb2JqZWN0LlxuZ2VuZGVyIDwtIGZhY3RvcihnZW5kZXJfbm90X2ZhY3RvcilcbmdlbmRlclxuXG4jIENvZXJjZSBpbnRvIG51bWVyaWMgdmVjdG9yIGFmdGVyIGNvbnZlcnRpbmcgdG8gZmFjdG9yIGRhdGEgb2JqZWN0LlxuYXMubnVtZXJpYyhnZW5kZXIpIn0= Lets look at another example where factor is useful. We want to see the distribution of price of each quality for the wine dataset. Upon plotting, it gives us a scatter plot, which makes it hard for us to see the distribution. Thus we convert the quality vector which is numeric initially, to factor and the plot it again. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ3aW5lIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0LzU0MVdJTkUuY3N2XCIpXG5wbG90KHdpbmUkUVVBTElUWSx3aW5lJFBSSUNFKVxuI3dlIHdhbnQgdG8gc2VlIHRoZSBkaXN0cmlidXRpb24gb2YgcHJpY2Ugb2YgZWFjaCBxdWFsaXR5LCBidXQgaXQgZ2l2ZXMgdXMgYSBzY2F0dGVyIHBsb3QsIHdoaWNoIG1ha2VzIGl0IGhhcmQgZm9yIHVzIHRvIHNlZSB0aGUgZGlzdHJpYnV0aW9uLlxuaXMuZmFjdG9yKHdpbmUkUVVBTElUWSlcbiN0aGUgcmVzdWx0IGlzIGZhbHNlLCB3aGljaCBtZWFucyBxdWFsaXR5IGlzIGEgbnVtZXJpYyB2YWx1ZSByYXRoZXIgdGhhbiBhIGZhY3RvclxuXG5mYWN0b3JfcXVhbGl0eSA8LSBmYWN0b3Iod2luZSRRVUFMSVRZKVxuI2NvbnZlcnQgcXVhbGl0eSB2YWx1ZXMgaW50byBmYWN0b3JzXG5wbG90KGZhY3Rvcl9xdWFsaXR5LHdpbmUkUFJJQ0UpXG4jbm93IHdlIGNhbiBnZW5lcmF0ZSB0aGUgYm94IHBsb3QgYW5kIHNlZSB0aGUgZGlzdHJpYnV0aW9uIGNsZWFybHkuIn0= 5.3 Coercing Values in data frames Before coercing data into data frames, lets look at small examples. Lets look at a coerced vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBsb29rIGF0IGEgY29lcmNlZCB2ZWN0b3IuXG5cbiN2ZWN0b3IgY29udGFpbmluZyA0IGVsZW1lbnRzXG5teVZlY3Q8LWMoXCJSb2JlcnRcIiwgXCJFdGhhblwiLCA2LCA0KVxubXlWZWN0XG5cbiNZb3Ugd2lsbCBub3RpY2UgdGhhdCB0aGUgbGFzdCB0d28gZWxlbWVudHMgLCB3aGljaCBhcmUgYW4gaW50ZWdlcnMsIGFyZSBjb2VyY2VkIGludG8gYSBjaGFyYWN0ZXIgdHlwZS5cblxuI2NsYXNzKCkgaXMgdXNlZCB0byBjaGVjayB0aGUgdHlwZSBvZiBhbiBvYmplY3RcbmNsYXNzKG15VmVjdCkifQ== We see that when a vector has elements of mixed data types, they gets coerced into a type with precedence over other types. For example in the above case there were character elements and numeric elements types in the vector. But character type has precedence over numeric type and hence the whole vector is coerced into character type. We can check the types of vectors using a specific type of is function: is.character(), is.double(), is.integer(), is.logical(),etc. There are many other types under the is function, for checking if the data object given is a dataframe, factor, etc. Lets look at the examples. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjdmVjdG9yIGNvbnRhaW5pbmcgNCBlbGVtZW50c1xubXlWZWN0PC1jKFwiUm9iZXJ0XCIsIFwiRXRoYW5cIiwgNiwgNClcbm15VmVjdFxuXG4jIENoZWNrIGlmIHZlY3RvciBpcyBvZiBDaGFyYWN0ZXIgdHlwZS5cbmlzLmNoYXJhY3RlcihteVZlY3QpXG5cbiMgQ2hlY2sgaWYgdmVjdG9yIGlzIG9mIG51bWVyaWMgdHlwZS5cbmlzLm51bWVyaWMobXlWZWN0KVxuXG4jIFVzZSBUUlVFIGFuZCBGQUxTRSAob3IgVCBhbmQgRikgdG8gY3JlYXRlIGxvZ2ljYWwgdmVjdG9yc1xubG9nX3ZlYyA8LSBjKFRSVUUsIEZBTFNFLCBULCBGKVxuXG4jIENoZWNrIGlmIHZlY3RvciBpcyBvZiBsb2dpY2FsIHR5cGUuXG5pcy5sb2dpY2FsKGxvZ192ZWMpIn0= We saw how to check the type of the data. But if you want to convert a column into your choice of data type, you can use the specific type of as function: as.character(), as.double(), as.integer(), as.logical(),etc. Again as we saw above about the is function types, there are also many other types of the as function. Lets look at the example of coercing a vector into character type. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjdmVjdG9yIGNvbnRhaW5pbmcgNCBlbGVtZW50c1xubXlWZWN0PC1jKDIsIDMsIDYsIDQsIFRSVUUsIEZBTFNFKVxubXlWZWN0XG5cbiMgRmlyc3QgbGV0cyBsb29rIGF0IHRoZSBjbGFzcyBvZiB0aGUgdmVjdG9yXG5jbGFzcyhteVZlY3QpXG5cbiMgQ29lcmNlIHRoZSB2ZWN0b3IgdG8gQ2hhcmFjdGVyIHR5cGUuIFxuYXMuY2hhcmFjdGVyKG15VmVjdCkgXG5cbiMgWW91IGNhbiBzZWUgdGhhdCB0aGUgZWxlbWVudHMgb2YgdGhlIG51bWVyaWMgdmVjdG9yIGFyZSBjb2VyY2VkIGludG8gY2hhcmFjdGVyIHR5cGUuIn0= Lets look at an example of coercing a mixed type vector into numeric type. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJteXZlYzwtYyhcIlJvYmVydFwiLCBcIjIyXCIsIDQ1KVxubXl2ZWNcblxuIyBGaXJzdCBsZXRzIGxvb2sgYXQgdGhlIGNsYXNzIG9mIHRoZSB2ZWN0b3JcbmNsYXNzKG15dmVjKVxuXG4jIENvZXJjZSB0aGUgY2hhcmFjdGVyIHZlY3RvciB0byBudW1lcmljIHR5cGUuIFxuYXMubnVtZXJpYyhteXZlYykgXG5cbiMgWW91IGNhbiBzZWUgdGhhdCB0aGUgZWxlbWVudHMgb2YgdGhlIG1peGVkIHZlY3RvciBhcmUgY29lcmNlZCBpbnRvIG51bWVyaWMgdHlwZS5cblxuXG5cbm15dmVjMiA8LSBjKFRSVUUsIEZBTFNFLCBGLCBULCBUKVxubXl2ZWMyXG5cbiMgRmlyc3QgbGV0cyBsb29rIGF0IHRoZSBjbGFzcyBvZiB0aGUgdmVjdG9yXG5jbGFzcyhteXZlYzIpXG5cbiMgQ29lcmNlIHRoZSBsb2dpY2FsIHZlY3RvciB0byBudW1lcmljIHR5cGUuIFxuYXMubnVtZXJpYyhteXZlYzIpIFxuXG4jIFlvdSBjYW4gc2VlIHRoYXQgdGhlIGVsZW1lbnRzIG9mIHRoZSBtaXhlZCB2ZWN0b3IgYXJlIGNvZXJjZWQgaW50byBudW1lcmljIHR5cGUuIn0= We can see in the above example, while converting the character type vector to numeric if we encounter, numbers in character type, they get converted to numeric type. But the characters in character type, are not not converted, and instead we get a warning saying NAs introduced by coercion. Also, while converting a logical vector to numeric vector, we see that TRUE or T is coerced as 1 and FALSE or F is coerced as 0. Now lets look at how to coerce data column and rewrite it into the dataframe. Suppose in the Moody dataset, you want to change the categorical vector of letter grade to numeric grades between 1 to 5, where A=1, B=2, , F=5. First, you will convert the grade column vector to factor using the factor() function. Then, convert the grade column with the command as.numeric() to numeric column. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBDb252ZXJ0IHRoZSBjYXRlZ29yaWNhbCBjb2x1bW4gZ3JhZGUgdG8gZmFjdG9yIGRhdGEgY29sdW1uLlxubW9vZHkkZ3JhZGU8LWZhY3Rvcihtb29keSRncmFkZSlcbmhlYWQobW9vZHkkZ3JhZGUpXG5cbiMgTm93IGNvbnZlcnQgdGhlIGxldmVscyB0byBudW1lcmljIHVzaW5nIHRoZSBhcy5udW1lcmljIGZ1bmN0aW9uXG5tb29keSRncmFkZSA8LSBmYWN0b3IoYXMubnVtZXJpYyhtb29keSRncmFkZSkpXG5oZWFkKG1vb2R5JGdyYWRlKSJ9 We can see that the outcome of the above code, gives us a moody dataframe with grade column as a numeric column converted from the previous categorical column. We can also see that the we used the as.numeric() function inside the factor function while converting from categorical to numeric, to maintain the levels information of the grade column. Now, suppose you also want to change the labels of the grade column. Lets change the grades from capital letters to small letters, i.e. A -&gt; a, B -&gt; b, and so on. To do this, we can provide our user defined labels vector to the labels attribute of the factor() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBDb252ZXJ0IHRoZSBjYXRlZ29yaWNhbCBjb2x1bW4gZ3JhZGUgdG8gZmFjdG9yIGRhdGEgY29sdW1uIHdpdGggdXNlciBkZWZpbmVkIGxhYmVscy5cbm1vb2R5JGdyYWRlIDwtIGZhY3Rvcihtb29keSRncmFkZSxsYWJlbHMgPSBjKFwiYVwiLFwiYlwiLFwiY1wiLFwiZFwiLFwiZlwiKSlcbmhlYWQobW9vZHkkZ3JhZGUpIn0= We can see that the capital letter are now transformed to small letters. 5.4 Merging Two Relational Data Frames. Often, we have data from multiple sources/multiple databases, files etc. To perform analysis, we need to merge these dataframes together with one or more common key variables. In R the merge() function allows merging two data frames by common columns or row names. This function allows you to perform different SQL joins, like left join, inner join, right join or full join, among others. We will look at merging datasets in R with this function, along with examples. Consider the following 2 datasets. First is a smaller just 4 record data subset of the Moody dataset. Table 5.1: Small subset of Moody Dataset STUDENTID SCORE GRADE ON_SMARTPHONE ASKS_QUESTIONS FINALEXAM 65446 23.67 D never always 12.874804 79686 8.41 F never never 5.044093 56400 69.76 C never always 23.585730 16792 95.51 A never always 23.476748 Second is another dataset of students with respective GPA and Majors. Table 5.2: Small dataset of students information STUDENTID GPA Major 65446 1.559626 computer science 79686 3.813033 economics 56400 2.840912 political science 10001 2.664000 economics NOTE: We can see from the above snippets of the above the top 3 records in both dataset have same STUDENTID, but the 4th records in both datasets are of different students. The most important element while discussing the examples below, will focus on what happens to the 4th records of both datasets when using the various merge options and attributes. 5.4.1 Inner Join This is the most usual type of join of datasets that you can perform. It consists of merging two dataframes in one that contains common elements of both. In order to merge the two datasets, you just have to pass them to the merge() function without the need of changing other arguments. Inner join merge is the default merge of the merge() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aG91dCBhbnkgYXR0cmlidXRlcy5cbm1lcmdlKG1vb2R5X2RmLHN0dWRlbnRfZGYpIn0= We can see that there are only 3 record in the output. The reason being that, the studentid of the fourth record in both the dataset did not match. And thus the merge function did not know which datasets record to be kept and which not. Also the reason the merge function tried to match and merge the two datasets, is by using the first columns from both the datasets, which in both case was the STUDENTID column. We can also do the same process, and get he same outcome, by defining the index column by yourself. Lets look at this in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aCB0aGUgXCIgYnkgXCIgIGF0dHJpYnV0ZS5cbm1lcmdlKG1vb2R5X2RmLHN0dWRlbnRfZGYsYnkgPSBcIlNUVURFTlRJRFwiKSJ9 As we can see, the output remains the same. But we understand that we can define any other common column as the index column based on which the merging can occur. IMPORTANT NOTE: There are also arguments like by.x and by.y which correspond to indexing based on one of the column from the left(first) or right(second) datasets respectively. This could come extremely handy, when the two datasets you want to merge, have different column name for the index column. For example, suppose in the two dataset that we have considered above, the first dataset had students records indexed by the studentid column where the indexing column name is studentid, but in the second dataset the indexing column even though with same student ids as entries but with the column name of stu-id. Now while merging, you can face error since the merge() function will have trouble finding the two index columns to match since they are named differently in the two datasets. Here you can provide the argument by.x = \"studentid\" , by.y = \"stu-id\" in the function while merging. 5.4.1.1 Another example Suppose you have the happiness index dataset, Table 5.3: Happiness Index Dataset for all countries IDN AGE COUNTRY GENDER IMMIGRANT INCOME HAPPINESS 88364 29 Kyrgyzstan Male 0 103305 8.35 37692 41 Afghanistan Male 0 51682 4.44 57856 20 Azerbaijan Female 0 72381 6.24 49453 62 South Korea Female 0 65658 5.66 93485 63 Jordan Female 1 109581 3.17 97976 36 Congo-Kinshasa Female 0 112432 9.43 where you have the survey data of people of various countries with records of information about AGE, COUNTRY, GENDER, IMMIGRANT, INCOME, and HAPPINESS. You can do analysis on the above dataset per country, per age group,etc. But if you want to do analysis based on per continent, then you will have to create lists of all the countries in each continent, and then subset using the appropriate subset method/s from section below 5.5. Alternate method will be acquiring another dataset, with information of each country and its respective continent, and do merge, which we can then use to subset easily. Lets look at an example of this process below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCB0aGUgSGFwcGluZXNzIGluZGV4IGRhdGFzZXQuXG5oYXBweTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvSEFQUElORVNTMjAxNy5jc3ZcIilcbmhlYWQoaGFwcHkpXG5cbiMgTm93IGxldHMgbG9hZCB0aGUgc2ltcGxlIGRhdGFzZXQgb2YgY291bnRyeSBhbmQgY29udGluZW50cy5cbmNvbnRpbmVudHM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L2NvdW50cnktY29udGluZW50cy5jc3ZcIilcbmhlYWQoY29udGluZW50cylcblxuIyBOb3cgd2UgY2FuIHVzZSB0aGUgbWVyZ2UgZnVuY3Rpb24gdG8gaW5jbHVkZSB0aGUgY29udGluZW50cyBvZiBlYWNoIGNvdW50cnkgaW4gdGhlIGhhcHBpbmVzcyBkYXRhc2V0IGFnYWluc3QgZWFjaCBvdGhlci5cbmhhcHB5LmM8LW1lcmdlKGhhcHB5LGNvbnRpbmVudHMpXG5oZWFkKGhhcHB5LmMpXG5cbmhhcHB5LmNbc2FtcGxlKG5yb3coaGFwcHkuYyksMTApLF0ifQ== We can see from the output of the above example, the new dataframe created in happy.c after applying merge() function on the Happiness index dataset and the country-continents dataset, the CONTINENT column is added from the country-continents dataset into the happyness index dataset. And each country in the happy.c dataframe has now the value of its respective continent in the the CONTINENT column. 5.4.2 Full Join Full Join is also known as the outer join or the full outer join. It merges all the columns of both datasets into one. For those records with non-intersecting index elements, Full join keeps both the records, and fills the missing values with NA , i.e. Not Available(NA) keyword. In order to create this type of full join of the two dataframes in R, we need to set the argument all to TRUE or T. Lets look at this in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aCB0aGUgXCIgYWxsIFwiICBhdHRyaWJ1dGUuXG5tZXJnZShtb29keV9kZixzdHVkZW50X2RmLGFsbCA9IFRSVUUpIn0= We can see that the first record of the output with studentid = 10001 was present in the second dataset only, thus the values corresponding to the columns of the first dataset are set to NA. Similarly, the same occurs with the record with studentid = 16792, which was only present in the first dataset, and thus has NA in the place of columns of second dataset. 5.4.3 Left Join The left join in R involves matching all the rows in the first data frame with the corresponding records on the second dataframe. To create this left join, you just have to set the argument all.x to TRUE or T. Recall while doing the full join, we set the argument all to TRUE or T. Similarly, since we consider x as the first dataset or the left dataset, we will set the argument of all.x where the .x is the key to select the first dataset. We have seen in the snippets above, the student with studentid = 16792 is only present in the first dataset but not the second. So lets see the result of merging using the left join in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aCB0aGUgXCIgYWxsIFwiICBhdHRyaWJ1dGUuXG5tZXJnZShtb29keV9kZixzdHVkZW50X2RmLGFsbC54ID0gVFJVRSkifQ== We can see that the record of student with studentid = 16792 has NA as the entry in the columns merged from the right dataset. Also, the record of student with studentid = 10001 is completely excluded, since it belongs to the second dataset. 5.4.4 Right Join The right join merge involves joining all the rows in the second data frame with the corresponding records on the first dataframe. The right join is opposite to that of left join. In consequence, here, you will need to set the argument all.y to TRUE or T, since we consider the right dataset or the second dataset as y. We have seen in the snippet above, that the student with studenid = 10001 is only present in the second dataset but not the first. So lets see the result of merging using the right join in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cblxuIyBVc2UgdGhlIG1lcmdlIGZ1bmN0aW9uLCB3aXRoIHRoZSBcIiBhbGwgXCIgIGF0dHJpYnV0ZS5cbm1lcmdlKG1vb2R5X2RmLHN0dWRlbnRfZGYsYWxsLnkgPSBUUlVFKSJ9 We can see that the record of student with studentid = 10001 has NA as the entry in the columns merged from the left dataset. Also, the record of student with studentid = 16792 is completely excluded, since it belongs to the first dataset. 5.5 Slicing and Dicing. R was made especially for data analysis and graphics. SQL was made especially for databases. They are allies in this field of data science. The data structure in R that most closely matches a SQL table is a data frame. The terms rows and columns are used in both. There is an R package called sqldf that allows you to use SQL commands to extract data from an R data frame. We will not use this package in the examples but look at a way the operations in SQL translate to basic R commands that we have studied in previous chapter 4. In R we have seen how subsetting of rows and columns happen using the subset function in earlier chapters 4.4. Please review this section before proceeding ahead. 5.5.1 Subsetting on Columns ( DICING ) So lets start with dicing the dataframe. In other words, lets look at subsetting operations on columns. Columns in SQL are also called fields. In R it is commonly called variables. In SQL the subset of columns is determined by SELECT statement. We can do these type of SQL operation in R using the normal subsetting method, either using the subset() function or using the square brackets [ ]. NOTE: In most of the examples below, to avoid printing of the complete dataset after any operations, we have used the head() function to truncate the output to only top 6 rows. However you can always remove the function or change the limit or output records to your choice by passing additional attribute n = user_defined_limit to the head() function. Just to recap subsetting on columns, 5.5.1.1 Subset single column. Remember: You can either use the column names or the column location index, to dice the dataframe. Suppose we want to subset the moody dataset only the grade column. Lets look at this example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBMZXRzIHN1YnNldCB0aGUgZ3JhZGUgY29sdW1uIGZvcm0gdGhlIG1vb2R5IGRhdGFzZXQgYW5kIGxvb2sgYXQgaXRzIGZpcnN0IGZldyBlbGVtZW50cy4uXG5oZWFkKG1vb2R5WywnZ3JhZGUnLGRyb3A9Rl0pXG5cbiMgV2l0aG91dCBgZHJvcD1GYCBpbiB0aGUgYXR0cmlidXRlLCB5b3Ugd2lsbCBnZXQgb25seSB0aGUgdmFsdWVzIG9mIHRoZSBjb2x1bW4uXG5oZWFkKG1vb2R5WywnZ3JhZGUnXSkifQ== We can see that only one column is selected form the dataframe. The drop = F attribute is provided to keep the dataframe structure. You can also see the effect of not using the drop = F attribute in the above example. Note: In some cases, where you want to use the subsetted column with other function, e.g. mean(subsetted_column) you must not use the drop=F attribute, otherwise it will result in error. 5.5.1.2 Subset multiple column. Suppose you want to subset multiple columns by name, you can create a vector or the column names you want to subset and then include it wile subsetting. Lets look at the example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNyZWF0ZSBhIHZlY3RvciBvZiBjb2x1bW4gbmFtZSB0aGF0IHdlIHdhbnQgdG8gc3Vic2V0LlxuY29sdW1uTmFtZXM8LSBjKFwiZ3JhZGVcIixcInNjb3JlXCIpXG5cbiMgSW5jbHVkaW5nIHRoZSBhYm92ZSB2ZWN0b3Igd2lsZSBzdWJzZXR0aW5nLlxuaGVhZChtb29keVssY29sdW1uTmFtZXNdKSJ9 We can see that only the two column of grade and score are kept in the subset. Similarly, we can include the multiple column names and get subset. 5.5.1.3 Subset on all columns You can get all the columns in the subset, by keeping the space after the comma blank. This gives the complete set of columns. Suppose you did some slicing on the dataframe and want to keep all the columns in the output, you can just keep the space after the comma blank while subsetting. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNhbiBzZWUgdGhhdCB3ZSBzbGljZWQgdGhlIGRhdGFmcmFtZSB0byBvbmx5IGtlZXAgdGhlIHJlY29yZHMgb2Ygc3R1ZGVudHMgd2l0aCBncmFkZSBcIkFcIi4gXG5oZWFkKG1vb2R5W21vb2R5JGdyYWRlPT1cIkFcIiwgXSlcblxuXG4jIFdlICB3aWxsIGxvb2sgYXQgc2xpY2luZyBpbiB0aGUgc3Vic2VxdWVudCBzZWN0aW9ucy4ifQ== 5.5.2 Subsetting on Rows ( SLICING ) Now that we have seen dicing Or subsetting on columns, which is similar to the select statement of SQL, we will now look at slicing on the dataframe. Or in other words subsetting on rows. There are many statements of SQL that does subsetting on rows, i.e. SELECT, WHERE, AND, OR, IN, LIKE, LIMIT, and many more. We will look at few of them, by implementing them using the basic R functions. 5.5.2.1 Subsetting based on single condition. We will look at a subsetting condition based on value. For subsetting based on value, you can use the relational operators e.g. &gt; , &lt; , &gt;= , &lt;= , == , etc between the attribute name and the value. Lets look at this in the following example. Suppose you want to keep all the observations of where score of students are greater than 80. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNhbiBzZWUgdGhhdCB3ZSBzbGljZWQgdGhlIGRhdGFmcmFtZSB0byBvbmx5IGtlZXAgdGhlIHJlY29yZHMgb2Ygc3R1ZGVudHMgd2l0aCBzY29yZSBncmVhdGVyIHRoYW4gODAuIFxuaGVhZChtb29keVttb29keSRzY29yZT44MCwgXSkifQ== We can see from the above result, the subset has only records of students having score greater than 80. This example is similar to using where statement in SQL. 5.5.2.2 Subsetting based on multiple conditions. Similar to the above example, suppose you want to subset based on multiple conditions. To do this, we will use the logical operators e.g. AND (\" &amp; \") , OR (\" | \") , NOT (\" ! \") between the various conditions. Lets look at an example for this type of subsetting. Suppose you want to slice the records of the moody dataset, based on two conditions: - Students with grade equal to \" A \" - AND - Students with score greater than 90. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNhbiBzZWUgdGhhdCB3ZSBzbGljZWQgdGhlIGRhdGFmcmFtZSB0byBvbmx5IGtlZXAgdGhlIHJlY29yZHMgb2Ygc3R1ZGVudHMgd2l0aCBzY29yZSBncmVhdGVyIHRoYW4gOTAgQU5EIHdpdGggZ3JhZGUgZXF1YWwgdG8gXCJBXCIgLiBcbmhlYWQobW9vZHlbbW9vZHkkc2NvcmU+OTAgJiBtb29keSRncmFkZSA9PSBjKFwiQVwiKSwgXSkifQ== We can see that the records of students with score greater than 90 and grade equal to A are kept, rest all records are removed. This example is similar to using the AND, OR, NOT clause in SQL. 5.5.2.3 Subset based on multiple values. We will look at subsetting the dataframe based on one condition with multiple values. Suppose you want to subset the moody dataset, based on the students grade, but you want to keep students records with grade equal to both B and C Well you can use multiple conditions as seen above with an AND clause between the two conditions with different values on the same variable/columns, but there is a simple and useful way to do this with just one conditional statement. We will make use of a vector of all the values that we want to use, and then assign this vector to the condition statement. Lets look at this in the following example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBsZXRzIGNyZWF0ZSBhIHZlY3RvciBvZiB2YWx1ZXMgcmVxdWlyZWQgaW4gdGhlIGNvbmRpdGlvbmFsIHN0YXRlbWVudFxuY29uZFZhbHVlczwtIGMoXCJCXCIsXCJDXCIpXG5cbiMgaGVyZSB3ZSBjYW4gc2VlIHRoYXQgd2Ugc2xpY2VkIHRoZSBkYXRhZnJhbWUgdG8gb25seSBrZWVwIHRoZSByZWNvcmRzIG9mIHN0dWRlbnRzIHdpdGggZ3JhZGUgZXF1YWwgdG8gXCJCXCIgb3IgXCJDXCIgLiBcbmhlYWQobW9vZHlbbW9vZHkkZ3JhZGUgPT0gY29uZFZhbHVlcywgXSlcbnVuaXF1ZShtb29keVttb29keSRncmFkZSA9PSBjb25kVmFsdWVzLCBdJGdyYWRlKVxuXG4jIHdlIGNhbiBhbHNvIGRpcmVjdGx5IHdyaXRlIHRoZSB2ZWN0b3Igd2l0aG91dCBhc3NpZ25pbmcgYSB2YXJpYWJsZS5cbmhlYWQobW9vZHlbbW9vZHkkZ3JhZGUgPT0gYyhcIkJcIixcIkNcIiksXSlcbnVuaXF1ZShtb29keVttb29keSRncmFkZSA9PSBjKFwiQlwiLFwiQ1wiKSxdJGdyYWRlKSJ9 We can see that the output has only records of students with grades B or C. And both the methods, result in same output. This example is similar to the IN operator of the SQL 5.5.2.4 Subset based on a partial/complete text/character. We will look at subsetting the dataset based on a specific pattern of text/characters. This type of subsetting proves useful in text columns where each record has one or more than one sentence, and you want to search for a particular keyword or pattern. Most simple example would be of a survey dateset, where each record in the dataset consists of text paragraph, answering the questions asked in the survey, and you want to figure out the count of particular keywords in each response. Lets look at an example based on the Happiness dataset. We would like to find the subset of countries with the letters \" and \" in their name. eg. Iceland, Uganda, Poland, etc. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJoYXBweTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvSEFQUElORVNTMjAxNy5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBTdWJzZXQgdXNpbmcgdGhlIGdyZXAgZnVuY3Rpb24gdG8gZmluZCB0aGUgcGF0dGVybiBcImFuZFwiIGluIHRoZSBuYW1lcyBvZiB0aGUgY291bnRyaWVzXG5oZWFkKGhhcHB5W2dyZXAoXCJhbmRcIixoYXBweSRDT1VOVFJZLGlnbm9yZS5jYXNlID0gVCksXSlcbnVuaXF1ZShoYXBweVtncmVwKFwiYW5kXCIsaGFwcHkkQ09VTlRSWSxpZ25vcmUuY2FzZSA9IFQpLF0kQ09VTlRSWSkifQ== We can see the output has subset of the happy dataset with records of only those countries with the pattern and in its name. To do this we have used the grep() function, which is a really important function for finding patterns in text and data. We dont need to study this grep() function in detail, but one can find very good resources explaining it online. This example is similar to the LIKE operator in SQL. 5.6 Group By Now that we have done Slicing and Dicing, we will like to apply some functions and gain measured information form the subsets. Although there is no straightforward, direct/ one step function to perform the function as that of the GROUP BY from SQL, but we can get the required functionality, by combining various functions step by step from the R.7 commands list 2.2 and the things we learned in this section 5 and the revision section 4. This section will involve use of the table(), tapply() function to apply the functions like mean, count, sum, etc on the subsets categorical or numerical columns. More importantly, we will look at a very useful example below, which will tie together all that we have learned until now. Suppose you want to get the statistics/numbers of average scores per grade and frequency of students per grade, and then use this table afterwards. So the SQL query will look something like SELECT grade, avg(score) as averagescore, count(*) as student_number FROM moody GROUP BY grade. To implement this above query functionality in R we would fisrt need to use the tapply function to find get the average score per grade and frequency per grade, and then combine it. Lets look at this process in the code below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuXG4jIENyZWF0ZSBhIHRhYmxlIG9mIGZyZXF1ZW5jeSBvZiBzdHVkZW50cyBwZXIgZ3JhZGUuXG5ncmFkZS5jb3VudCA8LSB0YXBwbHkobW9vZHkkZ3JhZGUsbW9vZHkkZ3JhZGUsbGVuZ3RoKVxuXG4jIENyZWF0ZSBhIHRhYmxlIG9mIGF2ZXJhZ2Ugb2Ygc3R1ZGVudHMgc2NvcmUgcGVyIGdyYWRlLlxuZ3JhZGUubWVhbiA8LSB0YXBwbHkobW9vZHkkc2NvcmUsbW9vZHkkZ3JhZGUsbWVhbilcblxuIyBXZSBub3cgY29tYmluZSB0aGUgdHdvIHRhYmxlcyB0b2dldGhlciB1c2luZyBjYmluZCBhbmQgc3RvcmUgaXQgYXMgZGF0YS5mcmFtZSBmb3Igc2ltcGxlIHBvc3QtcHJvY2Vzc2luZy5cbm91dDwtYXMuZGF0YS5mcmFtZShjYmluZChncmFkZS5jb3VudCxncmFkZS5tZWFuKSlcbm91dCJ9 We can see the combined table of both the average scores and frequency per grade. Now suppose we want to go one step ahead and want to order the out table from the example above, based on decreasing value of frequency of students per grade. To do this, we introduce the order() function. Order() Function The order() function returns a permutation of the order of the elements of a vector. You can decide by passing the argument to order the elements in ascending or descending order. An important thing to note, is that for our use for the example we discussed above, we will use the order function as a subset parameter. Lets look at this in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIilcbmdyYWRlLmNvdW50IDwtIHRhcHBseShtb29keSRncmFkZSxtb29keSRncmFkZSxsZW5ndGgpXG5ncmFkZS5tZWFuIDwtIHRhcHBseShtb29keSRzY29yZSxtb29keSRncmFkZSxtZWFuKVxub3V0PC1hcy5kYXRhLmZyYW1lKGNiaW5kKGdyYWRlLmNvdW50LGdyYWRlLm1lYW4pKVxuXG5vdXRcblxuIyBOb3cgbGV0cyBvcmRlciB0aGUgb3V0IGRhdGEsIGJhc2VkIG9uIHRoZSBncmFkZS5jb3VudCBjb2x1bW4sIGluIGFzY2VuZGluZyBvcmRlci5cbm91dFtvcmRlcihvdXRbLCdncmFkZS5jb3VudCddKSxdXG5cbiMgSWYgeW91IHdhbnQgdGhlIG91dHB1dCBpbiBkZXNjZW5kaW5nIG9yZGVyIGp1c3QgcGFzcyAnVFJVRScgb3IgJ1QnIHRoZSAgZGVjcmVhc2luZyBhcmd1bWVudCBvZiB0aGUgb3JkZXIgZnVuY3Rpb24uXG5vdXRbb3JkZXIob3V0WywnZ3JhZGUuY291bnQnXSxkZWNyZWFzaW5nID0gVCksXSJ9 We saw how we can use implement ordering in R. This is similar to using the ORDER BY statement of SQL. Another thing we can do is subsetting on the output. Suppose you want to keep only those grade records in the out data with frequency of students greater than 150 for particular grade. To do this we will use the technique studied in the slicing section 5.5.2. Lets look at the working of the above example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIilcbmdyYWRlLmNvdW50IDwtIHRhcHBseShtb29keSRncmFkZSxtb29keSRncmFkZSxsZW5ndGgpXG5ncmFkZS5tZWFuIDwtIHRhcHBseShtb29keSRzY29yZSxtb29keSRncmFkZSxtZWFuKVxub3V0PC1hcy5kYXRhLmZyYW1lKGNiaW5kKGdyYWRlLmNvdW50LGdyYWRlLm1lYW4pKVxuXG5vdXRcblxuIyBUbyBrZWVwIHRoZSByZWNvcmRzIHdoZXJlIGZyZXF1ZW5jeSBvZiBzdHVkZW50cyBpbiBwYXJ0aWN1bGFyIGdyYWRlIGlzIGdyZWF0ZXIgdGhhbiAxNTAuXG5vdXRbb3V0JGdyYWRlLmNvdW50PjE1MCxdIn0= We see that the B Grade had only 108 students in the record, it is removed from the out dataframe. This is similar to using the HAVING clause of SQL. 5.7 Handling Date and Time in dataframes. one of the most common issue that a novice or even an experienced R user can face is of handling date and time information available into the dataset, and importing it to use as a variable that is appropriate ans usable during analysis. Also getting R to agree that your data contains the dates and times can be tricky sometimes. We will see an example where the usual R data import fails to read date and time as actually date and time. To simplify this issue, we use a package called lubridate, which makes it easier to work with dates and times and converts them into POSIXct format. POSIXct is a class of data recognized by R as being a date or date and time. Lubridates functions handle wide variety of formats and separators, which simplifies the parsing process. Lets look how easy it is to use it and convert date and time input to be used in analysis. First we will look at converting date in character format to POSIXct. First we will convert \"20200317\"which is in year-month-date format. To convert this we will use the ymd() function of the lubridate package. Second we will convert \"03-17-2020\"which is in month-date-year format. To convert this we will use the mdy() function of the lubridate package. Third we will convert \"17/03/2020\"which is in date-month-year format. To convert this we will use the dmy() function of the lubridate package. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGx1YnJpZGF0ZSkgIyBpbmNsdWRlIHRoZSBsdWJyaWRhdGUgbGlicmFyeS5cblxuIyBGaXJzdCB3ZSB1c2UgdGhlIHltZCgpIGZ1bmN0aW9uLlxueW1kKFwiMjAyMDAzMTdcIilcblxuIyBTZWNvbmQgd2UgdXNlIHRoZSBtZHkoKSBmdW5jdGlvbi5cbm1keShcIjAzLTE3LTIwMjBcIilcblxuIyBUaGlyZCB3ZSB1c2UgdGhlIGRteSgpIGZ1bmN0aW9uLlxuZG15KFwiMTcvMDMvMjAyMFwiKSJ9 We can see the output of all the 3 function is the same, this means that the functions used have successfully converted all the input character type dates into the standardized POSIXct data type. Now lets look at converting time in character format to POSIXct. First we will convert \"18:20\" which is in hour-minutes format. To convert this we ill use the hm() function of the lubridate package. Second we will convert \"18:20:30\" which is in hour-minute-second format. To convert this we ill use the hms() function of the lubridate package. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGx1YnJpZGF0ZSkgIyBpbmNsdWRlIHRoZSBsdWJyaWRhdGUgbGlicmFyeS5cblxuIyBGaXJzdCB3ZSB3aWxsIHVzZSB0aGUgaG0oKSBmdW5jdGlvbi5cbmhtKFwiMTg6MjBcIilcblxuIyBTZWNvbmQgd2Ugd2lsbCB1c2UgdGhlIGhtcygpIGZ1bmN0aW9uLlxuaG1zKFwiMTg6MjA6MzBcIikifQ== We can see that the output of the 2 functions above are in POSIXct format and has the information of hours minutes and seconds annotated properly. There are various other functions in the lubridate package like for various use case, but we will not cover them since they are not useful here. To learn more about it you can visit the official lubridate package vignette linked here: lubridate Now coming back to the main example of avoiding issues/errors while importing date and time attributes present in dataset. For this we will look at the AirQualityUCI dataset. And here is the snippet of the dataset below. Table 5.4: Air Quality Dataset of amount of elements and pollutants in air. Date Time CO Tin.Oxide Non.Metanic.HydroCarbons Benzene 3/10/2004 18:00:00 2.6 1360 150 11.9 3/10/2004 19:00:00 2.0 1292 112 9.4 3/10/2004 20:00:00 2.2 1402 88 9.0 3/10/2004 21:00:00 2.2 1376 80 9.2 3/10/2004 22:00:00 1.6 1272 51 6.5 3/10/2004 23:00:00 1.2 1197 38 4.7 You will see from the dataset that the date and time columns are imported correctly. But in fact, and as we will see in the code below, the date column is of type character and the time is also of type character. Now to convert these columns into POSIXct supported date time columns we will use the lubridate functions. And then we will count the number of records in the dataset per year using the year() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhcTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvQWlyUXVhbGl0eVVDSS5jc3ZcIikgI3dlYiBsb2FkXG5hcTwtYXFbLDE6Nl0gIyBSZWR1Y2luZyB0aGUgbnVtYmVyIG9mIGNvbHVtbnMuXG5cbmhlYWQoYXEpXG5cbiMgTGV0cyBsb29rIGF0IHRoZSB0eXBlIG9mIHRoZSBEYXRlIGNvbHVtbiBhZnRlciBpbXBvcnRpbmcgdGhlIGRhdGFzZXQuXG5jbGFzcyhhcSREYXRlKSAjIERhdGUgQ29sdW1uXG5cbiMgTm93IGxldHMgdXNlIHRoZSBtZHkoKSBmdW5jdGlvbiB3aGljaCBjb252ZXJ0cyB0aGUgbW9udGgtZGF5LXllYXIgZm9ybWF0IHRvIFBPU0lYY3QgZm9ybWF0LlxuYXEkRGF0ZTwtbWR5KGFxJERhdGUpXG5oZWFkKGFxKVxuXG4jIE5vdyBsZXRzIGNoZWNrIHRoZSB0eXBlIG9mIHRoZSBEYXRlIGNvbHVtbiBhZ2Fpbi5cbmNsYXNzKGFxJERhdGUpXG5cblxuIyBMZXRzIGNyZWF0ZSBhIGZyZXF1ZW5jeSB0YWJsZSBmb3IgdGhlIGZyZXF1ZW5jeSBvZiByZWNvcmRzIHBlciB5ZWFyIHVzaW5nIHRoZSB0YWJsZSgpIGFuZCB5ZWFyKCkgZnVuY3Rpb25cbnRhYmxlKHllYXIoYXEkRGF0ZSkpIn0= We can see that the original type of the date column was \"character\" but then after using the lubridates function, we converted it to a suitable POSIXct format of \"Date\". Then we were easily able to subset the dataset based on the year, and get the frequency count of the records per year, as seen from the table for the years 2004 and 2005. Similarly, we can also convert the time column and probably use it later in analysis process. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhcTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvQWlyUXVhbGl0eVVDSS5jc3ZcIikgI3dlYiBsb2FkXG5hcTwtYXFbLDE6Nl0gIyBSZWR1Y2luZyB0aGUgbnVtYmVyIG9mIGNvbHVtbnMuXG5cbmhlYWQoYXEpXG5cbiMgTGV0cyBsb29rIGF0IHRoZSB0eXBlIG9mIHRoZSBUaW1lIGNvbHVtbiBhZnRlciBpbXBvcnRpbmcgdGhlIGRhdGFzZXQuXG5jbGFzcyhhcSRUaW1lKSAjIFRpbWUgQ29sdW1uXG5cbiMgU2ltaWxhcmx5IGZvciB0aGUgdGltZSBjb2x1bW4gbGV0cyB1c2UgdGhlIGhtcygpIGZ1bmN0aW9uLlxuYXEkVGltZTwtaG1zKGFxJFRpbWUpXG5cbiMgTm93IGxldHMgY2hlY2sgdGhlIHR5cGUgb2YgdGhlIFRpbWUgY29sdW1uIGFnYWluLlxuY2xhc3MoYXEkVGltZSkifQ== We can see that the original type of the time column was \"character\" but then after using the lubridates function, we converted it to a suitable POSIXct format of \"Period\" which is used to represent time information. EOC "],["classification.html", "Chapter 6 Data Modeling and Prediction techniques for Classification. 6.1 Decision Tree. 6.2 Use of Rpart 6.3 Visualize the Decision tree 6.4 Rpart Control 6.5 Prediction using rpart. 6.6 Split the data yourself. 6.7 Cross Validation", " Chapter 6 Data Modeling and Prediction techniques for Classification. 6.1 Decision Tree. Decision tree is one of the most powerful and popular tool for classification and prediction. It is a supervised learning predictive model that uses a set of binary rules to calculate a target value. It is a flow chart like tree structure, where each internal node has a test on a particular attribute, each branch denotes the outcome of the test, and each leaf node holds a class label/ numeric value. The reason decision tree are very popular are: - It is able to generate rules easier to understand as compared to other models.. - It require much less computations for performing modeling and prediction. - Both continuous/numerical and categorical variables are handled easily while creating the decision trees. There are a few drawbacks too while using decision trees in certain case of inputs and tasks. But for the scope of discussion of this course we wont go into much details of it. Also, we wont go into the depth of the internals of the formation/creation of the decision tree and its underlying algorithm, but we will look at decision tree as a way to create prediction model for both classification and regression. 6.2 Use of Rpart Recursive Partitioning and Regression Tree RPART library is a collection of routines which implement Classification and Regression Tree (CART) which is a type of Decision Tree.The resulting model can be represented as a binary tree. The library associated with this RPART is called rpart. Install this library using install.packages(\"rpart\"). Syntax for building the decision tree using rpart(): rpart( formula , method, data, control,...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... method: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use anova for regression, class for classification, etc. data: here we provide the dataset on which we want to fit the decision tree on. control: here we provide the control parametes for the decision tree. Explained more in detail in section further in this chapter. For more info on the rpart function visit rpart documentation Lets look at an example on the Moody 2019 dataset. We will use the rpart() function with the following inputs: prediction -&gt; GRADE predictors -&gt; SCORE, ON_SMARTPHONE, ASKS_QUESTIONS, LEAVES_EARLY, LATE_IN_CLASS data -&gt; moody dataset method -&gt; class for classification. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG5ycGFydChHUkFERSB+IFNDT1JFK09OX1NNQVJUUEhPTkUrQVNLU19RVUVTVElPTlMrTEVBVkVTX0VBUkxZK0xBVEVfSU5fQ0xBU1MsIGRhdGEgPSBtb29keVssLWMoMSldLG1ldGhvZCA9IFwiY2xhc3NcIikifQ== We can see that the output of the rpart() function is the decision tree with details of, node -&gt; node number split -&gt; split conditions/tests n -&gt; number of records in either branch i.e. subset yval -&gt; output value i.e. the target predicted value. yprob -&gt; probability of obtaining a particular category as the predicted output. Using output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section 6.5 But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section. 6.3 Visualize the Decision tree To visualize and understand the rpart() tree output in the easiest way possible, we use a library called rpart.plot. The function rpart.plot() of the rpart.plot library is the function used to visualize decision trees. The rpart.plot library is a front-end wrapper to the library prp which is the most basic library for plotting decision trees. prp allows various aesthetic modifications for visualizing the decision tree. We will look at a few examples of using prp below. But, first lets look at a example to visualize the output decision tree in the previous example on Moody dataset using rpart.plot() NOTE: The online runnable code block does not support rpart.plot and prp library and functions, thus the output of the following code examples are provided directly. # First lets import the rpart library library(rpart) # Import dataset moody &lt;- read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv&quot;) # Use of the rpart() function. tree &lt;- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody,method = &quot;class&quot;) # Now lets import the rpart.plot library to use the rpart.plot() function. library(rpart.plot) # Use of the rpart.plot() function to visualize the decision tree. rpart.plot(tree) Output Plot of rpart.plot() function We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about the grade category. the outcome probability of each grade category. the records percentage out of total records. To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides rpart.plot and Plotting with rpart.plot (PDF) Note that for any beginner using rpart.plot() function is the easiest way. But if you want to learn another way of plotting rpart trees then the following function can be used. So,another form of plotting rpart trees in a very minimalistic way is using the plot rpart i.e. prp() function, which is actually the working function behind rpart.plot(). Lets look at a same example like above but using prp(). # First lets import the rpart library library(rpart) # Import dataset moody &lt;- read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv&quot;) # Use of the rpart() function. tree &lt;- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody[,-c(1)],method = &quot;class&quot;) # Now lets import the rpart.plot library to use the rpart.plot() function. library(rpart.plot) # Use of the prp function to visualize the decision tree. prp(tree) Output Plot of prp() function We can see that the output of the prp() function is a very minimalist tree, without any colors with minimum required information. There are other arguments that can be passed to the prp() function to increase the aesthetic look and the information provided. To learn those extra arguments visit this guide prp() NOTE: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment. 6.4 Rpart Control We will now look at the control argument used in rpart() function, which is one of the important argument. The control argument of rpart() function is used to manually decide the control parameters of the decision tree. The advantages of using control method: - It restricts the height of the decision tree. - It avoids overfitting on the training dataset. - It can be used to eliminate attributes that affect less significantly on the splitting constraints. - Helps to terminate the creation process of tree earlier, thus reducing required computational time. The disadvantages of using control method: - It creates risk of generating trees with lesser accuracy compared to uncontrolled tree. - It could hamper the splitting condition selection in negative way. - Could result in underfitting, if control parameters not chosen carefully. As we can see, that controlling the decision tree provides us with lot of advantage in certain condition, we also risk in reducing the accuracy of the prediction from using the tree. Thus these control methods must be applied only in certain case, where the uncontrolled method takes large amount of time to create the tree, and overfits the train data. When the datasets have more significantly higher count of columns but less records of data, using control methods could be suitable. Now lets look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function. rpart.control( *minsplit*, *minbucket*, *cp*,...) minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -&gt; the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition. minbucket: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -&gt; the minimum number of observation in the terminal/leaf node of the trees must be 500 or above. cp: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by cp, will not be made in the tree. For more information of the other arguments of the rpart.control() function visit rpart.control Note: The ratio of minsplt to minbucket is 3:1. Thus if only one of the minsplit/minbucket is provided the other value is set using the above ratio. Also if both values are provided, unless the values are not in the above ratio, the rpart.control() the resorts to the default value. Also note, the default value of cp is 0.01. Let look at few examples. Suppose you want to set the control parameter minsplit=200. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIGxpYnJhcnkocnBhcnQpXG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbiB3aXRoIHRoZSBjb250cm9sIHBhcmFtZXRlciBtaW5zcGxpdD0yMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiAuLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKG1pbnNwbGl0ID0gMjAwKSlcblxuIyBDaGVjayB0aGUgY291bnQgb2Ygb2JzZXJ2YXRpb24gYXQgZWFjaCBzcGxpdCB0ZXN0LiBUbyBkbyB0aGlzIHdlIGZpbmQgdGhlIGNvdW50IGF0IGVhY2ggbm9uLWxlYWYvbm9uLXRlcm1pbmFsIG5vZGUuXG50cmVlJGZyYW1lW3RyZWUkZnJhbWUkdmFyIT1cIjxsZWFmPlwiLGMoXCJ2YXJcIixcIm5cIildXG5cbiMgbGlicmFyeShycGFydC5wbG90KVxuIyBycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=200 in rpart.control() function We can see from the output of tree$splits and the tree plot, that at each split the total amount of observations are above 200. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits. Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24gd2l0aCB0aGUgY29udHJvbCBwYXJhbWV0ZXIgbWluc3BsaXQ9MjAwXG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gLiwgZGF0YSA9IG1vb2R5WywtYygxKV0sbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5idWNrZXQgPSAxMDApKVxuXG4jIENoZWNrIHRoZSBjb3VudCBvZiBvYnNlcnZhdGlvbiBpbiBlYWNoIGxlYWYgbm9kZS5cbnRyZWUkZnJhbWVbdHJlZSRmcmFtZSR2YXI9PVwiPGxlYWY+XCIsYyhcInZhclwiLFwiblwiKV1cblxuIyBsaWJyYXJ5KHJwYXJ0LnBsb3QpXG4jIHJwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=100 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. Lets now use the cp parameter and see its effect on the tree. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24gd2l0aCB0aGUgY29udHJvbCBwYXJhbWV0ZXIgY3A9MC4wMDVcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiAuLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKGNwID0gMC4wMDUpKVxuXG4jIENoZWNrIHRoZSBhY2N1cmFjeSBpbmNyZWFzZSBmYWN0b3IgYXQgZWFjaCBzcGxpdC5cbnRyZWUkY3B0YWJsZVxuXG4jIGxpYnJhcnkocnBhcnQucGxvdCkpXG4jIHJwYXJ0LnBsb3QodHJlZSkifQ== Output tree plot of after setting cp=0.005 in rpart.control() function We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005. Now we saw the most important control parameters of the rpart.control function. Remember there are other parameters too, which you can study if you wish, but studying just these 3 discussed above are sufficient for the scope of this course. Also, note we have not check the effects of the control parameters on the prediction accuracy of the decision tree. Using the control parameters you could either increase the accuracy, but also risk, decreasing the accuracy. So choosing the controls parameter very carefully is very important to push the accuracy in the right direction. 6.5 Prediction using rpart. Now that we saw process to create a decision tree and also plotting it, we will like to use the output tree to predict the required attribute. From the moody example, we are trying to predict the grade of students. Lets look at the predict() function to predict the outcomes. predict(*object*,*data*,*type*,...) object: the generated tree from the rpart function. data: the data on which the prediction is to be performed. type: the type of prediction required. One of vector, prob, class or matrix. Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbi5cbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStPTl9TTUFSVFBIT05FK0FTS1NfUVVFU1RJT05TK0xFQVZFU19FQVJMWStMQVRFX0lOX0NMQVNTLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZCkifQ== We see that the output of the predict function is a vector of grades corresponding to each record of the Moody dataframe. Each index has a grade among A, B, C, D, F. Although we see the output, how do we compare the accuracy and correctness of the outputs. Lets look at one of the basic test we can do is perform a record by record comparison of the grade already in the dataset and the predicted grade, in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbi5cbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStPTl9TTUFSVFBIT05FK0FTS1NfUVVFU1RJT05TK0xFQVZFU19FQVJMWStMQVRFX0lOX0NMQVNTLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZClcblxuIyBMZXRzIGNoZWNrIHRoZSBjb3JyZWN0bmVzcyBvZiBlYWNoIHByZWRpY3Rpb24gZm9yIGVhY2ggcmVjb3JkXG5tZWFuKG1vb2R5JEdSQURFPT1wcmVkKSoxMDAifQ== Using just a row by row comparison we can see that the outcomes of the predicted grades and the original grades from the Moody dataset, are matching 93.73%. Thus our prediction accuracy is 93.73% and the error rate is 6.27%, which is very good. This prediction accuracy calculated on the training dataset is called training accuracy. Notice that we just compared the predicted grades with the already present grades from the Moody dataset, the same dataset on which the tree was built. In such scenario, one can say that you are predicting the grades on data already considered while training. So in some sense, you just output the known grades, and did not do any useful prediction. But to really see the use of our tree, we must predict on a data which has never been used in the training of the tree, OR, where the Prof. Moody has not assigned the grades. In the latter case it is difficult to prove the accuracy, without actually checking with Prof. Moody, if he/she would have assigned the same grade as the grade predicted by our model. But we have a solution for the former case, where we can predict grades on a subset of data, which we have not used while training. For that we would need to split the provided data into 2 parts, Training and Testing and then repeat the training process on the training dataset and the prediction on testing dataset. 6.6 Split the data yourself. We introduced the first and basic model for data analysis, Decision tree, in the section above. But we found that we need to perform a basic routine before dwelling deep into the creation of decision tree. The routine is to split the dataset in multiple parts, to check the accuracy of our trees prediction. This routine is the first step you perform after acquiring a cleaned dataset. The most useful splitting of dataset is done in 2 parts, Training and Testing. While splitting, the split ratio between training and testing should be decided properly. Mostly, training data is kept bigger,and testing is done on a relatively smaller subset. But the ratio should not be too biased, where there are only few observations in test data compared to training data. Usually, the math behind splitting is that, even after split, the smaller subset, i.e. the test subset should represent the distribution of the complete dataset. This means the test data should at-least have few record of each possible combination of attributes categories if categorical data, or, if numerical data then the numerical distribution is same as of the complete dataset. Thus, typically the train-test ratio is 80-20 or 70-30 or in some case even 60-40. Also, while selecting the records to assign to either training/testing data, they should be randomly picked from the original data, so as to avoid unbalanced distribution. We will look at a small example of splitting the complete dataset into training and testing dataset with a 70-30 ratio. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFNwbGl0IHJhbmRvbWx5IGludG8gMiBzZXRzIHdpdGggY2VydGFpbiByYXRpby9wcm9iYWJpbGl0eS5cbmlkeCA8LSBzYW1wbGUoIDE6Miwgc2l6ZSA9IG5yb3cobW9vZHkpLCByZXBsYWNlID0gVFJVRSwgcHJvYiA9IGMoLjcsIC4zKSlcbm1vb2R5LnRyYWluIDwtIG1vb2R5W2lkeCA9PSAxLF1cbm1vb2R5LnRlc3QgPC0gbW9vZHlbaWR4ID09IDIsXVxuXG5ucm93KG1vb2R5KVxubnJvdyhtb29keS50cmFpbilcbm5yb3cobW9vZHkudHJhaW4pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnRlc3QpXG5ucm93KG1vb2R5LnRlc3QpL25yb3cobW9vZHkpIn0= As we can see we split the original data with 1580 rows into two dataset, training data with almost 70% of rows of the original, and testing data with almost 30% of the original. Notice that we used a random sampling of the data, and not just sequential, to avoid any unbalanced distribution of attributes. Now, we looked at a method to split the dataset into training and testing data. But there is another type of splitting of the dataset which involves splitting the data into 3 parts namely, training, cross-validation and testing. We will look at the use of cross-validation and the process, in the next section 6.7. Typically, the ratio of train-validation-test is 60-20-20 or 50-25-25. Before that lets look at a simple method to perform a 3 way split with ratio 60-20-20. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFNwbGl0IHJhbmRvbWx5IGludG8gMyBzZXRzIHdpdGggY2VydGFpbiByYXRpby9wcm9iYWJpbGl0eS5cbmlkeCA8LSBzYW1wbGUoIDE6Mywgc2l6ZSA9IG5yb3cobW9vZHkpLCByZXBsYWNlID0gVFJVRSwgcHJvYiA9IGMoMC42LCAwLjIsIDAuMikpXG5tb29keS50cmFpbiA8LSBtb29keVtpZHggPT0gMSxdXG5tb29keS52YWxpZGF0aW9uIDwtIG1vb2R5W2lkeCA9PSAyLF1cbm1vb2R5LnRlc3QgPC0gbW9vZHlbaWR4ID09IDMsXVxuXG5ucm93KG1vb2R5KVxubnJvdyhtb29keS50cmFpbilcbm5yb3cobW9vZHkudHJhaW4pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnZhbGlkYXRpb24pXG5ucm93KG1vb2R5LnZhbGlkYXRpb24pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnRlc3QpXG5ucm93KG1vb2R5LnRlc3QpL25yb3cobW9vZHkpIn0= We can see that the dataset is split into 3 parts, with 60% in training data, 20% in validation data, and 20% in testing data. 6.7 Cross Validation Cross validation is a model validation technique for assessing generalization of the results of statistical analysis to an independent dataset. In other words, it is a technique to estimate the accuracy of a predictive models performance in practice. The goal of cross-validation is to test the models ability to predict new data that was not used in estimating/training it, in order to avoid problems like over-fitting and selection bias, and to give an insight on how the model will generalize to an independent dataset(i.e., an unknown dataset). Cross-validation also helps in selecting and fine-tuning the hyper-parameters of the models. In our case of decision tree, the hyper parameters could be the control parameters that determind the size of the decision tree, which in-turn determines the accuracy of the tree. One round of cross-validation involves partitioning data into complementary subsets, and then performing model training on one subset, and validating the results on the other subset. In most methods, multiple rounds of cross-validation are performed using different partitions in each round, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the models predictive performance. Another use of cross-validation is when you dont have the test data, and hence, you dont have a way to determine the true accuracy of the model. Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. The accuracy on the validation data is called cross-validation accuracy, while that on the train data is called training accuracy. Lets not dive too deep into the theory of this cross validation technique, but lets learn about the cross_validate() function, that helps us achieve this. cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*) data: The dataset on which cross validation is to be performed. tree: The decision tree generated using rpart. n_iter: Number of iterations. split_ratio: The splitting ratio of the data into train data and validation data. method: Method of the prediction. class for classification. The way the function works is as follows: It randomly partitions your data into training and validation. It then constructs the following two decision trees on training partition: The tree that you pass to the function. The tree constructed on all attributes as predictors and with no control parameters. -It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees. The first column corresponds to the cross-validation accuracy on the tree that you pass; the second is the cross-validation accuracy on the tree without any control and all attributes. The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting. We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired. The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant. Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration. Lets look at the cross_validate() function in action in the example below. We will pass the tree with formula as GRADE ~ SCORE+ON_SMARTPHONE+LEAVES_EARLY, and control parameter, with minsplit=100. And for cross_validate() function, we will usen_iter=5, and split_raitio=0.7 NOTE: Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use install.packages(&quot;devtools&quot;) devtools::install_github(&quot;devanshagr/CrossValidation&quot;) CrossValidation::cross_validate() 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 NOTE: If you encounter error while running the cross-validation function that said new levels encountered in test, make sure the dataset is imported again with read.csv() attribute stringsAsFactors as TRUE or T. For more information about the inner-working of the cross_validate() function visit cross_validate() We can see in the output the Training accuracy, the table of cross-validation accuracy at each iteration for both the passed tree and the tree on all attribute and also their averages and variances. Few Observation from the selected example above are: For the tree passed with selected attributes and some control parameters, the cross-validation accuracys (i.e. accuracy values in the accuracy_subset column) are fairly high for all iterations and have very low variance. They are close to the training accuracy which indicates we are not overfitting. Observe that the accuracy at each iteration of the accuracy_subset and accuracy_all column are relatively, close but not exact, suggesting that there are more attributes or other control parameters that can be included to the passed tree, to further increase the accuracy, thus closing the gap. Thus using cross-validation we were able to figure out with certainty, that the passed tree, is not the best tree that can be created using the training data. Also, we saw whether the generated tree overfits the training data or not. EOC "],["regression.html", "Chapter 7 Data Modelling and Prediction techniques for Regression. 7.1 Linear Regression. 7.2 Regression using RPART", " Chapter 7 Data Modelling and Prediction techniques for Regression. In chapter 6 we studied modeling and prediction techniques for Classification, where we predicted the class of the output variable, using Decision tree based algorithms. In this chapter we will study techniques for regression based prediction and create models for it. As opposite the the classification where we predict a particular class of the output variable, here we will predict a numerical/continuous value. We will look at two methods of performing this regression based modeling and prediction, first simple linear regression and second regression using decision tree. 7.1 Linear Regression. Linear regression is a linear approach to modeling the relationship between a scalar response (\\(Y\\)) and one or more explanatory variables(\\(X_i\\), where i is the number of explanatory variables). Scalar response means the predicted output. These variables are also known as dependent variables i.e. they are derived by applying some law/rule or function onto some other variable/s Usually in linear regression, models are used to predict only one scalar variable. But there are two subtype if these models: First when there is only one explanatory variable and one output variable. This type of linear regression model known as simple linear regression. Second, when there are multiple predictors, i.e. explanatory/dependent variables for the output variable. This type of linear regression model known as multiple linear regression. But in the case of prediction of multiple correlated output variables, we call this type of prediction using linear regression model as multivariate linear regression. Explanatory variables are the predictors on which the output predictions are based on. These variables are also known as independent variables and are independently sufficient to be used as predictors in regression models. Linear models fitted to various different type of data spread. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. Credits: Wikipedia. Since the relationship between the explanatory variables and the output variable is modeled linearly, these models are called as linear models. To do this, we need to find a linear regression equation for the set of input predictors and the output variable. But without going into the mathematics of finding this linear regression equation, we will use a tool/function provided in R to model and predict the output variable. 7.1.1 Linear regression using lm() function Syntax for building the regression model using the lm() function is as follows: lm(formula, data, ...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the linear regression model is to be trained. For more info on the lm() function visit lm() Lets look at the example on the RealEstate dataset. A snippet of the Realestate Dataset is given below. Table 7.1: Snippet of Real Estate Dataset Price Bedrooms Bathrooms Size 795000 3 3 2371 399000 4 3 2818 545000 4 3 3032 909000 4 4 3540 109900 3 1 1249 324900 3 3 1800 192900 4 2 1603 215000 3 2 1450 999000 4 3 3360 319000 3 2 1323 Now we can build a simple linear regression model to predict the Price attribute based on the various other attributes present in the dataset, as shown above. Since we will be predicting only one attribute values, this model will be called simple linear regression model. For the first example we will predict the price value of house using only size attribute as the predictor. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxucmVhbGVzdGF0ZTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvUmVhbEVzdGF0ZS5jc3ZcIilcblxuIyBzcGxpdGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhyZWFsZXN0YXRlKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC43LCAuMykpXG50cmFpbiA8LSByZWFsZXN0YXRlW2lkeCA9PSAxLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVtpZHggPT0gMixdXG5cbiMgVXNlIHRoZSBsbSgpIGZ1bmN0aW9uIHRvIHByZWRpY3QgdGhlIHByaWNlIGJhc2VkIG9uIHNpemUgb2YgdGhlIGhvdXNlLlxuIyBUaHVzIHRoaXMgaXMgYW4gZXhhbXBsZSBvZiBzaW1wbGUgbGluZWFyIHJlZ3Jlc3Npb24gc2luY2Ugb25seSBvbmUgcHJlZGljdG9yIGFuZCBvbmUgb3V0cHV0IHZhbHVlIGlzIHVzZWQuXG5zaW1wbGUuZml0IDwtIGxtKFByaWNlflNpemUsZGF0YT10cmFpbilcblxuIyBzdW1tYXJ5IG9mIHRoZSBtb2RlbFxuc3VtbWFyeShzaW1wbGUuZml0KVxuXG4jIExpbmVhciByZWxhdGlvbiBiZXR3ZWVuIHRoZSBQcmljZSBhbmQgU2l6ZSBhdHRyaWJ1dGUuXG5wbG90KHRyYWluJFNpemUsdHJhaW4kUHJpY2UpXG5hYmxpbmUoc2ltcGxlLmZpdCAsIGNvbD1cInJlZFwiKVxuXG4jIFByZWRpY3RpbmcgdmFsdWVzIG9uIHRoZSB0ZXN0IGRhdGFzZXQuXG5QcmVkaWN0ZWRQcmljZS5zaW1wbGUgPC0gcHJlZGljdChzaW1wbGUuZml0LHRlc3QpXG4jIFByZWRpY3RlZCBWYWx1ZXNcbmhlYWQoYXMuaW50ZWdlcih1bm5hbWUoUHJlZGljdGVkUHJpY2Uuc2ltcGxlKSkpXG4jIEFjdHVhbCBWYWx1ZXNcbmhlYWQodGVzdCRQcmljZSkifQ== We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The plot of Size vs Price, and the red line represents the fitted line or the linear model line which will be used for prediction. The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the actual values, the predicted values are some times close,some time far and few are very far. We saw above an example of simple linear regression model, where only one predictor was used for predicting a single output attribute. Now we will see an example of multiple linear regression model, where there can be multiple predictors to predict a single output attribute. (Note: Please do not confuse this with the multivariate linear regression.) Let look at an example of predicting the Price of the real estate, based on 3 attributes Size, Number of Bedrooms and Number of Bathrooms. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxucmVhbGVzdGF0ZTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvUmVhbEVzdGF0ZS5jc3ZcIilcblxuIyBzcGxpdGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhyZWFsZXN0YXRlKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC43LCAuMykpXG50cmFpbiA8LSByZWFsZXN0YXRlW2lkeCA9PSAxLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVtpZHggPT0gMixdXG5cblxuIyBVc2UgdGhlIGxtKCkgZnVuY3Rpb24gdG8gcHJlZGljdCB0aGUgcHJpY2UgYmFzZWQgb24gc2l6ZSwgYmF0aHJvb21zIGFuZCBiZWRyb29tcyBvZiB0aGUgaG91c2UuXG4jIFRodXMgdGhpcyBpcyBhbiBleGFtcGxlIG9mIG11bHRpcGxlIGxpbmVhciByZWdyZXNzaW9uIHNpbmNlIG11bHRpcGxlIHByZWRpY3RvciBhbmQgb25lIG91dHB1dCB2YWx1ZSBpcyB1c2VkLlxubXVsdGlwbGUuZml0IDwtIGxtKFByaWNlflNpemUgKyBCYXRocm9vbXMgKyBCZWRyb29tcyxkYXRhPXRyYWluKVxuXG4jIHN1bW1hcnkgb2YgdGhlIG1vZGVsXG5zdW1tYXJ5KG11bHRpcGxlLmZpdClcblxuIyBQcmVkaWN0aW5nIHZhbHVlcyBvbiB0aGUgdGVzdCBkYXRhc2V0LlxuUHJlZGljdGVkUHJpY2UubXVsdGlwbGUgPC0gcHJlZGljdChtdWx0aXBsZS5maXQsdGVzdClcbiMgUHJlZGljdGVkIFZhbHVlc1xuaGVhZChhcy5pbnRlZ2VyKHVubmFtZShQcmVkaWN0ZWRQcmljZS5tdWx0aXBsZSkpKVxuIyBBY3R1YWwgVmFsdWVzXG5oZWFkKHRlc3QkUHJpY2UpIn0= We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the previous model based on just the Size as predictor, here, when we used 3 predictors, we have more accurate predictions, thus increasing the overall accuracy of the model. 7.1.2 Calculating the Error using mse() As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test wont prove useful now in our numerical predictions scenario. Also we dont want to eyeball everytime we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique. To do this we will use the Mean Squared Error(MSE). The MSE is a measure of the quality of an predictor/estimator It is always non-negative Values closer to zero are better. The equation to calculate the MSE is as follows: \\[\\begin{equation} MSE=\\frac{1}{n} \\sum_{i=1}^{n}{(Y_i - \\hat{Y_i})^2} \\\\ \\text{where $n$ is the number of data points, $Y_i$ are the observed value}\\\\ \\text{and $\\hat{Y_i}$ are the predicted values} \\end{equation}\\] To implement this, we will use the mse() function present in the Metrics Package, so remember to install the Metrics package and use library(Metrics) in the code for local use. The syntax for mse() function is very simple: mse(actual,predicted) actual: vector of the actual values of the attribute we want to predict. predicted: vector of the predicted values obtained using our model. Now lets look at the MSE of the previous example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxucmVhbGVzdGF0ZTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvUmVhbEVzdGF0ZS5jc3ZcIilcblxuIyBzcGxpdGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhyZWFsZXN0YXRlKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC43LCAuMykpXG50cmFpbiA8LSByZWFsZXN0YXRlW2lkeCA9PSAxLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVtpZHggPT0gMixdXG5cbiMgVXNlIHRoZSBsbSgpIGZ1bmN0aW9uIHRvIHByZWRpY3QgdGhlIHByaWNlIGJhc2VkIG9uIHNpemUgb2YgdGhlIGhvdXNlLlxuc2ltcGxlLmZpdCA8LSBsbShQcmljZX5TaXplLGRhdGE9dHJhaW4pXG4jIFByZWRpY3RpbmcgdmFsdWVzIG9uIHRoZSB0ZXN0IGRhdGFzZXQuXG5QcmVkaWN0ZWRQcmljZS5zaW1wbGUgPC0gcHJlZGljdChzaW1wbGUuZml0LHRlc3QpXG4jIFByZWRpY3RlZCBWYWx1ZXNcbmhlYWQoYXMuaW50ZWdlcih1bm5hbWUoUHJlZGljdGVkUHJpY2Uuc2ltcGxlKSkpXG4jIEFjdHVhbCBWYWx1ZXNcbmhlYWQodGVzdCRQcmljZSlcblxuIyBMZXRzIHVzZSB0aGUgbXNlKCkgZnVuY3Rpb24gdG8gXG5tc2UodGVzdCRQcmljZSxQcmVkaWN0ZWRQcmljZS5zaW1wbGUpIn0= We can see the MSE is too large above 200 billion, and this is huge value could be understandable as we are taking the squared differences of all the records that we predicted. The main intention is to get this huge value to as low as possible possibly near zero, which could be difficult but can be achieved upto a relative error by using a better model and training data. 7.2 Regression using RPART Since we have already used the rpart library for performing decision tree algorithms also referred as CART(classification and regression tree) algorithms, we will now look at this type algorithm for regression based prediction. Remember we have discussed the usage of Rpart in the section 6.2 in great detail. Thus for using Rpart for regression based prediction we will need to provide the rpart() functions, method attribute, with the keyword anova. For more details on the use of Rpart for prediction please refer to section 6.2. Lets look at an example of regression based prediction using Rpart for the Price attribute of the Real estate Dataset with Size, Number of Bedrooms and Number of Bathrooms as predictors. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubGlicmFyeShNb2RlbE1ldHJpY3MpXG5cbiMgTG9hZCB0aGUgZGF0YXNldC5cbnJlYWxlc3RhdGU8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1JlYWxFc3RhdGUuY3N2XCIpXG5cbiMgc3BsaXRpbmcgdGhlIGRhdGFzZXQgaW50byB0cmFpbmluZyBhbmQgdGVzdGluZy5cbmlkeCA8LSBzYW1wbGUoIDE6Miwgc2l6ZSA9IG5yb3cocmVhbGVzdGF0ZSksIHJlcGxhY2UgPSBUUlVFLCBwcm9iID0gYyguNywgLjMpKVxudHJhaW4gPC0gcmVhbGVzdGF0ZVtpZHggPT0gMSxdXG50ZXN0IDwtIHJlYWxlc3RhdGVbaWR4ID09IDIsXVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbiB0byBwcmVkaWN0IHRoZSBwcmljZSBiYXNlZCBvbiB0aGUgc2l6ZSwgYmF0aHJvb21zIGFuZCBiZWRyb29tcyBvZiB0aGUgaG91c2UuXG5ycGFydC5maXQgPC0gcnBhcnQoUHJpY2V+U2l6ZStCYXRocm9vbXMrQmVkcm9vbXMsZGF0YT10cmFpbixtZXRob2QgPSBcImFub3ZhXCIpXG5cbiMgUHJlZGljdGluZyB2YWx1ZXMgb24gdGhlIHRlc3QgZGF0YXNldC5cblByZWRpY3RlZFByaWNlLnJwYXJ0IDwtIHByZWRpY3QocnBhcnQuZml0LHRlc3QpXG4jIFByZWRpY3RlZCBWYWx1ZXNcbmhlYWQoYXMuaW50ZWdlcih1bm5hbWUoUHJlZGljdGVkUHJpY2UucnBhcnQpKSlcbiMgQWN0dWFsIFZhbHVlc1xuaGVhZCh0ZXN0JFByaWNlKVxuXG4jIExldHMgdXNlIHRoZSBtc2UoKSBmdW5jdGlvbiB0byBcbm1zZSh0ZXN0JFByaWNlLFByZWRpY3RlZFByaWNlLnJwYXJ0KSJ9 Output tree plot of rpart() model using for regression using anova method We can see, The output decision tree of the rpart() function The predicted values obtained using the model created by the rpart() function. The MSE of the model on the testing dataset. An important point to note while using decision trees for regression purpose, is that since the underlying process of modeling is still a decision tree, the output still represent a set of distinct classes, even though the values of the classes are numeric. Thus we can see that the predicted values are repeated even for varying inputs. Hence Decision tree must be used carefully when used for regression based prediction models. EOC "],["models.html", "Chapter 8 Additional Modeling techniques. 8.1 Four Line Method for creating most type of prediction models in R 8.2 Random Forest 8.3 SVM 8.4 Neural Network.", " Chapter 8 Additional Modeling techniques. In this chapter, we will see some additional machine learning models used in practice, for various purpose. After studying both classification models and regression models in the previous 2 chapters 6 &amp; 7 respectively, we will now look into other generic models used for classification and/or regression purpose. Below is the list some of the widely used algorithms with their use case(either classification/regression or both) and training and prediction complexities for using particular learning models. Usage and Complexity of various machine learning algorithms. Credits:thekerneltrip.com As we can see that many of these algorithms can be used for classification and regression all together, as we saw in the case of the Decision tree models using Rpart in section 6.1, and also some model used for only a particular type of prediction e.g. linear regression. We will look at few algorithms from the above list: Random Forest Support Vector Machine (SVM) Neural Networks 8.1 Four Line Method for creating most type of prediction models in R But before we learn about these algorithms, let us see a four line method to build models using any of the above algorithms using R. We can safely assume that the data going to be used to build the model, has been pre-processed and based on requirements split into the required subsets. To see how to split the data refer to section 6.6. The zeroth step now will be obviously to install and load the packages that contain the ML algorithm. To do that on your local machine, use the following code. # Install the library install.packages(&quot;package name&quot;) # Load the library in R library(package_name) Once we have the algorithm library loaded, we then proceed to build the model. pred.model = model_function(formula, data, method*,...) model_function(): the function present in the library to build the model. e.g. rpart() formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the ML model is to be trained on. Remember never used the test data to build the model. method: (OPTIONAL) Used to denote the method of prediction or underlying algorithm. This parameter could be present in some model_function() but not all. Prediction using the predict() function on the training data to assess the models performance/accuracy in next step. pred = predict(pred.model, newdata = train) predict(): the common function for all models used for prediction. pred.model: output of the step 1. newdata: here we assign the data on which the prediction is to be done. Evaluate error in Training phase. We use the mse() function for finding the accuracy of the model. To read more in dept about the mse() function refer to section 7.1.2. mse(actual, pred) actual: vector of the actual values of the attribute we want to predict. pred: vector of the predicted values obtained using our model. Repeat steps 0,1,2 and 3 by changing the ML algorithm or manipulating dataset to perform better when used to train using ML model, so as to achieve as low MSE value as possible. Finally we predict on the testing data using the same predict function as in step 2 but replacing the train data with test data. pred = predict(pred.model, newdata = test) These are the 4 steps to follow while performing any prediction task using ML models in R. We can also add one more step between step 3 and 4, which is step of performing the cross validation process on the newly built models. This can be done either manually, or by using third party libraries. One such library is the rModeling package, which has function crossValidation() which can be used for any type of model_functions(). For more information visit crossValidation() Before we proceed to the next section, please look at the snippet of the earnings.csv dataset, which we will be using for predicting the Earnings attribute based on various other attributes provided in the dataset, using different prediction models. Table 8.1: Snippet of Earnings Dataset GPA Number_Of_Professional_Connections Earnings Major Graduation_Year Height Number_Of_Credits Number_Of_Parking_Tickets 2221 2.72 36 10282.54 Humanities 1963 69.51 122 3 5198 1.62 45 13152.07 Vocational 1962 67.90 120 1 2767 2.28 6 10236.24 Humanities 1978 66.02 120 0 8799 2.60 6 9740.00 Buisness 2001 66.26 121 1 9826 2.43 5 5022.88 Other 1977 69.98 124 1 4207 3.19 8 13313.53 Vocational 1991 69.70 123 0 1382 2.40 26 9757.62 STEM 1968 65.45 123 1 6815 3.18 27 11698.26 Professional 1987 63.98 121 2 604 2.49 54 9735.08 STEM 1962 67.51 122 0 1486 2.95 10 9706.23 STEM 1981 68.94 122 0 Now that we saw the general structure of the model and took a glace at the dataset we will be using, lets look at few of the algorithms as we promised from the list above. 8.2 Random Forest Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the value that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees. In simpler terms, the random forest algorithm creates multiple decision trees based on varying attributes and biases, and then predicts the output for each tree, and aggregates this prediction into one final output by some technique like majority count or average/etc. Visual Representation of a Random Forest learning model. Credits: Random Forest Wikipedia The main idea behind Random Forest arise from a method called ensemble learning method. Ensemble learning is the method of solving a problem by building multiple ML models and combining them. It is primarily used to improve the performance of classification, prediction, and function approximation models. Forests are type of ensemble learning methods, where they act like, pulling together all of decision tree algorithm efforts. Taking the teamwork of many trees thus improving the performance of a single random decision tree. Random decision forests correct for decision trees habit of overfitting to their training set. Now lets look at an example of prediction by the random forest model using the randomForest() function present in the randomForest library package. For more information about the randomForest() function and its attributes visit randomforest() Thus following the 4 step method of prediction for predicting a numerical attribute Earnings using the randomForest() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJhbmRvbUZvcmVzdClcbmxpYnJhcnkoTW9kZWxNZXRyaWNzKVxuXG4jIExvYWQgdGhlIGRhdGFzZXQuXG5lYXJuaW5nZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvZWFybmluZ3MuY3N2XCIsc3RyaW5nc0FzRmFjdG9ycyA9IFQpXG5cbiMgc3BsaXR0aW5nIHRoZSBkYXRhc2V0IGludG8gdHJhaW5pbmcgYW5kIHRlc3RpbmcuXG5pZHggPC0gc2FtcGxlKCAxOjIsIHNpemUgPSBucm93KGVhcm5pbmdkYXRhKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC44LCAuMikpXG50cmFpbiA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMSxdXG50ZXN0IDwtIGVhcm5pbmdkYXRhW2lkeCA9PSAyLF1cblxuIyAxLiBCdWlsZCBwcmVkaWN0aW9uIG1vZGVsIHVzaW5nIHJhbmRvbUZvcmVzdCgpIGZ1bmN0aW9uLlxucHJlZC5tb2RlbCA8LSByYW5kb21Gb3Jlc3QoRWFybmluZ3Mgfi4sIGRhdGEgPSB0cmFpbilcblxuIyBMZXRzIHNlZSB0aGUgc3VtbWFyeSBvZiB0aGUgcmFuZG9tRm9yZXN0IG1vZGVsLlxucHJlZC5tb2RlbFxuXG4jIDIuIFByZWRpY3QgdXNpbmcgdGhlIG5ld2x5IGJ1aWx0IG1vZGVsIG9uIHRoZSB0cmFpbmluZyBkYXRhc2V0LlxucHJlZC50cmFpbiA8LSBwcmVkaWN0KHByZWQubW9kZWwsbmV3ZGF0YSA9IHRyYWluKVxuXG4jIDMuIEV2YWx1YXRlIGVycm9yIG9uIHRyYWluaW5nIHVzaW5nIHRoZSBtc2UoKSBmdW5jdGlvbi5cbm1zZSh0cmFpbiRFYXJuaW5ncyxwcmVkLnRyYWluKVxuXG4jIDQuIFByZWRpY3Qgb24gdGhlIHRlc3RpbmcgZGF0YS5cbnByZWQudGVzdCA8LSBwcmVkaWN0KHByZWQubW9kZWwsbmV3ZGF0YSA9IHRlc3QpXG5cbiMgQWRkaXRpb25hbGx5IHNpbmNlIGhlcmUgd2UgaGF2ZSB0aGUgYWN0dWFsL3JlYWwgcHJlZGljdGlvbiB2YWx1ZXMgd2UgY2FuIGFsc28gY2hlY2sgdGhlIGFjY3VyYWN5IG9mIG91ciBwcmVkaWN0aW9uIG9uIHRlc3RpbmcgZGF0YS5cbm1zZSh0ZXN0JEVhcm5pbmdzLHByZWQudGVzdCkifQ== We can see, the summary of the output randomForest model with details of: Formula used. Type of random forest Number of trees created in the forest Number of variables used at each split. And some performance parameter. The mean squared error of the predicted values using training sub-dataset. The mean squared error of the predicted values using the testing sub-dataset. Note: Since a random forest is an ensemble learning method, it will usually take a lot more time to train that its counterparts. Thus you can see a significant waiting/execution time while running the above code and acquiring answer. 8.3 SVM Support-Vector Machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification(mostly) and regression(also works in some cases) analysis. The goal of the SVM is to find a hyperplane in an N-dimensional space (where N corresponds with the number of features) that distinctly classifies/regresses the data points. The accuracy of the results directly correlates with the hyperplane that we choose. We should find a plane that has the maximum distance between data points of both classes. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier. Support Vector Machine Linear classification hyperplane(line) example. Credits: Support Vector Machines Wikipedia Note that the dimension of the hyperplane depends on the number of features. If the number of input features is two, then the hyperplane is just a line. If the number of input features is three, then the hyperplane becomes a two-dimensional plane. It becomes difficult to draw on a graph a model when the number of features exceeds three. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Example of the kernal trick for non-linear classifier. Credits: Support Vector Machines Wikipedia Why is this called a support vector machine? Support vectors are data points closest to the hyperplane. They directly influence the position and orientation of the hyperplane and minimizes the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These points thus help to build our SVM model. SVM is great because it gives quite accurate results with minimum computation power. Lets look at the example of Support vector machine algorithm in use for predicting the Earnings attribute of the Earnings dataset. We will use the svm() function from the e1071 package. For more information about this function and its attributes visit svm() Thus following the 4 step model for prediction and using the svm() function as the model function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGUxMDcxKVxubGlicmFyeShNb2RlbE1ldHJpY3MpXG5cbiMgTG9hZCB0aGUgZGF0YXNldC5cbmVhcm5pbmdkYXRhPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9lYXJuaW5ncy5jc3ZcIixzdHJpbmdzQXNGYWN0b3JzID0gVClcblxuXG4jIHNwbGl0dGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhlYXJuaW5nZGF0YSksIHJlcGxhY2UgPSBUUlVFLCBwcm9iID0gYyguOCwgLjIpKVxudHJhaW4gPC0gZWFybmluZ2RhdGFbaWR4ID09IDEsXVxudGVzdCA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMixdXG5cbiMgMS4gQnVpbGQgcHJlZGljdGlvbiBtb2RlbCB1c2luZyBzdm0oKSBmdW5jdGlvbi5cbnByZWQubW9kZWwgPC0gc3ZtKEVhcm5pbmdzIH4uLCBkYXRhID0gdHJhaW4pXG5cbiMgTGV0cyBzZWUgdGhlIHN1bW1hcnkgb2YgdGhlIHN2bSBtb2RlbC5cbnByZWQubW9kZWxcblxuIyAyLiBQcmVkaWN0IHVzaW5nIHRoZSBuZXdseSBidWlsdCBtb2RlbCBvbiB0aGUgdHJhaW5pbmcgZGF0YXNldC5cbnByZWQudHJhaW4gPC0gcHJlZGljdChwcmVkLm1vZGVsLG5ld2RhdGEgPSB0cmFpbilcblxuIyAzLiBFdmFsdWF0ZSBlcnJvciBvbiB0cmFpbmluZyB1c2luZyB0aGUgbXNlKCkgZnVuY3Rpb24uXG5tc2UodHJhaW4kRWFybmluZ3MscHJlZC50cmFpbilcblxuIyA0LiBQcmVkaWN0IG9uIHRoZSB0ZXN0aW5nIGRhdGEuXG5wcmVkLnRlc3QgPC0gcHJlZGljdChwcmVkLm1vZGVsLG5ld2RhdGEgPSB0ZXN0KVxuXG4jIEFkZGl0aW9uYWxseSBzaW5jZSBoZXJlIHdlIGhhdmUgdGhlIGFjdHVhbC9yZWFsIHByZWRpY3Rpb24gdmFsdWVzIHdlIGNhbiBhbHNvIGNoZWNrIHRoZSBhY2N1cmFjeSBvZiBvdXIgcHJlZGljdGlvbiBvbiB0ZXN0aW5nIGRhdGEuXG5tc2UodGVzdCRFYXJuaW5ncyxwcmVkLnRlc3QpIn0= We can see, the summary of the output svm model with details of: Formula used. Type of SVM model The SVM Kernel Used And some performance parameter. Also, the Number of Support Vectors. The mean squared error of the predicted values using training sub-dataset. The mean squared error of the predicted values using the testing sub-dataset. 8.4 Neural Network. An Artificial Neural Network (ANN), usually simply called neural network(NN) is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. Thus in other words, a neural network is a sequence of neurons connected by synapses, which reminds of the structure of the human brain. However, the human brain is even more complex, and a NN is just a model that mimics a human brain. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The signal at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times. A visual representation of typical neural network with various nodes and edges along with layers. Credits: Artificial Neural Network Wikipedia What is great about neural networks is that they can be used for basically any task from spam filtering to computer vision. However, they are normally applied for machine translation, anomaly detection and risk management, speech recognition and language generation, face recognition, and more. To accommodate such a wide variety of application, neural nets are transformed and models in various different ways. To find multiple types of neural networks please visit Neural Network Zoo Now lets try to implement a neural network learning model for the Earnings prediction problem of Earnings dataset. To do this we will use the 4 step method of prediction and use the nnet() function from the nnet package as the model_function. Let look at the nnet() function and its parameters. nnet(formula,data,size,linout,...) formula and data are the same as mentions in Step 1 of section 8.1. size: denotes the number of units in the hidden layer. linout: Assign TRUE is predicting numerical value. Default is FALSE, for predicting categorical value. rang: set the initial random weights on each edge. maxit: maximum number of iterations. decay: weight decay parameter. Can also be understood as learning rate. Note: The nnet() function can only create a single hidden layer neural network model. To create more complex models please use different packages like neuralnet or h2o, deepnet, etc Now lets use the nnet() function for predction. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KG5uZXQpXG5saWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxuZWFybmluZ2RhdGE8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L2Vhcm5pbmdzLmNzdlwiLHN0cmluZ3NBc0ZhY3RvcnMgPSBUKVxuXG4jIHNwbGl0dGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhlYXJuaW5nZGF0YSksIHJlcGxhY2UgPSBUUlVFLCBwcm9iID0gYyguOCwgLjIpKVxudHJhaW4gPC0gZWFybmluZ2RhdGFbaWR4ID09IDEsXVxudGVzdCA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMixdXG5cbiMgMS4gQnVpbGQgcHJlZGljdGlvbiBtb2RlbCB1c2luZyBubmV0KCkgZnVuY3Rpb24uXG5wcmVkLm1vZGVsIDwtIG5uZXQoRWFybmluZ3MvMjAwMDAgfi4sIGRhdGEgPSB0cmFpbiwgc2l6ZSA9IDUwLCBkZWNheT01ZS01LG1heGl0ID0gNTAwLGxpbm91dCA9IFQpXG5cbiMgTGV0cyBzZWUgdGhlIHN1bW1hcnkgb2YgdGhlIG5uZXQgbW9kZWwuXG5wcmVkLm1vZGVsXG5cbiMgMi4gUHJlZGljdCB1c2luZyB0aGUgbmV3bHkgYnVpbHQgbW9kZWwgb24gdGhlIHRyYWluaW5nIGRhdGFzZXQuXG5wcmVkLnRyYWluIDwtIHByZWRpY3QocHJlZC5tb2RlbCxuZXdkYXRhID0gdHJhaW4pKjIwMDAwXG5cbiMgMy4gRXZhbHVhdGUgZXJyb3Igb24gdHJhaW5pbmcgdXNpbmcgdGhlIG1zZSgpIGZ1bmN0aW9uLlxubXNlKHRyYWluJEVhcm5pbmdzLHByZWQudHJhaW4pXG5cbiMgNC4gUHJlZGljdCBvbiB0aGUgdGVzdGluZyBkYXRhLlxucHJlZC50ZXN0IDwtIHByZWRpY3QocHJlZC5tb2RlbCxuZXdkYXRhID0gdGVzdCkqMjAwMDBcblxuIyBBZGRpdGlvbmFsbHkgc2luY2UgaGVyZSB3ZSBoYXZlIHRoZSBhY3R1YWwvcmVhbCBwcmVkaWN0aW9uIHZhbHVlcyB3ZSBjYW4gYWxzbyBjaGVjayB0aGUgYWNjdXJhY3kgb2Ygb3VyIHByZWRpY3Rpb24gb24gdGVzdGluZyBkYXRhLlxubXNlKHRlc3QkRWFybmluZ3MscHJlZC50ZXN0KSJ9 NOTE: If you see a very high value of MSE after running the above code, please re-run it. Usually you will find the MSE to be better than all the models we have studied uptil now for the Earnings prediction problem. We can see from the output, the summary of the output, neural network model, with details of: Number of weights in the complete neural network Initial Value and Final Value of the model weights along with iter value. Structure of the neural network in I-H-O format where the numbers, I is input, H is hidden and O is output nodes. Input node attributes. Note that the Majors column attribute are split into unique number of factors, thus creating new individual attributes. Output node attributes. Network Options. The mean squared error of the predicted values using training sub-dataset. The mean squared error of the predicted values using the testing sub-dataset. After Comparing all these models, we can see that the MSE values for the 3 models are SVM &gt; Random Forest &gt; Neural Network. This suggest one trend that, to get as best result as possible, one must invest most time in choosing the right model,and use the model with cleaned dataset for training. Eventually, since we use R language here, the code for model creation just boils down to few lines of code, 4 steps to be more accurate. But since we might find one model works better than other, we must choose the best fit model. Also, we saw the trend of time required for training of the models studied above was SVM &gt; RandomForest &gt; Neural Net. This also suggest a proportional relationship with the time required for a particular model to train and in turn producing the best possible results. Although one can say that, we can just use Neural Networks all the time, well this statement is true to some extent, but depending on the resources, the complexity of the data, and the complexity of the model itself, one needs to make some trade-offs. One should not try to throw a ball just few meters with a cannon, using mere hands will do the job. In essence, do not try to overuse the neural net model for the sake of adding 2 mumbers, simple addition will suffice. Studying these tradeoffs and more models in depth though is out of this course scope. EOC "],["predblogs.html", "Chapter 9 Blog: Prediction Challenges 9.1 General Structure of the Prediction Challenges. 9.2 Prediction Challange 1. 9.3 Prediction Challenge 2. 9.4 Prediction Challenge 3. 9.5 Prediction Challenge 4.", " Chapter 9 Blog: Prediction Challenges Until we have studied multiple methods of data analysis in sections 2.2,5, statistical testing in sections 3, &amp; building prediction models for both classification 6 and regression 7 along with advanced ML models 8. Now its time to utilize them in various ways for analysis and prediction of data. To do this, in this course, we have designed few prediction challenges, which test your ability to implement skills learnt in the course until now. First challenge is a basic prediction challenge using only data analysis using the freestyle techniques from section 2.2. Then onwards, prediction challenges used multitude of modeling techniques which were studied in 6 and 7. 9.1 General Structure of the Prediction Challenges. Usually there is a task to be performed in each prediction challenge. Either predicting a numerical of categorical values is the task of each challenge. The way to perform those task are constrained differently for different prediction challenges based on levels of difficulty and ML models to be used. The submission will take place on Kaggle which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submission. The datasets provided for each prediction challenge is as follows: Training Dataset. It is used for training and cross-validation purpose in the prediction challenge. This data has all the training attributes along and the ideal values of the prediction attribute. Models for prediction are to be trained using this dataset only. Testing Dataset. It is used for prediction only. It consists of all the attributes that were used for training, but it does not contain any values of the actual prediction attributes, which is actually the attribute that the prediction challenge predicts. Since its only used for prediction purpose and is not involved in training of the models, it is thus not involved in the cross-validation phase too. Submission Dataset. After prediction using the testing dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g. ID or name,etc) and second the predicted attribute column. Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle. To read the datasets use the read.csv() function and for writing the dataset to the file, use the write.csv() function. Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle. To avoid this, you can add the parameter, row.names = F in the write.csv() function. e.g. write.csv(*dataframe*,*fileaddress*,row.names = F). Now lets look at the prediction challenges that took place in this course along with the top submissions by students. 9.2 Prediction Challange 1. In Prediction challange 1, the task was to predict a categorical value using only free-style prediction. For this prediction challenge we used our favorite dataset, the Moody dataset, and predicted the Grade category of all students. The Grade category had only 2 factors: Pass OR Fail. Let look at a snippet of the moody dataset used for training in this challenge. Table 9.1: Snippet of Moody Dataset(TRAINING) for Prediction Challenge 1 Studentid Attendance Major Questions Score Seniority Texting Grade 29998 40 Stat Rarely 65 Freshman Always Pass 29999 30 Cs Always 46 Senior Rarely Fail 30000 90 Communication Rarely 60 Senior Always Pass 30001 5 Polsci Always 46 Senior Rarely Pass 30002 67 Cs Rarely 36 Senior Always Fail 30003 20 Stat Rarely 50 Senior Rarely Pass 30004 78 Stat Rarely 0 Senior Always Pass 30005 27 Polsci Rarely 45 Junior Always Fail 30006 81 Polsci Rarely 6 Sophomore Always Fail 30007 50 Communication Rarely 97 Senior Always Pass We can see that there are multiple attributes like Score, Attendence, Major, etc. that can be used as predictors, and then there is Grade attribute with ideal values for each record of student which will be used while training and then will be predicted on the testing dataset. 9.2.1 How the data was generated for Challenge 1 Professor Moody data set has been synthetically generated using random generator which follows probabilistic rules implementing secret patterns which we embedded in the data. These patterns are presented below in the form of decision tree. For example a rule that statistics major with score over 60, pass the class - reflects the generated data in which high percentage (but not 100%) of such students indeed pass Moodys class. There will always be random exceptions to these rules. But majority of stat students with score above 60 will pass the class The data is based on a tree given by the following conditions: Tree which is embedded in the data (secret pattern for Moody -challenge1/2) Major Stat Score &gt; 60 Pass Score &lt;= 60 Fail Comm Score &gt;40 Pass Score &lt;=40 Texting = Rarely Fail Texting = Always PAss Polsci Score &gt;50 Pass Score &lt;=50 Questions = rare Fail Questions = always Pass Cs Score &gt;70 Pass Score &lt;=70 Seniority= Freshman Score &gt;50 Pass Score &lt;=50 Attendance &gt;=60 Score &gt; 40 Pass Score &lt;=40 Fail Attendance &lt;60 Fail Seniority= Sophomore Score &gt;50 Pass Score &lt;=50 Fail Seniority= Junior Fail Seniority = Senior Fail We can see this in pictorial representation below based on each subset of Majors. For Stats Major: We can see that the rule was very simple, with the final grade decided based on only the Score attribute of the students. Thus finding this pattern would have been easier for students For Communication Major Students: The grade prediction for students from the Communication Major was based not only on the Score attribute but was also based on Texting attribute of the students records. As we can see, finding this pattern would have been not that difficult. For Political Science Major Students: The grade prediction for students from the Political Science Major was based not only on the Score attribute but was also based on Questions attribute of the students records. As we can see, finding this pattern would have been not that difficult. For Computer Science Major Students: The grade prediction for students from the Computer Science Major was the most involved and was based on various students attribute. Attributes like Score, Seniority and Attendance were involved in prediction, and the subsetting conditions were very complex. As we can see, finding this huge pattern would have been very difficult for students. If you want to see a well detailed data analysis of the dataset based on Majors as subset, then please look at Rohit Manjunaths submission in the Top Submission section for prediction challenge 1. How the data was generated using R You can see a simple way to write the data using the above patterns. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgRGF0YVxuZGF0PC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NMjAyMXRlc3Qtc3R1ZGVudHMuY3N2XCIsc3RyaW5nc0FzRmFjdG9ycyA9IFQpXG5cbiMgQ3JlYXRlIGFuIGFsbCBGYWlsIENhdGVnb3J5L1ByZWRpY3RlZCBWYWx1ZSBWZWN0b3IgYW5kIGFwcGVuZCBpdCB0byB0aGUgdGVzdGluZyBkYXRhc2V0LlxuZGF0JEdyYWRlPC1yZXAoJ0ZhaWwnLCBucm93KGRhdCkpXG5cbiMgVGhlIHZhcmlvdXMgY29uZGl0aW9ucyB1c2VkIHRvIHByZWRpY3QgZ3JhZGUgYXR0cmlidXRlLlxuXG4jIEZvciBNYWpvciA9IFN0YXRcbmRhdFtkYXQkTWFqb3I9PSdTdGF0JyAmIGRhdCRTY29yZT42MCxdJEdyYWRlIDwtJ1Bhc3MnXG5cbiMgRm9yIE1ham9yID0gQ29tbVxuZGF0W2RhdCRNYWpvcj09J0NvbW11bmljYXRpb24nICYgZGF0JFNjb3JlPjQwLF0kR3JhZGUgPC0nUGFzcydcbmRhdFtkYXQkTWFqb3I9PSdDb21tdW5pY2F0aW9uJyAmIGRhdCRTY29yZTw9NDAgJiBkYXQkVGV4dGluZz09J0Fsd2F5cycsXSRHcmFkZSA8LSdQYXNzJ1xuXG4jIEZvciBNYWpvciA9IFBvbHNjaVxuZGF0W2RhdCRNYWpvcj09J1BvbHNjaScgJiBkYXQkU2NvcmU+NTAsXSRHcmFkZSA8LSdQYXNzJ1xuZGF0W2RhdCRNYWpvcj09J1BvbHNjaScgJiBkYXQkU2NvcmU8PTUwICYgZGF0JFF1ZXN0aW9ucz09J0Fsd2F5cycsXSRHcmFkZSA8LSdQYXNzJ1xuXG4jIEZvciBNYWpvciA9IENzXG5kYXRbZGF0JE1ham9yPT0nQ3MnICYgZGF0JFNjb3JlPjcwLF0kR3JhZGUgPC0nUGFzcydcbmRhdFtkYXQkTWFqb3I9PSdDcycgJiBkYXQkU2NvcmU8PTcwICYgZGF0JFNlbmlvcml0eT09J0ZyZXNobWFuJyAmIGRhdCRTY29yZT41MCxdJEdyYWRlIDwtJ1Bhc3MnXG5kYXRbZGF0JE1ham9yPT0nQ3MnICYgZGF0JFNjb3JlPD03MCAmIGRhdCRTZW5pb3JpdHk9PSdGcmVzaG1hbicgJiBkYXQkQXR0ZW5kYW5jZSA+PTYwICYgZGF0JFNjb3JlPjQwLF0kR3JhZGUgPC0nUGFzcydcbmRhdFtkYXQkTWFqb3I9PSdDcycgJiBkYXQkU2NvcmU8PTcwICYgZGF0JFNlbmlvcml0eT09J1NvcGhvbW9yZScgJiBkYXQkU2NvcmU+NTAsXSRHcmFkZSA8LSdQYXNzJ1xuXG5cbiMgQ29tcGFyZSBpdCB3aXRoIHRoZSBpZGVhbCBwcmVkaWN0aW9ucyBmb3IgY2hlY2tpbmcgYWNjdXJhY3kgb2Ygb3VyIHByZWRpY3Rpb25zLlxuYW5zd2VyczwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTTIwMjF0ZXN0X2Fuc3dlci5jc3ZcIixzdHJpbmdzQXNGYWN0b3JzID0gVClcbm1lYW4oZGF0JEdyYWRlPT1hbnN3ZXJzJEdyYWRlKSAjIEFjY3VyYWN5In0= Thus we can see, using the ideal patterns the students would have expected to score around 83% accuracy. 9.2.2 Top Submissions for Challenge 1. Students with accuracy over 60% were considered passed for this prediction challenge. Jeremy Prasad Jeremys PPT Jeremy performed exceptionally well in this prediction challenge. His approach was a iterative learning process, where at each step after performing analysis he tried to decrease the error more and more. He started with a very basic model, of using just the score attribute with a hard threshold for pass or fail grade based on the score value. After this, to increase accuracy, he analysed the data more found which attributes effect the prediction of the data, and which are not really useful After finding these highly effective attributes, he wrote concrete set of attributs that can be used to assign the grade. Most of them were dependent on 2-3 attributes like Major-Senioriy-Score, Major-Score, or Major-Questions-Score,etc. This gave him a much better accuracy value for prediction. Rohit Manjunath Rohits PPT Rohit performed well in this prediction challenge, and has a different approach than that of Jeremys. In Rohits approach, instead of finding the minimum global threshold of pass or fail based on score, he found the threshold for the maximum score, above which every student passed the class. He then analysed the data based on the Majors first and then found interval threshold for each Majors scores. For some Majors, to increase accuracy, he further explored other attributes in detail to find which effects the final grade. Rohit obtained accuracy of almost 85%. 9.3 Prediction Challenge 2. In Prediction Challenge 2, we introduced the use of Decision Tree algorithm for prediction model building, to complete the same task as we saw in the Prediction Challenge 1. This was intended to see the first learning model in action, and also to see the ease in which the process of prediction can be completed using such prediction model against the trivial data analysis techniques. The datasets for this prediction challenge were the same as those in the prediction challenge 1. Since, the task in the prediction challenge was to predict a categorical value(Grade value) the learning algorithm allowed to be used in this task was the Decision Tree algorithm based on the CART model. Read more about how to use decision trees in section 6.1 . To implement this algorithm, students were allowed to use the RPART package 6.2 With rpart() doing most work of prediction in this task, the students were also asked to provide validation for their models prediction power/accuracy. This involved use of cross-validation techinques, which for the ease of this course level was provided in a custom function, see 6.7. 9.3.1 How the data was generated to Challenge 2 As we saw that the prediction task and the datasets in challenge 2 are similar to that of the challenge 1. Thus the data analysis of the challenge 1 would applicable in this case too. 9.3.2 Top Submissions for Challenge 2 Since rpart() is a very powerful function to find patterns with higher accuracy, the passing criteria for this challenge was above 80% accuracy score. Kevin Larkin Kevins PPT This was the top submission in terms of accuracy score on Kaggle. Kevin used the rpart() function, for modeling, with all the attributes of the training dataset except Studentid. To increase the accuracy of his model, he used the rpart.control() function parameters, especially the cp parameter of the function, which increased the splitting accuracy. Kevin acheived an accuracy score of over 86% on the test dataset for this challenge. Michael Ryvin Michaels PPT This was the second best submission as per accuracy score on Kaggle. Michael used the rpart() function, along with some control parameters for creating the decision tree. Michael achieved an accuracy score of over 86% on the test dataset. Shuohao Ping Shuohaos PPT This was the third best submission as per accuracy score on Kaggle. Shuohao used multiple iterations to create his final model. In each iteration, Shuohao tried to vary the control parameters and its values to find the best fit model after cross-validation. Shuohao, acheived an accuracy score of over 86% on the test dataset. 9.4 Prediction Challenge 3. After studying prediction of categorical data in the previous 2 prediction challenges, in prediction challenge 3, the task was to predict Earnings a numerical variable, using any ML algorithm. Earnings variable is part of the Earnings dataset which has details about a persons connections, GPA, Major,etc, and using these attributes, the students had to predict the numerical value of earnings of each person in the dataset. Students were recommended to first find some correlation between data by using free-style analysis, and then proceed to using ML models. This was included so as to show the effect of human intervention/input on the selection and performance of ML model, and also to avoid the trap of blindly applying the most costly ML model which might perform well, but is a overkill to perform task which could be completed using other less costly models. ( Cost here refers to the computation resources and time involved in training the models. ) To read more about prediction of a numerical variable in R, see section 7 and 8 Lets look at a snippet of the Earnings dataset used for training the models below. Table 9.2: Snippet of Earnings Dataset(TRAINING) for Prediction Challenge 3 GPA Number_Of_Professional_Connections Earnings Major Graduation_Year Height Number_Of_Credits Number_Of_Parking_Tickets 2.50 1 9756.15 STEM 2001 64.22 124 1 2.98 1 9709.03 STEM 2001 69.55 120 0 2.98 23 9711.37 STEM 1996 68.98 120 1 3.35 5 9656.15 STEM 2008 69.23 124 1 2.47 37 9751.92 STEM 1981 70.45 123 0 2.75 2 9728.30 STEM 2000 65.26 121 0 1.66 17 9847.59 STEM 2001 65.91 121 0 2.59 10 9743.36 STEM 1990 66.35 123 0 1.89 7 9793.38 STEM 1975 70.42 121 1 1.89 22 9810.38 STEM 1997 65.18 122 0 We can see that there are multiple attributes like GPA,Major,Graduation_Year,Height,etc. that can be used as predictors, and then there is Earnings attribute with ideal values for each record of student which will be used while training and then will be predicted on the testing dataset. 9.4.1 How the data was generated for Challenge 3 In this challenge, the Earnings variable was calculated using the attributes like GPA, Connections and Graduation Year in some cases. The main attribute on which the data is subsetted is the Education attribute. Then further, there is a predetermined polynomial formula based on the various attributes. The main idea behind these formulas, is to include a linear/ quadratic relation between the predictors and the Earnings attribute which will be predicted. These mathematical relation can be modeled using the most simplest linear regression algorithm to the most complex neural nets. The ideal formulas are listed below: Stem earn = -100 * gpa +10000 Humanities earn = 100* gpa + 10000 Vocational earn = 100 * gpa + 13000 Professional earn = -100gpa +12000 other earn = connection ^2 +5000 business earn = gpa * 100 * parity +10000 where parity = 1 if graduation year = even 0 if graduation year = odd As we can see, these formulas are mostly linear, while the formula for other education attribute is quadratic. Also, for Business education attribute subjects, the formula is dependent on an additional attribute. For more detailed data analysis please view the document attached here. Pred 3 Analysis How the data was modeled in R eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXRcbmRhdDwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvRWFybmluZ3NfVGVzdF9hbnN3ZXIuY3N2XCIsc3RyaW5nc0FzRmFjdG9ycyA9IFQpXG5cbiMgQ3JlYXRlIGEgcHJlZGljdGlvbiBjb2x1bW4gdG8gc3RvcmUgcHJlZGljdGVkIHZhbHVlcyBhbmQgYXBwZW5kIHRoYXQgY29sdW1uIHRvIGRhdGFzZXQuXG5kYXQkcHJlZEVhcm5pbmdzIDwtIHJlcCgwLG5yb3coZGF0KSlcblxuXG4jIFByZWRpY3QgRWFybmluZ3Mgb2Ygc3ViamVjdHMgd2l0aCBFZHVjYXRpb24gaW4gU1RFTSBmaWVsZC5cbmRhdFtkYXQkTWFqb3I9PSdTVEVNJyxdJHByZWRFYXJuaW5ncyA8LSAoLTEwMCpkYXRbZGF0JE1ham9yPT0nU1RFTScsXSRHUEEgKyAxMDAwMClcblxuIyBQcmVkaWN0IEVhcm5pbmdzIG9mIHN1YmplY3RzIHdpdGggRWR1Y2F0aW9uIGluIEh1bWFuaXRpZXMgZmllbGQuXG5kYXRbZGF0JE1ham9yPT0nSHVtYW5pdGllcycsXSRwcmVkRWFybmluZ3MgPC0gKDEwMCpkYXRbZGF0JE1ham9yPT0nSHVtYW5pdGllcycsXSRHUEEgKyAxMDAwMClcblxuIyBQcmVkaWN0IEVhcm5pbmdzIG9mIHN1YmplY3RzIHdpdGggRWR1Y2F0aW9uIGluIFZvY2F0aW9uYWwgZmllbGQuXG5kYXRbZGF0JE1ham9yPT0nVm9jYXRpb25hbCcsXSRwcmVkRWFybmluZ3MgPC0gKDEwMCpkYXRbZGF0JE1ham9yPT0nVm9jYXRpb25hbCcsXSRHUEEgKyAxMzAwMClcblxuIyBQcmVkaWN0IEVhcm5pbmdzIG9mIHN1YmplY3RzIHdpdGggRWR1Y2F0aW9uIGluIFByb2Zlc3Npb25hbCBmaWVsZC5cbmRhdFtkYXQkTWFqb3I9PSdQcm9mZXNzaW9uYWwnLF0kcHJlZEVhcm5pbmdzIDwtICgtMTAwKmRhdFtkYXQkTWFqb3I9PSdQcm9mZXNzaW9uYWwnLF0kR1BBICsgMTIwMDApXG5cbiMgUHJlZGljdCBFYXJuaW5ncyBvZiBzdWJqZWN0cyB3aXRoIEVkdWNhdGlvbiBpbiBPdGhlciBmaWVsZHMuXG5kYXRbZGF0JE1ham9yPT0nT3RoZXInLF0kcHJlZEVhcm5pbmdzIDwtIChkYXRbZGF0JE1ham9yPT0nT3RoZXInLF0kTnVtYmVyX09mX1Byb2Zlc3Npb25hbF9Db25uZWN0aW9uc14yICsgNTAwMClcblxuIyBQcmVkaWN0IEVhcm5pbmdzIG9mIHN1YmplY3RzIHdpdGggRWR1Y2F0aW9uIGluIEJ1c2luZXNzIGZpZWxkLlxuZGF0W2RhdCRNYWpvcj09J0J1aXNuZXNzJyxdJHByZWRFYXJuaW5ncyA8LSAoMTAwKmRhdFtkYXQkTWFqb3I9PSdCdWlzbmVzcycsXSRHUEEqKChkYXRbZGF0JE1ham9yPT0nQnVpc25lc3MnLF0kR3JhZHVhdGlvbl9ZZWFyKzEpJSUyKSArIDEwMDAwKVxuXG5cbiMgQ29tcGFyZSB0aGUgcHJlZGljdGVkIEVhcm5pbmdzIHZhbHVlcyB3aXRoIHRoZSBpZGVhbCBFYXJuaW5ncyB2YWx1ZXMgaW4gdGVzdCBkYXRhLlxubGlicmFyeShNb2RlbE1ldHJpY3MpXG5tc2UoZGF0JEVhcm5pbmdzLGRhdCRwcmVkRWFybmluZ3MpIn0= We can see that after using the ideal formulas, we get an MSE of around 3300. Thus students who took part in this prediction challenge and scored around this MSE score would be top submissions. 9.4.2 Top Submissions for Challenge 3 For this prediction challenge, the MSE score below 30000 was considered a Passing score. Seok Yim Seoks PPT This was the top submission based on MSE score, with a final score less than 100. The approach to solving this challenge was really well implemented. First, he looked at the dataset on whole, tried to find some interesting patterns. Then, after finding the patterns, he did not predict on the complete dataset using one big model, but subseted the data based on one attribute, and then modeled the ML model on these small subsets. This not only reduced the MSE to such low levels, thus increasing accuracy, but also led to faster model learning time, and prediction time. Nick Whelan Nicks PPT This was another top submission based on MSE score, with final score less than 100. The approach to solving the task was different compared to Seoks implementation, but was equally good, with nearly the same prediction power/accuracy. Nick tried to use the randomForest algorithm on the whole dataset as the initial model, but the MSE turned out to be near 25,000. Then he did some free-style analysis and found the linear relationship between various subsets of dataset with the earnings value. To implement this he used the fundamentals of linear regression very well while creating a learning model, and also used a quadratic model where needed. This resulted in a very accurate model with low MSE score. Bennett Garcia Bennetts PPT Bennett had a final MSE score of below 100 and was one of the top submissions for this challenge. A significantly different learning model was used by Bennett to achieve this low MSE. He first analyzed the data, and found attributes on which the dataset can be subsetted on. Then, he here used Neural Networks as models for prediction on those subsets. This Neural Network approach was very well implemented. 9.5 Prediction Challenge 4. Challenge 4 was a relatively newer challenge, and was built to test and combine all that has been learnt from the previous challenges. In this challenge, there was a scenario as described below: Mysterious box was found on the beach. Despite spending probably years in the water, it still works! But what does it do? It has four inputs (electric) &amp; a switch. Setting these inputs and different switch positions emits various weird and scary sounds as output in response to the electric signals. It sizzles, gurgles, hisses, ominously tics like a bomb,etc..but nothing happens - just sounds. So no harm will happen to surroundings. As we can see from the scenario, the task now in this challenge, is to predict the sounds that the Mysterios Box will make upon providing various set of inputs and different switch positions. Henceforth, we will refer to this mysterious box as Black Box. Also, since there are only finite number of sounds the box can make, the output sounds attribute is a categorical value, which will be predicted in this task. Students were recommended to first find some correlation between data by using free-style analysis, and then proceed to using any ML models. To read more about prediction in R, see sections 6,7 and 8 Lets look at a snippet of the Mysterious Box/ Black Box dataset used for training the models below. The training describes which sounds has been noted in the laboratory in nearly 20,000 experiments combining different input signals and switch positions. Table 9.3: Snippet of Black Box Dataset(TRAINING) for Prediction Challenge 4 ID INPUT1 INPUT2 INPUT3 INPUT4 SWITCH SOUND 86623 30 31 72 29 Low Gargle 57936 87 76 31 79 Low Tick 54301 16 33 87 41 Low Tick 2678 64 77 91 59 Minimum Beep 65827 33 72 53 66 High Beep 22420 5 50 26 50 High Gargle 2285 82 72 60 73 High Tick 62571 44 85 100 8 Minimum Kaboom 49229 92 31 100 64 Low Gargle 63532 28 51 77 4 Low Gargle We can see that there are multiple attributes like INPUT1,2,3,4 and Switch that can be used as predictors, and then there is Sound attribute with ideal values for each record of the experiment record which will be used while training and then will be predicted on the testing dataset. 9.5.1 How the data was generated for Challenge 4 This challenge was the most involved of the 4 challenges in this blog. There was no direct and straight forward answer to this challenge, but it required more data analysis, as compared to the other challenge. Although the relation of the Sound attribute was dependent on the 4 input and the switch position, figuring the relation between various inputs and the correct switch position was a non-trivial task. The solution to this challenge involved creating a new numeric variable(Say OUTPUT) which will be dependent on the 4 input values, and also the various switch positions. The relation between the inputs and the OUTPUT variable is given below: The ordering of Switch position is given as: Low = 1 Minimum = 2 Medium = 3 Maximum = 4 High = 5 if Switch == 1 i.e. &quot;Low&quot; then OUTPUT = Input 1+ 5 * Input 2 - 2 * Input3 + sample(2:5,1) if Switch == 2 i.e. &quot;Minimum&quot; then OUTPUT = 3* Input 2 - 2 * Input 4 + sample(2:3,1) else i.e. Position other than &quot;Low&quot; and &quot;Minimum&quot; then OUTPUT = Input1 ^2 -1.5 * Input 3 + sample(5:10,1) Then SOUND totally depends on OUTPUT attribute, but is distributed probabilistically over all possible sound. For example, the SOUND when OUTPUT&gt;150 is distributed as 0, 0, 10, 0, 10, 60, 20. This number list corresponds to Gargle, Tick, Beep, Kaboom, Rumble, Sizzle, Hiss. And thus we can see that &quot;Sizzle&quot; sound has the max probability of 60%, and is this the most likely sound when the OUTPUT value is above 150. OUTPUT &gt; 150 -&gt; Max Probability of finding &quot;Sizzle&quot; 100 &lt; OUTPUT &lt; 150 -&gt; Max Probability of finding &quot;Rumble&quot; 70 &lt; OUTPUT &lt; 100 -&gt; Max Probability of finding &quot;Kaboom&quot; 50 &lt; OUTPUT &lt; 70 -&gt; Max Probability of finding &quot;Hiss&quot; 20 &lt; OUTPUT &lt; 50 -&gt; Max Probability of finding &quot;Tick&quot; OUTPUT &lt; 20 -&gt; Max Probability of finding &quot;Gargle&quot; As we can see, that the 4 Input attributes are used to calculate the OUTPUT values based on a polynomial formula, and the particular formula is choosen by the Switch attribute. After finding the OUTPUT values, then the decision tree like structure can be implemented to assign Sound attribute to corresponding OUTPUT value based on the chart above. How the data was generated for Challenge 4 in R # Load The Data dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22_answer.csv&quot;,stringsAsFactors = T) dat$OUTPUT &lt;- rep(0,nrow(dat)) dat$OUTPUT &lt;- ((dat$INPUT1^2) - (1.5*dat$INPUT3) + sample(5:10,1)) dat[dat$SWITCH == &quot;Low&quot;,]$OUTPUT &lt;- (dat[dat$SWITCH == &quot;Low&quot;,]$INPUT1 + (5*dat[dat$SWITCH == &quot;Low&quot;,]$INPUT2) - (2*dat[dat$SWITCH == &quot;Low&quot;,]$INPUT3) + sample(2:5,1)) dat[dat$SWITCH == &quot;Minimum&quot;,]$OUTPUT &lt;- ((3*dat[dat$SWITCH == &quot;Minimum&quot;,]$INPUT2) - (2*dat[dat$SWITCH == &quot;Minimum&quot;,]$INPUT4) + sample(2:3,1)) dat$predSound &lt;- rep(&#39;Empty&#39;,nrow(dat)) dat[dat$OUTPUT&gt;150,]$predSound&lt;-&#39;Sizzle&#39; dat[dat$OUTPUT&gt;=100 &amp; dat$OUTPUT&lt;150,]$predSound&lt;-&#39;Rumble&#39; dat[dat$OUTPUT&gt;=70 &amp; dat$OUTPUT&lt;100,]$predSound&lt;-&#39;Kaboom&#39; dat[dat$OUTPUT&gt;=50 &amp; dat$OUTPUT&lt;70,]$predSound&lt;-&#39;Hiss&#39; dat[dat$OUTPUT&gt;=20 &amp; dat$OUTPUT&lt;50,]$predSound&lt;-&#39;Tick&#39; dat[dat$OUTPUT&lt;20,]$predSound&lt;-&#39;Gargle&#39; mean(dat$SOUND==dat$predSound) 9.5.2 Top Submissions for Challenge 4 Since this challenge involved stochastically generated data, the prediction accuracy required for passing this challenge was above 60%. Nicole Coria Nicoles PPT This was the top submission based on accuracy score, with a final score more than 68.7% The approach to solving this challenge was iterative and trail and error based. First, since the task is to predict categorical data, she decided to use rpart(directly). Then, over iteration, by varying the control parameters of rpart, she tried to find the model with the highest accuracy. Use of cross-validation also helped in finding the best fit model. Atharva Patil Atharvas PPT This was another top submission based on accuracy score, with final score above 68% The approach to solving the task was very well implemented, using external resources too. Atharva tried to analyze the data first. To do this, he used Prof. Imielinskis online platform called Boundless Analytics. This online platform has ability to analyze the data automatically, and create plots which only matter or provide more information about the data. It eliminates the need to perform the data analysis manually. Then, he proceeded by building the model using the rpart() function and control parameters. Andrew Scovell Andrews PPT Bennett had a final accuracy score of above 68% and was one of the top submissions for this challenge. He did a very extensive data analysis using all the attributes of the dataset. He also tried analyzing using mean, sums, standard deviation, etc of the numerical inputs. Using the control parameters of the rpart() function he tried to find the best fitting model, and used cross-validation to avoid overfitting. To perform any of the above challenges yourself, visit the appropriate links. Prediction Challenge 1 https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d Prediction Challenge 2 https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b Prediction Challenge 3 https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd Prediction Challenge 4 https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe "]]
