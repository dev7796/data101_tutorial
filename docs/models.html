<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Additional Modeling techniques. | DATA 101 Shortest Textbook</title>
  <meta name="description" content="This is a example of a interactive book for DATA 101 Course thought by Prof. Tomasz Imielinski (http://data101.cs.rutgers.edu/) Find the demo site at (https://dev7796.github.io/Data101_TextBook/) Published using Bookdown for R." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Additional Modeling techniques. | DATA 101 Shortest Textbook" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a example of a interactive book for DATA 101 Course thought by Prof. Tomasz Imielinski (http://data101.cs.rutgers.edu/) Find the demo site at (https://dev7796.github.io/Data101_TextBook/) Published using Bookdown for R." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Additional Modeling techniques. | DATA 101 Shortest Textbook" />
  
  <meta name="twitter:description" content="This is a example of a interactive book for DATA 101 Course thought by Prof. Tomasz Imielinski (http://data101.cs.rutgers.edu/) Find the demo site at (https://dev7796.github.io/Data101_TextBook/) Published using Bookdown for R." />
  



<meta name="date" content="2021-08-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression.html"/>
<link rel="next" href="predblogs.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DATA 101 Book</a>
<a href="https://github.com/dev7796/Data101_TextBook" target="blank">Github Source</a>
</li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#setting-up-r"><i class="fa fa-check"></i><b>1.1</b> Setting Up R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="dataexp.html"><a href="dataexp.html"><i class="fa fa-check"></i><b>2</b> Data Exploration</a>
<ul>
<li class="chapter" data-level="2.1" data-path="dataexp.html"><a href="dataexp.html#plots"><i class="fa fa-check"></i><b>2.1</b> Plots</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="dataexp.html"><a href="dataexp.html#scatter-plot"><i class="fa fa-check"></i><b>2.1.1</b> Scatter Plot</a></li>
<li class="chapter" data-level="2.1.2" data-path="dataexp.html"><a href="dataexp.html#bar-plot"><i class="fa fa-check"></i><b>2.1.2</b> Bar Plot</a></li>
<li class="chapter" data-level="2.1.3" data-path="dataexp.html"><a href="dataexp.html#box-plot"><i class="fa fa-check"></i><b>2.1.3</b> Box Plot</a></li>
<li class="chapter" data-level="2.1.4" data-path="dataexp.html"><a href="dataexp.html#mosaic-plot"><i class="fa fa-check"></i><b>2.1.4</b> Mosaic Plot</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="dataexp.html"><a href="dataexp.html#freestyle"><i class="fa fa-check"></i><b>2.2</b> Free Style data exploration with just seven R commands " R.7 "</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="dataexp.html"><a href="dataexp.html#subset"><i class="fa fa-check"></i><b>2.2.1</b> Subset</a></li>
<li class="chapter" data-level="2.2.2" data-path="dataexp.html"><a href="dataexp.html#table"><i class="fa fa-check"></i><b>2.2.2</b> Table</a></li>
<li class="chapter" data-level="2.2.3" data-path="dataexp.html"><a href="dataexp.html#tapply"><i class="fa fa-check"></i><b>2.2.3</b> tapply</a></li>
<li class="chapter" data-level="2.2.4" data-path="dataexp.html"><a href="dataexp.html#dataframe"><i class="fa fa-check"></i><b>2.2.4</b> data.frame()</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="dataexp.html"><a href="dataexp.html#Moodyexplaination"><i class="fa fa-check"></i><b>2.3</b> Professor Moody Puzzle</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stateval.html"><a href="stateval.html"><i class="fa fa-check"></i><b>3</b> p-value: Simple Statistical Evaluation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="stateval.html"><a href="stateval.html#permtest"><i class="fa fa-check"></i><b>3.1</b> Permutation Test</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="stateval.html"><a href="stateval.html#permonestep"><i class="fa fa-check"></i><b>3.1.1</b> Permutation Test One Step</a></li>
<li class="chapter" data-level="3.1.2" data-path="stateval.html"><a href="stateval.html#permfunction"><i class="fa fa-check"></i><b>3.1.2</b> Permutation Function</a></li>
<li class="chapter" data-level="3.1.3" data-path="stateval.html"><a href="stateval.html#aniceexample"><i class="fa fa-check"></i><b>3.1.3</b> Exercise - How p-value is affected by difference of means and standard deviations</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="stateval.html"><a href="stateval.html#bonferroni"><i class="fa fa-check"></i><b>3.2</b> Multiple Hypothesis - Bonferroni Correction.</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="stateval.html"><a href="stateval.html#examples-for-multiple-hypothesis-testing."><i class="fa fa-check"></i><b>3.2.1</b> Examples for Multiple hypothesis testing.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Data Modeling and Prediction techniques for Classification.</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#decisiontree"><i class="fa fa-check"></i><b>4.1</b> Decision Tree.</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#rpart"><i class="fa fa-check"></i><b>4.2</b> Use of Rpart</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#rpartplot"><i class="fa fa-check"></i><b>4.3</b> Visualize the Decision tree</a></li>
<li class="chapter" data-level="4.4" data-path="classification.html"><a href="classification.html#rpartcontrol"><i class="fa fa-check"></i><b>4.4</b> Rpart Control</a></li>
<li class="chapter" data-level="4.5" data-path="classification.html"><a href="classification.html#rpartpredict"><i class="fa fa-check"></i><b>4.5</b> Prediction using rpart.</a></li>
<li class="chapter" data-level="4.6" data-path="classification.html"><a href="classification.html#splitdata"><i class="fa fa-check"></i><b>4.6</b> Split the data yourself.</a></li>
<li class="chapter" data-level="4.7" data-path="classification.html"><a href="classification.html#crossvalidation"><i class="fa fa-check"></i><b>4.7</b> Cross Validation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>5</b> Data Modelling and Prediction techniques for Regression.</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression.html"><a href="regression.html#linear-regression."><i class="fa fa-check"></i><b>5.1</b> Linear Regression.</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="regression.html"><a href="regression.html#lm"><i class="fa fa-check"></i><b>5.1.1</b> Linear regression using lm() function</a></li>
<li class="chapter" data-level="5.1.2" data-path="regression.html"><a href="regression.html#mse"><i class="fa fa-check"></i><b>5.1.2</b> Calculating the Error using mse()</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="regression.html"><a href="regression.html#regression-using-rpart"><i class="fa fa-check"></i><b>5.2</b> Regression using RPART</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>6</b> Additional Modeling techniques.</a>
<ul>
<li class="chapter" data-level="6.1" data-path="models.html"><a href="models.html#model4step"><i class="fa fa-check"></i><b>6.1</b> Four Line Method for creating most type of prediction models in R</a></li>
<li class="chapter" data-level="6.2" data-path="models.html"><a href="models.html#randomforest"><i class="fa fa-check"></i><b>6.2</b> Random Forest</a></li>
<li class="chapter" data-level="6.3" data-path="models.html"><a href="models.html#svm"><i class="fa fa-check"></i><b>6.3</b> SVM</a></li>
<li class="chapter" data-level="6.4" data-path="models.html"><a href="models.html#nnet"><i class="fa fa-check"></i><b>6.4</b> Neural Network.</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="predblogs.html"><a href="predblogs.html"><i class="fa fa-check"></i><b>7</b> Prediction Challenges</a>
<ul>
<li class="chapter" data-level="7.1" data-path="predblogs.html"><a href="predblogs.html#general-structure-of-the-prediction-challenges."><i class="fa fa-check"></i><b>7.1</b> General Structure of the Prediction Challenges.</a></li>
<li class="chapter" data-level="7.2" data-path="predblogs.html"><a href="predblogs.html#prediction-challange-1."><i class="fa fa-check"></i><b>7.2</b> Prediction Challange 1.</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="predblogs.html"><a href="predblogs.html#how-the-data-was-generated-for-challenge-1"><i class="fa fa-check"></i><b>7.2.1</b> How the data was generated for Challenge 1</a></li>
<li class="chapter" data-level="7.2.2" data-path="predblogs.html"><a href="predblogs.html#top-submissions-for-challenge-1."><i class="fa fa-check"></i><b>7.2.2</b> Top Submissions for Challenge 1.</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="predblogs.html"><a href="predblogs.html#prediction-challenge-2."><i class="fa fa-check"></i><b>7.3</b> Prediction Challenge 2.</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="predblogs.html"><a href="predblogs.html#how-the-data-was-generated-to-challenge-2"><i class="fa fa-check"></i><b>7.3.1</b> How the data was generated to Challenge 2</a></li>
<li class="chapter" data-level="7.3.2" data-path="predblogs.html"><a href="predblogs.html#top-submissions-for-challenge-2"><i class="fa fa-check"></i><b>7.3.2</b> Top Submissions for Challenge 2</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="predblogs.html"><a href="predblogs.html#prediction-challenge-3."><i class="fa fa-check"></i><b>7.4</b> Prediction Challenge 3.</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="predblogs.html"><a href="predblogs.html#how-the-data-was-generated-for-challenge-3"><i class="fa fa-check"></i><b>7.4.1</b> How the data was generated for Challenge 3</a></li>
<li class="chapter" data-level="7.4.2" data-path="predblogs.html"><a href="predblogs.html#top-submissions-for-challenge-3"><i class="fa fa-check"></i><b>7.4.2</b> Top Submissions for Challenge 3</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="predblogs.html"><a href="predblogs.html#prediction-challenge-4."><i class="fa fa-check"></i><b>7.5</b> Prediction Challenge 4.</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="predblogs.html"><a href="predblogs.html#how-the-data-was-generated-for-challenge-4"><i class="fa fa-check"></i><b>7.5.1</b> How the data was generated for Challenge 4</a></li>
<li class="chapter" data-level="7.5.2" data-path="predblogs.html"><a href="predblogs.html#top-submissions-for-challenge-4"><i class="fa fa-check"></i><b>7.5.2</b> Top Submissions for Challenge 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>8</b> Appendix</a>
<ul>
<li class="chapter" data-level="8.1" data-path="dataexp.html"><a href="dataexp.html#dataframe"><i class="fa fa-check"></i><b>8.1</b> c() &amp; data.frame() &amp; class()</a></li>
<li class="chapter" data-level="8.2" data-path="appendix.html"><a href="appendix.html#basicfunction"><i class="fa fa-check"></i><b>8.2</b> summary(), mean(),length(), max(),min(), sd(),nrow(), ncol(), dim()</a></li>
<li class="chapter" data-level="8.3" data-path="appendix.html"><a href="appendix.html#cut"><i class="fa fa-check"></i><b>8.3</b> Cut</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="appendix.html"><a href="appendix.html#questionwhat-would-r-say"><i class="fa fa-check"></i><b>8.3.1</b> QuestionWhat would R say?</a></li>
<li class="chapter" data-level="8.3.2" data-path="appendix.html"><a href="appendix.html#questionwhat-would-r-say-1"><i class="fa fa-check"></i><b>8.3.2</b> QuestionWhat would R say?</a></li>
<li class="chapter" data-level="8.3.3" data-path="appendix.html"><a href="appendix.html#questionwhat-would-r-say-2"><i class="fa fa-check"></i><b>8.3.3</b> QuestionWhat would R say?</a></li>
<li class="chapter" data-level="8.3.4" data-path="appendix.html"><a href="appendix.html#a-complex-example"><i class="fa fa-check"></i><b>8.3.4</b> A complex example</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="appendix.html"><a href="appendix.html#basicexamples"><i class="fa fa-check"></i><b>8.4</b> What would R say?</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="appendix.html"><a href="appendix.html#question"><i class="fa fa-check"></i><b>8.4.1</b> Question</a></li>
<li class="chapter" data-level="8.4.2" data-path="appendix.html"><a href="appendix.html#question-1"><i class="fa fa-check"></i><b>8.4.2</b> Question</a></li>
<li class="chapter" data-level="8.4.3" data-path="appendix.html"><a href="appendix.html#question-2"><i class="fa fa-check"></i><b>8.4.3</b> Question</a></li>
<li class="chapter" data-level="8.4.4" data-path="appendix.html"><a href="appendix.html#question-3"><i class="fa fa-check"></i><b>8.4.4</b> Question</a></li>
<li class="chapter" data-level="8.4.5" data-path="appendix.html"><a href="appendix.html#question-4"><i class="fa fa-check"></i><b>8.4.5</b> Question</a></li>
<li class="chapter" data-level="8.4.6" data-path="appendix.html"><a href="appendix.html#question-5"><i class="fa fa-check"></i><b>8.4.6</b> Question</a></li>
<li class="chapter" data-level="8.4.7" data-path="appendix.html"><a href="appendix.html#question-6"><i class="fa fa-check"></i><b>8.4.7</b> Question</a></li>
<li class="chapter" data-level="8.4.8" data-path="appendix.html"><a href="appendix.html#question-7"><i class="fa fa-check"></i><b>8.4.8</b> Question</a></li>
<li class="chapter" data-level="8.4.9" data-path="appendix.html"><a href="appendix.html#question-8"><i class="fa fa-check"></i><b>8.4.9</b> Question</a></li>
<li class="chapter" data-level="8.4.10" data-path="appendix.html"><a href="appendix.html#question-9"><i class="fa fa-check"></i><b>8.4.10</b> Question</a></li>
<li class="chapter" data-level="8.4.11" data-path="appendix.html"><a href="appendix.html#question-10"><i class="fa fa-check"></i><b>8.4.11</b> Question</a></li>
<li class="chapter" data-level="8.4.12" data-path="appendix.html"><a href="appendix.html#question-11"><i class="fa fa-check"></i><b>8.4.12</b> Question</a></li>
<li class="chapter" data-level="8.4.13" data-path="appendix.html"><a href="appendix.html#question-12"><i class="fa fa-check"></i><b>8.4.13</b> Question</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="appendix.html"><a href="appendix.html#create-column"><i class="fa fa-check"></i><b>8.5</b> Create Column</a></li>
<li class="chapter" data-level="8.6" data-path="appendix.html"><a href="appendix.html#factor"><i class="fa fa-check"></i><b>8.6</b> Factor Function: factor()</a></li>
<li class="chapter" data-level="8.7" data-path="appendix.html"><a href="appendix.html#coerce"><i class="fa fa-check"></i><b>8.7</b> Coercing Values in data frames</a></li>
<li class="chapter" data-level="8.8" data-path="appendix.html"><a href="appendix.html#merge"><i class="fa fa-check"></i><b>8.8</b> Merging Two Relational Data Frames.</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="appendix.html"><a href="appendix.html#inner-join"><i class="fa fa-check"></i><b>8.8.1</b> Inner Join</a></li>
<li class="chapter" data-level="8.8.2" data-path="appendix.html"><a href="appendix.html#full-join"><i class="fa fa-check"></i><b>8.8.2</b> Full Join</a></li>
<li class="chapter" data-level="8.8.3" data-path="appendix.html"><a href="appendix.html#left-join"><i class="fa fa-check"></i><b>8.8.3</b> Left Join</a></li>
<li class="chapter" data-level="8.8.4" data-path="appendix.html"><a href="appendix.html#right-join"><i class="fa fa-check"></i><b>8.8.4</b> Right Join</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="appendix.html"><a href="appendix.html#sliceanddice"><i class="fa fa-check"></i><b>8.9</b> Slicing and Dicing.</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="appendix.html"><a href="appendix.html#dicing"><i class="fa fa-check"></i><b>8.9.1</b> Subsetting on Columns ( DICING )</a></li>
<li class="chapter" data-level="8.9.2" data-path="appendix.html"><a href="appendix.html#slicing"><i class="fa fa-check"></i><b>8.9.2</b> Subsetting on Rows ( SLICING )</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="appendix.html"><a href="appendix.html#group-by"><i class="fa fa-check"></i><b>8.10</b> Group By</a></li>
<li class="chapter" data-level="8.11" data-path="appendix.html"><a href="appendix.html#handling-date-and-time-in-dataframes."><i class="fa fa-check"></i><b>8.11</b> Handling Date and Time in dataframes.</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DATA 101 Shortest Textbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="models" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Additional Modeling techniques.</h1>
<script src="files/js/dcl.js"></script>
<p>In this chapter, we will see some additional machine learning models used in practice, for various purpose.</p>
<p>After studying both classification models and regression models in the previous 2 chapters <a href="classification.html#classification">4</a> &amp; <a href="regression.html#regression">5</a> respectively, we will now look into other generic models used for classification and/or regression purpose.</p>
<p>Below is the list some of the widely used algorithms with their use case(either classification/regression or both) and training and prediction complexities for using particular learning models.</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/MLmodelcomplexity.png" alt="" />
<p class="caption">Usage and Complexity of various machine learning algorithms. Credits:thekerneltrip.com</p>
</div>
<p>As we can see that many of these algorithms can be used for classification and regression all together, as we saw in the case of the Decision tree models using Rpart in section <a href="classification.html#decisiontree">4.1</a>, and also some model used for only a particular type of prediction e.g. linear regression.</p>
<p>We will look at few algorithms from the above list:</p>
<ul>
<li>Random Forest</li>
<li>Support Vector Machine (SVM)</li>
<li>Neural Networks</li>
</ul>
<hr />
<div id="model4step" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Four Line Method for creating most type of prediction models in R</h2>
<p>But before we learn about these algorithms, let us see a four line method to build models using any of the above algorithms using R.</p>
<p>We can safely assume that the data going to be used to build the model, has been pre-processed and based on requirements split into the required subsets. To see how to split the data refer to section <a href="classification.html#splitdata">4.6</a>.</p>
<ol start="0" style="list-style-type: decimal">
<li>The zeroth step now will be obviously to install and load the packages that contain the ML algorithm. To do that on your local machine, use the following code.</li>
</ol>
<pre class="text"><code># Install the library
install.packages(&quot;package name&quot;)

# Load the library in R
library(package_name)</code></pre>
<ol style="list-style-type: decimal">
<li>Once we have the algorithm library loaded, we then proceed to build the model.</li>
</ol>
<ul>
<li><code>pred.model = model_function(formula, data, method*,...)</code>
<ul>
<li><em>model_function()</em>: the function present in the library to build the model. e.g. <em>rpart()</em></li>
<li><em>formula</em>: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on.
<ul>
<li><code>prediction ~ predictor1 + predictor2 + predictor3 + ...</code></li>
</ul></li>
<li><em>data</em>: here we provide the dataset on which the ML model is to be trained on. Remember never used the test data to build the model.</li>
<li><em>method</em>: (OPTIONAL) Used to denote the method of prediction or underlying algorithm. This parameter could be present in some model_function() but not all.</li>
</ul></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Prediction using the predict() function on the training data to assess the models performance/accuracy in next step.</li>
</ol>
<ul>
<li><code>pred = predict(pred.model, newdata = train)</code>
<ul>
<li><em>predict()</em>: the common function for all models used for prediction.</li>
<li><em>pred.model</em>: output of the step 1.</li>
<li><em>newdata</em>: here we assign the data on which the prediction is to be done.</li>
</ul></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Evaluate error in Training phase. We use the mse() function for finding the accuracy of the model. To read more in dept about the mse() function refer to section <a href="regression.html#mse">5.1.2</a>.</li>
</ol>
<ul>
<li><code>mse(actual, pred)</code>
<ul>
<li><em>actual</em>: vector of the actual values of the attribute we want to predict.</li>
<li><em>pred</em>: vector of the predicted values obtained using our model.</li>
</ul></li>
</ul>
<p>Repeat steps 0,1,2 and 3 by changing the ML algorithm or manipulating dataset to perform better when used to train using ML model, so as to achieve as low MSE value as possible.</p>
<ol start="4" style="list-style-type: decimal">
<li>Finally we predict on the testing data using the same predict function as in step 2 but replacing the train data with test data.</li>
</ol>
<ul>
<li><code>pred = predict(pred.model, newdata = test)</code></li>
</ul>
<p>These are the 4 steps to follow while performing any prediction task using ML models in R.</p>
<p>We can also add one more step between step 3 and 4, which is step of performing the cross validation process on the newly built models.</p>
<p>This can be done either manually, or by using third party libraries.</p>
<p>One such library is the <code>rModeling</code> package, which has function <em>crossValidation()</em> which can be used for any type of model_functions(). For more information visit <a href="https://www.rdocumentation.org/packages/rModeling/versions/0.0.3/topics/crossValidation">crossValidation()</a></p>
<p>Before we proceed to the next section, please look at the snippet of the <em>earnings.csv</em> dataset, which we will be using for predicting the <em>Earnings</em> attribute based on various other attributes provided in the dataset, using different prediction models.</p>
<div style="border: 1px solid #ddd; padding: 5px; overflow-x: scroll; width:100%; ">
<table>
<caption>
<span id="tab:unnamed-chunk-37">Table 6.1: </span>Snippet of Earnings Dataset
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
GPA
</th>
<th style="text-align:right;">
Number_Of_Professional_Connections
</th>
<th style="text-align:right;">
Earnings
</th>
<th style="text-align:left;">
Major
</th>
<th style="text-align:right;">
Graduation_Year
</th>
<th style="text-align:right;">
Height
</th>
<th style="text-align:right;">
Number_Of_Credits
</th>
<th style="text-align:right;">
Number_Of_Parking_Tickets
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
2807
</td>
<td style="text-align:right;">
1.83
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
10172.60
</td>
<td style="text-align:left;">
Humanities
</td>
<td style="text-align:right;">
1982
</td>
<td style="text-align:right;">
69.75
</td>
<td style="text-align:right;">
123
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
1421
</td>
<td style="text-align:right;">
2.78
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
9730.43
</td>
<td style="text-align:left;">
STEM
</td>
<td style="text-align:right;">
1977
</td>
<td style="text-align:right;">
66.24
</td>
<td style="text-align:right;">
125
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
862
</td>
<td style="text-align:right;">
1.77
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
9828.33
</td>
<td style="text-align:left;">
STEM
</td>
<td style="text-align:right;">
1983
</td>
<td style="text-align:right;">
69.02
</td>
<td style="text-align:right;">
120
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
4210
</td>
<td style="text-align:right;">
1.98
</td>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
13191.68
</td>
<td style="text-align:left;">
Vocational
</td>
<td style="text-align:right;">
1989
</td>
<td style="text-align:right;">
65.56
</td>
<td style="text-align:right;">
121
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
697
</td>
<td style="text-align:right;">
1.69
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
9810.15
</td>
<td style="text-align:left;">
STEM
</td>
<td style="text-align:right;">
1998
</td>
<td style="text-align:right;">
67.43
</td>
<td style="text-align:right;">
122
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
6160
</td>
<td style="text-align:right;">
2.42
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
11746.90
</td>
<td style="text-align:left;">
Professional
</td>
<td style="text-align:right;">
2015
</td>
<td style="text-align:right;">
65.98
</td>
<td style="text-align:right;">
122
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
9237
</td>
<td style="text-align:right;">
2.01
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
5006.94
</td>
<td style="text-align:left;">
Other
</td>
<td style="text-align:right;">
2004
</td>
<td style="text-align:right;">
67.95
</td>
<td style="text-align:right;">
121
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
3117
</td>
<td style="text-align:right;">
2.60
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
10258.07
</td>
<td style="text-align:left;">
Humanities
</td>
<td style="text-align:right;">
1971
</td>
<td style="text-align:right;">
64.22
</td>
<td style="text-align:right;">
121
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
6191
</td>
<td style="text-align:right;">
2.25
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
11767.14
</td>
<td style="text-align:left;">
Professional
</td>
<td style="text-align:right;">
1967
</td>
<td style="text-align:right;">
68.33
</td>
<td style="text-align:right;">
126
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:left;">
2778
</td>
<td style="text-align:right;">
2.64
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
10264.30
</td>
<td style="text-align:left;">
Humanities
</td>
<td style="text-align:right;">
1971
</td>
<td style="text-align:right;">
69.30
</td>
<td style="text-align:right;">
121
</td>
<td style="text-align:right;">
5
</td>
</tr>
</tbody>
</table>
</div>
<p>Now that we saw the general structure of the model and took a glace at the dataset we will be using, lets look at few of the algorithms as we promised from the list above.</p>
<hr />
</div>
<div id="randomforest" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Random Forest</h2>
<p>Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the value that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees.</p>
<p>In simpler terms, the random forest algorithm creates multiple decision trees based on varying attributes and biases, and then predicts the output for each tree, and aggregates this prediction into one final output by some technique like majority count or average/etc.</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/randomforest2.png" alt="" />
<p class="caption">Visual Representation of a Random Forest learning model. Credits: Random Forest Wikipedia</p>
</div>
<p>The main idea behind Random Forest arise from a method called ensemble learning method.</p>
<p>Ensemble learning is the method of solving a problem by building multiple ML models and combining them. It is primarily used to improve the performance of classification, prediction, and function approximation models.</p>
<p>Forests are type of ensemble learning methods, where they act like, pulling together all of decision tree algorithm efforts. Taking the teamwork of many trees thus improving the performance of a single random decision tree.</p>
<p>Random decision forests correct for decision trees’ habit of overfitting to their training set.</p>
<p>Now lets look at an example of prediction by the random forest model using the <em>randomForest()</em> function present in the <code>randomForest</code> library package. For more information about the <em>randomForest()</em> function and its attributes visit <strong><a href="https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest">randomforest()</a></strong></p>
<p>Thus following the 4 step method of prediction for predicting a numerical attribute “Earnings” using the randomForest() function.</p>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJhbmRvbUZvcmVzdClcbmxpYnJhcnkoTW9kZWxNZXRyaWNzKVxuXG4jIExvYWQgdGhlIGRhdGFzZXQuXG5lYXJuaW5nZGF0YTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvZWFybmluZ3MuY3N2XCIsc3RyaW5nc0FzRmFjdG9ycyA9IFQpXG5cbiMgc3BsaXR0aW5nIHRoZSBkYXRhc2V0IGludG8gdHJhaW5pbmcgYW5kIHRlc3RpbmcuXG5pZHggPC0gc2FtcGxlKCAxOjIsIHNpemUgPSBucm93KGVhcm5pbmdkYXRhKSwgcmVwbGFjZSA9IFRSVUUsIHByb2IgPSBjKC44LCAuMikpXG50cmFpbiA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMSxdXG50ZXN0IDwtIGVhcm5pbmdkYXRhW2lkeCA9PSAyLF1cblxuIyAxLiBCdWlsZCBwcmVkaWN0aW9uIG1vZGVsIHVzaW5nIHJhbmRvbUZvcmVzdCgpIGZ1bmN0aW9uLlxucHJlZC5tb2RlbCA8LSByYW5kb21Gb3Jlc3QoRWFybmluZ3Mgfi4sIGRhdGEgPSB0cmFpbilcblxuIyBMZXRzIHNlZSB0aGUgc3VtbWFyeSBvZiB0aGUgcmFuZG9tRm9yZXN0IG1vZGVsLlxucHJlZC5tb2RlbFxuXG4jIDIuIFByZWRpY3QgdXNpbmcgdGhlIG5ld2x5IGJ1aWx0IG1vZGVsIG9uIHRoZSB0cmFpbmluZyBkYXRhc2V0LlxucHJlZC50cmFpbiA8LSBwcmVkaWN0KHByZWQubW9kZWwsbmV3ZGF0YSA9IHRyYWluKVxuXG4jIDMuIEV2YWx1YXRlIGVycm9yIG9uIHRyYWluaW5nIHVzaW5nIHRoZSBtc2UoKSBmdW5jdGlvbi5cbm1zZSh0cmFpbiRFYXJuaW5ncyxwcmVkLnRyYWluKVxuXG4jIDQuIFByZWRpY3Qgb24gdGhlIHRlc3RpbmcgZGF0YS5cbnByZWQudGVzdCA8LSBwcmVkaWN0KHByZWQubW9kZWwsbmV3ZGF0YSA9IHRlc3QpXG5cbiMgQWRkaXRpb25hbGx5IHNpbmNlIGhlcmUgd2UgaGF2ZSB0aGUgYWN0dWFsL3JlYWwgcHJlZGljdGlvbiB2YWx1ZXMgd2UgY2FuIGFsc28gY2hlY2sgdGhlIGFjY3VyYWN5IG9mIG91ciBwcmVkaWN0aW9uIG9uIHRlc3RpbmcgZGF0YS5cbm1zZSh0ZXN0JEVhcm5pbmdzLHByZWQudGVzdCkifQ==
</div>
<p>We can see,</p>
<ul>
<li>the summary of the output randomForest model with details of:
<ul>
<li>Formula used.</li>
<li>Type of random forest</li>
<li>Number of trees created in the forest</li>
<li>Number of variables used at each split.</li>
<li>And some performance parameter.</li>
</ul></li>
<li>The mean squared error of the predicted values using training sub-dataset.</li>
<li>The mean squared error of the predicted values using the testing sub-dataset.</li>
</ul>
<p>Note: Since a random forest is an ensemble learning method, it will usually take a lot more time to train that its counterparts. Thus you can see a significant waiting/execution time while running the above code and acquiring answer.</p>
<hr />
</div>
<div id="svm" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> SVM</h2>
<p>Support-Vector Machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification(mostly) and regression(also works in some cases) analysis.</p>
<p>The goal of the SVM is to find a hyperplane in an N-dimensional space (where N corresponds with the number of features) that distinctly classifies/regresses the data points. The accuracy of the results directly correlates with the hyperplane that we choose. We should find a plane that has the maximum distance between data points of both classes.</p>
<p>Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/svm2.png" alt="" />
<p class="caption">Support Vector Machine Linear classification hyperplane(line) example. Credits: Support Vector Machines Wikipedia</p>
</div>
<p>Note that the dimension of the hyperplane depends on the number of features. If the number of input features is two, then the hyperplane is just a line. If the number of input features is three, then the hyperplane becomes a two-dimensional plane. It becomes difficult to draw on a graph a model when the number of features exceeds three.</p>
<p>In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/Kernel_Machine.svg" alt="" />
<p class="caption">Example of the kernal trick for non-linear classifier. Credits: Support Vector Machines Wikipedia</p>
</div>
<p>Why is this called a support vector machine? <em>Support vectors</em> are <em>data points</em> closest to the hyperplane. They directly influence the position and orientation of the hyperplane and minimizes the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These points thus help to build our SVM model.</p>
<p>SVM is great because it gives quite accurate results with minimum computation power.</p>
<p>Lets look at the example of Support vector machine algorithm in use for predicting the <em>Earnings</em> attribute of the Earnings dataset.</p>
<p>We will use the <em>svm()</em> function from the <em>e1071</em> package. For more information about this function and its attributes visit <strong><a href="https://www.rdocumentation.org/packages/e1071/versions/1.7-6/topics/svm">svm()</a></strong></p>
<p>Thus following the 4 step model for prediction and using the “<em>svm()</em>” function as the model function.</p>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGUxMDcxKVxubGlicmFyeShNb2RlbE1ldHJpY3MpXG5cbiMgTG9hZCB0aGUgZGF0YXNldC5cbmVhcm5pbmdkYXRhPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9lYXJuaW5ncy5jc3ZcIixzdHJpbmdzQXNGYWN0b3JzID0gVClcblxuXG4jIHNwbGl0dGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhlYXJuaW5nZGF0YSksIHJlcGxhY2UgPSBUUlVFLCBwcm9iID0gYyguOCwgLjIpKVxudHJhaW4gPC0gZWFybmluZ2RhdGFbaWR4ID09IDEsXVxudGVzdCA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMixdXG5cbiMgMS4gQnVpbGQgcHJlZGljdGlvbiBtb2RlbCB1c2luZyBzdm0oKSBmdW5jdGlvbi5cbnByZWQubW9kZWwgPC0gc3ZtKEVhcm5pbmdzIH4uLCBkYXRhID0gdHJhaW4pXG5cbiMgTGV0cyBzZWUgdGhlIHN1bW1hcnkgb2YgdGhlIHN2bSBtb2RlbC5cbnByZWQubW9kZWxcblxuIyAyLiBQcmVkaWN0IHVzaW5nIHRoZSBuZXdseSBidWlsdCBtb2RlbCBvbiB0aGUgdHJhaW5pbmcgZGF0YXNldC5cbnByZWQudHJhaW4gPC0gcHJlZGljdChwcmVkLm1vZGVsLG5ld2RhdGEgPSB0cmFpbilcblxuIyAzLiBFdmFsdWF0ZSBlcnJvciBvbiB0cmFpbmluZyB1c2luZyB0aGUgbXNlKCkgZnVuY3Rpb24uXG5tc2UodHJhaW4kRWFybmluZ3MscHJlZC50cmFpbilcblxuIyA0LiBQcmVkaWN0IG9uIHRoZSB0ZXN0aW5nIGRhdGEuXG5wcmVkLnRlc3QgPC0gcHJlZGljdChwcmVkLm1vZGVsLG5ld2RhdGEgPSB0ZXN0KVxuXG4jIEFkZGl0aW9uYWxseSBzaW5jZSBoZXJlIHdlIGhhdmUgdGhlIGFjdHVhbC9yZWFsIHByZWRpY3Rpb24gdmFsdWVzIHdlIGNhbiBhbHNvIGNoZWNrIHRoZSBhY2N1cmFjeSBvZiBvdXIgcHJlZGljdGlvbiBvbiB0ZXN0aW5nIGRhdGEuXG5tc2UodGVzdCRFYXJuaW5ncyxwcmVkLnRlc3QpIn0=
</div>
<p>We can see,</p>
<ul>
<li>the summary of the output svm model with details of:
<ul>
<li>Formula used.</li>
<li>Type of SVM model</li>
<li>The SVM Kernel Used</li>
<li>And some performance parameter.</li>
<li>Also, the Number of Support Vectors.</li>
</ul></li>
<li>The mean squared error of the predicted values using training sub-dataset.</li>
<li>The mean squared error of the predicted values using the testing sub-dataset.</li>
</ul>
<hr />
</div>
<div id="nnet" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Neural Network.</h2>
<p>An Artificial Neural Network (ANN), usually simply called neural network(NN) is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons.</p>
<p>Thus in other words, a neural network is a sequence of neurons connected by synapses, which reminds of the structure of the human brain. However, the human brain is even more complex, and a NN is just a model that mimics a human brain.</p>
<p>An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The “signal” at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges.</p>
<p>Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.</p>
<p>Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs.</p>
<p>Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/neuralnetwork.svg" alt="" />
<p class="caption">A visual representation of typical neural network with various nodes and edges along with layers. Credits: Artificial Neural Network Wikipedia</p>
</div>
<p>What is great about neural networks is that they can be used for basically any task from spam filtering to computer vision. However, they are normally applied for machine translation, anomaly detection and risk management, speech recognition and language generation, face recognition, and more.</p>
<p>To accommodate such a wide variety of application, neural nets are transformed and models in various different ways. To find multiple types of neural networks please visit <strong><a href="https://www.asimovinstitute.org/neural-network-zoo/">Neural Network Zoo</a> </strong></p>
<p>Now lets try to implement a neural network learning model for the Earnings prediction problem of Earnings dataset.</p>
<p>To do this we will use the 4 step method of prediction and use the <em>nnet()</em> function from the “nnet” package as the model_function.</p>
<p>Let look at the <em>nnet()</em> function and its parameters.</p>
<ul>
<li><code>nnet(formula,data,size,linout,...)</code>
<ul>
<li><em>formula</em> and <em>data</em> are the same as mentions in Step 1 of section <a href="models.html#model4step">6.1</a>.</li>
<li><em>size</em>: denotes the number of units in the hidden layer.</li>
<li><em>linout</em>: Assign TRUE is predicting numerical value. Default is FALSE, for predicting categorical value.</li>
<li><em>rang</em>: set the initial random weights on each edge.</li>
<li><em>maxit</em>: maximum number of iterations.</li>
<li><em>decay</em>: weight decay parameter. Can also be understood as learning rate.</li>
</ul></li>
<li><strong>Note</strong>: The <em>nnet()</em> function can only create a single hidden layer neural network model. To create more complex models please use different packages like <a href="https://www.rdocumentation.org/packages/neuralnet/versions/1.44.2">neuralnet</a> or <em>h2o, deepnet</em>, etc</li>
</ul>
<p>Now lets use the <em>nnet()</em> function for predction.</p>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KG5uZXQpXG5saWJyYXJ5KE1vZGVsTWV0cmljcylcblxuIyBMb2FkIHRoZSBkYXRhc2V0LlxuZWFybmluZ2RhdGE8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L2Vhcm5pbmdzLmNzdlwiLHN0cmluZ3NBc0ZhY3RvcnMgPSBUKVxuXG4jIHNwbGl0dGluZyB0aGUgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nLlxuaWR4IDwtIHNhbXBsZSggMToyLCBzaXplID0gbnJvdyhlYXJuaW5nZGF0YSksIHJlcGxhY2UgPSBUUlVFLCBwcm9iID0gYyguOCwgLjIpKVxudHJhaW4gPC0gZWFybmluZ2RhdGFbaWR4ID09IDEsXVxudGVzdCA8LSBlYXJuaW5nZGF0YVtpZHggPT0gMixdXG5cbiMgMS4gQnVpbGQgcHJlZGljdGlvbiBtb2RlbCB1c2luZyBubmV0KCkgZnVuY3Rpb24uXG5wcmVkLm1vZGVsIDwtIG5uZXQoRWFybmluZ3MvMjAwMDAgfi4sIGRhdGEgPSB0cmFpbiwgc2l6ZSA9IDUwLCBkZWNheT01ZS01LG1heGl0ID0gNTAwLGxpbm91dCA9IFQpXG5cbiMgTGV0cyBzZWUgdGhlIHN1bW1hcnkgb2YgdGhlIG5uZXQgbW9kZWwuXG5wcmVkLm1vZGVsXG5cbiMgMi4gUHJlZGljdCB1c2luZyB0aGUgbmV3bHkgYnVpbHQgbW9kZWwgb24gdGhlIHRyYWluaW5nIGRhdGFzZXQuXG5wcmVkLnRyYWluIDwtIHByZWRpY3QocHJlZC5tb2RlbCxuZXdkYXRhID0gdHJhaW4pKjIwMDAwXG5cbiMgMy4gRXZhbHVhdGUgZXJyb3Igb24gdHJhaW5pbmcgdXNpbmcgdGhlIG1zZSgpIGZ1bmN0aW9uLlxubXNlKHRyYWluJEVhcm5pbmdzLHByZWQudHJhaW4pXG5cbiMgNC4gUHJlZGljdCBvbiB0aGUgdGVzdGluZyBkYXRhLlxucHJlZC50ZXN0IDwtIHByZWRpY3QocHJlZC5tb2RlbCxuZXdkYXRhID0gdGVzdCkqMjAwMDBcblxuIyBBZGRpdGlvbmFsbHkgc2luY2UgaGVyZSB3ZSBoYXZlIHRoZSBhY3R1YWwvcmVhbCBwcmVkaWN0aW9uIHZhbHVlcyB3ZSBjYW4gYWxzbyBjaGVjayB0aGUgYWNjdXJhY3kgb2Ygb3VyIHByZWRpY3Rpb24gb24gdGVzdGluZyBkYXRhLlxubXNlKHRlc3QkRWFybmluZ3MscHJlZC50ZXN0KSJ9
</div>
<p><strong>NOTE</strong>: If you see a very high value of MSE after running the above code, please re-run it. Usually you will find the MSE to be better than all the models we have studied uptil now for the Earnings prediction problem.</p>
<p>We can see from the output,</p>
<ul>
<li>the summary of the output, neural network model, with details of:
<ul>
<li>Number of weights in the complete neural network</li>
<li>Initial Value and Final Value of the model weights along with iter value.</li>
<li>Structure of the neural network in I-H-O format where the numbers, I is input, H is hidden and O is output nodes.</li>
<li>Input node attributes.
<ul>
<li>Note that the Majors column attribute are split into unique number of factors, thus creating new individual attributes.</li>
</ul></li>
<li>Output node attributes.</li>
<li>Network Options.</li>
</ul></li>
<li>The mean squared error of the predicted values using training sub-dataset.</li>
<li>The mean squared error of the predicted values using the testing sub-dataset.</li>
</ul>
<hr />
<p>After Comparing all these models, we can see that the MSE values for the 3 models are SVM &gt; Random Forest &gt; Neural Network. This suggest one trend that, to get as best result as possible, one must invest most time in choosing the right model,and use the model with cleaned dataset for training.</p>
<p>Eventually, since we use R language here, the code for model creation just boils down to few lines of code, 4 steps to be more accurate. But since we might find one model works better than other, we must choose the best fit model.</p>
<p>Also, we saw the trend of time required for training of the models studied above was SVM &gt; RandomForest &gt; Neural Net. This also suggest a proportional relationship with the time required for a particular model to train and in turn producing the best possible results.</p>
<p>Although one can say that, we can just use Neural Networks all the time, well this statement is true to some extent, but depending on the resources, the complexity of the data, and the complexity of the model itself, one needs to make some trade-offs. One should not try to throw a ball just few meters with a cannon, using mere hands will do the job. In essence, do not try to overuse the neural net model for the sake of adding 2 mumbers, simple addition will suffice. Studying these tradeoffs and more models in depth though is out of this course scope.</p>
<hr />
<p><strong>EOC</strong></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="predblogs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdemo.html"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
