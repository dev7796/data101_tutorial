# Introduction {#intro}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->

<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->


<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->

<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(iris, 20), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->

<!-- You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015]. -->



The objective of this textbook is to provide you with the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in a statistically convincing manner, perhaps in a colorful and visually appealing way.

Questions which you will have to anticipate and you will have to answer are
- How do you know that your findings are not random?
- And fundamental of all questions:
- **So what?**

Even the most impressing looking results may come up randomly. And you will be asked this question along with the question *‚Äúwhat was your p-value and how did you compute it‚Äù*

And even if you convince your audience that your results are not random, you will have to be ready to explain why your audience should care about the results you reported. In other words, is there any actionable value in your results? Or they are just simply interesting, good to know, but no one really needs to care much about them otherwise? Hopefully it is the former not the latter.


In the following sections we will address these questions and go through the process of data exploration, validation, and presentation.

- We will start with making plots, follow with free style data exploration ‚Äì which allows us to form the leads, that is hypotheses. Then we will follow with simple statistical tests which will allow us to validate these hypothesis and defend our findings against randomness claims. - We will learn how to calculate p-values and how to use them to defend our findings. 
- We will use as few R commands as possible and reach our goal in the shortest possible path. In fact we will demonstrate how using just 7 R commands we can perform quite sophisticated data exploration.  In the appendix, we show many more useful commands of R which eventually you would have to use. However, our goal in this short textbook, is to present the  shortest path to data analysis which will let you import the data, plot it, make some analysis yourself and use R-libraries.  In this textbook and in this class we do not teach how to clean the data (data wrangling) and how to deal with a wide variety of data types. We also do not address complex data transformations such as multi-frame operations like merge (we show them in appendix).  We also do not explain how different machine learning methods work, we only show you how to use them. It is similar to teaching one how to drive a car without knowing how a car engine works.


<!--chapter:end:chapters/intro.Rmd-->

# üîñ Best DataBlog of 2022 {#b2022}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt12"> Ella Walmsley </button> 
<div id="bt12" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best1.pdf&embedded=true" width="100%" height="500px"></embed>
</div>



<!--chapter:end:chapters/best_work_2022.Rmd-->

# üîñ Data League Leaderboard {#DLL}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/chapters/Leaderboard'2022.csv")
# head(moody)
temp<-knitr::kable(
  moody[1:nrow(moody),10], caption = 'Leaderboard 2022',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<!--chapter:end:chapters/Leaderboard.Rmd-->

# üîñ Data puzzles secrets

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#freestyle12">Data Exploration</button>
<div id="freestyle12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1wvWxu_DEEoYNq0qKEkepbqu2-x0SGsXGQiE2iWbv2Iw/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

## Moody Data Puzzle

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv") #web load
# head(moody)
temp<-knitr::kable(
  head(moody, 5), caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
Moody Data Puzzle is our first example of a data puzzle. By data puzzles we mean synthetically generated data sets which have some embedded patterns. Your goal is to find the embedded pattern(s). You may also find patterns similar  (implied) by patterns embedded in the data puzzle. This is fine too. The goal of data puzzles is to excite you about exploratory data analysis. In many ways it is like a game.

**Puzzle description:**

Professor Moody has been teaching statistics 101 class for many years. His teaching evaluations went considerably south with the chief complaint:  he DOES NOT seem to assign grades fairly.   Students compared their scores among themselves and found quite a bit of discrepancies! But their complaints went nowhere since Professor promptly disappeared after posting the final grades and scores.

A new brave TA,  managed to get hold of the carefully maintained grading table (spanning multiple years) of professor Moody by ‚Ä¶.messing a bit with Moody's computer‚Ä¶.well,  let's not explain the details because he would get in trouble. What he found out was a remarkably structured account of how professor Moody assigns his grades.  

Looks like Professor Moody is in fact very alert in class. He is aware of  what students do,  detecting texting during class and remembering exactly who was dozed off in  class.  He also keeps the mysterious "participation index" which is a numerical score from 0 to 1. This is probably related to questions asked and answered by students as well as their general attentiveness in class.  Remarkable but a little creepy, isn't it?

What is the best advice the new TA, can give future students how to get a good grade in Professor Moody's class? What factors influence the grade besides the score? Back your recommendation up with plots and evidence from the attached data.

**What are examples of patterns we are looking for here?**   

Here are some:

- ‚ÄúStudents who text a lot‚Äù have lower chance to get an A in the class‚Äù
- ‚ÄúStudents whose participation is lower than 0.25 fail the class more often‚Äù
- ‚ÄúDozing off does not matter if your score is more than 90, you still get an A‚Äù
- ‚ÄúIf you score is less than 30, you fail the class regardless of what your other attributes are‚Äù

### Secrets Revealed- Patterns in Professor Moody's data? 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MDP12">My Patterns</button>
<div id="MDP12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/HW3-Moody-My-patterns.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
<br>
Many student solutions falsely attribute higher grades to higher values of participation attribute..  

This is a classic example of a hidden variable described in the reference attached below. 

The truth is that participation attribute value impacts the  score  attribute value. Generally, the higher the participation in Moody‚Äôs class, the higher the score.  But it is the  score attribute which has a direct impact on the grade. Thus, it is the score which is  the real "hidden variable" impacting  the final grade. 

Thus, the score already reflects participation. Professor Moody seemed to look only at texting and dozing off attributes in grade determination (see the power points above  with the explanation) 


Compare with examples of hidden variables in the following reference about correlation and causation.

https://www.stewartmath.com/precalc_7e_dp/precalc_7e_dp6.html


### Best Student's Submissions 2022

<button class="btn btn-primary" data-toggle="collapse" data-target="#PPT2021">Lauretta Martin</button>
<button class="btn btn-primary" data-toggle="collapse" data-target="#PPT2020">Sanjaya Budhathoki</button>
<button class="btn btn-primary" data-toggle="collapse" data-target="#PPT2019">Sandhya Senthilkumar</button>

<div id="PPT2021" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/LM-HW3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="PPT2020" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/SB-HW3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="PPT2019" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/HW3-SS.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
<!--
#### Snippet 1

```{r,tut=TRUE,height=300}
moody2022 <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")


```

#### Snippet 2

```{r,tut=TRUE,height=300}
moody2022 <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

```

#### Snippet 3

```{r,tut=TRUE,height=300}
moody2022 <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

```

#### Snippet 4

```{r,tut=TRUE,height=300}
moody2022 <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

```

#### Snippet 5

```{r,tut=TRUE,height=300}
moody2022 <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

```
-->


<!--
#### Snippet 1

Lets look at the general distribution of Grades and scores

```{r,tut=TRUE,height=400}
moody2022 <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

#Barplot General Grade distribution
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(moody2022$GRADE), col = colors, main = 'General Grade Distribution',
        ylab = 'Number of students', xlab = 'Letter Grade')

#Boxplot grade distribution by score
colors<- c('red','blue','cyan','yellow','green', 'orange')
boxplot(SCORE~GRADE, data=moody2022, xlab = "Letter Grade", ylab="Score", 
        main="Distribution of Scores by Grade", col = colors, border = "black")

```

#### Snippet 2

Does Texting affect grades?

```{r,tut=TRUE,height=600}
moody2022 <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

#barplot using varibale always TEXTING_IN_CLASS
alwaysTexting <- moody2022[which(moody2022$TEXTING_IN_CLASS == 'always'), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(alwaysTexting$GRADE), col = colors, 
        main = 'Grade Distribution for student always texting', xlab = 'letter Grade',
        ylab = 'Number of Students')

#barplot using varibale sometimes TEXTING_IN_CLASS
sometimesTexting <- moody2022[which(moody2022$TEXTING_IN_CLASS == 'sometimes'), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(sometimesTexting$GRADE), col = colors, 
        main = 'Grade Distribution for student sometimes texting', xlab = 'letter Grade',
        ylab = 'Number of Students')

#barplot using varibale rarely TEXTING_IN_CLASS
rarelyTexting <- moody2022[which(moody2022$TEXTING_IN_CLASS == 'rarely'), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(rarelyTexting$GRADE), col = colors, 
        main = 'Grade Distribution for student rarely texting', xlab = 'letter Grade',
        ylab = 'Number of Students')

#barplot using varibale never TEXTING_IN_CLASS
neverTexting <- moody2022[which(moody2022$TEXTING_IN_CLASS == 'never'), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(neverTexting$GRADE), col = colors, 
        main = 'Grade Distribution for student never texting', xlab = 'letter Grade',
        ylab = 'Number of Students')

```

#### Snippet 3

Does Dozing off affect grades?

```{r,tut=TRUE,height=600}
moody2022 <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

#barplot using varibale always Dozing_off
alwaysDozes <- moody2022[which(moody2022$DOZES_OFF == 'always'), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(alwaysDozes$GRADE), col = colors, 
        main = 'Grade Distribution for student always Dozing', xlab = 'letter Grade',
        ylab = 'Number of Students')

#barplot using varibale rarely Dozing_off
rarelyDozes <- moody2022[which(moody2022$DOZES_OFF == 'rarely'), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(rarelyDozes$GRADE), col = colors, 
        main = 'Grade Distribution for student rarely Dozing', xlab = 'letter Grade',
        ylab = 'Number of Students')

#barplot using varibale sometimes Dozing_off
sometimesDozes <- moody2022[which(moody2022$DOZES_OFF == 'sometimes'), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(sometimesDozes$GRADE), col = colors, 
        main = 'Grade Distribution for student sometimes Dozing', xlab = 'letter Grade',
        ylab = 'Number of Students')

#barplot using varibale nevers Dozing_off
neverDozes <- moody2022[which(moody2022$DOZES_OFF == 'never'), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(neverDozes$GRADE), col = colors, 
        main = 'Grade Distribution for student never Dozing', xlab = 'letter Grade',
        ylab = 'Number of Students')


```

#### Snippet 4

Does Particiaption affect grades?

```{r,tut=TRUE,height=500}
moody2022 <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

#barplot using varibale 0-0.5 Participation
lessParticipation <- moody2022[which(moody2022$PARTICIPATION >= 0 & moody2022$PARTICIPATION < 0.5), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(lessParticipation$GRADE), col = colors, 
        main = 'Grade Distribution for student with low Participation', xlab = 'Letter Grade',
        ylab = 'Number of Students')

#barplot using varibale 0.5-1 Participation
highParticipation <- moody2022[which(moody2022$PARTICIPATION >= 0.5 & moody2022$PARTICIPATION <= 1), ]
colors<- c('red','blue','cyan','yellow','green', 'orange')
barplot(table(highParticipation$GRADE), col = colors, 
        main = 'Grade Distribution for student with high Participation', xlab = 'Letter Grade',
        ylab = 'Number of Students')
```
-->


<!--
#### Snippet 1

Does participation affect grade or score?

```{r,tut=TRUE,height=400}
moody <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

barplot(table(moody$GRADE), col=rainbow(5), main="Bar Plot of Grades", xlab = "Grades", ylab = "Number of Students", 
        border=NA, ylim=c(0, 200))


boxplot(moody$PARTICIPATION~moody$GRADE, main = "Participation vs. Grade", xlab = "Participation", ylab = "Grade", 
        col = rainbow(5))


boxplot(moody$SCORE~moody$GRADE, main = "Scores vs. Grades", col = rainbow(5), xlab = "Grades", 
        ylab = "Number of Students", ylim=c(0, 120))


plot(moody$PARTICIPATION, moody$SCORE, main = "Participation vs. Score", xlab = "Participation", ylab = "Score", 
     col = "blue")

```

#### Snippet 2

Does Dozing off affect grades?

```{r,tut=TRUE,height=400}
moody <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

barplot(table(moody$GRADE[moody$DOZES_OFF == "never"]), col=rainbow(5), main="Grades vs. Never Sleeping in Class", 
        xlab = "Grades", ylab = "Number of Students", border=NA, ylim=c(0, 200))

barplot(table(moody$GRADE[moody$DOZES_OFF == "sometimes"]), col=rainbow(5), main="Grades vs. Sometimes Sleeping in Class", 
        xlab = "Grades", ylab = "Number of Students", border=NA, ylim=c(0, 200))

barplot(table(moody$GRADE[moody$DOZES_OFF == "always"]), col=rainbow(5), main="Grades vs. Always Sleeping in Class", 
        xlab = "Grades", ylab = "Number of Students", border=NA, ylim=c(0, 200))

```

#### Snippet 3

Does Texting in class affect grades?

```{r,tut=TRUE,height=400}
moody <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

barplot(table(moody$GRADE[moody$TEXTING_IN_CLASS == "never"]), col=rainbow(5), main="Grades vs. Never Texting in Class", 
        xlab = "Grades", ylab = "Number of Students", border=NA, ylim=c(0, 200))

barplot(table(moody$GRADE[moody$TEXTING_IN_CLASS == "rarely"]), col=rainbow(5), main="Grades vs. Rarely Texting in Class", 
        xlab = "Grades", ylab = "Number of Students", border=NA, ylim=c(0, 200))

barplot(table(moody$GRADE[moody$TEXTING_IN_CLASS == "always"]), col=rainbow(5), main="Grades vs. Always Texting in Class", 
        xlab = "Grades", ylab = "Number of Students", border=NA, ylim=c(0, 200))

```
-->

## Movies Data Hunt

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
**Puzzle description:**

contains  imdb scores of 12,800+ movies  along with several attributes including budget, gross  genre, content rating etc.

What are the most promising  alternative hypotheses  about imdb scores to test?  Name your three top candidates along with the evidence which backs them up:  either in the form of R instruction(s)  or plot.

### Secrets Revealed- Patterns in Movies data? 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MV12">Secrets Revealed</button>
<div id="MV12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/profhw4.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

### Best Student's Submissions 2022

<button class="btn btn-primary" data-toggle="collapse" data-target="#hw41">Joshua Sze</button>
<button class="btn btn-primary" data-toggle="collapse" data-target="#hw42">Andrew Fasano</button>

<div id="hw41" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/student1hw4.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="hw42" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/studen2hw4.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

## Minimarket Data Hunt 

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Minimarket.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Minimarket Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
**Puzzle description:**

Each row of Minimarket.csv contains one customer transaction which is represented as binary vector (treat this as NUM values). 1 means that customer bought an item, 0 - means that customer did not but that item. For example if customer bought Bread but did not buy Butter you will see 1 in the Bread column and 0 in the Butter column.

**Summary**

Here's what you'd do: <br>
1. Come up with a null hypothesis: "Bread does not impact the sales of butter" <br>
2. Come up with an alternative hypothesis: "Bread impacts the sale of butter" <br>
3. Compute the mean value of the Butter column for all the rows where Bread value = 0. Let's say this is mean1.<br>
4. Compute the mean value of the Butter column for all the rows where Bread value = 1. Let's say this is mean2.


### What were the secret associations  between  items in the minimarket?

<button class="btn btn-primary" data-toggle="collapse" data-target="#MV12">Secrets Revealed</button>
<div id="MV12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ecrets-revealed-prediction-minimarket.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

## Predicting grades in Professor Moody's class



### How did I cook the Professor Moody Prediction challenge data? 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MV13">Secrets Revealed</button>
<div id="MV13" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/Secrets-revealed-prediction-challenge.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/FreeStyle.Rmd-->

# Setting Up R 

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

- **Important Instructions**
  - Installation of R is required before installing RStudio
    - "R‚Äù is a programming language, and,
    - ‚ÄúRStudio‚Äù is an Integrated Development Environment (IDE) which provides you a platform to code in R.

- __How to download and install R & RStudio?__

  - _Downloading and installing R._

    - For Windows Users.
      - Click on the link provided below or copy paste it on your favourite browser and go to the website.
          - [https://cran.r-project.org/bin/windows/base/](https://cran.r-project.org/bin/windows/base/)
      - Click on the link at top left where it says ‚ÄúDownload R 4.0.3 for windows‚Äù or the latest at the time of your installation.
      - Open the downloaded file and follow the instructions as it is.

    - For MAC Users.
      - Click on the link provided below or copy paste it on your favourite browser and go to the website.
          - [https://cloud.r-project.org/bin/macosx/](https://cloud.r-project.org/bin/macosx/)
      - Under ‚ÄúLatest release‚Äù, click on ‚ÄúR-4.0.3.pkg‚Äù or the latest at the time of your installation.
      - Open the downloaded file and follow the instructions as it is.
      
 
  - _Downloading and installing RStudio._
  
    - For Windows Users.
      - Click on the link below or copy paste it in your favourite browser.
          - [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/)
      - Scroll down almost till the end of the web page until you find a section named ‚ÄúAll Installers‚Äù.
      - Click on the download link beside ‚ÄúWindows 10/8/7‚Äù to download the windows version of RStudio.
      - Install RStudio by clicking on the downloaded file and following the instructions as it is.

    - For MAC Users.
      - Click on the link below or copy paste it in your favourite browser.
          - [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/)
      - Scroll down almost till the end of the web page until you find a section named ‚ÄúAll Installers‚Äù.
      - Click on the link beside ‚ÄúmacOS 10.13+‚Äù to start your download the MAC version of RStudio.
      - Install RStudio by clicking on the downloaded file and following the instructions as it is.


---

## Create New Project {#setting}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->

<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->


<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->

<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(iris, 20), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->

<!-- You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015]. -->

After installing R studio successfully the first step is to create a project R studio. 

- Step 1: Go to **File -> New Project**

![New Project](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/1_text.png)

- Step 2: Select **New Directory**

![New Directory](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/2_text.png)

- Step 3: Select **New Project**

![New Project](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/3_text.png)

- Step 4: Give your preferred directory name like **"Data101_Assignmnets"**

![Directory Name](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/4_text.png)

- Step 5: Click on Create Project and finally the R studio should look like

![Rstudio](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/4.5_text.png)


---

## How to upload a data set?
<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive()

```

 - To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon.

- There are two options while accessing the dataset from your local machine:
  1. To avoid giving long directory paths for accessing the dataset, one should use the command **getwd()** to get the current working directory and store the dataset in the same directory. 
  
![Getwd](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/5_text.png)

- To access the dataset stored in the same directory one can use the following: **read.csv("MOODY_DATA.csv")**.

![Store the moody dataset in the same directory](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/6_text.png)

  2. One can also store the dataset at a different location and can access it using the following command: (Suppose the dataset is stored inside the folder Data101_Tutorials on the desktop)
  
    - For Windows Users.
      - Example: read.csv("C:/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv")

    - For MAC Users.
      - Example: read.csv("/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv")
      

**Note: **
The directory path given here is the current working directory hosted on *Github* where the dataset has been stored.
```{r, tut=TRUE}

# Read in the data
df <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

# Print out `df`
head(df)
```

---

## Saving your work

- To save your work go to **File -> Save**. It will ask you to give a name for your **.R file** and then click on Save.

![Save](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/7_text.png)

- After making modifications to your saved file, you will need to save the file again. 
If the name of the file on the top is in <span style="color: red;"> Red Color </span> indicates that the file have **unsaved** changes.

![Unsaved File](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/8_text.png)


- Go to **File -> Save** to save your .R file again. After saving the file the color of the file name i.e. **HW1.R** will again change back to **black**.

![Saved File](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/9_text.png)

**Note: ** You can create multiple files inside the same project such as for your each homework assignments

## General R References

https://www.w3schools.com/r/ <br>
https://cran.r-project.org/doc/contrib/Short-refcard.pdf <br>
https://www.amazon.com/Statistics-Engineers-Scientists-William-Navidi/dp/0073376337/ref=pd_lpo_3?pd_rd_i=0073376337&psc=1

## Textbook Concepts

- Hypothesis testing: \@ref(ztest), \@ref(ptest) 

- Difference of means hypothesis testing: \@ref(ptest)

- Null Hypothesis: \@ref(ztest)

- Alternative Hypothesis: \@ref(ztest)

- z-value: \@ref(ztest)

- critical value: \@ref(ztest)

- significance level: \@ref(ztest)

- p-value: \@ref(ztest)

- Bonferroni correction: \@ref(Mtest)

- Chi square test: \@ref(chitest)

- Independence: \@ref(chitest)

- Multiple Hypothesis testing: \@ref(Mtest)

- False Discovery Proportion: \@ref(Mtest)

- Contingency Matrix: \@ref(chitest)

- Bayesian Reasoning: \@ref(br)

- Prior odds: \@ref(br)

- Posterior odds: \@ref(br)

- Likelihood ratio: \@ref(br)

- False positive: \@ref(br)

- True positive: \@ref(br)

- Crossvalidation: \@ref(crossvalidation)

- Decision trees: \@ref(prpart)

- Linear regression: \@ref(lr)

- Recursive partitioning: \@ref(lr)

- MSE: \@ref(lr)

- Prediction accuracy: \@ref(lr)

- Training: \@ref(lr)

- Testing: \@ref(lr)


## R functions used in this class

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

- **Elementary instructions: ** c() \@ref(vector),  mean() \@ref(mean),   nrow() \@ref(nrow), rep(), sd() \@ref(sd), cut() \@ref(cut)

- **Plots: **  plot() \@ref(scartterplot), barplot() \@ref(barplot), boxplot() \@ref(boxplot)  mosaicplot() \@ref(mosaicplot)

- **Data Transformations: ** subset() \@ref(subset), tapply() \@ref(tapply),  table() \@ref(table), aggregate() 

- **Library functions: ** chisq.test() \@ref(chitest), pnorm() \@ref(pnorm), Permutation() \@ref(ptest), rpart() \@ref(prpart), predict() \@ref(rpartpredict), lm() \@ref(lm), crossvalidation() \@ref(crossvalidation)

- **Parameters of rpart: ** minsplit \@ref(rpartcontrol), minbucket \@ref(rpartcontrol), cp \@ref(rpartcontrol)



## Data sets

### Moody

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

summary(moody)
```

### Movies

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

summary(movies)
```

### Traffic

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
traffic<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')
# head(moody)
temp<-knitr::kable(
  traffic[sample(1:nrow(traffic),5), ], caption = 'Snippet of Traffic Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
traffic<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')

summary(traffic)
```

### Hindex

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
hindex<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv") #web load
# head(moody)
temp<-knitr::kable(
  hindex[sample(1:nrow(hindex),5), ], caption = 'Snippet of Hindex Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
hindex <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv")

summary(hindex)
```


### Prediction 1 Dataset

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Snippet of Moody Predicition 1 dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv")

summary(moody)
```

### Midterm, Project and Final Exam distribution in Prof. Moody class 

**Assumptions:** Midterm, Project and Final Exam are all out of 100

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Midterm, Project and Final Exam distribution in Prof. Moody class',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")

summary(moody)
```


### Minimarket

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Minimarket dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
minimarket<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

summary(minimarket)
```







<!--chapter:end:chapters/Setting_up_R.Rmd-->

# üîñ Plots {#plots}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#plots12">Plots</button>
<div id="plots12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1L7ml_mwV7ms3eZ2qbznGgNdarhTfrSP6PWq-RxNUgQM/edit?usp=sharing" width="100%" height="500px"></embed>
</div>
<!--
<button class="btn btn-primary" data-toggle="collapse" data-target="#plots11">Lecture PPT</button>
<div id="plots11" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/Plots-2022.pdf&embedded=true" width="100%" height="500px"></embed>
</div> -->

## Vector {#vector}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive( greedy = FALSE)

```
- A vector is simply a list of items that are of the same type.

<!-- To combine the list of items to a vector, use the **c()** function and separate the items by a comma.
- All of the items inside a vector to are of the same type like numerical or categorical. -->

### Snippet 1

Lets look at example of creating a vector:

```{r,tut=TRUE,height=300}

#Lets create 3 vectors with title, author and year.
color <- c('Red','Blue','Yellow','Green')

#Lets look at how the created vectors look.
color
```

### Snippet 2

Create a vector with numerical values in a sequence, use the **:** operator:

```{r,tut=TRUE,height=300}

#Lets create a vectors with numerical sequence.
year <- 2018:2022

#Lets look at how the created vectors look.
year

```
<!--
<br />

- To find out how many items a vector has, use the **length()** function:

```{r,tut=TRUE,height=300}

#Lets create a vectors with categorical values
author <- c('Foreman', 'John', 'Said')

# You can access the vector items by referring to its index number inside brackets []. The first item has index 1, the second item has index 2, and so on:
author[1]

#Lets look at the size of a vectors.
length(author)

```

<br />

- To sort items in a vector alphabetically or numerically, use the **sort()** function and to change the value of a specific item, refer to the **index number**:

```{r,tut=TRUE,height=300}

#Lets create a vectors with categorical values
title <- c('Data Smart','Orientalism','False Impressions','Making Software')

#sorting titles
sort(title)

# change the title 'Data Smart' to 'Smart Data'
title[1] <- "Smart Data"

#Lets look at how the updated vectors look.
title 
```

-->

---

## Data Frames

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive( greedy = FALSE)

```
- Data Frames are data displayed in a format as a table.
<!--
- Data Frames can have different types of data inside it. While the first column can be character, the second and third can be numeric or logical. 

- Following are the characteristics of a data frame.
  - The column names should be non-empty.
  - The row names should be unique.
  - The data stored in a data frame can be of numeric, factor or character type.
  - Each column should contain same number of data items.

Use the **data.frame()** function to create a data frame:-->

### Snippet 1

Populating the dataframe:

```{r,tut=TRUE,height=300}

# Load the dataset into the moody variable
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

# Now lets view the dataframe moody with just 5-6 tuples
head(moody)
```

### Snippet 2 

Get the summary of the dataframe:

```{r,tut=TRUE,height=300}

# Load the dataset into the moody variable
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

# Use the summary() function to summarize the data from a Data Frame:
summary(moody)
```
<!--
- Just like vectors, you can access specific data **(Slicing)** in dataframes using brackets. 
- But now, instead of just using one indexing vector, we use two indexing vectors: one for the rows and one for the columns. -->

### Snippet 3 

Use the notation **data[rows, columns]**, which selects the subsets of rows and columns:

```{r,tut=TRUE,height=500}

# Load the dataset into the moody variable
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

# Return row 1
moody[1, ]

# Return column 5
moody[, 5]

# Rows 1:5 and column 2
moody[1:5, 2]

# Give me rows 1-3 and columns 2 and 4 of moody
moody[1:3, c(2:4)]
```

---

## Table {#table}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive( greedy = FALSE)

```

- **table()** displays frequency distribution of its arguments. 

<!--
- Tables (i.e. frequency distribution table) can be created using **table()** along with some of its variations. 
- To use **table()**, simply add in the variables you want to tabulate separated by a comma. 
- You must reference the variable using **dataset$variable**. 
- By default, missing values are excluded from the counts; if you want a count for these missing values you must specify the argument **useNA=‚Äúifany‚Äù** or **useNA=‚Äúalways‚Äù**. -->
### Snippet 1

The below examples show how to use this function:

```{r, tut=TRUE,height=400}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load

#lets make a table for the grades of students and counts of students for each Grade. 
grades <- table(moody$grade)

#lets see the above frequency distrbuted tables
grades
```

### Code Review
#### What would R say?

```{r, tut=TRUE,height=400}
moody <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

table(moody[moody$questions!='always',]$grade)
#What will R say?

# A. error
# B. distribution of grades for students who always ask questions
# C. distribution of grades for students who do not always ask questions 
```

#### What would R say?

```{r, tut=TRUE,height=400}
moody <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

table(moody[moody$questions==('always','never'),]$grade)
#What will R say?

# A. error.
# B. distribution of grades for students who always or never ask questions.  
# C. distribution of grades for students who do not ask questions always or never. 
```

<br />

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
# head(moody)

temp<-knitr::kable(
  head(moody, 5), caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<!-- --- -->

<!-- ### Topics visited in this sub-chapter -->

<!-- * Scatter Plot -->
<!-- * Barplot -->
<!-- * Boxplot -->
<!-- * Mosaic Plot -->

---

## Scatter Plot {#scartterplot}

- Scatter Plot are used to plot two numerical variables.
- Hence it is used when both the labels are numerical values.


Lets look at example of scatter plot using Moody.

```{r,tut=TRUE,height=700}
# Let's look at a 2 attribute scatter plot.
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
plot(moody$participation,moody$score,ylab="score",xlab="participation",main=" Participation vs Score",col="red")


```

---

## Bar Plot {#barplot}

- A bar plot are used to plot a categorical variable. 
- This rectangle height is proportional to the value of the variable in the vector.
<!--
- Barplots are also used to graphically represent the distribution of a categorical variable, after converting the categorical vector into a table(i.e. frequency distribution table)
- In a bar plot, you can also give different colors to each bar. -->



```{r, tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#lets make a table for the grades of students and counts of students for each Grade. 

t<-table(moody$grade)

#once we have the table lets create a barplot for it.

barplot(t,xlab="Grade",ylab="Number of Students",col=colors, 
        main="Barplot for student grade distribution",border="black")
```


---

##  Box Plot {#boxplot}

- A boxplot is used to display a numerical variable.
- A boxplot shows the distribution of data in a dataset. 

![Boxplot](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/boxplot1.png)
<br />

- A boxplot shows the following things:
  - Minimum
  - Maximum
  - Median
  - First quartile
  - Third quartile
  - Outliers
  
<!--
- You can create a single boxplot using just a vector or a multiple boxplot using a formula.
- When you write a formula, you should use the Tilde (~) operator. This column name on the left side of this operator goes on the y axis and the column name on the right side of this operator goes on the x axis.-->



```{r,tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#Suppose you want to find the distribution of students score per Grade. We use box plot for getting that. 
boxplot(score~grade,data=moody,xlab="Grade",ylab="Score", main="Boxplot of grade vs score",col=colors,border="black")

# the circles represent outliers.
```


<!-- ## 4. Histogram -->

<!-- Refer Slide 15. -->

<!-- ```{r} -->

<!-- #Suppose you want to find the frequecy/distribution of cars with mileage in particular range. We use histogram for this.  -->

<!-- hist(automobile$`city-mpg`,xlim = c(0,100),xlab = 'milage', main = "Histogram of Car milage",col=colors,border="black") -->

<!-- # You can Change column range using breaks. -->

<!-- ``` -->


<!-- For more detail,reference and example refer Slides -->



---
##  Mosaic Plot {#mosaicplot}

- Mosaic plot is used to visualize two categorical variables.

<!--
- Mosaic plot is a graphical method for visualizing data from two or more qualitative variables.
- The length of the rectangles in the mosaic plot represents the frequency of that particular value.
- The width and length of the mosaic plot can be used to interpret the frequencies of the elements.
- For example, if you want to plot the number of individuals per letter grade using a smartphone, you want to look at a mosaic plot. -->


```{r,tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#suppose you want to find numbers of students with a particular grade based on their texting habits. Use Mosiac-plot.

mosaicplot(moody$grade~moody$texting,xlab = 'Grade',ylab = 'Texting habit', main = "Mosiac of grade vs texing habit in class",col=colors,border="black")


```

## Additional References

https://www.datamentor.io/r-programming/plot-function/

<!--
```
{r child="./chapters/datatransformation.Rmd"}
```
 
 -->

<!--chapter:end:chapters/plots.Rmd-->

# üîñ Data Transformation

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#subset12">Data Transformation</button>
<div id="subset12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1--AeLFVrESqyWO7iNiIBRW7pX-VSYtBco1o8bOpOatE/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

## Basic Functions

### mean() {#mean}

- **mean()** function is used to find the average of values in a numerical vector.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#Lets look at the mean of score column.
mean(moody$score)
```


### length()

- **length()** function is used to get the number of elements in any vector

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#Lets look at the length of the grade column 
length(moody$grade)
```

### max()

- **max()** function is used to get the maximum value in a numerical vector.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#lets look at the maximum value of the score in the score column
max(moody$score)
```

### min()

- **min()** function is used to get the minimum value in a numerical vector

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#Lets look at the minimum value of score in the score column.
min(moody$score)

```

### sd() {#sd}

- **sd()** function is used to find the standard deviation of numerical vector

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#Lets look at the standard deviation of score column
sd(moody$score)
```

---

## Subset {#subset}

### Snippet 1- example of subset function {#nrow}

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
#Subset of rows
moody_never_smartphone<-subset(moody,ON_SMARTPHONE=="never")
nrow(moody)
nrow(moody_never_smartphone)

```

### Snippet 2- example of  subset  function

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
#Subset of rows
moody1<-subset(moody,ON_SMARTPHONE=="never")
# You can see only student never on smartphone are in the subset.
table(moody1$ON_SMARTPHONE) 

```

### Snippet 3- subset as subframe 

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
#Alternate way to subset.
moody2<-moody[moody$ON_SMARTPHONE=="never", ]
# You can see a similar table as above.
table(moody2$ON_SMARTPHONE) 

```

### Snippet 4- subsetting columns

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
colnames(moody)
#subset of columns
moody3<-subset(moody, select = -c(1))
ncol(moody3)
# You can see the number of columns has been reduced by 1, due to sub-setting without column 1
ncol(moody3)

```

### Snippet 5- sub-setting rows and columns

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
#Subset of Rows and Columns
moody1<-subset(moody, select = c(2:4), ON_SMARTPHONE == "never")
colnames(moody1)
#Notice that only 3 columns are remaining
dim(moody1)

```

### Code Review
#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
moody[moody$SCORE>=90,3]
# What will R say?


# A. Get subset of all columns which contains students who scored more than equal to 90
# B. error
# C. get all score values which are more than equal to 90
# D. get subset of only the grades of students with score greater than equal to 90


```

#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

moody[moody$SCORE>=80.0 & moody$GRADE =='B',] 
# What will R say?

# A. subset of moody data frame who got B grade.
# B. error.
# C. subset of moody data frame with score greater than 80.
# D. subset of moody data frame with score more than 80 and got B grade.


```

---

## tapply {#tapply}

- **tapply()** computes a measure (mean, median, min, max, etc..) or a function for each factor variable in a vector. It is a very useful function that lets you create a subset of a vector and then apply some functions to each of the subset.
- tapply(numerical, categorical, aggregagte function)

### Snippet 1-  Example of tapply followed by barplot

```{r,height=700}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")


# To apply tapply() on SCORE factored on ON_SMARTPHONE

moody1<-tapply(moody$SCORE,moody$ON_SMARTPHONE,mean)
moody1 # We can see it calculated mean value of the score by students with respect to their use of phone in class.

barplot(moody1,col = "cyan",xlab = "Labels", ylab = "mean_val",main = "tapply() example 1",las = 2, cex.names = 0.75)#plot

```
<!--
### Snippet 2 (*)

```{r,height=700}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

#Lets factor the grades on on_smartphone as well as grade category.
moody2<-tapply(moody$GRADE,list(moody$ON_SMARTPHONE,moody$GRADE),length)
# We can see it calculated count of the grade of student with respect to their in-class smartphone usage  and grade category.
moody2
barplot(moody2,col=c("red","cyan","orange","blue"),main = "tapply() example 2",beside = TRUE,legend=rownames(moody2))
```
-->
### Code Review
#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

tapply(moody, GRADE, SCORE, min)
# What will R say?

# A. minimum score for each grade
# B. minimum grade for each score
# C. minimum grade only 
# D. Error.


```
<!--
#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

tapply(moody$GRADE, list(moody$ON_SMARTPHONE,moody$GRADE), length)
# What will R say?

# A. Total no. of students grade for each values of on_smartphone attribute
# B. mean value of on_smartphone attribute for each grade
# C. mean category of on_smartphone only 
# D. error.
```
-->
---

## Derived Attribute

R allows creating new data frame attributes (columns) ‚Äúon the fly‚Äù.  These are new vectors, which are often defined as functions of existing attributes. Hence, the name - derived  attributes. 

Derived attributes will play an important role  in data exploration as well as in building prediction models. Very often, derived attributes allow discovery of important patterns in data. Similarly, derived attributes may be more predictive than original attributes in the imported data sets.

The term feature engineering is often used in machine learning to describe creation of derived attributes.

### Snippet 1  - Making  new categorical attribute. 
 
The  line 4 initializes the new attribute PF (Pass/Fail) to "Pass".  The line 5 replaces "Pass" by ‚ÄúFail‚Äù  for students who received F. This new attribute, PF, will allow exploratory analysis to find ‚ÄúHow to pass Professor Moody‚Äôs class‚Äù. The answer to this question may be different than then answer to ‚ÄúHow to get a good grade in Professor Moody‚Äôs class‚Äù.


```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

# Cut Example using breaks - Cutting data using defined vector. 
moody$PF<-'Pass'
moody[moody$GRADE=='F',]$PF<-'Fail'

# lets see our added column PF
moody

```



### Cut 

- **cut()** function  divides the range of x into intervals. Provides ability to label intervals as well. It plays important role in defining derived attributes from attributes which are numerical.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

# Cut Example using breaks - Cutting data using defined vector. 
score1 <- cut(moody$SCORE,breaks=c(0,50,100),labels=c("F","P"))
table(score1)

```

### Code Review
#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

cut(moody$SCORE, breaks=c(0,25,70,100),labels=c("low", "medium", "high"))
#What would R say?

# A. 5 intervals of attribute score
# B. 3 intervals (0,25) (25,70) (75,100)
# C. 3 categorical values "low", "medium" and "high" for different score intervals
# D. 3 separate datasets with similar score values

```

#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

output<-cut(moody$SCORE, 5)
summary(output)
#What would R say?

# A. 5 intervals of attribute score of unequal count of elements
# B. 5 intervals of attribute score of equal count of elements
# C. 5 categorical values for different score intervals
# D. 5 separate dataset with similar score values

```

#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

output<-cut(moody$ASKS_QUESTIONS, 2)
summary(output)
#What would R say?

# A. 2 intervals of attribute ask_questions of unequal count of elements in each interval
# B. 2 intervals of attribute ask_questions of equal count of elements in each interval
# C. 2 categorical values for different ask_questions intervals
# D. Error.

```

#### More complex  example of defining derived attributes

The next snippet  illustrates defining a new numerical attribute, $adjustedScore of a student in the Moody data frame.  

Score is adjusted by the value of participation attribute in the following way:   

- If participation is larger than 0.5 - a bonus proportional to participation * 10 is added to the score.  

- If participation is smaller than 0.5, a penalty of 1-participation) * 10 is subtracted from the score. 

In this way, for someone with very small participation, the 10 point penalty will be imposed (10 points subtracted from the score). Conversely,  someone with perfect participation (1.0) will receive a 10 point bonus. 

##### Snippet 1

```{r,height=700}

moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")


moody$conditional <-0
moody[moody$participation<0.50, ]$conditional <- moody[moody$participation<0.50, ]$score -10*(1-moody[moody$participation<0.50, ]$participation)
moody[moody$participation>=0.50, ]$conditional <- moody[moody$participation>=0.50, ]$score +10*moody[moody$participation>=0.50, ]$participation

# print the column names
colnames(moody)

# lets look at the conditional attribute 
head(moody)

#subset the moody dataset rows = 1 to 10 and cols = 1,5
moody[1:10, c(1,5)]

#subset the moody dataset rows = 1 to 10 and cols = 1,5,6
moody[1:10, c(1,5,6)]

# print summary of inidividual columns
summary(moody$score)
summary(moody$conditional)

# Plotting the conditional attribute using boxplot
boxplot(moody$conditional,col = c("red"),main="Complex Example")

# Plotting the score attribute using boxplot
boxplot(moody$score,col = c("blue"),main="Complex Example")

```

<!--chapter:end:chapters/tapply_subsetting.Rmd-->

# üîñ Hypothesis Testing: z-test {#ztest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#HPT12">Hypothesis Testing</button> <button class="btn btn-primary" data-toggle="collapse" data-target="#CLT12">Central Limit Theorem</button>
<div id="HPT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/17zX82imn7S3_r9Uls3pqInIwdPx1NIQW-O22ynSa9Sg/edit?usp=sharing" width="100%" height="500px"></embed>
</div>
<div id="CLT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/15VRfffVukRxQ8V9Oh_KRG8T7dIuPjlXaYySdcqvXryQ/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<br>

- **Great Resource: **     <a href = "https://www.khanacademy.org/math/statistics-probability/significance-tests-confidence-intervals-two-samples/comparing-two-means/v/hypothesis-test-for-difference-of-means" target="_blank"> <button class="btn btn-primary" data-toggle="collapse" data-target="#KH12">Khan Academy</button> </a>

## Introduction

The following synthetic data describes daily traffic on weekday and weekend days in Lincoln and Holland tunnels. The data frame has three attributes: TUNNEL, DAY and VOLUME_PER_MINUTE.  Below we show a small sample of the TRAFFIC data frame

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
traffic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv") #web load
# head(moody)
temp<-knitr::kable(
  traffic[sample(1:nrow(traffic),10), ], caption = 'Snippet of Traffic Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>

The following snippet **6.2** shows the code for hypothesis test of difference of means.

Is the mean traffic (VOLUME_PER_MINUTE)  in the Holland tunnel bigger than  mean  traffic (VOLUME_PER_MINUTE)  in the Lincoln? 

## Snippet 1: Shows the code for z-test to test the following hypotheses{#pnorm}

**Null Hypothesis** - Traffic in Holland tunnel is the same as traffic in Lincoln tunnel.

**Alternative Hypothesis** - Traffic in the Holland Tunnel is larger than traffic in the Lincoln  tunnel.

In the snippet **6.2**  we end up calculating the p-value which leads to rejection of Null hypothesis (good news for data scientist, bad for the sceptic).  Indeed, p-value is less than the significance level  of 5%.
This means, that under null hypothesis it is extremely unlikely (less than 5% chance) to see the result which is at least as big as the  observed difference of means.

```{r,tut=TRUE,height=850}

TRAFFIC<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')

#data clean and subset, either
lincoln.data <- subset(TRAFFIC, TRAFFIC$TUNNEL == "Lincoln")
holland.data <- subset(TRAFFIC, TRAFFIC$TUNNEL == "Holland")

#traffic at lincoln
lincoln.traffic <- lincoln.data$VOLUME_PER_MINUTE

#traffic at holland
holland.traffic <- holland.data$VOLUME_PER_MINUTE

# standard deviation of two samples.
sd.lincoln <- sd(lincoln.traffic)
sd.holland <- sd(holland.traffic)
sd.lincoln
sd.holland

#length of lincoln and holland
len_lincoln <- length(lincoln.traffic)
len_holland <- length(holland.traffic)
len_lincoln
len_holland

#standard deviation of difference traffic
sd.lin.hol <- sqrt(sd.lincoln^2/len_lincoln + sd.holland^2/len_holland)
sd.lin.hol

#means of two samples
mean.lincoln <- mean(lincoln.traffic)
mean.holland <- mean(holland.traffic)
mean.lincoln
mean.holland

#z score
zeta <- (mean.lincoln - mean.holland)/sd.lin.hol
zeta

#plot red line
plot(x=seq(from = -5, to= 5, by=0.1),y=dnorm(seq(from = -5, to= 5,  by=0.1),mean=0),type='l',xlab = 'mean difference',  ylab='possibility')
abline(v=zeta, col='red')

#get p
p = 1-pnorm(zeta)
p
```

## Snippet 2: Make your own data and see how p-value changes

How p-value is affected by difference of means and standard deviations.

We will build two distributions ourselves - varying the means and standard deviations. We will use **rnorm()** to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. You can do it by changing values of mean and standard deviation in the **rnorm()** function.

Clearly the further apart the mean values are - the lower the p-value. But how do standard deviations affect the p-value?  See for yourself.

Build the data frame with two attributes: Cat and Val, using **rnorm()** function

```{r,tut=TRUE,height=700}
Val1<-rnorm(10,mean=25, sd=10)
Val2<-rnorm(10,mean=35, sd=10)
Cat1<-rep("GroupA",10)  
Cat2<-rep("GroupB",10)  
Cat<-c(Cat1,Cat2) 
Val<-c(Val1,Val2)

d<-data.frame(Cat,Val)
Observed_Difference<-mean(d[d$Cat=='GroupB',2])-mean(d[d$Cat=='GroupA',2])
Observed_Difference

dA<-d[d$Cat=='GroupA',]
dA
dB<-d[d$Cat=='GroupB',]
dB

meanA<-mean(dA$Val)
meanB<-mean(dB$Val)

sdA<-sd(dA$Val)
sdB<-sd(dB$Val)

lenA<-nrow(dA)
lenA
lenB<-nrow(dB)
lenB

sd.A.B <- sqrt(sdA^2/lenA +sdB^2/lenB)
sd.A.B

zeta <- (meanB - meanA)/sd.A.B
zeta
p<-1-pnorm(zeta)
p
```

## Additional References

https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/p-value/ <br>
http://www.z-table.com/ <br>
https://www.statisticshowto.com/probability-and-statistics/z-score/ <br>
https://sixsigmastudyguide.com/z-scores-z-table-z-transformations/

<!--chapter:end:chapters/z-test.Rmd-->

# üîñ Hypothesis Testing: Permutation Test {#ptest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#PT12">Permutation Test</button> 
<div id="PT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1Dat8r1wUbNzFCqbjxj_jbH8T11BFwmOJOxK5T_mbnUE/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

## Snippet 1

**Exercise** - How p-value is affected by difference of means and standard deviations

We will build our two distributions ourseleves - varying the means and standard deviations.  We will use **rnorm()** to generate normal distributions with given means and standard deviations. Then we will use permutation test (can be z-test as well) to test difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values.

Build the data frame with two attributes: **Cat** and **Val**, using **rnorm()** function

```{r,tut=TRUE,height=600}
Val1<-rnorm(10,mean=25, sd=10)
Val2<-rnorm(10,mean=30, sd=10)
 
Cat1<-rep("GroupA",10)  # for example GroupA can be Holland Tunnel
Cat2<-rep("GroupB",10)  # for example Group B will be Lincoln Tunnel

Cat1
Cat2

#The rep command will repeat, the variables will be of type character and will contain 10 values each.

Cat<-c(Cat1,Cat2) # A variable with first 10 values GroupA and next 10 values GroupB
Cat

Val<-c(Val1,Val2)
Val

d<-data.frame(Cat,Val)
d

Observed_Difference<-mean(d[d$Cat=='GroupB',2])-mean(d[d$Cat=='GroupA',2])
Observed_Difference

#This will calculate the mean of the second column (having 10 random values for each group), and the mean of groupB values is subtracted from the mean of groupA values, which will give you the value of the difference of the mean.
 
 #Try changing mean and sd values. When you run this you will see that the difference is sometimes negative #or sometimes positive.
```

## Snippet 2

Do this in your R studio, since we cannot install our package in data camp service we are using to run the code snippets

```{r,tut=TRUE,ex="permutationtestfunction",type="pre-exercise-code"}
traffic<-read.csv('https://raw.githubusercontent.com/kunal0895/RDatasets/master/TRAFFIC.csv')
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```


```{r,tut=TRUE,ex="permutationtestfunction",type="sample-code",height=700}
#install.packages("devtools")
#devtools::install_github("devanshagr/PermutationTestSecond")

#PermutationTestSecond::Permutation(d, "Cat", "Val",10000, "GroupA", "GroupB")
Permutation(traffic, "TUNNEL", "VOLUME_PER_MINUTE",1000,"Holland", "Lincoln")
 
 #The Permutation function returns the absolute value of the difference. So the red line is the absolute value of the observed difference. You will see a histogram having a normal distribution with a red showing the observed difference.
```

## Snippet 3

One permutation at a time

```{r,tut=TRUE,height=400}
traffic<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')

ranNum <- sample(1:nrow(traffic),nrow(traffic))
ranNum[1:5]

VOLUME_PER_MINUTE<-traffic$VOLUME_PER_MINUTE[ranNum]
TUNNEL<-traffic$TUNNEL

Permuted_traffic<-data.frame(TUNNEL, VOLUME_PER_MINUTE)

mean(traffic[traffic$TUNNEL=='Lincoln', ]$VOLUME_PER_MINUTE) -mean(traffic[traffic$TUNNEL=='Holland', ]$VOLUME_PER_MINUTE)

mean(Permuted_traffic[Permuted_traffic$TUNNEL=='Lincoln', ]$VOLUME_PER_MINUTE)-mean(Permuted_traffic[Permuted_traffic$TUNNEL=='Holland', ]$VOLUME_PER_MINUTE)
```

<!--chapter:end:chapters/Permutaion_Test.Rmd-->

# üîñ Chi Square Analysis {#chitest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#CS12">Chi Square</button> 
<div id="CS12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1h-h2S5lW6ReFwdeJKNflPpE0iCjoS08T0mpgSLiFv88/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<br>

- **Great Resource: **     <a href = "https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-goodness-fit/v/chi-square-statistic" target="_blank"> <button class="btn btn-primary" data-toggle="collapse" data-target="#KH12">Khan Academy</button> </a>

## Snippet 1

```{r,tut=TRUE,height=300}
Expected <-matrix(c(200,420,180, 40,120,40), nrow=3, ncol=2)
Observed<-matrix(c(200,420,180,35,120,45), nrow=3, ncol=2)
Expected
Observed
chisq.test(Observed)
```

## Snippet 2

```{r,tut=TRUE,height=300}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")
moody$IN<-'Out_Slice'
moody[moody$DOZES_OFF=='never' & moody$TEXTING_IN_CLASS=='always', ]$IN<-'In_Slice'
d<-table(moody$GRADE, moody$IN)
d
chisq.test(d)
```

## Snippet 3


```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")
data<-table(movies$content, movies$genre)
chisq.test(data)
```


<!--chapter:end:chapters/Chisquare.Rmd-->

# üîñ Multiple Hypothesis Testing {#Mtest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#MPT12"> Multiple Hypothesis Testing</button> 
<div id="MPT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1dCbhnuGMsXYJEltQXUfPhl8dCTAqJWCZ0xI-0IhnxAc/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
hindex<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv") #web load
# head(moody)
temp<-knitr::kable(
  hindex[sample(1:nrow(hindex),10), ], caption = 'Snippet of Hindex Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

## Snippet 1 - Benjamini-Hochberg Algorithm

```{r,tut=TRUE,height=450}
p<-sort(round(runif(100, min=0, max=0.05), 4))
p
p<-p+0.0003
p
#implement Benjamini-Hochberg formula
q<-rep(0.05,100)
q
r=c(1:100)
q<-round(q*r/100,4)
temp<-p<q
#Select p-values which correspond to discoveries (reject NULL)
maxindex<-max(which(temp=='TRUE'))
p[1:maxindex]
```

## Snippet 2

Happiness Index synthetic data set which is used in my slides for multiple hypotheses testing

- How to order by aggregate?

- First make data frame out of tapply? Use  aggregate  and list functions.

```{r,tut=TRUE,height=450}
Hindex <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv") #web load

Hindex<-aggregate(Hindex$HAPPINESS, list(Hindex$COUNTRY), mean)
colnames(Hindex)<- c("Country","AverageH")
#renames columns of the Hindex data frame
colnames(Hindex)

Hindex[order(Hindex$AverageH),]
```

## Additional References 

https://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/

<!--chapter:end:chapters/Multiple_Hypothesis.Rmd-->

# Code Review: Exploratory Queries in R {#cr}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Movies Dataset Example 
### Snippet 1: What is the mean imdb of low budget comedies?


```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

mean(movies[movies$Budget=='Low' & movies$genre=='Comedy', ]$imdb_score)

```

### Snippet 2: What is standard deviation of imdb score of  high gross Family movies?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

sd(movies[movies$Gross=='High' & movies$genre =='Family',]$imdb)

```

### Snippet 3: What is the lowest imdb score among high budget movies?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

min(movies[movies$Budget=='High',]$imdb)

```

### Snippet 4: How many low budget movies generated high gross income?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

nrow(movies[movies$Budget=='Low' & movies$Gross =='High',])

```

### Snippet 5: What is imdb score of the first non-US movie in the movies data frame?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

#You can use this simple command to quickly find out
head(movies[movies$country!='USA', ]$imdb_score)

```

### Snippet 6: What is the least frequent genre among UK movies?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

#You can use this code to find out
table(movies[movies$country=='UK',]$genre, movies[movies$country=='UK',]$country)

```

### Snippet 7: Which content rating has the lowest average imdb score?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

#You can use this code to find out
tapply(movies$imdb, movies$content, mean)

```

### Snippet 8: Movies from which country have the smallest average imdb score?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

#Better compute it, since there are too many countries for visual inspection
MA<-aggregate(movies$imdb_score, list(movies$country), mean)
colnames(MA)<-c("Country", "Mimdb")
MA<-MA[order(-MA$Mimdb), ]
MA[1,]
```

### Snippet 9: What is the least frequent genre in movies data frame?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

z<-table(movies$genre)
sort(z,decreasing=FALSE)[1]
```

### Snippet 10: z value = 2.4, whats the p-value?

```{r,tut=TRUE,height=300}

1-pnorm(2.4)

```

## Census Dataset Example

### Snippet 11: For the individual over 50, which profession has the highest average capital gain?
```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

age_greater_than_49 = subset(census_data, census_data$AGE >= 50)
aged_capitalgains = tapply(age_greater_than_49$CAPITALGAINS, age_greater_than_49$PROFESSION, mean)
aged_capitalgains

```

### Snippet 12: Which profession has the highest average capital gains; Sales or Tech-support?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example12_data = tapply(census_data$CAPITALGAIN, census_data$PROFESSION, mean)
example12_data
```

### Snippet 13: What is most frequent profession of people with less than 10 years od of education?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example13_data = table(subset(census_data, YEARS <= 10)$PROFESSION)
example13_data
```

### Snippet 14: What is minimum number of years of education for people with Exec-managerial specialty?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example14_data = min(subset(census_data, PROFESSION == "Exec-managerial")$YEARS)
example14_data
```

### Snippet 15: What is the most frequent degree for natives of the United States?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example15_data = table(subset(census_data, NATIVE == "United-States")$EDUCATION)
example15_data
```

### Snippet 16: What is the least frequent degree for people with at least 12 years of education?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example16_data = table(subset(census_data, YEARS >= 12)$EDUCATION)
example16_data
```


<!--chapter:end:chapters/code_review.Rmd-->

# üîñ Common Sense Judgement and Probability {#common}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#common12">Common Sense Judgement and Probability</button>
<div id="common12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1IuqMFWih6WuUAH2gCztfUjDl2SL7OUV5y7VyDs6_EGo/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/common_sense.Rmd-->

# üîñ Bayesian Reasoning {#br}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#BR12">Bayesian Reasoning</button> 
<div id="BR12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1gwI6y7yyqi8GWdPuS9dZlXsZs-1r1TsumQb-PL4Hu5s/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

## Snippet 1: Covid Odds after positive Home Test. 

```{r,tut=TRUE,height=600}
#Belief = "Have Covid"
#Observation = Covid Test
#How much the probability of having covid increases upon positive COVID-test?
#We use the odds formulation of Bayesian Theorem
# we begin with prior odds of having Covid:  P(Covid)/(1-P(Covid)
PriorHaveCovid<-0.01
PriorCovidOdds<-PriorHaveCovid/(1-PriorHaveCovid)
PriorCovidOdds
#True positive:  Probability of having positive Covid test when having covid  = P(PositiveCovidTest|HaveCovid)
TruePositive<-0.99
#False positive = Probability of having positive Covid test when not having covid = P(PostiveCovidTest/DoNotHaveCovid)
FalsePositive<-0.001
LikelihoodRatio<-TruePositive/FalsePositive
PosteriorCovidOdds<-LikelihoodRatio*PriorCovidOdds
PosteriorHaveCovid<- PosteriorCovidOdds/(1+PosteriorCovidOdds)
PosteriorHaveCovid
```

## Snippet 2: What are the odds that an 'F' student is a freshman?

```{r,tut=TRUE,height=600}
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv')
#Belief - Student is a freshman
#Observation - Failed the class
Prior<-nrow(moody[moody$Seniority =='Freshman',])/nrow(moody)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(moody[moody$Grade=='F' & moody$Seniority=='Freshman',])/nrow(
  moody[moody$Seniority =='Freshman',]),2)
TruePositive
FalsePositive<-round(nrow(moody[moody$Grade=='F'& moody$Seniority !='Freshman',])/nrow(moody[moody$Seniority !='Freshman',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
round(Posterior,2)
```


## Snippet 3: What are the odds that a 'A' student with the score less than 80 is a psychology major?

```{r,tut=TRUE,height=600}
#Belief - what we do not know. #Is a student a psychology #major?
#Observation = what we do #know. They got an A and less #than 80 in score

moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv')
Prior<-nrow(moody[moody$Major =='Psychology',])/nrow(moody)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(moody[moody$Score <80 & moody$Grade=='A'& moody$Major=='Psychology',])/nrow(moody[moody$Major=='Psychology',]),2)
TruePositive
FalsePositive<-round(nrow(moody[moody$Score <80 & moody$Grade=='A'& moody$Major!='Psychology',])/nrow(moody[moody$Major!='Psychology',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior
```

<!--chapter:end:chapters/bayesian_reasoning.Rmd-->

# üîñ Prediction Challenges {#Pc1}

Prediction challenges differentiate data 101 class at Rutgers from many similar classes at other universities.

Here is how we are different:

Typically prediction model building is illustrated using real world data. This sounds very attractive and of course is the ultimate goal - but using real world data for your first prediction models is not a good idea. Here is why:

- Real data sets needs cleaning - and this requires more elaborate programming skills than in data 101. But even if data is "clean" - that is uploadable to R studio.

- Even if real data is clean, it is often "unrelatable" - like say some animal laboratory results or drug tests. It takes time to learn the data, it may also require some domain expertize to fully comprehend the data. 

- It may take a lot of effort to discover trends and patterns in real data. Sometimes these trends are very weak.

This is why we create our prediction challenges synthetically. Our data is relatable and synthetic. For example we start with data which each student can relate to - Grading. These challenges called Professor Moody prediction challenges call for predicting the grade in class based on attributes such as major, seniority, score in class as well as behavioral characterstics in class - texting, dozing off and asking questions. 

Data sets for our prediction challenges are generated with hidden patterns which make prediction challenges more fun to work on. They are truly data puzzles. For example, it may be the case that asking a lot of questions in class is negatively correlated with the grade. Professor Moody does not like to be bothered! This suprising pattern could have been injected in one of our data sets. 

Data generation for our data puzzles is accomplished with our own tool, called Data Puzzle Generator. Using data Puzzle Generator one can generate data sets with embedded patterns in very short time. One can also built on the previous data puzzles and add or remove patterns - creating new data puzzles

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## General Structure of the Prediction Challenges

The submission will take place on Kaggle which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submission.

The datasets provided for each prediction challenge is as follows:

- **Training Dataset**
  - It is used for training and cross-validation purpose in the prediction challenge.
  - This data has all the training attributes along and the  values of the attribute wich is predicted (so called, Target attribute).
  - Models for prediction are to be trained using this dataset only.  
  - Training data set is the set which is used when you build your prediction model - since this is the only data set which has all values of target attribute.

- **Testing Dataset**

  - It is used for applying your prediction model to new data. You do it only when you are finished with building your prediction model.

  - Testing data set consists of all the attributes that were used for training, but it does not contain any values of the target attribute.

  - It is disjoint with the training data set - it contains new data and it is missing the target variable.

- **Submission Dataset**

  - After prediction using the ‚Äútesting‚Äù dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g. ID or name,etc) and second the predicted attribute column.
  - Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle.

To read the datasets use the read.csv() function and for writing the dataset to the file, use the write.csv() function. Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle.

To avoid this, you can add the parameter, row.names = F in the write.csv() function. e.g. write.csv(*dataframe*,*fileaddress*,row.names = F).

## Challenge 1 - Freestyle prediction of grades in yet another MOODY data set

This is the next in the sequence of data puzzles about grading methods of the eccentric professor Moody. Professor Moody found out that his former grading methods were leaked to the student by treacherous TA and changed his grading methods (and the TA).

Unfortunately, again the data was leaked to the students (Professor Moody does not use passwords). It indicates that Professor Moody may be tougher on certain majors and also may apply different grading criteria for different student seniority levels

Can you build a prediction model which will mimic Moody's grading as closely as possible?

Attached  are three files : One <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv" download="M2022train.csv">M2022train.csv</a> with  original Professor Moody grading data and another, the <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022testS.csv" download="M2022testS.csv">M2022testS.csv</a> data with missing GRADE column.  Finally <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submissionS.csv" download="M2022submissionS.csv">M2022submissionS.csv</a> is the file you will submit to Kaggle of course after filling up the GRADE column.  

 Your job is to predict the grades in the testing file, adding to it GRADE attribute  with predicted grades. 

This test submission will be handled through Kaggle (just the error computation part)  and  Canvas just like for any assignments so far (Kaggle submission instructions coming). Kaggle will automatically calculate your prediction error.  In this case, of Professor Moody data, it will be a fraction of  grades which your prediction model have predicted incorrectly. 

**Data League:** https://data101.cs.rutgers.edu/?q=node/155 <br>
**Kaggle competition:** <br>
**Kaggle submission instructions:** https://data101.cs.rutgers.edu/?q=node/150 <br>
**Canvas HW9:** https://rutgers.instructure.com/courses/159918/assignments/1953810

### This is how an 'A' in this course looks like

<button class="btn btn-primary" data-toggle="collapse" data-target="#pc12"> Seok Yim' 2021 </button> 
<button class="btn btn-primary" data-toggle="collapse" data-target="#pc13"> Muskan Burman' 2021 </button> 
<div id="pc12" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/seok.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="pc13" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/muskan.pdf&embedded=true" width="100%" height="500px"></embed>
</div>


### Best Student's Submissions 2022

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt15"> Upsham Naik </button> 

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt13"> Jeevanandan Ramasamy </button> 
<div id="bt15" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best2.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
<div id="bt13" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

## Challenge 2 - Same data but using rpart - decision tree

It is the same DATA as prediction challenge 1.  Just use **rpart()** this time.  Lets see if you can do better (certainly faster) with the rpart than with freestyle prediction. You have to use rpart, but you can use it as part of your prediction model and combine it with your model which you submitted for HW9.  We will talk about rpart in detail in recitations and lectures next week.

**FOR THIS PREDICTION CHALLENGE:**  Have to use rpart function (and predict of course). Have to use and show crossvalidation (use crossvalidate(). Explain in ppts how you used crossvalidation.

Use rpart contol functions - like minbucket and minsplit as well as different subsets of attributes - when corssvalidating. Make sure you explain in your ppts what "controls" have you tried and eventually used.
 
This test submission will be handled through Kaggle (just the error computation part)  and  Canvas just like for any assignments so far (Kaggle submission instructions coming). Kaggle will automatically calculate your prediction error.  In this case, of Professor Moody data, it will be a fraction of  grades which your prediction model have predicted incorrectly. 


**Data League:** https://data101.cs.rutgers.edu/?q=node/155 <br>
**Kaggle competition:** <br>
**Kaggle submission instructions:** https://data101.cs.rutgers.edu/?q=node/150 <br>
**Canvas HW10:** https://rutgers.instructure.com/courses/159918/assignments/1961012

### Best Student's Submissions 2022

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt14"> Jeevanandan Ramasamy </button> 
<div id="bt14" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best4.pdf&embedded=true" width="100%" height="500px"></embed>
</div>


<!--chapter:end:chapters/Prediction_challenges.Rmd-->

# üîñ Free Style: Prediction {#P1}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#p12"> Prediction - Free Style </button> 
<div id="p12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1pA0bzMGr_Tu2CXsgtT9Ks5apHqxWh8l1XXBVvwWr6zc/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

## Snippet 1: Example of a simple freestyle prediction model

```{r,tut=TRUE,height=400}

test<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")

summary(test)

myprediction<-test
decision <- rep('F',nrow(myprediction))
decision[myprediction$Score>40] <- 'D'
decision[myprediction$Score>60] <- 'C'
decision[myprediction$Score>70] <- 'B'
decision[myprediction$Score>80] <- 'A'
myprediction$Grade <-decision
error <- mean(test$Grade!= myprediction$Grade)
error
```

## Snippet 2: How to build a freestyle (your own code) prediction model?

```{r,tut=TRUE,height=500}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")

# How do we build a freestyle prediction model?  Definitely start with plots like the boxplot from the section 5 (data exploration).  But then follow up with exploratory queries as in the recent quizzes. Examples here use table()  functon and look for situations when one grade is absoutely dominant. This would be your prediction. Thus, the goal is to slice the data using subsetting in such a way that for each slice you get a clear "winner grade". Then combine these subset rules into decision vector - just as we did in snippet 14.1.

# Below some examples of such exploratory queries with clear grade winners.

summary(moody)
table(moody$Grade)
table(moody[moody$Score>80,]$Grade)
table(moody[moody$Score>80 & moody$Major=='Psychology',]$Grade)
table(moody[moody$Score<40 & moody$Major=='Economics',]$Grade)
table(moody[moody$Score<40 & moody$Seniority=='Freshman',]$Grade)
```

## Snippet 3: One-step crossvalidation

```{r,tut=TRUE,height=900}
train<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")
summary(train)
#scramble the train frame
v<-sample(1:nrow(train))
v[1:5]
trainScrambled<-train[v, ]
#one step crossvalidation
trainSample<-trainScrambled[nrow(trainScrambled)-10:nrow(trainScrambled), ]
myprediction<-trainSample

#prediction model - free style
#How to test how good your model is?
#Crossvalidation:  Divide train data set into two disjoint subsets T (train) and train MINUS T, the complement of T. 
#You use T to derive your prediction model and the complement of T (train MINUS T) to validate (test it).
# We assume that you created prediction model looking just at the subset of training data T=trainScrambled[1:990,  ]. 
#Since for crossvalidation we train on a subset T of the training data set and validate (test) on the complement of T. 
#In this case T= trainScrambled[1:990,  ] and complement of T (to validate/test) is stored as trainSample.
#You can do it multiple times. And observe the error and its stability.
#You build your model using the decision vector.  Here is very SIMPLISTIC MODEL which is just illustration. Your model should have much better error and be more sophisticated. 

decision <- rep('F',nrow(myprediction))
decision[myprediction$Score>40] <- 'D'

decision[myprediction$Score>60] <- 'C'

decision[myprediction$Score>70] <- 'B'

decision[myprediction$Score>80 ] <- 'A'

myprediction$Grade <-decision
error <- mean(trainSample$Grade!= myprediction$Grade)
error   
```



## Snippet 4: Preparing submission.csv for Kaggle

```{r,tut=TRUE,height=500}
# Here you just need the test table (without grades) to apply your prediction model and calculate predicted grades. And submission data frame to fill it in with the predicted #grades

test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022testSNoGrade.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')

myprediction<-test
#Here is your model. I just show example of trivial prediction model
decision <- rep('F',nrow(myprediction))
decision[myprediction$Score>40] <- 'D'
decision[myprediction$Score>60] <- 'C'
decision[myprediction$Score>70] <- 'B'
decision[myprediction$Score>80] <- 'A'
#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-decision
submission
# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle

```



<!--chapter:end:chapters/Free_style.Rmd-->

# üîñ Predictions with rpart {#prpart}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#dt12"> Prediction with rpart </button> 
<div id="dt12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1Yw11iu0FnUbiNKrwEKyWHxZ1oXUVG0APfI9acEbB91o/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

Decision trees are one of the most powerful and popular tools for classification and prediction. The reason decision trees are very popular is that they can  generate rules which are  easier to understand as compared to other models. They require much less computations for performing modeling and prediction. Both continuous/numerical and categorical variables are handled easily while creating the decision trees.


## Use of Rpart {#rpart}

Recursive Partitioning and Regression Tree `RPART` library is a collection of routines which implements a Decision Tree.The resulting model can be represented as a binary tree.

The library associated with this `RPART` is called `rpart`. Install this library using `install.packages("rpart")`.

Syntax for building the decision tree using rpart():

- `rpart( formula , method, data, control,...)`
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *method*: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use "anova" for regression, "class" for classification, etc.
  - *data*: here we provide the dataset on which we want to fit the decision tree on.
  - *control*: here we provide the control parameters for the decision tree. Explained more in detail in the section further in this chapter.
  
  
For more info on the rpart function visit [rpart documentation](https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart)

Lets look at an example on the Moody 2022 dataset.

- We will use the rpart() function with the following inputs:
  - prediction -> GRADE
  - predictors -> SCORE, DOZES_OFF, TEXTING_IN_CLASS, PARTICIPATION
  - data -> moody dataset
  - method -> "class" for classification.


### Snippet 1
```{r,tut=TRUE,height=300}
library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function.
rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody,method = "class")

```
We can see that the output of the rpart() function is the decision tree with details of, 

- node -> node number
- split -> split conditions/tests
- n -> number of records in either branch i.e. subset
- yval -> output value i.e. the target predicted value.
- yprob -> probability of obtaining a particular category as the predicted output.

Using the output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section \@ref(rpartpredict)

But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section.

## Visualize the Decision tree {#rpartplot}

To visualize and understand the rpart() tree output in the easiest way possible, we use a library called `rpart.plot`. The function `rpart.plot()` of the rpart.plot library is the function used to visualize decision trees.

*NOTE*: The online runnable code block does not support `rpart.plot` library and functions, thus the output of the following code examples are provided directly.

### Snippet 2
```{r,tut=TRUE,height=500}
# First lets import the rpart library
library(rpart)

# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function.
rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody,method = "class")

# Now lets import the rpart.plot library to use the rpart.plot() function.
#library(rpart.plot)

# Use of the rpart.plot() function  to visualize the decision tree.
#rpart.plot(tree)
```
![Output Plot of *rpart.plot()* function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt.png)

We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about 

- the grade category.
- the outcome probability of each grade category.
- the records percentage  out of total records.

To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides [rpart.plot](https://www.rdocumentation.org/packages/rpart.plot/versions/3.0.9/topics/rpart.plot) and [Plotting with rpart.plot (PDF)](http://www.milbo.org/doc/prp.pdf)

---
**NOTE**: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment.
---

## Rpart Control {#rpartcontrol}

Now let's look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function.

- `rpart.control( *minsplit*, *minbucket*, *cp*,...)`
 - *minsplit*: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -> the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition.
 - *minbucket*: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -> the minimum number of observation in the terminal/leaf node of the trees must be 500 or above.  
 - *cp*: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by *cp*, will not be made in the tree.
 

For more information of the other arguments of the `rpart.control()` function visit [rpart.control](https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart.control)

Let look at few examples.

Suppose you want to set the control parameter minsplit=200. 

### Snippet 3: Minsplit = 200

```{r,tut=TRUE,height=500}
library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter minsplit=200
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minsplit = 200))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)
```
![Output tree plot of after setting minsplit=200 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt2.png)

### Snippet 4: Minsplit = 100

```{r,tut=TRUE,height=500}
library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter minsplit=100
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minsplit = 100))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)
```
![Output tree plot of after setting minsplit=100 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt8.png)

We can see from the output of `tree$splits` and the tree plot, that at each split the total amount of observations are above 200 and 100. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits.

Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters.

### Snippet 5: Minbucket = 100

```{r,tut=TRUE,height=500}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter Minbucket=100
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minbucket = 100))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)

```
![Output tree plot of after setting minbucket=100 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt3.png)

We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size.

### Snippet 6: Minbucket = 200

```{r,tut=TRUE,height=500}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter Minbucket=200
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minbucket = 200))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)

```
![Output tree plot of after setting minbucket=200 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt4.png)

We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 200. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size.

Lets now use the `cp` parameter and see its effect on the tree.

### Snippet 7: cp = 0.05

```{r}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter cp=0.2
tree <- rpart(GRADE ~ ., data = moody,method = "class",control=rpart.control(cp = 0.05))

tree

#library(rpart.plot)
#rpart.plot(tree)


```
![Output tree plot of after setting cp=0.05 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt6.png)


### Snippet 8: cp = 0.005

```{r}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter cp=0.005
tree <- rpart(GRADE ~ ., data = moody,method = "class",control=rpart.control(cp = 0.005))

tree

#library(rpart.plot)
#rpart.plot(tree)


```
![Output tree plot of after setting cp=0.005 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt7.png)
We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005.


## Cross Validation {#crossvalidation}

Overfitting takes place when you have a high accuracy on training dataset, but a low accuracy on the test dataset. But how do you know whether you are overfitting or not? Especially since you cannot determine accuracy on the test dataset? That is where cross-validation comes into play.

Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. The partition is defined by split ratio. If split ratio =0.7, 70% of the training dataset will be used for the actual training of your model (rpart or lm), and 30 % will be used for validation (or testing). The accuracy of this validation data is called cross-validation accuracy.

To know if you are overfitting or not, compare the training accuracy with the cross-validation accuracy. If your training accuracy is high, and cross-validation accuracy is low, that means you are overfitting.

- `cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*)`
  - *data*: The dataset on which cross validation is to be performed.
  - *tree*: The decision tree generated using rpart.
  - *n_iter*: Number of iterations.
  - *split_ratio*: The splitting ratio of the data into train data and validation data.
  - *method*: Method of the prediction. "class" for classification.

The way the function works is as follows:

- It randomly partitions your data into training and validation. 
- It then constructs the following two decision trees on training partition:
  -  The tree that you pass to the function.
  -  The tree is constructed on all attributes as predictors and with no control parameters.
-It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees.

The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting.

We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired.

The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant.

Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration.


Let's look at the cross_validate() function in action in the example below.

We will pass the tree with formula as `GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION`, and control parameter, with `minsplit=100`. 
And for cross_validate() function, we will use` n_iter=5, and split_raitio=0.7` 

---
**NOTE:** Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use 
---

```
install.packages("devtools") 
devtools::install_github("devanshagr/CrossValidation")
CrossValidation::cross_validate()
```

### Snippet 9

```{r,tut=TRUE,ex="crossvalidate",type="pre-exercise-code"}

cross_validate <- function(df, tree, n_iter, split_ratio, method = 'class')
{
  # training data frame df
  df <- as.data.frame(df)

  # mean_subset is a vector of accuracy values generated from the specified features in the tree object
  mean_subset <- c()

  # mean_all is a vector of accuracy values generated from all the available features in the data frame
  mean_all <- c()

  # control parameters for the decision tree
  contro = tree$control

  # the following snippet will create relations to generate decision trees
  # relation_all will create a decision tree with all the features
  # relation_subset will create a decision tree with only user-specified features in tree
  dep <- all.vars(terms(tree))[1]
  indep <- list()
  relation_all = as.formula(paste(dep, '.', sep = "~"))
  i <- 1
  while (i < length(all.vars(terms(tree)))) {
    indep[[i]] <- all.vars(terms(tree))[i + 1]
    i <- i + 1
  }
  b <- paste(indep, collapse = "+")
  relation_subset <- as.formula(paste(dep, b, sep = "~"))

  # creating train and test samples with the given split ratio
  # performing cross-validation n_iter times
  for (i in 1:n_iter) {
    sample <-
      sample.int(n = nrow(df),
                 size = floor(split_ratio * nrow(df)),
                 replace = F)
    train <- df[sample,]
    testing  <- df[-sample,]
    type = typeof(unlist(testing[dep]))

    # decision tree for regression if the method specified is "anova"
    if (method == 'anova') {
      first.tree <-
        rpart(
          relation_subset,
          data = train,
          control = contro,
          method = 'anova'
        )
      second.tree <- rpart(relation_all, data = train, method = 'anova')
      pred1.tree <- predict(first.tree, newdata = testing)
      pred2.tree <- predict(second.tree, newdata = testing)
      mean1 <- mean((as.numeric(pred1.tree) - testing[, dep]) ^ 2)
      mean2 <- mean((as.numeric(pred2.tree) - testing[, dep]) ^ 2)
      mean_subset <- c(mean_subset, mean1)
      mean_all <- c(mean_all, mean2)
    }

    # decision tree for classification
    # if the method specified is not "anova", then this block is executed
    # if the method is not specified by the user, the default option is to perform classification
    else{
      first.tree <-
        rpart(
          relation_subset,
          data = train,
          control = contro,
          method = 'class'
        )
      second.tree <- rpart(relation_all, data = train, method = 'class')
      pred1.tree <- predict(first.tree, newdata = testing, type = 'class')
      pred2.tree <-
        predict(second.tree, newdata = testing, type = 'class')
      mean1 <-
        mean(as.character(pred1.tree) == as.character(testing[, dep]))
      mean2 <-
        mean(as.character(pred2.tree) == as.character(testing[, dep]))
      mean_subset <- c(mean_subset, mean1)
      mean_all <- c(mean_all, mean2)
    }
  }

  # average_accuracy_subset is the average accuracy of n_iter iterations of cross-validation with user-specified features
  # average_acuracy_all is the average accuracy of n_iter iterations of cross-validation with all the available features
  # variance_accuracy_subset is the variance of accuracy of n_iter iterations of cross-validation with user-specified features
  # variance_accuracy_all is the variance of accuracy of n_iter iterations of cross-validation with all the available features
  cross_validation_stats <-
    list(
      "average_accuracy_subset" = mean(mean_subset, na.rm = T),
      "average_accuracy_all" = mean(mean_all, na.rm = T),
      "variance_accuracy_subset" = var(mean_subset, na.rm = T),
      "variance_accuracy_all" = var(mean_all, na.rm = T)
    )

  # creating a data frame of accuracy_subset and accuracy_all
  # accuracy_subset contains n_iter accuracy values on cross-validation with user-specified features
  # accuracy_all contains n_iter accuracy values on cross-validation with all the available features
  cross_validation_df <-
    data.frame(accuracy_subset = mean_subset, accuracy_all = mean_all)
  return(list(cross_validation_df, cross_validation_stats))
}
```


```{r,tut=TRUE, ex="crossvalidate",type="sample-code",height=500}
# First lets import the rpart library
library(rpart)
# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv',stringsAsFactors = T)
# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS, data = moody,method = "class",control = rpart.control(minsplit = 100))
tree
# Now lets predict the Grades of the Moody Dataset.
pred <- predict(tree, moody, type="class")
head(pred)
# Lets check the Training Accuracy
mean(moody$GRADE==pred)
# Lets us the cross_validate() function.
cross_validate(moody,tree,5,0.7)
```

You can see that the cross-validation accuracies for the tree that was passed (accuracy_subset) are fairly high and close to our training accuracy of 84%. This means we are not overfitting. Also observe that accuracy_subset and accuracy_all have the same values, which means that the only relevant attributes are score and participation, and adding more attributes doesn't make any difference to the tree. Finally, the values in accuracy_subset are reasonably close to each other, which mean low variance.


## Prediction using rpart. {#rpartpredict}

Now that we have seen the process to create a decision tree and also plot it, we will like to use the output tree to predict the required attribute.

From the moody example, we are trying to predict the grade of students. Lets look at the `predict()` function to predict the outcomes.

- `predict(*object*,*data*,*type*,...)`
  - *object*: the generated tree from the rpart function.
  - *data*: the data on which the prediction is to be performed.
  - *type*: the type of prediction required. One of "vector", "prob", "class" or "matrix".

Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset.

### Snippet 10

```{r,tut=TRUE,height=500}
# First lets import the rpart library
library(rpart)

# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody ,method = "class")
tree

# Now lets predict the Grades of the Moody Dataset.
pred <- predict(tree, moody, type="class")
head(pred)
```

## Snippet 11: Your Model with rpart

```{r,tut=TRUE,height=600}
#How to combine your freestyle prediction model with the rpart? 

#One way of doing it is to divide the data sets into two mutually exclusive subsets (which cover all data also).  How do you make these subsets?  Unfortunately there is no algorithm for this and it is more relying on how well is your model doing for different slices of the data.  

#In this example (similarly to snippet 16.7 where we combine two rpart models, we assume that initial split we decided on is based on SCORE. But instead of having two rpart models  (16.7), we will use our prediction  model from prediction challenge 1  for SCORE >50 and rpart for SCORE <=50.

#Lets assume that yourPrediction is our model from Prediction Challenge 1 (your entire code has to be applied here to the data set (moody, below)

moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

#rpartModel<-rpart(GRADE~., data=moody[moody$SCORE<=50,]);
#pred_rpartModel <- predict(rpartModel, newdata=moody[moody$SCORE<=50,], type="class")
#pred_yourModel <- yourPrediction[moody$SCORE<=50]
#myprediction<-moody

## Here we combine two models - our model from prediction 1 challenge and rpart.

#decision <- rep('F',nrow(myprediction))
#decision[myprediction$SCORE>50] <- pred_yourModel
#decision[myprediction$SCORE<=50] <-as.character(pred_rpartModel )
#myprediction$GRADE <-decision
#error <- mean(moody$GRADE!= myprediction$GRADE
#error
```

## Snippet 12: Freestyle +  rpart: Combining rpart prediction models

```{r,tut=TRUE,height=600}

library(rpart)
# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')
model1<-rpart(GRADE~., data=moody[moody$SCORE>50,]);
model2<-rpart(GRADE~., data=moody[moody$SCORE<=50,]);
model1
model2
pred1 <- predict(model1, newdata=moody[moody$SCORE>50,], type="class")
pred2 <- predict(model2, newdata=moody[moody$SCORE<=50,], type="class")
myprediction<-moody
decision <- rep('F',nrow(myprediction))
decision[myprediction$SCORE>50] <- as.character(pred1)
decision[myprediction$SCORE<=50] <-as.character(pred2)
myprediction$GRADE <-decision
error <- mean(moody$GRADE!= myprediction$GRADE)
error

```

## Snippet 13: Submission with rpart

```{r,tut=TRUE,height=600}
library(rpart)
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022testSNoGrade.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')
train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv")

tree <- rpart(Grade ~ Major+Score+Seniority, data = train, method = "class",control=rpart.control(minbucket = 200))
tree

prediction <- predict(tree, test, type="class")

#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-prediction 

# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle
```

<!--chapter:end:chapters/Decision_trees.Rmd-->

# üîñ Linear Regression {#lr}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#lr12"> Linear Regression </button> 
<div id="lr12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1RuAidmZGDoTRYMjONLEsw3bfx_aQb-f3C0V4Jjk4Vzs/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

Linear regression is a linear approach to modeling the relationship between a numerical response ($Y$) and one or more independent variables ($X_i$).

Usually in linear regression, models are used to predict only one scalar variable. But there are two subtype if these models:
- First when there is only one explanatory variable and one output variable. This type of linear regression model known as simple linear regression.
- Second, when there are multiple predictors, i.e. explanatory/dependent variables for the output variable. This type of linear regression model known as multiple linear regression.

![Linear models fitted to various different type of data spread. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. Credits: Wikipedia.](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling2/lmvariants.svg)

## Linear regression using lm() function {#lm}

Syntax for building the regression model using the *lm()* function is as follows:

- `lm(formula, data, ...)`
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *data*: here we provide the dataset on which the linear regression model is to be trained.
  
For more info on the *lm()* function visit [lm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm)

Lets look at the example on the Moody dataset.

```{r,echo=FALSE}
moodyNUM<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv')
temp<-knitr::kable(
  head(moodyNUM, 10), caption = 'Snippet of Moody Num Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

Now we can build a simple linear regression model to predict the ClassScore attribute based on the various other attributes present in the dataset, as shown above.

Since we will be predicting only one attribute values, this model will be called simple linear regression model.

### Snippet 1: How much do Midterm, Project and Final Exam count?

```{r,tut=TRUE,height=500}
moodyNUM<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv')
split<-0.7*nrow(moodyNUM)
split
moodyNUMTr<-moodyNUM[1:split,]
moodyNUMTr
moodyNUMTs<-moodyNUM[split:nrow(moodyNUM),]
#We use linear regression to #find out the weights of #Midterm, Project and Final #Exam in calculation of the #final class score. Each of #them are scored out of 100 and #the final class score is also #scored out of 100 as weighted #sum of Midterm, Project and #Final Exam scores.
train <- lm(ClassScore~.,  data=moodyNUMTr)
train
pred <- predict(train,newdata=moodyNUMTs)
mean((pred - moodyNUMTs$ClassScore)^2)
```

We can see that, 

- The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc.
- The predicted values are obtained from the predict function using the trained model and the test data.

## Calculating the Error using mse() {#mse}

As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test won't prove useful now in our numerical predictions scenario.

We don't want to eyeball every time we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique.

To do this we will use the Mean Squared Error(MSE).

- The MSE is a measure of the quality of an predictor/estimator
- It is always non-negative
- Values closer to zero are better.

The equation to calculate the MSE is as follows:

\begin{equation}
MSE=\frac{1}{n} \sum_{i=1}^{n}{(Y_i - \hat{Y_i})^2}
\\ \text{where $n$ is the number of data points, $Y_i$ are the observed value}\\ \text{and $\hat{Y_i}$ are the predicted values}
\end{equation}

To implement this, we will use the *mse()* function present in the Metrics Package, so remember to install the Metrics package and use `library(Metrics)` in the code for local use.

The syntax for *mse()* function is very simple:

- `mse(actual,predicted)`
  - *actual*: vector of the actual values of the attribute we want to predict.
  - *predicted*: vector of the predicted values obtained using our model.


## Snippet 2: Cross Validate your prediction

```{r,tut=TRUE,height=600}
library(ModelMetrics)

train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")
#scramble the train frame
v<-sample(1:nrow(train))
v[1:5]
trainScrambled<-train[v, ]

#one step crossvalidation
n <- 100
trainSample<-trainScrambled[nrow(trainScrambled)-n:nrow(trainScrambled), ]
testSample <- trainScrambled[1:n,]

lm.tree <- lm(ClassScore~.,  data=trainSample)
lm.tree

pred <- predict(lm.tree,newdata=testSample)
pred

mse(testSample$ClassScore,pred)

  
```

We can see that,

- The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc.
- The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the previous model we are using the cross validation technique to check that if we have more accurate predictions, thus increasing the overall accuracy of the model.

## Snippet 3: Submission with lm

```{r,tut=TRUE,height=600}
library(rpart)
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM_test.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')
train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")

tree <- lm(ClassScore~.,  data=train)
tree

prediction <- predict(tree, newdata=test)

#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-prediction 

# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle
```

<!--chapter:end:chapters/Linear_regression.Rmd-->

# üîñ Machine Learning-Prediction Loop {#MLP}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

- **Lecture slides: **     <button class="btn btn-primary" data-toggle="collapse" data-target="#ML12"> Prediction Loop </button> 
<div id="ML12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1n-7uFUUS40SwZO31rKxblplHtkAZMvQDXQzA3lY4PPU/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/Prediction_loop.Rmd-->

# üîñ Boundless Analytics {#BA}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Snippet 1: Chi square hunt

```{r,tut=TRUE,height=600}

# Say, the Boundless analytics provides us with the slice:  Beer =='Lager' &  Day =='Weekend' and Snacks ='Crackers' and anchor attribute is Location.  You can calculate Chisq for this slice and the Location attribute to test if distribution of locations is affected if we limit ourselves only to transactions selling Lager and Crackers on Weekends?  

# The most interesting slice-anchor attribute combinations are the ones with the largest chisq test and lowest p-value. Nevertheless do not forget about multiple hypothesis correction - since we can on chi-square hunt here!

Minimarket<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

Minimarket$IN<-'Out_Slice'
Minimarket[Minimarket$Beer=='Lager' & Minimarket$Day=='Weekend' &  Minimarket$Snacks =='Crackers', ]$IN<-'In_Slice'
d<-table(Minimarket$Location, Minimarket$IN)
chisq.test(d)

```

<!--chapter:end:chapters/BoundlessAnalytics.Rmd-->

