# Introduction {#intro}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->

<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->


<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->

<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(iris, 20), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->

<!-- You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015]. -->


The objective of this textbook is to provide you with the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them.  In other words, to introduce you to data science. We call it an active textbook, since students can interact with the book by running and modifying snippets of R code. Students can also test themselves using Query and Code Roulette - on  questions and simple coding tasks.  Thus, an active textbook interacts with its reader, helps to run code and also asks students questions and gives them simple coding tasks. Concepts which we discuss in the book are widely covered on the web with countless youtube video tutorials.  We briefly introduce the basic concepts here as well, but our focus is on problem solving and simple coding.  In other words, honing the skills to do something with  the data, not just talk about it.   To learn a concept you have to know how to code it. To put it bluntly, no coding, no learning! 

Active textbook is data-centric. We  stress that prior to data analysis and exploration, you have to know your data.  Paraphrasing the famous saying that  real estate is about “location, location and location”, data science is about data, data and data.  Using numerous data sets we guide the student through the process of getting acquainted with their data. We call these data sets - data puzzles, since each of them is synthetically created and has hidden patterns embedded in the process of data generation.  For each of our data puzzles we show the process of getting familiar with the data  beginning from  simple scripts called queries and proceeding through as-hoc hypothesis testing as well as Bayesian reasoning. As we said,  each data puzzle hides interesting and non-trivial patterns which are to be discovered.  This process of discovery makes data science similar to the work of a detective. Discoveries range from  grading methods of Professor Moody, factors influencing  quality of a party,  voter profiles in  local town elections or quality of sleep determinants. 

Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in a statistically convincing manner, perhaps in a colorful and visually appealing way.  Finally, you will be able to apply some basic machine learning methods to build, train and test prediction models.  All of this will be accomplished in a succinct and crisp way using a small subset of R instructions. 

 We assume no prior programming background.  We will teach you as little R as necessary to achieve the goals of this book: explore the data, visualize it,  verify hypotheses and build prediction models.  Thus, you will be able to do a good chunk of work which data scientists do.  We will accomplish this goal through active snippets of executable code. These are examples of R code (around 100 executable snippets of code) embedded in  the textbook itself.  More importantly, you will be able to modify the code and execute the modified code without need to install any application on your machine.  This will allow you to understand the code in the book through the “what if”  exploratory process. Thus, every code snippet is just an invitation to endless modifications. This is why we call this textbook - active.  
 
Another unique aspect of this textbook is its reliance on data puzzles. These are synthetic data sets with embedded patterns and rules generated by our tool called **DataMaker**. We will present our data puzzles (dynamic list, may vary from year to year) in section 8   following  introduction to plots, then we will proceed to freestyle data exploration.  This will allow us to learn  more about our data,  form the leads, and finally state our hypotheses.  We will follow up by an elementary introduction to hypothesis testing through a permutation test.  We will learn how to calculate p-values and how to use them to defend our findings against the  randomness trap. This will be particularly important in case of multiple hypotheses when one has to be particularly careful to avoid “false positives”  . We will introduce Bayesian reasoning and learn how to compute posterior odds of a belief given an observation.  All these important concepts will be introduced via executable snippets of code and “what if” practicing.  We will then enter the key section of the book - the data puzzles section.  In the data puzzle section, for each data set we will go through the process of getting to know the data and using the concepts learned so far by executing code tailored to each of the data puzzles. 

 In the second part of the book we discuss prediction models. We focus on decision trees (rpart() - recursive partitioning)  and linear regression. But we also show how to use other machine learning methods from the rich R-library. We go over cross-validation and show how to build prediction models which combine multiple machine learning models.  We stress the importance of knowing your data first, instead of just blind application of machine learning packages.  Humans in the loop is very important and prior data exploration and visualization leads to improved quality prediction models.  Students can practice prediction model building on especially prediction snippets to make themselves prepared for Kaggle based prediction challenge competition which takes the last months of the data 101 class. The last leaderboard of 2022 challenge is presented here <a href="https://data101.cs.rutgers.edu/?q=node/151"> LeaderBoard </a>.


We will use as few R functions  as possible to achieve our goals. In fact we will demonstrate how using less than ten R functions is sufficient for us. In the appendix, we show many more useful commands of R which eventually you would have to use. However, our goal in this short textbook, is to present the shortest path to data analysis which will let you import the data, plot it, make some analysis yourself and use R-libraries to build machine learning models.  In this textbook and in this class we do not teach how to clean the data (data wrangling) and how to deal with a wide variety of data types. We also do not address complex data transformations such as multi-frame operations like merge function. We also do not explain how different machine learning methods work, we only show you how to use them. It is similar to teaching one how to drive a car without knowing how a car engine works. 

Sections \@ref(rconcepts) and \@ref(rfunctions) provide the lists of all concepts which we cover in our active textbook and all R functions which are needed.  Notice how small the set of R functions is.  It is important for programming novices to start small and also see how far this small set of functions can get you. 

Our **question roulette** allows self-testing on nearly 100 questions relevant to the material. Each question is answered, but students are encouraged first to answer questions themselves and only then follow it with checking the correct answer.  The **code roulette**, on the other hand, consists of around 100 of simple common data science coding tasks.  








<!--chapter:end:chapters/intro.Rmd-->

# Setting Up R 

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

- **Important Instructions**
  - Installation of R is required before installing RStudio
    - "R” is a programming language, and,
    - “RStudio” is an Integrated Development Environment (IDE) which provides you a platform to code in R.

- __How to download and install R & RStudio?__

  - _Downloading and installing R._

    - For Windows Users.
      - Click on the link provided below or copy paste it on your favourite browser and go to the website.
          - [https://cran.r-project.org/bin/windows/base/](https://cran.r-project.org/bin/windows/base/)
      - Click on the link at top left where it says “Download R 4.0.3 for windows” or the latest at the time of your installation.
      - Open the downloaded file and follow the instructions as it is.

    - For MAC Users.
      - Click on the link provided below or copy paste it on your favourite browser and go to the website.
          - [https://cloud.r-project.org/bin/macosx/](https://cloud.r-project.org/bin/macosx/)
      - Under “Latest release”, click on “R-4.0.3.pkg” or the latest at the time of your installation.
      - Open the downloaded file and follow the instructions as it is.
      
 
  - _Downloading and installing RStudio._
  
    - For Windows Users.
      - Click on the link below or copy paste it in your favourite browser.
          - [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/)
      - Scroll down almost till the end of the web page until you find a section named “All Installers”.
      - Click on the download link beside “Windows 10/8/7” to download the windows version of RStudio.
      - Install RStudio by clicking on the downloaded file and following the instructions as it is.

    - For MAC Users.
      - Click on the link below or copy paste it in your favourite browser.
          - [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/)
      - Scroll down almost till the end of the web page until you find a section named “All Installers”.
      - Click on the link beside “macOS 10.13+” to start your download the MAC version of RStudio.
      - Install RStudio by clicking on the downloaded file and following the instructions as it is.


---

## Create New Project {#setting}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->

<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->


<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->

<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(iris, 20), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->

<!-- You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015]. -->

After installing R studio successfully the first step is to create a project R studio. 

- Step 1: Go to **File -> New Project**

![New Project](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/1_text.png)

- Step 2: Select **New Directory**

![New Directory](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/2_text.png)

- Step 3: Select **New Project**

![New Project](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/3_text.png)

- Step 4: Give your preferred directory name like **"Data101_Assignmnets"**

![Directory Name](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/4_text.png)

- Step 5: Click on Create Project and finally the R studio should look like

![Rstudio](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/4.5_text.png)



## How to upload a data set?
<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive()

```

### Grades dataset

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv" download="moody2022.csv">moody2022.csv</a>

Grades in Professor Moody’s class data set. 

Our working data set  will be the Moody data set which stores data about students' grades in a large signature class taught by Professor Moody. The data set stores individual scores of students in class, their major, seniority and GPA.  Data scientists may ask many questions such as, given the student’s score in class, does the final grade depend on the major and/or student’s seniority?  For example, is it more difficult for computer science majors to earn an A, pass the class, than, say for students majoring in psychology?  Does GPA play any role in  grading?  It should not - but maybe it does?  We are still far away from being able to ask such questions, for now we will use Moody data set in code snippets which illustrates the core R functions which we will use in the active textbook.

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Grades Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Grades Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

 - To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon.

- There are two options while accessing the dataset from your local machine:
  1. To avoid giving long directory paths for accessing the dataset, one should use the command **getwd()** to get the current working directory and store the dataset in the same directory. 
  
![Getwd](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/5_text.png)

- To access the dataset stored in the same directory one can use the following: **read.csv("moody2022.csv")**.

![Store the moody dataset in the same directory](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/2.3.1.png)

  2. One can also store the dataset at a different location and can access it using the following command: (Suppose the dataset is stored inside the folder Data101_Tutorials on the desktop)
  
    - For Windows Users.
      - Example: read.csv("C:/Users/Desktop/Data101_Tutorials/moody2022.csv")

    - For MAC Users.
      - Example: read.csv("/Users/Desktop/Data101_Tutorials/moody2022.csv")
      

**Note: **
The directory path given here is the current working directory hosted on *Github* where the dataset has been stored.
```{r, tut=TRUE}

# Read in the data
df <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

# Print out `df`
head(df)
```



## Saving your work

- To save your work go to **File -> Save**. It will ask you to give a name for your **.R file** and then click on Save.

![Save](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/2.3.2.png)

- After making modifications to your saved file, you will need to save the file again. 
If the name of the file on the top is in <span style="color: red;"> Red Color </span> indicates that the file have **unsaved** changes.

![Unsaved File](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/2.3.3.png)


- Go to **File -> Save** to save your .R file again. After saving the file the color of the file name i.e. **HW1.R** will again change back to **black**.

![Saved File](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/2.3.4.png)

**Note: ** You can create multiple files inside the same project such as for your each homework assignments

## General R References

https://www.w3schools.com/r/ <br>
https://cran.r-project.org/doc/contrib/Short-refcard.pdf <br>
https://www.amazon.com/Statistics-Engineers-Scientists-William-Navidi/dp/0073376337/ref=pd_lpo_3?pd_rd_i=0073376337&psc=1 <br>
https://data101.cs.rutgers.edu/laboratory/

## Textbook Concepts {#rconcepts}

- Hypothesis testing: \@ref(ztest)

- Difference of means hypothesis testing: \@ref(ztest)

- Null Hypothesis: \@ref(ztest)

- Alternative Hypothesis: \@ref(ztest)

- z-value: \@ref(ztest)

- critical value: \@ref(ztest)

- significance level: \@ref(ztest)

- p-value: \@ref(ztest)

- Bonferroni correction: \@ref(Mtest)

- Chi square test: \@ref(chitest)

- Independence: \@ref(chitest)

- Multiple Hypothesis testing: \@ref(Mtest)

- False Discovery Proportion: \@ref(Mtest)

- Contingency Matrix: \@ref(chitest)

- Bayesian Reasoning: \@ref(br)

- Prior odds: \@ref(br)

- Posterior odds: \@ref(br)

- Likelihood ratio: \@ref(br)

- False positive: \@ref(br)

- True positive: \@ref(br)

- Crossvalidation: \@ref(crossvalidation)

- Decision trees: \@ref(prpart)

- Linear regression: \@ref(lr)

- Recursive partitioning: \@ref(lr)

- MSE: \@ref(lr)

- Prediction accuracy: \@ref(lr)

- Training: \@ref(lr)

- Testing: \@ref(lr)


## R functions used in this class {#rfunctions}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

- **Elementary instructions: ** c() \@ref(vector),  mean() \@ref(mean),   nrow() \@ref(nrow), rep(), sd() \@ref(sd), cut() \@ref(cut)

- **Plots: **  plot() \@ref(scartterplot), barplot() \@ref(barplot), boxplot() \@ref(boxplot)  mosaicplot() \@ref(mosaicplot)

- **Data Transformations: ** subset() \@ref(subset), tapply() \@ref(tapply),  table() \@ref(table), aggregate() 

- **Library functions: ** chisq.test() \@ref(chitest), pnorm() \@ref(permutaion), Permutation() \@ref(permutaion), rpart() \@ref(prpart), predict() \@ref(rpartpredict), lm() \@ref(lm), crossvalidation() \@ref(crossvalidation)

- **Parameters of rpart: ** minsplit \@ref(rpartcontrol), minbucket \@ref(rpartcontrol), cp \@ref(rpartcontrol)


<!--
## Data sets

### Moody

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv" download="moody2022_new.csv">moody2022_new.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

summary(moody)
```

### Movies

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv" download="Movies2022F-4.csv">Movies2022F-4.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

summary(movies)
```

### Traffic

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv" download="Traffic2022.csv">Traffic2022.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
traffic<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')
# head(moody)
temp<-knitr::kable(
  traffic[sample(1:nrow(traffic),5), ], caption = 'Snippet of Traffic Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
traffic<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')

summary(traffic)
```

### Hindex

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv" download="Hindex.csv">Hindex.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
hindex<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv") #web load
# head(moody)
temp<-knitr::kable(
  hindex[sample(1:nrow(hindex),5), ], caption = 'Snippet of Hindex Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
hindex <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv")

summary(hindex)
```


### Prediction 1 Dataset

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv" download="M2022train.csv">M2022train.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Snippet of Moody Predicition 1 dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv")

summary(moody)
```

### Midterm, Project and Final Exam distribution in Prof. Moody class 

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv" download="MoodyNUM.csv">MoodyNUM.csv</a>

**Assumptions:** Midterm, Project and Final Exam are all out of 100

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Midterm, Project and Final Exam distribution in Prof. Moody class',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")

summary(moody)
```


### Minimarket

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv" download="HomeworkMarket2022.csv">HomeworkMarket2022.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Minimarket dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
minimarket<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

summary(minimarket)
```

-->





<!--chapter:end:chapters/Setting_up_R.Rmd-->

# 🔖 Basic R Intructions

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

In this section we introduce the absolutely basic R instructions, call it R101, which will be sufficient for the entire data 101 class. This is a very small subset of the entire R. The good news is that using this very small subset of R we can accomplish all coding objectives for data 101! 

The set we present  below is a mix of simple arithmetic aggregate functions such as mean() \@ref(mean), max() \@ref(max), sum() , basic data structures such as vectors and data frames and finally, two  core functions defined for data frames: subset() \@ref(subset), tapply() \@ref(tapply) and table() \@ref(table) function defined on vectors. 


## Vector {#vector}

- A vector is simply a list of items that are of the same type.

<!-- To combine the list of items to a vector, use the **c()** function and separate the items by a comma.
- All of the items inside a vector to are of the same type like numerical or categorical. -->

### Categorical vectors

Lets look at example of creating a vector:

```{r,tut=TRUE,height=300}

#Lets create 3 vectors with title, author and year.
color <- c('Red','Blue','Yellow','Green')

#Lets look at how the created vectors look.
color
```

### Numerical vectors

Create a vector with numerical values in a sequence, use the **:** operator:

```{r,tut=TRUE,height=300}

#Lets create a vectors with numerical sequence.
year <- 2018:2022

#Lets look at how the created vectors look.
year

```

---

<!--
<br />

- To find out how many items a vector has, use the **length()** function:

```{r,tut=TRUE,height=300}

#Lets create a vectors with categorical values
author <- c('Foreman', 'John', 'Said')

# You can access the vector items by referring to its index number inside brackets []. The first item has index 1, the second item has index 2, and so on:
author[1]

#Lets look at the size of a vectors.
length(author)

```

<br />

- To sort items in a vector alphabetically or numerically, use the **sort()** function and to change the value of a specific item, refer to the **index number**:

```{r,tut=TRUE,height=300}

#Lets create a vectors with categorical values
title <- c('Data Smart','Orientalism','False Impressions','Making Software')

#sorting titles
sort(title)

# change the title 'Data Smart' to 'Smart Data'
title[1] <- "Smart Data"

#Lets look at how the updated vectors look.
title 
```

-->
## Data Frames

- Data Frames are data displayed in a format as a table.
<!--
- Data Frames can have different types of data inside it. While the first column can be character, the second and third can be numeric or logical. 

- Following are the characteristics of a data frame.
  - The column names should be non-empty.
  - The row names should be unique.
  - The data stored in a data frame can be of numeric, factor or character type.
  - Each column should contain same number of data items.

Use the **data.frame()** function to create a data frame:-->

### Data Frame creation

Data frames will serve as containers of imported data - typically data provided in csv format, like the moody data set above. Snippet 4.21 shows how to populate a data frame using read.csv() instruction. Notice that the moody data frame which is the container for the imported  data set  will automatically inherit attribute names (columns) of the underlying data set.

```{r,tut=TRUE,height=300}

# Load the dataset into the moody variable
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

# Now lets view the dataframe moody with just 5-6 tuples
head(moody)
```

### Data frame subsetting  

We can select subsets of columns and subsets of rows for a data frame using the following the notation data[rows, columns]:

```{r,tut=TRUE,height=500}

# Load the dataset into the moody variable
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

# Return row 1
moody[1, ]

# Return column 5
moody[, 5]

# Rows 1:5 and column 2
moody[1:5, 2]

# Give me rows 1-3 and columns 2 and 4 of moody
moody[1:3, c(2:4)]
```

---





## Table {#table}

<!--
- Tables (i.e. frequency distribution table) can be created using **table()** along with some of its variations. 
- To use **table()**, simply add in the variables you want to tabulate separated by a comma. 
- You must reference the variable using **dataset$variable**. 
- By default, missing values are excluded from the counts; if you want a count for these missing values you must specify the argument **useNA=“ifany”** or **useNA=“always”**. -->

### Table()

The below examples show how to use this function:

```{r, tut=TRUE,height=400}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv") #web load

#lets make a table for the grades of students and counts of students for each Grade. 
grades <- table(moody$Grade)

#Joint distribution of grade and major
table(moody$Grade, moody$Major)


```




## Basic Functions

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Snippet of moody Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Snippet of moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

### mean() {#mean}

- **mean()** function is used to find the average of values in a numerical vector.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

#Lets look at the mean of score column.
mean(moody$Score)
```


### length()

- **length()** function is used to get the number of elements in any vector

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

#Lets look at the length of the grade column 
length(moody$Grade)
```

### max() {#max}

- **max()** function is used to get the maximum value in a numerical vector.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

#lets look at the maximum value of the score in the score column
max(moody$Score)
```

### min()

- **min()** function is used to get the minimum value in a numerical vector

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

#Lets look at the minimum value of score in the score column.
min(moody$Score)

```

### sd() {#sd}

- **sd()** function is used to find the standard deviation of numerical vector

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

#Lets look at the standard deviation of score column
sd(moody$Score)
```




Now we are ready to introduce basic data transformation techniques such as slicing and dicing.  Slicing, otherwise known as **subsetting**, allows the selection of data frame subsets. These subsets are defined by boolean conditions built from Attribute op value pairs where op is one of the arithmetic operators such as =, !=, < etc.  For example (Score >70)& (Grade ==’A’) refers to a subset of a data frame describing students who scored more than 70 points and got an A.  

Dicing refers to eliminating some of the attributes from a data frame  - it is vertical slicing - which results in a more “narrow” frame.
Finally we can also expand our data frame with new, so called derived, attributes. This is a very useful operation in data analysis since it allows so-called **“feature engineering”**. These new user-defined features can lead to totally new insights into the data. 

## Subset {#subset}

The following snippets demonstrate two ways of subsetting a data frame: first  through explicit function subset() and second through the native sub-data frame notation df[ ].

### subset() {#nrow}


```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")
#Subset of rows
moody_psychology<-subset(moody, Major== 'Psychology')
nrow(moody)
nrow(moody_psychology)

```

### Subframe


```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

#Alternate way to subset.
moody[moody$Major=="Psychology", ]
moody[moody$Major!="Psychology", ]
moody[moody$Score >80, ]
moody[moody$Score >80 & moody$Grade == 'B', ]

```

### Subsetting columns

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")
colnames(moody)
#subset of columns
moody3<-subset(moody, select = -c(1))
ncol(moody3)
# You can see the number of columns has been reduced by 1, due to sub-setting without column 1
ncol(moody3)

```

### Subsetting rows and columns


```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")
#Subset of Rows and Columns
moody1<-subset(moody, select = c(2:4), Major=="Psychology")
colnames(moody1)
#Notice that only 3 columns are remaining
dim(moody1)

```



One of the most important R instructions is **tapply**. It allows parallel execution of an aggregate function for different values of a categorical variable.

## tapply {#tapply}

tapply()  has four arguments:  the data frame (df), numerical attribute of df, categorical attribute of df and aggregate function (mean, max, min etc).  Syntax of df is as follows:
 
```tapply(df$numerical attribute, df$categorical attribute, aggregate function)```

- tapply() first slices data frame  df by different values of a categorical attribute and then computes an aggregate  (mean, median, min, max, etc..) of a numerical attribute  to each slice. 


### tapply()

```{r,height=300}

moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

#Distribution of grades for seniors who major in Economics
tapply(moody[moody$Seniority == 'Junior',]$Score, moody[moody$Seniority == 'Junior',]$Grade,mean)
```

### Combining table() and subset()


```{r,height=300}

moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022.csv")

#Distribution of grades for juniors
table(moody[moody$Seniority == 'Junior',]$Grade)
#Distribution of grades for seniors who major in Economics
table(moody[moody$Seniority == 'Senior' & moody$Major == 'Economics', ,]$Grade)
```


<!--
### Snippet 2 (*)

```{r,height=700}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

#Lets factor the grades on on_smartphone as well as grade category.
moody2<-tapply(moody$GRADE,list(moody$ON_SMARTPHONE,moody$GRADE),length)
# We can see it calculated count of the grade of student with respect to their in-class smartphone usage  and grade category.
moody2
barplot(moody2,col=c("red","cyan","orange","blue"),main = "tapply() example 2",beside = TRUE,legend=rownames(moody2))
```



### Cut 

- **cut()** function  divides the range of x into intervals. Provides ability to label intervals as well. It plays important role in defining derived attributes from attributes which are numerical.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

# Cut Example using breaks - Cutting data using defined vector. 
score1 <- cut(moody$SCORE,breaks=c(0,50,100),labels=c("F","P"))
table(score1)

```


#### More complex  example of defining derived attributes

The next snippet  illustrates defining a new numerical attribute, $adjustedScore of a student in the Moody data frame.  

Score is adjusted by the value of participation attribute in the following way:   

- If participation is larger than 0.5 - a bonus proportional to participation * 10 is added to the score.  

- If participation is smaller than 0.5, a penalty of 1-participation) * 10 is subtracted from the score. 

In this way, for someone with very small participation, the 10 point penalty will be imposed (10 points subtracted from the score). Conversely,  someone with perfect participation (1.0) will receive a 10 point bonus. 

##### Snippet 1

```{r,height=700}

moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")


moody$conditional <-0
moody[moody$participation<0.50, ]$conditional <- moody[moody$participation<0.50, ]$score -10*(1-moody[moody$participation<0.50, ]$participation)
moody[moody$participation>=0.50, ]$conditional <- moody[moody$participation>=0.50, ]$score +10*moody[moody$participation>=0.50, ]$participation

# print the column names
colnames(moody)

# lets look at the conditional attribute 
head(moody)

#subset the moody dataset rows = 1 to 10 and cols = 1,5
moody[1:10, c(1,5)]

#subset the moody dataset rows = 1 to 10 and cols = 1,5,6
moody[1:10, c(1,5,6)]

# print summary of inidividual columns
summary(moody$score)
summary(moody$conditional)

# Plotting the conditional attribute using boxplot
boxplot(moody$conditional,col = c("red"),main="Complex Example")

# Plotting the score attribute using boxplot
boxplot(moody$score,col = c("blue"),main="Complex Example")

```
-->


<!--chapter:end:chapters/tapply_subsetting.Rmd-->

# 🔖 Plots {#plots}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

When you import your data to R studio one of the first things you do is plot. Data visualization is a key components of data analysis. Before we talk about plots, we introduce some very basis data structures in R: vectors, data frames and tables.  These are introduced below in the form of code snippets that you can run and modify. 

**Then we are ready to plot!**

We will introduce several basic plots such as scatter plot, bar plot, boxplot and mosaic plot.  How do we know which plot to apply?  It depends on whether the variables to be plotted are categorical or  numerical.   Below we show a simple table which can serve as a guide which plot to use depending on types of variables to be plotted. 

<table>
<tr>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
NUM x NUM 
</td>
<td>
scatter plot
</td>
</tr>

<tr>
<td>
CAT x CAT
</td>
<td>
mosaic plot
</td>
</tr>

<tr>
<td>
CAT x NUM 
</td>
<td>
box plot
</td>
</tr>

<tr>
<td> NUM
</td>
<td> box plot, histogram
</td>
</tr>

<tr>
<td> CAT
</td>
<td> bargraph
</td>
</tr>
</table>


<!-- --- -->

<!-- ### Topics visited in this sub-chapter -->

<!-- * Scatter Plot -->
<!-- * Barplot -->
<!-- * Boxplot -->
<!-- * Mosaic Plot -->

## Scatter Plot {#scartterplot}

- Scatter Plot are used to plot two numerical variables.
- Hence it is used when both the labels are numerical values.


Lets look at example of scatter plot using Moody.

```{r,tut=TRUE,height=700}
# Let's look at a 2 attribute scatter plot.
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
plot(moody$participation,moody$score,ylab="score",xlab="participation",main=" Participation vs Score",col="red")


```



## Bar Plot {#barplot}

- A bar plot are used to plot a categorical variable. 
- This rectangle height is proportional to the value of the variable in the vector.
<!--
- Barplots are also used to graphically represent the distribution of a categorical variable, after converting the categorical vector into a table(i.e. frequency distribution table)
- In a bar plot, you can also give different colors to each bar. -->



```{r, tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#lets make a table for the grades of students and counts of students for each Grade. 

t<-table(moody$grade)

#once we have the table lets create a barplot for it.

barplot(t,xlab="Grade",ylab="Number of Students",col=colors, 
        main="Barplot for student grade distribution",border="black")
```



##  Box Plot {#boxplot}

- A boxplot is used to display a numerical variable.
- A boxplot shows the distribution of data in a dataset. 

![Boxplot](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/boxplot1.png)
<br />

- A boxplot shows the following things:
  - Minimum
  - Maximum
  - Median
  - First quartile
  - Third quartile
  - Outliers
  
<!--
- You can create a single boxplot using just a vector or a multiple boxplot using a formula.
- When you write a formula, you should use the Tilde (~) operator. This column name on the left side of this operator goes on the y axis and the column name on the right side of this operator goes on the x axis.-->



```{r,tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#Suppose you want to find the distribution of students score per Grade. We use box plot for getting that. 
boxplot(score~grade,data=moody,xlab="Grade",ylab="Score", main="Boxplot of grade vs score",col=colors,border="black")

# the circles represent outliers.
```


<!-- ## 4. Histogram -->

<!-- Refer Slide 15. -->

<!-- ```{r} -->

<!-- #Suppose you want to find the frequecy/distribution of cars with mileage in particular range. We use histogram for this.  -->

<!-- hist(automobile$`city-mpg`,xlim = c(0,100),xlab = 'milage', main = "Histogram of Car milage",col=colors,border="black") -->

<!-- # You can Change column range using breaks. -->

<!-- ``` -->


<!-- For more detail,reference and example refer Slides -->


##  Mosaic Plot {#mosaicplot}

- Mosaic plot is used to visualize two categorical variables.

<!--
- Mosaic plot is a graphical method for visualizing data from two or more qualitative variables.
- The length of the rectangles in the mosaic plot represents the frequency of that particular value.
- The width and length of the mosaic plot can be used to interpret the frequencies of the elements.
- For example, if you want to plot the number of individuals per letter grade using a smartphone, you want to look at a mosaic plot. -->


```{r,tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#suppose you want to find numbers of students with a particular grade based on their texting habits. Use Mosiac-plot.

mosaicplot(moody$grade~moody$texting,xlab = 'Grade',ylab = 'Texting habit', main = "Mosiac of grade vs texing habit in class",col=colors,border="black")


```

## Misleading Graphs 

Beware of misleading graphs. 

The following graphs artificially exaggerate their claims by manipulating either the Y-axis or X-axis. Typically this effect can be achieved by  moving the beginning of the scale (Y or X) from zero to much larger value.  Such axes manipulations result in exaggerating otherwise minor trends and differences. 

![Figure 4.6.1](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading1.PNG)

Differences between calories are not as large as it appears on the graph above. The range is just between 590 and 720, but because of moving the origin of the X-axis to 590, the bottom three bars seem to be multiple times larger than the top 3. KFC and MacDonald look much better than they should. 

![Figure 4.6.2](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading5.PNG)

Number of people on welfare appears to be growing rapidly.  In fact,  it is growing by total of  around 12%  in 3 years. This is a far cry from   4 times – when judging from the height of the last bar as compared with the first, leftmost bar.


![Figure 4.6.3](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading6.PNG)

It seems like the fraction of Democrats who agree with the court is much higher than the fraction of Republicans or Independents (seems like 3x of democrats agree with the court as compared to Republicans).  By moving the origin of the Y-axis to 50, the difference of 8% points between Democrats and  republicans is grossly exaggerated. 

Similar effects are achieved in the graphs below – moving the origins of axes exaggerates the difference in bar sizes. 

![Figure 4.6.5](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading11.PNG)

![Figure 4.6.6](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading12.PNG)

The home price increase is not massive! It only appears to be by manipulation of the graph! Neither is average class score for Ms Smith vs Mr. Jones.

- **Optical Illusions**

Depending how pie-chart is presented some slices may appear much larger than they really are. For example item C is the same size as slice A, but it appears much larger.

![Figure 4.6.7](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading2.PNG)

Notice the graph below is reversed. What looks like a drop is in fact increasing!

![Figure 4.6.8](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading10.PNG)

- **Misleading Message**

In these two side-by-side graphs we have Y-axes labeled by different variables (number of countries and cases per 10000 respectively). This is why the comparison is a bit like apples and oranges. 

![Figure 4.6.9](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading3.PNG)

- **Correlation vs Causation**

This is one of many examples of graphs showing spurious correlation which looks like causation. The two unrelated quantities change in time very similarly but have nothing to do with one another!

![Figure 4.6.10](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading4.PNG)

- **One sided arguments**

The graph below shows only half of what should be the full argument.  What about people who did not go to college at all? We are missing the “base” here.

![Figure 4.6.11](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Misleading7.PNG)





## Additional References

<button class="btn btn-primary" data-toggle="collapse" data-target="#plots12">Plots</button>
<div id="plots12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1L7ml_mwV7ms3eZ2qbznGgNdarhTfrSP6PWq-RxNUgQM/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

https://www.datamentor.io/r-programming/plot-function/

<!--
```
{r child="./chapters/datatransformation.Rmd"}
```
 
 -->

<!--chapter:end:chapters/plots.Rmd-->

# Data science vs Databases {#dbds}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```


At the beginning there is always data.  In R, it is a data frame which is a set of rows and columns.  Our Moody data set is an example of a data set.  It contains thousands of rows and a handful of columns.  
But is it a complete data set or is it just “tip of an iceberg”?    Is our dataset  “all there  is”?  Or is it just a sample from a possibly much larger, unknown, universe?

Data science and statistics deal with situations when data is partial.  In other words we do not have the whole data set.  We only have a sample, even if this sample is very large.  No matter how large the data is, it is still just  a sample from a larger hypothetical data set that may involve future, unknown yet, data values. 

On the other hand ,in the field of databases we believe that our data set is complete. Data is “as presented”. We query such data and treat the answers as exact values, since we fundamentally trust that the data we have is correct and *complete*.   This is quite different from the data science approach which always treats data as an incomplete sample of the unknown, real, data set.  Thus, data science deals with approximations.

We formulate the main principle behind data science


**Data is always a sample**

**Example:** Airbnb data set.

Let airbnb be the data set describing Airbnb rentals in New York City. It has 5287 tuples and is a set of listings with room type, neighborhood, neighborhood group (more specific names of hundreds of NYC neighborhoods such as Tribeca, East Village, Park Slope etc), floor  and price. 

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnbSmall.csv") #web load
# head(moody)
temp<-knitr::kable(
  head(moody, 5), caption = 'Snippet of small airbnb Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  head(moody, 5), caption = 'Snippet of small airbnb Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

There are two approaches of treating this data set:

- **Approach 1:**  airbnb data set is complete. We just treat it as “as is". We can run queries against Airbnb data  sets and return definite query answers.  For example we can ask about the fraction of Airbnb rental offerings which are located in Brooklyn (it is around 42.1%). Or the average price of Brooklyn rental?  It is $179.06.  Answers to our queries are exact.   

- **Approach 2:** airbnb data set is incomplete. It is just a sample of much larger, unknown set of all prices of airbnb rentals *ever* (even though we do not have timestamps associated with rentals stored in the Airbnb table)  If this is the case, answers to queries are no longer definite and deterministic but rather estimators of *real values* which are unknown to us.  

Let us examine these same queries:  query Q1 about frequency of Brooklyn Airbnb rentals and query Q2 about mean price of Brooklyn Airbnb rentals.  

We begin by running this two queries against airbnb data, assuming it is *complete data set*.  In other words we follow Approach 1.  Thus, we treat the airbnb data set “as is” – and the answers to our queries are prices: 42.1% and $179.06 respectively.

## **Query 1:** Fraction of Brooklyn Airbnb Rentals 

The following query returns the required fraction of Brooklyn rentals

`round(nrow(airbnb[airbnb$neighbourhood_group=='Brooklyn',])/nrow(airbnb),3)`

calculates the fraction of Brooklyn Airbnb offerings – 42.1%. 

### Fraction of Brooklyn Airbnb rentals) 

```{r,tut=TRUE,height=300}

airbnb<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnbSmall.csv')   
colnames(airbnb)

round(nrow(airbnb[airbnb$neighbourhood_group=='Brooklyn',])/nrow(airbnb),3)

```

Now, let's follow Approach 2 and not trust that the airbnb data set is complete, but rather a sample of some larger “complete” airbnb data set. Under normal circumstances we will never know this “real” data set.  But here let us assume that we know about airbnbReal.  It comprises 35,573 tuples. Our data set airbnb was the random sample of 5278 tuples  out of the total of 35,573 of airbnbReal. Not surprisingly, the value of our query  will now return a different value (42.4%)  - making our original query answer just an estimate. 
  

### Fraction of Brooklyn Airbnb rentals in the *real* airbnb data set

```{r,tut=TRUE,height=300}

airbnbReal<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv')

round(nrow(airbnbReal[airbnbReal$neighbourhood_group=='Brooklyn',])/nrow(airbnb),3)

```

The answer to our query - the real fraction of Brooklyn rentals differs now from 42.4%. It is close, but not the same. Thus, the fraction 42.4% obtained from the sample is only an estimate of the real fraction of Brooklyn apartments.  The quality of this estimate depends on the size of our sample data set.  In our case that sample size was 5278 tuples. The smaller the size of the sample, the less accurate the estimate is. Data is just a sample. This is the data science approach. 
 

The following three histograms show distribution of frequency of Brooklyn apartments across 1000 samples of  different sizes.  We show three sample sizes here:  samples of 1000 apartments each, 100 apartments each and finally 5000 apartments each taken from the airbnbReal data set.  Notice how the spread of the frequency values depend on the sample size. The larger the data sample is, the “tighter” is the histogram. In the case of a sample size of 1000, we may obtain frequencies of Brooklyn offerings ranging from 38 to 46%.


![Figure 5.1: Distribution of the fraction of airbnb apartments which are located in Brooklyn - 1000 samples each of the size =1000](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Brooklyn-samples1000.png)

The Histogram below shows distribution of frequencies of Brooklyn Airbnb rentals if our database has only 100 offerings. Notice that now if we run 1000 such samples of size 100, the spread is much larger (it is determined by standard deviation) . Now we can get frequencies ranging from 30% to 55%!

![Figure 5.2: Distribution of the fraction of Brooklyn apartments - 1000 samples each of the size =100](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/BrooklynSamples100.png)

Finally, the last of the three histograms shows spread of frequencies over 1000 samples each of size 5000.  This time the spread is much smaller – and we range from 41 to 44% with some outliers of course.  All these histograms illustrate a simple fact that if we believe our data set to be incomplete and just a sample of a larger data set, we can only answer queries with estimates.  One of the key objectives of data science is to provide these estimates with some measure of confidence (see section 11)

![Figure 5.3: Distribution of the fraction of Brooklyn apartments - 1000 samples each of the size =5000](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/BrooklynSamples5000.png)

## **Query 2:**  Mean price of Brooklyn Airbnb rentals 
     
The following query returns the mean price of Brooklyn rentals

`round(mean(airbnb[airbnb$neighbourhood_group=='Brooklyn',]$price),2)`

This returns $179.06

### Mean price of Brooklyn airbnb rentals

```{r,tut=TRUE,height=300}

airbnb<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnbSmall.csv')  

round(mean(airbnb[airbnb$neighbourhood_group=='Brooklyn',]$price),2)

```

Trusting airbnb it is a complete data set and following approach (1), we view $179.06 as precise and definite answer.

As in case of query 1, we also follow approach (2) and assume that in fact airbnb is nothing but a sample of 5287 tuples from the airbnbReal data set of It comprises 35,573 tuples. Our data set airbnb was the random sample of 5278 tuples  out of the total of 35,573.  Not surprisingly, the value of our query  will now return a different value $179.84 making our originally returned mean price just an estimate.  Thus, the mean price of $179.06 was only an estimate of the real mean price of $179.84.

### Mean price of Brooklyn airbnb rentals in real Airbnb data set

```{r,tut=TRUE,height=300}

airbnb<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv')  

round(mean(airbnb[airbnb$neighbourhood_group=='Brooklyn',]$price),2)

```


In a similar way for Query 1, we present histograms of mean price of Brooklyn Airbnb offerings for 1000 samples of sizes 1000, 100 and 5000 respectively from airbnbReal data set of 35,573 tuples. Again we observe that the larger the sample, the tighter the histograms. For example, for samples of size 100, the mean price for a sample can range from $140s to $220s. 

These histograms for mean price illustrate a very important concept in statistics called Central Limit Theorem.   This remarkable theorem says that regardless of what the original distribution of data is, the distribution of means of samples follows normal distribution (the Bell curve), as illustrated in the histograms below. 

Again, if we treat the airbnb data set we have only as a sample, we have to provide measures of the quality of estimates such data provides for any query.  Only when we take data ‘as is’ can we talk about definite query answers. Otherwise, no matter how large our data set is, it is only a sample. This is probably the shortest summary of data science methodology. Nothing is exact. Everything is just a sample. As such, we need to do better than just mistrust the data – we should provide measures of quality of the estimates that are down from data as samples. 

![Figure 5.4: Distribution of mean Brooklyn airbnb rental price  for  1000 samples each of size =1000](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/BrooklynPriceSample1000.png)

![Figure 5.5: Distribution of mean Brooklyn airbnb rental price  for 1000 samples each of size =100](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/brooklyn2.png)


![Figure 5.6: Distribution of mean Brooklyn airbnb rental price  for 1000 samples each of size =5000](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/BrooklynPriceSample5000.png)

The following snippet (5.5) shows impact of different samples of airbnbReal data set on queries Q1 and Q2. You can change the size of a sample and run this code repetitively to see how the  values returned by query 1 and query 2 change.  Instead of obtaining this particular sample of 5287 of tuples from the aibnbReal we could have obtained a different random sample of 5287 tuples and get different estimators of frequency of Brooklyn  rentals as well of the mean price.  Change 5287 to any number (1000, 100,2000) and rerun the snippet 5.5

```{r,tut=TRUE,height=400}

airbnbReal<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv')  

v <- sample(1:nrow(airbnbReal))
airbnb<-airbnbReal[v,]
airbnb<-airbnb[1:5287,]
round(nrow(airbnb[airbnb$neighbourhood_group=='Brooklyn',])/nrow(airbnb),3)
round(mean(airbnb[airbnb$neighbourhood_group=='Brooklyn',]$price),2)

```

We can repeat similar histograms for any queries on the Airbnb data set such as 

- Q3:  Mean prices of Tribeca apartments on floors higher than 5th floor

- Q4:  Fraction of high floor offerings in Manhattan (high floor >6)

- Q5: Mean prices of private room apartments in Manhattan

- Q6:  Fraction of private rooms under $150 in Queens

Etc.

Each such query has an exact answer if we believe that data is complete and is only an estimate if we treat data as a sample. 

<!--chapter:end:chapters/datascience_databases.Rmd-->

# Fooled By Randomness {#fbr}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

In this section we show how random data can fool us. Often, random findings may look real, in fact so real that we mistake them for real discoveries.  It is one of the main objectives of statistics and data science to help identify such false discoveries.  This chapter provides a motivation and justification for hypothesis testing methods, multiple hypothesis corrections, p-values and permutation tests  which we discuss in subsequent sections of the active textbook.

Here we show how randomly generated data set can lead to seemingly astonishing discoveries – which are simply …random, although they look anything but random!

In order to illustrate false discoveries we have used our DataMaker data generation tool to generate a data set Cars describing hypothetical sales data for car dealerships in 15 major cities in the US. We have considered 10 major car makers, different seasons of the year (winter, spring, summer, fall). In addition  Cars stores information about buyer gender as well as buyer’s age.  Below we show a sample of several tuples of Cars




```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Cars Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Cars Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

We have generated the Cars data set  as completely random data set, with uniform distributions of  dealerships, car makes sold there, four seasons, buyer’s gender’s and their age.  Thus any buyer with any age is equally likely to buy any car in any of the dealerships.

We have run several queries on Cars dataset and demonstrated through the following snippets, how random data can lead to query findings which look “real”.  

Let us start with the first snippet which shows that indeed our data set follows uniform distribution and it has been randomly generated.
 
**Snippet 6.1:** Get to know your data
```{r, tut=TRUE, height = 500}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")

nrow(Cars)
summary(Cars)
unique(Cars$Dealership)
unique(Cars$Car)
unique(Cars$Season)
unique(Cars$Buyer)
table(Cars$Dealership)
table(Cars$Car)
table(Cars$Season)
table(Cars$Buyer)
tapply(Cars$Buyer_Age, Cars$Car, mean)
tapply(Cars$Buyer_Age, Cars$Dealership, mean)
tapply(Cars$Buyer_Age, Cars$Season, mean)

```

We can see that each car maker is sold around 9% of the time, and 9% of transactions take place in each of the dealerships from LA to NYC. We also see that around 25% of transactions take place in each season. Also buyers' gender is distributed equally 50:50 across all transactions for each car and for each dealership. In other words, nothing interesting is happening in this data – it is unrealistically uniform.

Nevertheless, even this purely random data leads to some seemingly exciting discoveries. We will first show a few examples of these false discoveries and then follow up with explanations how such discoveries can emerge out of pure randomness. How can we be so fooled and what are remedies to avoid trusting such random discoveries?.  In the second subsection we discuss sampling and dangers which emerge from drawing conclusions from data samples.


## False Discoveries

Here we list some of the false discoveries:

**Snippet 6.2:**  Among very young customers (Age <22) of  Chicago dealership  Dealership, 12.2% buy Hyundai and only 7.4% by GMC.  Thus, almost twice as many of the very young customers prefer Hyundai to GMC.

```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")

round(table(Cars[Cars$Dealership=='Chicago'& Cars$Buyer_Age <22,]$Car)/nrow(Cars[Cars$Dealership=='Chicago'& Cars$Buyer_Age<22,]),4)

```

**Snippet 6.3:**  Among senior customers (Age >70) of LA dealership, 13.1% buy Toyota and only 6.25% buy GMC.  Interesting findings! Twice as many seniors in LA  buy Toyota than GMC!


```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")

round(table(Cars[Cars$Dealership=='LA'& Cars$Buyer_Age >70 &Cars$Season=='Winter',]$Car)/nrow(Cars[Cars$Dealership=='LA'& Cars$Buyer_Age>70&Cars$Season=='Winter',]),4)

```


**Snippet 6.4:**   Gender distribution of Honda purchases in  Austin dealership favors female customers – 54%  Female to 46%  Male

```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")

round(table(Cars[Cars$Car=='Honda' &Cars$Dealership=='Austin',]$Buyer)/nrow(Cars[Cars$Car=='Honda'&Cars$Dealership=='Austin',]),4)
```

We discover seemingly a significant disparity between genders among Honda Buyers in Austin. 8% difference between female and male customer share seems significant. 

**Snippet 6.5:**  Mean age of Jeep buyers in Chicago in spring is  4 years higher than in summer. It is 54 in spring vs 50 in spring.

```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")

tapply(Cars[Cars$Car=='Jeep'&Cars$Dealership=='Chicago',]$Buyer_Age, Cars[Cars$Car=='Jeep'&Cars$Dealership=='Chicago',]$Season, mean)
```

 Younger Jeep Buyers in Chicago wait for summer!
 
**Snippet 6.6:**  The gender gap  among GMC buyers in LA who are in late twenties is 22%!, 61%  are female and only 39% are Male
 
```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")

round(table(Cars[Cars$Car=='GMC' &Cars$Dealership=='LA' & Cars$Buyer_Age >25 &Cars$Buyer_Age <30  ,]$Buyer)/nrow(Cars[Cars$Car=='GMC'&Cars$Dealership=='LA'& Cars$Buyer_Age >25 &Cars$Buyer_Age <30,]),4)
```

GMCs are drastically more popular among women than among men in LA. Certainly all these discoveries are false! Tricks that random data plays on us. 

There is a number of factors leading to these  false discoveries:
 
-   Subsets of data are pretty narrow – GMC buyers in winter in one dealership who are in a certain age bracket constitute a small subpopulation of the whole data set. So called Law of small numbers which we discuss later states that such  small data sets are  prone to extreme results.

-   There is an exponential number of queries which look like 6.1-6.5 and differ only by choice of parameters (age, dealership name, and car make name). Running a sufficient number of such queries every time changing one or more parameters,   we are simply bound to find some false positives. This is called multiple hypothesis testing.  Before we presented queries  6.1-6.5 above we have run a number of queries which did not generate false discoveries. This is sometimes called p-value hunt and multiple hypothesis testing and Bonferroni coefficient (discussed in section 9) takes this trial and error process under consideration.

-   Difference of means may be due to a simple random process. We need a permutation test, z-test and similar test to see if a conclusion like 6.3 is statistically legitimate. It is not. But we cannot just eyeball it and decide.

## More about Sampling

We have already  shown the distribution of prices of Brooklyn apartments across samples of different sizes. Here we show more examples of sampling and its impact on different queries.

We will begin by showing how we select a sample of a given size from the data set. Here for Cars, we select 5000 random tuples from the total of 50,000 tuples.



**Snippet 6.7:**  Sampling

```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")
 
v<-sample(1:nrow(Cars))[1:5000]
SampleCars<-Cars[v, ]
```

 The function sample() permutes the rows of Cars data frame. Then we select the first 5000 tuples from the shuffled Cars data frame (you may change this number). 
 
Now let us run several queries on the samples and compare them with the results of the same queries over the entire Cars data set.  Note: students are encouraged to run sampling code multiple times and see how query results change!  This should serve as a warning about trusting samples. One can also change the sample size  and see what effect does it have on a query.


**Snippet 6.8:** Effect of random samples of different sizes on the average age of a buyer.
 
```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")
v<-sample(1:nrow(Cars))[1:5000]
SampleCars<-Cars[v, ]
mean(SampleCars$Buyer_Age)
mean(Cars$Buyer_Age)
```

**Snippet 6.9:**  Effect of random samples of different sizes on the average age of a buyer of Ford in Chicago

```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")
v<-sample(1:nrow(Cars))[1:5000]
SampleCars<-Cars[v, ]
mean(SampleCars[SampleCars$Dealership=='Chicago' & SampleCars$Car=='Ford',]$Buyer_Age)
mean(Cars[Cars$Dealership=='Chicago'&Cars$Car=='Ford',]$Buyer_Age)
```

The more narrow is the query, the more discrepancy there is between query results over samples.

**Snippet 6.10:** Skewed car maker distributions among female buyers older than 45 in theSan Antonio dealership

```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")
v<-sample(1:nrow(Cars))[1:5000]
SampleCars<-Cars[v, ]
round(table(SampleCars[SampleCars$Buyer_Age>45 & SampleCars$Buyer=='Female'&SampleCars$Dealership=='San Antonio',]$Car)/nrow(SampleCars[SampleCars$Buyer_Age>45 & SampleCars$Buyer=='Female'&SampleCars$Dealership=='San Antonio',]),4)
round(table(Cars[Cars$Buyer_Age>45 & Cars$Buyer=='Female'&Cars$Dealership=='San Antonio',]$Car)/nrow(Cars[Cars$Buyer_Age>45 & Cars$Buyer=='Female'&Cars$Dealership=='San Antonio',]),4)
```
 
We observe a very skewed result here – Honda is best seller among Female buyers over 45 in San Antonio, with 14%  buyers of Honda vs just 5.4%  buyers of GMC.  

We have  generated three more samples and every time the best seller in this group of buyers was different!

- First Sample:
  - Jeep 14.5% (bestseller)
  - Nissan 4.8%  (worst seller)	- The gap of almost 10%!

- Second Sample:
  - Huyndai 11.8% 
  - Jeep  5.2% 
  
- Third Sample:
  - GMC (11.8%)
  - Chevrolet (4.7%)
  
Quite a variation!
 
Let us now take one specific Car make-, say GMC   and analyze its purchases among  buyers younger than 30 

**Snippet 6.11:**  Analysis of market for GMC buyers

```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")

v<-sample(1:nrow(Cars))[1:5000]
SampleCars<-Cars[v, ]
 
round(nrow(SampleCars[SampleCars$Car=='GMC' &SampleCars$Buyer_Age <30 ,])/nrow(SampleCars[SampleCars$Buyer_Age <30 ,]),4)
```

We can observe quite substantial variation of GMC market share among buyers younger than 30.  The market share varies from  6 to 10% among different samples. In general samples become more sensitive also with more narrow queries. If we only considered GMC market share (dropping age constraints), we would observe much smaller market share variation.
 
**Snippet 6.12**	Average age distribution grouped by all car makes in in Chicago

```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")

v<-sample(1:nrow(Cars))[1:500]
SampleCars<-Cars[v, ]
tapply(SampleCars[SampleCars$Dealership=='Chicago',]$Buyer_Age, SampleCars[SampleCars$Dealership=='Chicago',]$Car, mean)
```

Notice how for a smaller sample of 500 the distribution of average age varies for different car makes – from almost 60 for Subaru to around 40 for Nissan.  

## Confidence Intervals

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/ElectionsID.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Election Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Election Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br />

This data contains synthetic data on 200000 voters in some small county who voted either Democrat or Republican. The majority of voters (55%) voted republican, while the minority voted for Democrats. Typically votes are not counted simultaneously and we are all familiar with “calling elections” on the basis of a smaller subset of counted votes. 

In the following snippet we would like students to experience the effects of samples of different sizes. Samples may provide different results from an even more decisive win of Republicans (like 60%+) to a slight win by Democrats.


**Snippet 6.13:**  Distribution of votes for Democrats vs Republicans in a random  sample of 100 voters  may deviate from 45:55 split in the general population.  

```{r, tut=TRUE, height = 300}

# Read in the data
Elections <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/ElectionsID.csv")
colnames(Elections)

v <- sample(1:nrow(Elections))
Elections<-Elections[v,]
SampleE<-Elections[1:100,]
table(SampleE$Party)
```

Just run the code several times and observe how the distribution of votes between Democrats and Republicans changes.  Also, change the size of the sample from 100 to 1000 and perhaps even to 10000. Observe how more “stable” the results are and how much closer the vote distribution is now to the true vote distribution in the general population of voters.  


The next snippet is the only one when we  use a simple  for loop in this entire textbook. 

Below we display the histogram of 1000 samples of size 500 and we observe how the support for Democrats varies. Please observe that the histogram’s shape approached Bell’s curve with the mean of 45% (true voter support for Democrats). Notice however that there is a fraction of samples which would incorrectly have Democrats as winners, exceeding 50% of vote.

![Figure 6.1 Democratic votes](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/DemocraticVotes.png)



## Law of Small Numbers

The term “Law of small numbers”  has been coined by Kahneman in his book “Thinking fast, Thinking slow”.  

The law of small numbers states that small samples are more prone to extreme results than large samples.  Kahneman refers to the example of incidences of kidney cancers. It seems that kidney cancers are less frequent in small rural counties in mid-west.  - probably because life in small rural counties is more healthy, food is organic etc.  Unfortunately, kidney  cancers are also more frequent in small rural counties in midwest. This time (false) narrative can attribute it to poverty and poor access to healthcare. In fact none of the two narratives is correct. Small samples (small counties in this case) are simply more prone to extreme results.  This is analogous to selecting a small number of balls from an urn containing red and black balls. With a small number of balls (4) the extreme results (4 black, 0 red, 4 red, 0 black) occur more often than when the number of selected balls is larger, say 7. 

Here we illustrate the law of small numbers using our Car dealerships data set. For small sizes of samples (10), we observe that Winter is the season when car sales are dramatically higher than expected seasonal sales (about 25% of sales for each of the 4 seasons in our synthetic data set).  For different samples of size 10, we may conclude that winter is the season when sales are much lower than expected. Thus, the opposite effect!

Use the snippet by running the code several times and also changing the size of the sample from 10 to 20, 30 and down to 5 as well to observe effects of law of small numbers.

**Snippet 6.14** Law of small numbers snippet

```{r, tut=TRUE, height = 300}

# Read in the data
Cars <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")
v<-sample(1:nrow(Cars))
sampleCars<-Cars[v,]
table(sampleCars[1:10,]$Season)
```

## Randomness and repeatability

When data is random, “observed” trends do not repeat themselves.. They come and go, with different random samples. 

How likely is it that GMC are bought more often by females than males in Columbus, season after season, sample after sample?

If the data is always random, i.e. the next data is again randomly generated, we are likely to see the reversal of the trend. In fact every time we get the new data, data may be reversed. In data science we are looking for lasting trends, we would like to treat the data as representative of the general trend which outlasts the current sample. The whole data set after all is a sample of some unknown “universal data set”.  Here we demonstrate what happens when universal data set is in fact randomly uniform,
 
Notice that if we generate new samples, we may get a complete reversal of the trends, in the same dealership we may get Toyota as most popular and Honda as least popular, and for another sample Toyota and Honda may switch places!

Randomness  is the main force fooling us and has to be treated very seriously.  Eliminating randomness as the “cause” of our observations is one of the main goals of data science. Hypothesis testing allows us to take randomness into account and the concept of p-value is a quantitative measure of possible impact of random data.  
 

##  Data sets with embedded patterns

Using the DataMaker tool we have embedded the following two patterns into Cars data:
 
Subaru in Austin is bought by 95% females

GMC in Columbus is bought by 93% males
 
The following snippet shows the skewed distribution of females and males due to the pattern injected by DataMaker into the cars data set.
 
**Snippet 6.15** Car data set with embedded pattern

```{r, tut=TRUE, height = 300}

# Read in the data
CarsB <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Cars2022.csv")
table(CarsB[CarsB$Car=='Subaru'& CarsB$Dealership =='Austin',]$Buyer)
```

These two patterns are intended to be legitimate - not false discoveries. We have made them rather extreme, skewing the gender distribution in both cases above 90%. But what if we made it slightly more than the uniform – random distribution in Cars, for example 60:40 distribution?  The challenge for statistical evaluation methods is not to reject such True discoveries. The closer the real pattern is through to the possible random pattern, the more likely such False Negatives are. Thus, in the process of rejection of False Positives we may also fall into the trap of False Negatives and reject the legitimate discoveries. 
Being too cautious may deprive us from discovering a real business value in our data. Being too adventurous, may result in false discoveries. There is a cost associated with each of these two errors. Is it better to be too optimistic than to be too pessimistic?





## Hidden Variables 

Another common situation when data may fool us is the case of hidden variables. This is also known as the dilemma between correlation and causation. 

To illustrate the famous problem of hidden variables, we will use the following  synthetic data set describing incomes of law school graduates of top 100 universities, 5 years after graduation. 


Each tuple records the income of a graduate, ranking of the school they attended (from 1 to 100), tuition they paid, their SAT score and finally their GPA.  The common belief is that the rank of a school (and tuition paid) is the prime factor driving salaries up. The higher the school rank, the higher the 5-year after graduation salary.  This is what schools like to tell their prospective students - you pay higher tuition in return for higher future earnings. Great investment! 

Not quite so.

The real variable which drives salaries is SAT score.  Students with higher SAT scores are accepted to higher ranked schools, while students with lower SAT scores are chosen by lower ranked schools. Thus (as actually pointed out in famous New York Times article),  higher ranked schools simply accept stronger students and school “success” has less to do with how they teach but more with the academic quality of students they accept. 

This is illustrated by the linear relationship between the Income and SAT. There is also a linear relationship between the SAT scores and the ranking of the school (higher ranked schools accept candidates with higher SAT) as well as a linear relationship between Tuition and the school rank - the higher ranked schools charged higher tuitions. 

This is a fundamental question between Causality and Correlation. SAT is called Hidden variable which may possibly incorrectly indicate that Income is driven by school rank or by tuition, instead, both variables are driven by SAT score. Thus, in this data set it is a SAT score which drives Incomes.

SAT score is the hidden variable in this example.


```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/IncomeSchool.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of School income Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of School income Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br />

**Snippet 6.16**   Incomes of school graduates – the case of Hidden Variable

```{r, tut=TRUE, height = 400}
IS<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/IncomeSchool.csv")

plot(IS$SAT, IS$Income)
plot(IS$Tuition, IS$Income)
plot(IS$SchoolRank, IS$Income)

plot(IS$SchoolRank, IS$Income)
plot(IS$SAT, IS$Income)
plot(IS$SAT, IS$SchoolRank)

```

Notice also that GPA in this data set has really no correlation with Income. Of course in reality GPA may be correlated with income, here in our synthetic data set it is not. One can observe it by plotting Income as a function of GPA.






<!--chapter:end:chapters/fooled_by_randomness.Rmd-->

# Normal distribution {#nd}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

Let us now introduce the normal distribution and its characteristic Bell curve.

![Figure 7.1  Histogram of normal distribution generated in snippet 7.1](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/NormalPlot.png)

 There are two important parameters of normal distribution: mean and standard deviation. The mean of histogram from 7.1 is equal to 10 and the standard deviation is equal to 1.

Standard deviation is a very important measure of data spread. It is as important as the mean value, since it tells us how widely or narrowly data is spread around the mean. Mathematically, standard deviation is defined by the following formula:

\begin{equation}

\sigma = \sqrt{\frac{\sum(x_i - \mu)^2}{N}}\\

\text{where:}\\
\sigma \text{= population standard deviation}\\
\text{N = the size of the population}\\
x_i \text{= each value from the population}\\
\mu \text{= the population mean}\\

\end{equation} 




The squaring of differences between data points and the mean value is important since it treats negative and positive differences identically. Dividing by the number of data points is important for normalization.

Given a numerical variable x, sd(x) is the R function which computes standard deviation. It is demonstrated in the snippet 7.1

These will be useful later in hypothesis testing.

In the following snippet we use two functions rnorm() and pnorm(). Function rnorm() generates a number of data points according to normal distribution assuming the mean=10 and standard deviation (sd) =1.  Normal distribution is the core distribution in data science (although power law distribution is also gaining ground as we explain in a separate section).  Normal distribution plays a special role due to the Central Limit Theorem \@ref(sampling).

Feel free to change parameters of rnorm() in snippet 7.1 and see the effect of mean and standard deviation in the shapes of resulting bell curves.

The statement length(normal[normal >x]) shows the number of data points which are larger than x. In the snippet x=12. Notice how quickly  length(normal[normal >x]) goes down to zero when one increases x. The statement z<-(12-10)/10 calculate the z-value, which is defined as difference between x and the mean of the distribution divided by standard deviation. This helps normalization and allows talking about vastly different data sets in the same terms. 

Z-value is the distance measured in sigmas, where sigma stands for one standard deviation. 

The function pnorm(z) calculates the fraction of data points in normal distribution whose z-value is less than z. Regardless of what standard deviation and mean of a given data set is, this fraction is always the same for the same values of z.  Z-value is the common denominator normalizing all data distributions. With z-values we may disregard the range of data as well as its mean. It allows us to define distance from the mean in units which are standard deviations (sigmas). 

The reader is encouraged to change all the parameters of this snippet and see the effect of z-value on the number of values exceeding z. This fraction becomes negligible roughly around z approaching 3. For z-values larger than 5, one cannot observe a single value with z>5, unless the data set is very large.  Thus, the decline of the Gaussian Bell curve is very steep!


```{r,tut=TRUE,height=300}

normal<-rnorm(10000,mean=10, sd=1)
sd(normal)
mean(normal)
hist(normal, 50)
length(normal[normal >12])
z<-(12-10)/10
1-pnorm(z)


```

1-pnorm(z) returns the area under the bell curve with z value larger than z. In our case z=0.2. As you can check 1-pnorm(z) returns 0.42. Thus, 42% of the area under the bell curve is larger than z=0.2.

<!--chapter:end:chapters/normal_distribution.Rmd-->

# 🔖  Data Transformation with Derived Attributes

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

R allows creating new data frame attributes (columns) “on the fly”. These are new vectors, which are often defined as functions of existing attributes. Hence, the name - derived attributes.

Derived attributes will play an important role in data exploration as well as in building prediction models. Very often, derived attributes allow discovery of important patterns in data. Similarly, derived attributes may be more predictive than original attributes in the imported data sets.

The term feature engineering is often used in machine learning to describe creation of derived attributes.


## Making new categorical attributes

Here we define a new attribute PF (Pass/Fail) to “Pass”.  Students who got A, B or C, passed. Students who received F, failed. We are grouping values of Grade into two categories of a new categorical attribute PF. 

The line 5 replaces “Pass” by “Fail” for students who received F. 

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv") #web load

# Cut Example using breaks - Cutting data using defined vector. 
moody$PF<-'Pass'
moody[moody$GRADE=='F',]$PF<-'Fail'
moody

```

## Making categorical attribute from numerical attribute using function Cut()

- **cut()** function  divides the range of x into intervals. Provides ability to label intervals as well. It plays important role in defining derived attributes from attributes which are numerical.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv") #web load

# Cut Example using breaks - Cutting data using defined vector. 
score1 <- cut(moody$SCORE,breaks=c(0,50,100),labels=c("F","P"))
table(score1)

```


## Making new numerical attribute from numerical attributes

Suppose we would like to combine score and participation into one combined score. We can define a new numerical attribute from SCORE and PARTICIPATION . We can see that the moody data frame will be expanded by the additional attribute. 

```{r,height=300}

moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv") #web load

moody$Combined <- (moody$SCORE  + 10* moody$PARTICIPATION)
moody[1:10,]


```



## More complex  example of defining derived attributes

The way we define combined score attributes rewards students even for poor participation. Their combined score is always higher than their score in class even if their participation was quite low. It would make more sense to define combined score by either penalizing for poor performance or rewarding good performance. 

The next snippet illustrates defining  such a new numerical attribute, $adjustedScore of a student in the Moody data frame. adjustedScore penalizes low participation  or rewards for good participation. 

Score is adjusted by the value of participation attribute in the following way:
 
- If participation is larger than 0.5 - a bonus proportional to participation * 10 is added to the score.
- If participation is smaller than 0.5, a penalty of 1-participation) * 10 is subtracted from the score.

In this way, for someone with very small participation, the 10 point penalty will be imposed (10 points subtracted from the score). Conversely, someone with perfect participation (1.0) will receive a 10 point bonus.

```{r,height=700}

moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv") #web load

moody$conditional <-0
moody[moody$participation<0.50, ]$conditional <- moody[moody$participation<0.50, ]$score -10*(1-moody[moody$participation<0.50, ]$participation)

moody[moody$participation>=0.50, ]$conditional <- moody[moody$participation>=0.50, ]$score +10*moody[moody$participation>=0.50, ]$participation

#Lets see a sample of new rows of moody with the new derived attribute
moody[1:10,]


```

We are now able to transform our data by slicing and dicing rows and columns, using subset function (or sub-data frame), we can also add new attributes as shown above.  Data Transformation is critical not just in data exploration and plotting but foremost in building high quality prediction models as we will show later. 

## Additional references

<button class="btn btn-primary" data-toggle="collapse" data-target="#subset12">Data Transformation</button>
<div id="subset12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1--AeLFVrESqyWO7iNiIBRW7pX-VSYtBco1o8bOpOatE/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/Derived_attributes.Rmd-->

# 🔖 Hypothesis Testing {#ztest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

Randomness is the biggest enemy of data scientists. How to distinguish what's real from what's random? This is the goal of hypothesis testing.  We will introduce hypothesis testing through the permutation test. To illustrate the permutation test, let us start with a simple example of a dataset storing information about traffic in Lincoln and Holland tunnels. 

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
traffic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv") #web load
# head(moody)
temp<-knitr::kable(
  traffic[sample(1:nrow(traffic),10), ], caption = 'Snippet of Traffic Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br> 

We observe that Lincoln traffic is higher than Holland tunnel traffic by calculating average traffic volume per minute for each of the tunnels using the provided data.

We conclude with 68.54 mean volume per minute for Lincoln and 67.71  mean volume per minute for the Holland tunnel. This seems to indicate that Lincoln traffic is higher than Holland traffic.  But is it?  Or is it just random deviation? Perhaps if we took more measurements, the trend would be reversed? This is where the permutation test comes handy. First, let us  talk about the null hypothesis and the alternative hypothesis.


**Null hypothesis** for the Lincoln-Holland tunnel observation  is that,  not surprisingly,  there is no difference in traffic  between the two tunnels.   

The **alternative hypothesis** states that Lincoln tunnel is more busy than Holland tunnel.

Does observed data (observed traffic difference) provide us with enough evidence to *reject null hypothesis* and in fact support the alternative  hypothesis?  To answer this question we need to decide whether the observed result is reasonably likely to come up randomly under the condition that NULL hypothesis holds.  

How likely it is  that observed difference (D=0.83) comes *randomly*?
Permutation test helps us to estimate the chance that D=0.83 will come up randomly under the condition that traffic in Holland and Lincoln tunnels is equal.

In each permutation of the permutation test we  randomly scramble the traffic table once. Permutation test is run many times, typically 10,000, even 100,000 times, and each permutation simulates a random process by simply reassigning the traffic volume values randomly between tunnels. The numbers of traffic measurements in Holland and Lincoln  tunnels respectively remain the same. Existing values are scrambled  though - breaking any relationship between volume numbers and tunnel names. Each permutation is like rolling a dice. How often will this  random process produce the result which is at least as extreme as D=0.83 that we have observed? The less often it happens the more likely it is that what we have observed is NOT random. For example, if we can get our observed result only 3 times in 1000 rolls of a dice (permutation test) it means that with probability of 99.7% our observed result cannot be random.

Permutation test provides a palpable experience of randomness. Just roll the dice many times and see how often you can get the observed result or more. If you can get D>0.83 relatively often (above what is called significance level usually it is at least 5% of the time), then you cannot reject the null hypothesis. In other words the conclusion that your observation appeared RANDOMLY. Otherwise, we can conclude that observation was not random - and we reject the null hypothesis.

Notice, that every time we run the permutation test function we may get slightly different p-values. This is because permutations are random. The more times we run a permutation test, the closer it will approximate the “real” p-value.  Snippet 6.1 shows permutation test results for the Traffic data set. 

Another test which is often used for difference of means hypothesis testing is the z-test.  It is described very well in the attached link to the Khan Academy lecture.  Here we run z-test function in one of the following snippets.

## Permutation test {#permutaion}


The following snippet \@ref(permutaion) shows the code for hypothesis test of difference of means.

Is the mean traffic (VOLUME_PER_MINUTE)  in the Holland tunnel bigger than  mean  traffic (VOLUME_PER_MINUTE)  in the Lincoln? 

Do this in your R studio, since we cannot install our package in data camp service we are using to run the code snippets

```{r,tut=TRUE,ex="permutationtestfunction",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction",type="sample-code",height=500}
#install.packages("devtools")
#devtools::install_github("devanshagr/PermutationTestSecond")

#PermutationTestSecond::Permutation(d, "Cat", "Val",10000, "GroupA", "GroupB")
traffic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv")
Permutation(traffic, "TUNNEL", "VOLUME_PER_MINUTE",1000,"Holland", "Lincoln")
 
#The Permutation function returns the absolute value of the difference. So the red line is the absolute value of the observed difference. You will see a histogram having a normal distribution with a red showing the observed difference.
```

## z-test {#ztests}

**Null Hypothesis** - Traffic in Holland tunnel is the same as traffic in Lincoln tunnel.

**Alternative Hypothesis** - Traffic in the Holland Tunnel is larger than traffic in the Lincoln  tunnel.

In the snippet \@ref(ztests)  we end up calculating the p-value which leads to rejection of Null hypothesis (good news for data scientist, bad for the sceptic).  Indeed, p-value is less than the significance level  of 5%.
This means, that under null hypothesis it is extremely unlikely (less than 5% chance) to see the result which is at least as big as the  observed difference of means.

```{r,tut=TRUE,ex="ztestfunction1",type="pre-exercise-code"}

z_test <- function(data,col1,col2,sub1,sub2) {
  data <- as.data.frame(data)
  V1<-data[,col1]
  V2<-data[,col2]
  #data clean and subset, either
  lincoln.data <- subset(data, V1 == sub1)
  holland.data <- subset(data, V1 == sub2)
  
  #traffic at lincoln
  lincoln.traffic <- lincoln.data[,col2]
  #traffic at holland
  holland.traffic <- holland.data[,col2]
  
  # standard deviation of two samples.
  sd.lincoln <- sd(lincoln.traffic)
  sd.holland <- sd(holland.traffic)
  
  #length of lincoln and holland
  len_lincoln <- length(lincoln.traffic)
  len_holland <- length(holland.traffic)
  len_lincoln
  len_holland
  
  #standard deviation of difference traffic
  sd.lin.hol <- sqrt(sd.lincoln^2/len_lincoln + sd.holland^2/len_holland)
  sd.lin.hol
  
  #means of two samples
  mean.lincoln <- mean(lincoln.traffic)
  mean.holland <- mean(holland.traffic)
  mean.lincoln
  mean.holland
  
  #z score
  zeta <- (mean.lincoln - mean.holland)/sd.lin.hol
  print(paste(zeta," is the z-value"))
  
  #plot red line
  plot(x=seq(from = -5, to= 5, by=0.1),y=dnorm(seq(from = -5, to= 5,  by=0.1),mean=0),type='l',xlab = 'mean difference',  ylab='possibility')
  abline(v=zeta, col='red')
  
  #get p
  p = 1-pnorm(zeta)
  print(paste(p, " is the p-value"))
}
```

```{r,tut=TRUE,ex="ztestfunction1",type="sample-code",height=500}

TRAFFIC<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')

z_test(TRAFFIC,"TUNNEL", "VOLUME_PER_MINUTE","Lincoln", "Holland")

```


## Make your own data and see how p-value changes

For students familiar with basic descriptive statistics (mean, standard deviation)we build a synthetic data set ourselves and see how difference of means and difference of standard deviations affects the p-value. We will build our two distributions ourselves - varying the means and standard deviations. We will use **rnorm()** to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. 

Build the data frame with two attributes: Cat and Val, using rnorm() function. 
Our null hypothesis is that Group A and Group B  have identical mean(Val).

The alternative hypothesis is that the mean(Val) for Group B is higher than mean(Val) for Group A.
We will change the mean and standard deviation of the data distributions for Group A and Group B and see how these changes affect the p-value. We will first use a permutation test and a single-step permutation test (just to illustrate what happens each single step when we run a permutation test). Then we finish off with the z-test. 

### Permuation test

**Exercise** - How p-value is affected by difference of means and standard deviations

We will build our two distributions ourseleves - varying the means and standard deviations.  We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use permutation test (can be z-test as well) to test difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values.

Build the data frame with two attributes: **Cat** and **Val**, using **rnorm()** function

```{r,tut=TRUE,ex="permutationtestfunction1",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,ex="permutationtestfunction1", tut=TRUE,type="sample-code",height=600}
Val1<-rnorm(10,mean=25, sd=10)
Val2<-rnorm(10,mean=30, sd=10)
 
Cat1<-rep("GroupA",10)  # for example GroupA can be Holland Tunnel
Cat2<-rep("GroupB",10)  # for example Group B will be Lincoln Tunnel

Cat1
Cat2

#The rep command will repeat, the variables will be of type character and will contain 10 values each.

Cat<-c(Cat1,Cat2) # A variable with first 10 values GroupA and next 10 values GroupB
Cat

Val<-c(Val1,Val2)
Val

d<-data.frame(Cat,Val)
d

Permutation(d, "Cat", "Val",1000,"GroupA", "GroupB")

Observed_Difference<-mean(d[d$Cat=='GroupB',2])-mean(d[d$Cat=='GroupA',2])
Observed_Difference

#This will calculate the mean of the second column (having 10 random values for each group), and the mean of groupB values is subtracted from the mean of groupA values, which will give you the value of the difference of the mean.
 
 #Try changing mean and sd values. When you run this you will see that the difference is sometimes negative #or sometimes positive.
```

### One permutation at a time

```{r,tut=TRUE,height=400}
traffic<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')

ranNum <- sample(1:nrow(traffic),nrow(traffic))
ranNum[1:5]

VOLUME_PER_MINUTE<-traffic$VOLUME_PER_MINUTE[ranNum]
TUNNEL<-traffic$TUNNEL

Permuted_traffic<-data.frame(TUNNEL, VOLUME_PER_MINUTE)

mean(traffic[traffic$TUNNEL=='Lincoln', ]$VOLUME_PER_MINUTE) -mean(traffic[traffic$TUNNEL=='Holland', ]$VOLUME_PER_MINUTE)

mean(Permuted_traffic[Permuted_traffic$TUNNEL=='Lincoln', ]$VOLUME_PER_MINUTE)-mean(Permuted_traffic[Permuted_traffic$TUNNEL=='Holland', ]$VOLUME_PER_MINUTE)
```


### z-test

How p-value is affected by difference of means and standard deviations.

We will build two distributions ourselves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. You can do it by changing values of mean and standard deviation in the rnorm() function.

Clearly the further apart the mean values are - the lower the p-value. But how do standard deviations affect the p-value?  See for yourself.

Build the data frame with two attributes: Cat and Val, using rnorm() function

```{r,tut=TRUE,ex="ztestfunction",type="pre-exercise-code"}

z_test <- function(data,col1,col2,sub1,sub2) {
  data <- as.data.frame(data)
  V1<-data[,col1]
  V2<-data[,col2]
  #data clean and subset, either
  lincoln.data <- subset(data, V1 == sub1)
  holland.data <- subset(data, V1 == sub2)
  
  #traffic at lincoln
  lincoln.traffic <- lincoln.data[,col2]
  #traffic at holland
  holland.traffic <- holland.data[,col2]
  
  # standard deviation of two samples.
  sd.lincoln <- sd(lincoln.traffic)
  sd.holland <- sd(holland.traffic)
  
  #length of lincoln and holland
  len_lincoln <- length(lincoln.traffic)
  len_holland <- length(holland.traffic)
  len_lincoln
  len_holland
  
  #standard deviation of difference traffic
  sd.lin.hol <- sqrt(sd.lincoln^2/len_lincoln + sd.holland^2/len_holland)
  sd.lin.hol
  
  #means of two samples
  mean.lincoln <- mean(lincoln.traffic)
  mean.holland <- mean(holland.traffic)
  mean.lincoln
  mean.holland
  
  #z score
  zeta <- (mean.lincoln - mean.holland)/sd.lin.hol
  print(paste(zeta," is the z-value"))
  
  #plot red line
  plot(x=seq(from = -5, to= 5, by=0.1),y=dnorm(seq(from = -5, to= 5,  by=0.1),mean=0),type='l',xlab = 'mean difference',  ylab='possibility')
  abline(v=zeta, col='red')
  
  #get p
  p = 1-pnorm(zeta)
  print(paste(p, " is the p-value"))
}
```


```{r,tut=TRUE,ex="ztestfunction",height=700}
Val1<-rnorm(10,mean=25, sd=10)
Val2<-rnorm(10,mean=35, sd=10)
Cat1<-rep("GroupA",10)  
Cat2<-rep("GroupB",10)  
Cat<-c(Cat1,Cat2) 
Val<-c(Val1,Val2)

d<-data.frame(Cat,Val)
Observed_Difference<-mean(d[d$Cat=='GroupB',2])-mean(d[d$Cat=='GroupA',2])
Observed_Difference

z_test(d,"Cat", "Val","GroupB", "GroupA")
```




## Additional References

<button class="btn btn-primary" data-toggle="collapse" data-target="#HPT12">Hypothesis Testing</button> 

<button class="btn btn-primary" data-toggle="collapse" data-target="#KH12">Khan Academy Video</button>

<div id="HPT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1Fmd8GiwEHGNUudh5XIojn_4Vi03qxLOG8cidmpSXmuw/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<div id="KH12" class="collapse">https://www.khanacademy.org/math/statistics-probability/significance-tests-confidence-intervals-two-samples/comparing-two-means/v/hypothesis-test-for-difference-of-means
</div>


https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/p-value/ <br>
http://www.z-table.com/ <br>
https://www.statisticshowto.com/probability-and-statistics/z-score/ <br>
https://sixsigmastudyguide.com/z-scores-z-table-z-transformations/

<!--chapter:end:chapters/hypothesis_testing.Rmd-->

# 🔖 Test of Independence {#chitest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

We would like to test if a student's final grade in Professor Moody’s class  depends  on the student's major.  The null hypothesis in this case is the hypothesis of independence.  Independence means that the distribution of final grades is the same no matter what the major is. The alternative hypothesis is that the distribution of final grades  changes from major to major, rejecting the null hypothesis of independence. 

Notice that we do not specify how the grades depend on students' majors.  Do CS students get better grades than Psychology  majors? Do Economics majors get lower grades than Statistics majors?  We do not care about this. We are only testing here whether there is a relationship between Major and the final grade distribution.

We will describe the permutation test which scrambles our data randomly in such a way that any relationship between grades and major  (if it ever existed) is broken. We will run a permutation test a large number of times - possibly tens of thousands of times. Then we will determine how likely it is to randomly obtain the observed result.  But what is the observed result? It is a bit more complex than the difference of means which were observing the difference of means hypothesis testing. 

The observed result in our case is calculated by so called chi-square statistic which is calculated on the contingency table,  ```table(moody$Grade, moody$Major)```

To explain it, let us first define two tables: The observed contingency table and the expected contingency table. It is calculated by table() function.


**OBSERVED CONTINGENCY TABLE**
<table>
<tr>
<td>Grade/Major</td>
<td>CS</td>
<td>Economics</td>
<td>Psychology</td>
<td>Statistics</td>
</tr>

<tr>
<td>A</td>
<td>46</td>
<td>54</td>
<td>69</td>
<td>42</td>
</tr>

<tr>
<td>B</td>
<td>46</td>
<td>12</td>
<td>2</td>
<td>35</td>
</tr>

<tr>
<td>C</td>
<td>51</td>
<td>33</td>
<td>30</td>
<td>34</td>
</tr>

<tr>
<td>D</td>
<td>41</td>
<td>37</td>
<td>29</td>
<td>34</td>
</tr>

<tr>
<td>F</td>
<td>108</td>
<td>99</td>
<td>99</td>
<td>99</td>
</tr>

</table>


The second table we need is called the expected table. This is the hypothetical table of the relationship between grade and major,  assuming grade and major are completely independent.  Such a table  would be the result of  the same distribution of grades for each of the majors.  Notice that we 1000 students in the data set the expected table (i.e. table which have grades completely independent from majors) would  have the same distribution of grades for each major that over all students - which is shown by the **TOTAL** column.


**EXPECTED CONTINGENCY TABLE**
<table>
<tr>
<td>Grade/Major</td>
<td>CS</td>
<td>Economics</td>
<td>Psychology</td>
<td>Statistics</td>
</tr>

<tr>
<td>A</td>
<td>61</td>
<td>50</td>
<td>48</td>
<td>51</td>
</tr>

<tr>
<td>B</td>
<td>28</td>
<td>22</td>
<td>22</td>
<td>23</td>
</tr>

<tr>
<td>C</td>
<td>43</td>
<td>35</td>
<td>34</td>
<td>36</td>
</tr>

<tr>
<td>D</td>
<td>41</td>
<td>33</td>
<td>32</td>
<td>34</td>
</tr>

<tr>
<td>F</td>
<td>118</td>
<td>96</td>
<td>93</td>
<td>99</td>
</tr>

<tr>
<td>TOTAL</td>
<td>292</td>
<td>236</td>
<td>229</td>
<td>244</td>
<td>1000</td>
</tr>
</table>

We kept fractions - although these are number of students - therefore would have to be rounded up to integers 

The chi-square formula calculates the **“distance”** between the observed contingency table and the expected contingency table.

\begin{equation}

\sum \frac{(O_i - E_i)^2}{E_i}\\


\text{where:}\\
\text{O = observed values}\\
\text{E = expected values}\\

\end{equation}

For the two tables above the, 

\begin{equation}

\sum \frac{(O_i - E_i)^2}{E_i}  = 60.03\\

\end{equation}

To evaluate how far off is this observed result assuming that Grades are independent from Major, we run a permutation test which scrambles Grades and Majors randomly and every time computes the chi-square formula with the new observed table (the expected table is always the same).  Then, we see how many times out of, say 10,000 iterations of permutation test we obtain a result larger than the observed result of 60.03? This is the p-value. 

Permutation test for independence hypothesis gives us again a better feeling about the impact of randomness and whether the observed chi-square result for “similarity” of grade distributions for different majors can be obtained randomly. 
In the following snippet we run the chisq test which is based on the so-called chi square distribution. Here we simply show you a function which can calculate p-value, just like the z-test function does.  The explanation of the chi-square test is provided in attached link to the excellent Khan Academy video.

Permutation tests in both cases of difference of means and independence hypotheses give a better intuitive sense of how we answer the question - can the observed result be obtained randomly? 

Notice that the independence test is looking globally at two vectors and whether one depends on another. If we wanted to be more specific and know if psychology majors are more likely to get an A than CS majors, we can frame this as a difference of means hypothesis. Testing this hypothesis will be using the difference of means of frequencies of A’s among CS majors and psychology majors.  This could be done again by permutation test in section 8 or the z-test. 



## Chisq test

```{r,tut=TRUE,height=300}
Expected <-matrix(c(200,420,180, 40,120,40), nrow=3, ncol=2)
Observed<-matrix(c(200,420,180,35,120,45), nrow=3, ncol=2)
Expected
Observed
chisq.test(Observed)
```

## Chisq permutation test

```{r,tut=TRUE,ex="chisquarefunction",type="pre-exercise-code"}
library(dplyr)
options(warn=-1)
chi_permutation_test <- function(data,col1,col2,iter) {
  
  df <- data.frame(data)
  vals<- unique(df[[col2]])
  no_rows <- nrow(df)
  dt <- table(df[[col1]], df[[col2]])
  res <- chisq.test(dt)
  real_ans <- res$statistic
  ans_vec <- vector()
  total<-0
  for (x in 1:iter){
    
    new_data <- sample(x=vals,size=no_rows,replace = TRUE)
    
    dt_new <- table(df[[col1]], new_data)
    
    res_new <- chisq.test(dt_new)
    if(res_new$statistic > real_ans){
      total <- total + res_new$statistic
    }
    
    ans_vec <- append(ans_vec,res_new$statistic)
   
  }
  hist(ans_vec,main="Permutation Test for Chi-Square",xlab="Chi-Square Values", breaks=100)
  print(real_ans)
  if(total == 0){
    print('p < 0.001')
  }
  else{
    cat('p-value: ', total/iter)
  }
  abline(v=real_ans,col="blue",lwd=2)
}

```


```{r,tut=TRUE,ex="chisquarefunction",type="sample-code",height=500}

d<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")
head(d)

chi_permutation_test(d,"Seniority","Grade",10000)

```

## Chisq and slicing

```{r,tut=TRUE,height=300}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")
moody$IN<-'Out_Slice'
moody[moody$DOZES_OFF=='never' & moody$TEXTING_IN_CLASS=='always', ]$IN<-'In_Slice'
d<-table(moody$GRADE, moody$IN)
d
chisq.test(d)
```

## Contingency table


```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")
data<-table(movies$content, movies$genre)
chisq.test(data)
```

## Additional Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#CS12">Chi Square Permutation Test</button> 

<button class="btn btn-primary" data-toggle="collapse" data-target="#KH13">Khan Academy Video</button>

<div id="CS12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1GiiBTHhI-NDlY3YROCxl5AJu1w2Jc_-2iZuK0AjVpAw/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<div id="KH13" class="collapse">https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-goodness-fit/v/chi-square-statistic
</div>

<!--chapter:end:chapters/Chisquare.Rmd-->

# Sampling (and special role of normal distributions)

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)
```

In section \@ref(dbds) we already introduced the concept of  a sample. For educational purposes we assume that we also know the universe from which the sample was taken. This of course happens very rarely, if at all. Usually we do not know the universe.  

Here, we discuss how we can calculate the quality of estimation, when we do not know the universe which is sampled. Is this even possible?  Yes, it is. This is the power of statistics. We introduce confidence intervals for means and for proportions and show how such intervals can serve as estimators along with the confidence probabilities as measures of estimators quality. 

## Estimator of Proportions

Let us start with the estimation of proportion. This is the problem we witness in each election, when we see partial results and try to estimate who won, on the basis of the sample of votes which were counted so far. Different TV networks call a winner at different times and sometimes (although rarely) they may be wrong. 

How can we estimate the quality of the estimated proportion calculated on the basis of a sample?

For example assume that in our Local Election data puzzle, we consider a sample of 1000 voters, who are above 50 years old.  Let's assume that 62% of them voted for Royalists. Of course by now we know that each data set is  really, only a sample and each value computed from that sample is just an estimate of an unknown “real” value.  Thus, we do believe that 62% is the exact population proportion of voters under 50 who chose to vote for Royalists 

Instead of 62% we return so called confidence interval of proportions

\begin{equation}

<62-e, 62+e>

\end{equation}

 

Examples of such intervals would be  <59,65> (in this case e=3).  In addition, along with the confidence interval, we return the confidence level, say 99%, 95% etc. Thus, our answer has the form of an interval as well as confidence level of this interval. 

Technically, we estimate the unknown population proportion p* with the sample population proportion p (in our case it is 62%). This sample proportion comes from the universe of sample proportions and we must know something more about this universe to be able to infer the relationship between the p* and p. It makes sense to assume that this possible population proportions p*  are centered around p.  It is also reasonable to assume that the larger the size of the sample N, the tighter this population will cluster around p.  Thus, we assume that the mean of the population of samples is equal to observed population proportion p. 

Turns out that the set of all sample proportions will be normally distributed around the mean which is equal to p and with standard deviation which is equal to 

\begin{equation}

\sqrt{(p(1-p)/N)} \\

\text{where N is the size of the sample (see eg Martin Sternstein, “Statistics” )}\\

\end{equation}



For example, coming back to out example of sample of 1000 voters who are over 50 years old voting for Royalists, we calculate the mean = 0.62 and the standard deviation

**Snippet 11.1:**
```{r, tut=TRUE, height = 400}
sigma = Sqrt(0.62*0.38/1000)
pnorm(z)

#To calculate the probability of interval of <p-z*sigma, p+z*sigma>  we compute 
1-2(1-pnorm(z))

#this is the same as 
2*pnorm(z)-1
```


Thus for example, with p=0.62 and sigma = 0.0027,  we can get 99% interval with z=3, which would be 
The interval <61.2%, 62.8%>    with confidence of 99%

Notice that if our sample was 10 times smaller, N=100 then sigma = 0.048 and 99% confidence interval would be much wider
<57.2%, 66.8%>

##  Estimator of  Mean
Consider a sample of 500 movies taken from a movie data set (see data puzzles). The scheme contains imdb_score, genre, content, nationality etc.

Suppose we calculated the mean of imdb_score of comedies

**Snippet 11.2:**

```{r, tut=TRUE, height = 300}
movies <-read.csv('moviesSample.csv')    
mean(movies[movies$genre=='Comedy',]$imdb_score)
```

There are 160 comedies in our sample and the mean imdb score  of this sample of Comedies is equal to 6.52. But it is just an estimate. We only have a sample from an unknown larger data set (of course we may consider the original movies data set with thousands of movies as the universe of movies from which we have drawn the sample, but how do we know that it is the source of data for our sample?  Therefore we have to assume that we do not know the source of data for our movie sample.  Can we  still assess the quality of the imdb_score estimator based on this limited sample of 160 comedies?

Turns out that thanks to the central limit theorem we can. 

Our sample is one sample from the population of sample means. How do these means vary?  It is clear that the means of samples are centered around the mean of the entire, unknown population.  Again, the larger the size of a sample, the tighter the population of means of samples of this size is. The lesser the spread (standard deviations) of means around the mean of the population.

It turns out (see Martin Sterenstein, Statistics), that the standard deviation of the population of means

sigma-samples= sigma-population/sqrt(n)

This leads us to the central limit theorem, which says that regardless of what is the distribution of the original data, the distribution of means of samples from this population is normal. This is really a stunning finding, since it is so universal. It lets us be completely blind to the original distribution of data!  

**Central Limit Theorem**

- The set of all sample means is approximately normally distributed
- The mean of the set of sample means is equal to the mean of population
- The standard deviation, sigma-samples is equal to sigma-population/sqrt(n)

**Snippet 11.3:** Confidence interval for the mean imdb score of comedies

```{r, tut=TRUE, height = 300}
sigma<-sd(movies[movies$genre=='Comedy',]$imdb_score)
sigma <-sigma/sqrt(160)
sigma
```

Let's assume we would like to get a confidence interval with a confidence level of 99%, This means setting up an interval with plus minus 3z from the mean. Since sigma=0.077, 3z = 0.23. Thus 

<6.52-0.23, 6.52+0.23> =    <6.31, 6.83> with 99% confidence. 

Again, like in case of the confidence interval for population proportions, we replace the specific value of the mean imdb with the interval and its confidence level. 
Notice that, as sample size, we have not used N=500 but N=160, which is the number of Comedies – the size of the data frame subset which we use to calculate the mean. 

Also notice again that if our sample was ten times smaller, that is N=16 comedies, sigma would reach almost 0.25 and the width of the interval would be much wider -  1.5 (0.75 on each side).  This our mean estimator would be far less precise. 


<!--chapter:end:chapters/Sampling.Rmd-->

# 🔖 Multiple Hypothesis Testing {#Mtest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

We often consider multiple possible hypotheses in our search for discovery  to find one with lowest possible p-value. Consciously or subconsciously we are engaging, what is often called,  p-value hunting. We have to be very careful! We may “discover” what is simply random even if we correctly calculate p-value and compare it with the significance level. It is very important to learn about multiple hypothesis traps very early in the process of learning data science.  

For example assume that we are looking for associations between sales of individual items in a supermarket.  Does bread sell with butter? Does coffee sell with spring water?  There is an exponential number of possible combinations (N choose 2 to be exact, where N is the number of items).  For each such pair we perform hypothesis testing. If one test is performed at the 5% significance level and the corresponding null hypothesis is true, there is only a 5% chance of incorrectly rejecting the null hypothesis. However, if 100 tests are each conducted at the 5% significance level and all corresponding null hypotheses are true, the expected number of incorrect rejections (also known as false positives or Type I errors) is 5. If the tests are statistically independent from each other, the probability of at least one incorrect rejection is approximately 99.4%.  Thus, we will almost surely find one false positive! In other words we will be fooled by data. 

Bonferroni correction is a method to counteract the multiple hypothesis (often called multiple comparison problem. Make it harder to reject null hypotheses by dividing the significance level by number of hypotheses. The Bonferroni correction compensates for that increase by testing each individual hypothesis at a significance level of **α / m**  where  m is the number of hypotheses. For example, if a trial is testing me = 20 hypotheses with a desired α = 0.05, then the Bonferroni correction would test each individual hypothesis at

\begin{equation}
\alpha = \frac{0.05} {20} 
       = 0.0025
\end{equation}

Thus, there is a very simple remedy for multiple hypothesis traps. Just divide the significance level by the number of (potential) hypotheses tested.  This will make it harder, often much much harder to reject the null hypothesis and yell Eureka! Critics say that in fact Bonferroni correction is too conservative and too “pro-null”  and tough on alternative hypotheses to be acceptable.  

The unwanted side effect of Bonferroni correction is that  we may  fail to reject the null hypothesis too often. Bonferroni correction makes discovery sometimes too hard, making data scientists too conservative and accepting null hypothesis when they should be rejected.  It may also be the case that even Bonferroni correction will not protect us, as we will show in our example below.  But at least we will be much less likely to  make fools of ourselves coming with false discoveries leading potentially to very wrong business decisions. 

There are other less conservative methods of correcting for multiple hypotheses - such as the Benjamini-Hochberg method described in the attached slides.

We illustrate the p-value hunt in snippet 8.1 below. It is based on synthetic data set showing summer temperatures in New Jersey townships.
Table below describes the data set based on hypothetical temperature readings in various municipalities of New Jersey over summer. Is one city experiencing higher average temperatures than another? Can we find such a pair of cities? This is the ultimate p-value hunt. Let’s compare townshiships pair by pair, until we find a pair with sufficiently large differences of mean temperatures and sufficiently low p-value. Careful! You may come up with false discovery if you do not correct for multiple hypotheses!

The \@ref(snippet1)  shows several permutation tests for different pairs of townships and difference of means of temperatures hypothesis test. Two of four pairs show p–values less than customary significance level of 5%. Should we then reject the null hypothesis and conclude that indeed Ocean Grove is warmer than New Brunswick and that New Brunswick is warmer than Holmdel? Indeed, both pairs result in p-values significantly lower than 5%. If we incorrectly disregard the number of hypotheses considered, we may come to wrong conclusions supporting these two alternative hypotheses. But there are around 20 townships in the Temp data set. Thus there are around 200 possible hypotheses (200 pairs of townships) which we may consider in our p-value hunt. If we apply Bonferroni correction for N=200, the significance level will be 200 times lower, instead of 5%, it will be 0.025%. None of the two hypotheses (Ocean Grove vs New Brunswick and New Brunswick vs Holmdel) meets the new significance level. Indeed in both cases p-values are significantly larger than 0.025%. Thus, for none of the four pairs we can reject null hypotheses.

Now we can disclose that we have created our Temp data set completely randomly - assigning random temperatures between 50 and 100 degrees to each township. Thus, without Bonferroni coefficient we would be fooled by data, not once, but twice in our four tests. We would find a trend when it does not exist - it is simply random deviation.

It turns out however that even Bonferroni correction is not sufficient to protect us against incorrectly rejecting null hypothesis. Indeed, for Red Bank and Holmdel, we conclude that Red Band is warmer than Holmdel with p-value of 0.01%! (see the last permutation test in the snippet 1). This p-value falls even below significance level adjusted with Bonferroni correction (0.025%). It only shows that dealing with multiple hypotheses is a risky adventure. We may end up being fooled by data even when we apply Bonferroni correction. But at least we are less likely to fall into the trap of multiple hypotheses when we apply Bonferroni Correction.

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Tempratures.csv" download="Tempratures.csv">Tempratures.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
hindex<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Tempratures.csv") #web load
# head(moody)
temp<-knitr::kable(
  hindex[sample(1:nrow(hindex),10), ], caption = 'Snippet of Temperature Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

The Temp data set assigned random temperatures between 50 and 100 degrees to around 20 townships in New Jersey. 

Without Bonferroni correction we would have to incorrectly reject the null hypothesis in the last three permutation tests  (3rd, 4th and 5th) tests. We will still reject null hypothesis (and have false positive discovery) in the last, 5th case. Indeed, the 5th p-value is 0.0002 which is less than the significance level after Bonferroni correction (0.00025).  

### Multiple Permutation Tests {#snippet1}

```{r,tut=TRUE,ex="permutationtestfunction10",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction10",type="sample-code",height=400}

Temp <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Tempratures.csv") #web load

Permutation(Temp, "Township", "Temprature",10000, "Princeton", "Trenton")
Permutation(Temp, "Township", "Temprature",10000, "Passaic", "Newark")
Permutation(Temp, "Township", "Temprature",10000, "Ocean Grove", "New Brunswick")
Permutation(Temp, "Township", "Temprature",10000, "New Brunswick", "Holmdel")
Permutation(Temp, "Township", "Temprature",10000, "Red Bank", "Holmdel")
```
<!--
## Snippet 1 - Benjamini-Hochberg Algorithm {#Snippet1}

```{r,tut=TRUE,height=450}
p<-sort(round(runif(100, min=0, max=0.05), 4))
p
p<-p+0.0003
p
#implement Benjamini-Hochberg formula
q<-rep(0.05,100)
q
r=c(1:100)
q<-round(q*r/100,4)
temp<-p<q
#Select p-values which correspond to discoveries (reject NULL)
maxindex<-max(which(temp=='TRUE'))
p[1:maxindex]
```

## Snippet 2

Happiness Index synthetic data set which is used in my slides for multiple hypotheses testing

- How to order by aggregate?

- First make a data frame out of tapply? Use  aggregate  and list functions.

```{r,tut=TRUE,height=450}
Hindex <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv") #web load

Hindex<-aggregate(Hindex$HAPPINESS, list(Hindex$COUNTRY), mean)
colnames(Hindex)<- c("Country","AverageH")
#renames columns of the Hindex data frame
colnames(Hindex)

Hindex[order(Hindex$AverageH),]
```
-->

## Additional References 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MPT12"> Multiple Hypothesis Testing</button> 
<div id="MPT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1dCbhnuGMsXYJEltQXUfPhl8dCTAqJWCZ0xI-0IhnxAc/edit?usp=sharing" width="100%" height="500px"></embed>
</div>


https://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/

<!--chapter:end:chapters/Multiple_Hypothesis.Rmd-->

# 🔖 Bayesian Reasoning {#br}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

Bayesian Reasoning and Bayesian theorem are fundamental instruments to use both in data science as well as in real, everyday life. They are definitely part of data literacy and should be widely taught, especially among future doctors, lawyers and politicians.  In this section we will explain why Bayesian reasoning is so important and also teach the most simple and intuitive formulation of Bayesian theorem - the  Odds formulation.

Bayesian theorem is a calming tool  - the chances of bad things happening are lower than expected!  This is why Bayes helps  pessimists.  Take a situation at a doctor's office when a patient learns that a medical test for some potentially serious condition came positive. The doctor believes the test and the test is almost 100% accurate. Should the patient despair?  Not so fast. Bayes theorem allows the patient to ask the doctor some important questions. 

In bayesian reasoning we distinguish between two concepts = observation and belief.  Belief is something unknown, Observation is known. We use observation to modify the odds (probability) of the  belief from prior odds (before we learned about observation) and the posterior odds (after we learned about observation). For example a patient taking a covid test is concerned about having covid. But s/he does not know whether they have covid. Thus “having covid” is a belief.  Test result is an observation (positive or negative).  Given the prior odds of covid (say 1:100), and positive covid test what are the posterior odds of covid?  Bayesian theorem tells us how to compute posterior odds from prior odds, given the observation.
w
Odds formulation of Bayesian theorem states;

\begin{equation}

\text{POSTERIOR ODDS = LIKELIHOOD RATIO *  PRIOR ODDS} \\
               

\textbf{Prior odds}   \text{-  odds for the belief before observation (evidence)}\\

\textbf{Likelihood ratio}  \text{-  effect of observation, evidence. Can be larger or smaller than 1!!}\\

\textbf{Posterior odds} \text{-  New odds with observation(evidence) taken under consideration.}\\

\end{equation}

Let B a belief and O be an observation, then

\begin{equation}

\textbf{Prior odds} - \frac{P(B)}{P(\sim B)}\\

\textbf{Likelihood ratio}  - \frac{P(O|B)}{P(O|\sim B)}\\

\textbf{Posterior odds} - \frac{P(B|O)}{P(\sim B|O)}
\end{equation}

Let's discuss the multiplier – likelihood ratio in more detail.  It is the red colored part of the bayes Theorem:

\begin{equation}

\frac{P(B|O)}{P(\sim B|O)} =	\frac{P(O|B)}{P(O|\sim B)}  *   \frac{P(B)}{P(\sim B)} \\

\text{The red colored ratio is the ratio of true positive and false positive,}\\

\text{P(O|B) – True positive} \\
\text{P(O|$\sim$ B) – False positive}

\end{equation}

True positive is the conditional probability of seeing the observation given that our  belief is true. In our medical example it is the probability of testing positive for covid, given that in fact we have covid. 

False positive, on the other hand, is the probability of observation under condition that the belief is not true. For example in our case that covid test comes positive even when we do not have covid. 

In real life False positives are often overlooked. And this is the critical question we should ask the doctor or health professional who administers any test. What is the false positive of this test?   Since this is what we divide the true positive by. Even if the true positive is 99.9% (almost sure), if the false positive is, say 20% - the likelihood ratio is around 5.  In such a situation, a positive test increases the odds of having covid just 5 times. If prior odds of covid are 1:100, the posterior odds of covide after such a positive test are just 5:100, still minimal!.  Even if a false positive was 10%, the likelihood ratio of 10, would increase odds of covid 10 fold, to just 1:10 and false positive of 5%, would result in a likelihood ratio of 20 - still leading to higher odds of NOT having covid than having it!  

This is why false positives are so critical. IBut the main question that Bayes teaches us to ask is what are the prior odds.  Since if prior odds are very small (we are testing a really rare condition) then the likelihood ratio would have to be really large to make posterior odds significant.  For example if prior odds are one in a million, we need a likelihood ratio of more than half a million to actually make posterior odds better than proverbial fifty - fifty.

Hence to main questions we should ask our doctor upon hearing that the test results are positive are:

**What are the prior odds of the disease?**

**What is the false positive of the test (since we assume that the true positive of the test would usually be close to 100%)?**

In the following snippets we show how to calculate the posterior odds, while being tested for a disease and then closer to our data puzzles, how to calculate the posterior odds of getting an A in class, when scoring more than 85%.In all these situations we begin with identifying what is belief (the unknown), what is the observation (the known) and we use the snippets by plugging in some assumed values of prior odds, as well as true positives and false positives.


## Covid Odds after positive Home Test. 

```{r,tut=TRUE,height=600}
#Belief = "Have Covid"
#Observation = Covid Test
#How much the probability of having covid increases upon positive COVID-test?
#We use the odds formulation of Bayesian Theorem
# we begin with prior odds of having Covid:  P(Covid)/(1-P(Covid)
PriorHaveCovid<-0.01
PriorCovidOdds<-PriorHaveCovid/(1-PriorHaveCovid)
PriorCovidOdds
#True positive:  Probability of having positive Covid test when having covid  = P(PositiveCovidTest|HaveCovid)
TruePositive<-0.99
#False positive = Probability of having positive Covid test when not having covid = P(PostiveCovidTest/DoNotHaveCovid)
FalsePositive<-0.001
LikelihoodRatio<-TruePositive/FalsePositive
PosteriorCovidOdds<-LikelihoodRatio*PriorCovidOdds
PosteriorHaveCovid<- PosteriorCovidOdds/(1+PosteriorCovidOdds)
PosteriorHaveCovid
```

## What are the odds that an 'F' student is a freshman?

```{r,tut=TRUE,height=600}
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv')
#Belief - Student is a freshman
#Observation - Failed the class
Prior<-nrow(moody[moody$Seniority =='Freshman',])/nrow(moody)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(moody[moody$Grade=='F' & moody$Seniority=='Freshman',])/nrow(
  moody[moody$Seniority =='Freshman',]),2)
TruePositive
FalsePositive<-round(nrow(moody[moody$Grade=='F'& moody$Seniority !='Freshman',])/nrow(moody[moody$Seniority !='Freshman',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
round(Posterior,2)
```


## What are the odds that a 'A' student with the score less than 80 is a psychology major?

```{r,tut=TRUE,height=600}
#Belief - what we do not know. #Is a student a psychology #major?
#Observation = what we do #know. They got an A and less #than 80 in score

moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv')
Prior<-nrow(moody[moody$Major =='Psychology',])/nrow(moody)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(moody[moody$Score <80 & moody$Grade=='A'& moody$Major=='Psychology',])/nrow(moody[moody$Major=='Psychology',]),2)
TruePositive
FalsePositive<-round(nrow(moody[moody$Score <80 & moody$Grade=='A'& moody$Major!='Psychology',])/nrow(moody[moody$Major!='Psychology',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior
```

## Additinal Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#BR12">Bayesian Reasoning</button> 
<div id="BR12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1gwI6y7yyqi8GWdPuS9dZlXsZs-1r1TsumQb-PL4Hu5s/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/bayesian_reasoning.Rmd-->

# 🔖 Data puzzles {#dp}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

```{r setup, include=FALSE}
library(shiny)
library(shinythemes)
library(shinyjs)
library(shinymanager)
library(googlesheets4)
gs4_deauth()
```
## Introduction

Data Puzzles are synthetically generated datasets with some embedded patterns.  Patterns have various forms from relationships between attributes to rules of the form “if condition then value” between specific attribute-value pairs.  These patterns are stochastic and embedded in datasets using **DataMaker** -  our  Data Puzzle Generation Tool.

We use data puzzles extensively in the class assignments.  These range from data exploration and plotting through  hypothesis testing to prediction and machine learning.  After the assignment is completed we reveal the data secrets - the patterns which were embedded by DataMaker.  Students do not have to find exactly the embedded patterns, often they find related patterns which makes the “game” even more fun. 

In the following  we provide the list of data puzzles along with the underlying data sets.  Using DataMaker we change the patterns and even data sets from academic year to academic year..  We can also provide data puzzles of different levels of difficulty from the  one star (easy) to five star (most difficult)  ones. 

We will now proceed to nine datasets -first seven of them are synthetically created by DataMaker data puzzles with embedded secret patterns. Last two are real data sets - airbnb and titanic. Each data set is subject of a separate subsection 10.2-10.10. These sections are structured in a similar way. We start  each section with around  eight to ten practice snippets and follow with the Code Roulette link, where we randomly select a small coding task for students using the data set of this section. We provide the output which requested code returns when run on the data set. But we do not provide the code. This has to be written by a student. 

Each section 10.i starts with the snippet “Get to know your data”. There are around fifteen  R instructions there when executed return the column set of the data set, number of rows, summary of the data set, unique values of each attribute and various basic distributions.

For example, the first section, 10.2 starts with the following “Get to know your data” snippet. These are repeated for all 9 data sets. Here we explain each line below.

```
colnames(moody)
# returns all columns of moody

summary(moody)
# provides summary, distributions of different columns of moody

nrow(moody)
# returns  number of rows of moody

unique(moody$GRADE)
#provides unique values of GRADE attribute (grades)

unique(moody$DOZES_OFF)        
#provides unique values of  DOZES_OFF attribute (‘always’, ‘sometimes’, ‘never’)    

unique(moody$TEXTING_IN_CLASS)
#provides unique values of  TEXTING_IN_CLASS) attribute (‘always’, ‘sometimes’, ‘never’)    

table(moody$GRADE)
#Frequency distribution of GRADE

table(moody$DOZES_OFF)
#Frequency distribution ofDOZES_OFF

table(moody$TEXTING_IN_CLASS)
#Frequency distribution of TEXTING_IN_CLASS

tapply(moody$SCORE, moody$GRADE, mean)
#Provides a mean score for each of the grades. We expect it will be diminishing with the grade

tapply(moody$PARTICIPATION, moody$GRADE, mean)
#Provides mean participation  for each of the grades. We do not know what to expect. Perhaps  it will be diminishing with the grade

tapply(moody$SCORE, moody$DOZES_OFF, mean)
#Provides a mean score  for each of the values of DOZES_OFF. We do not know what to expect. Perhaps  it will be diminishing the more often a student DOZES_OFF?

tapply(moody$PARTICIPATION, moody$DOZES_OFF, mean)
#Provides mean participation  for each of the values of DOZES_OFF. We do not know what to expect. Perhaps  it will be diminishing the more often a student DOZES_OFF?
```

After we get to know the data, we follow with the number of simple coding tasks - some of them are:

- Exploratory queries - compute mean, max, min on a subset of the data set
- Hypothesis testing for difference of means of numerical attribute of the data set
- Calculation of posterior odds using Odd’s formulation of Bayes Theorem.
- Hypothesis testing for test of independence


## Strange grading methods of Professor Moody Data Puzzle

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv" download="moody2022_new.csv">moody2022_new.csv</a>

How to get a good grade in Professor Moody’s class?  

Professor Moody does not give final grades just on the basis of your total score alone. Our data shows that two students with the same total score may get widely varying final grades. Can you believe that you can even fail his class with a score as high as 82%? This is outrageous, isn’t it? 

DataMaker has generated  thousands of tuples  which in addition to the total score and final grade also store bizarre information about student behaviors in the class - do they often doze off? Does a student text a lot?  Does s/he ask a lot of questions?    Does it help if you ask a lot of questions? Does it hurt if you doze off a lot?


```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv") #web load
# head(moody)
temp<-knitr::kable(
  head(moody, 5), caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  head(moody, 5), caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

![ Figiure 1: Boxplot for student score and grade distribution \@ref(check1-9)](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/data_puzzle/1-3.png)

### 🧙 Secret patterns embedded by the Data Maker (hint) 

It is not surprising that Professor Moody does not like when students text in his class all the time. He also does not like the sleepers - who doze off and do not participate at all. But….if they do well in class nevertheless, and have convincing scores, he has no choice but give them grades their score imply.  However, if one has a border score, could be B, could be A, or can be B or can be C, then Moody apparently weights in the sleeping/texting data!

Always texting in class or always dozing off in class makes a difference but only for border line scores.  That is, for scores between 80 and 90, the grade will be a B when Texting_inclass = ‘always’ or Dozing_off =’always”. Situation is similar for border scores between C and B and F and D.

Check it out!


### Practice Snippets 

#### Snippet 1: Get familiar with the data set

```{r,tut=TRUE,height=400}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

colnames(moody)
summary(moody)
unique(moody$GRADE)
nrow(moody)
unique(moody$DOZES_OFF)
unique(moody$TEXTING_IN_CLASS)
table(moody$GRADE)
table(moody$DOZES_OFF)
table(moody$TEXTING_IN_CLASS)
tapply(moody$SCORE, moody$GRADE, mean)
tapply(moody$PARTICIPATION, moody$GRADE, mean)
tapply(moody$SCORE, moody$DOZES_OFF, mean)
tapply(moody$PARTICIPATION, moody$DOZES_OFF, mean)
tapply(moody$SCORE, moody$TEXTING_IN_CLASS, mean)
tapply(moody$PARTICIPATION, moody$TEXTING_IN_CLASS, mean)

```

#### Snippet 2

**Q:** Did you know that students who never doze off in class have more than twice as many A’s than students who sometimes doze off?

```{r,tut=TRUE,height=400}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

table(moody$DOZES_OFF, moody$GRADE)
```

#### Snippet 3

**Q:** Did you know that the students who scored over 85 and still received a B almost always dozed off all the time during class?

```{r,tut=TRUE,height=400}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

moody[(moody$SCORE>85) & (moody$GRADE=='B'), ]$DOZES_OFF
```

#### Snippet 4

**Q:** What gives a higher chance of failing, texting all the time or always dozing off during class? <br>
**A:** Always texting in class! Almost 40% chance of failing!

```{r,tut=TRUE,height=400}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

#Probability of failing the class when dozing off all the time
nrow(moody[moody$DOZES_OFF=='always' & moody$GRADE=='F',])/nrow(moody[moody$DOZES_OFF=='always',])
#Probability of failing the class when  always texting in class 
nrow(moody[moody$TEXTING_IN_CLASS=='always' & moody$GRADE=='F',])/nrow(moody[moody$TEXTING_IN_CLASS=='always',])

```

#### Snippet 5

**Q:**  What grade did a student who scores 39.57 and always dozed off received? <br>
**A:**  D

```{r,tut=TRUE,height=400}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

moody[moody$SCORE=='39.57'&moody$DOZES_OFF=='always',]$GRADE

```

#### Snippet 6

**Q:**  What are posterior odds that an A student never dozes off?  <br>
**A:** Posterior Odds = 2.66 <br>
Likelihood Ratio = 4 <br>
Prior Odds =0.56

```{r,tut=TRUE,height=400}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

Prior<-nrow(moody[moody$DOZES_OFF =='never',])/nrow(moody)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(moody[moody$GRADE=='A'& moody$DOZES_OFF=='never',])/nrow(moody[moody$DOZES_OFF=='always',]),2)
TruePositive
FalsePositive<-round(nrow(moody[moody$GRADE=='A'& moody$DOZES_OFF!='never',])/nrow(moody[moody$DOZES_OFF!='always',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior

```

#### Snippet 7

**Q:** Verify the hypothesis that C students have higher mean participation than F students? What is the p-value? <br>
**A:** Negative.  Fail to reject null hypothesis that mean participations of C and F students are the same with p=0.11

```{r,tut=TRUE,ex="permutationtestfunction3",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction3",type="sample-code",height=400}
#install.packages("devtools")
#devtools::install_github("devanshagr/PermutationTestSecond")

#PermutationTestSecond::Permutation(d, "Cat", "Val",10000, "GroupA", "GroupB") 

moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

PermutationTestSecond::Permutation(moody, "GRADE", "PARTICIPATION",10000, "C", "F")

```

#### Snippet 8

**Q:** What is the mean score of students who  always doze off in class  and what is the most frequent grade that they received? <br>
**A:** The mean score is 50.26 and the most frequent grade is D.


```{r,tut=TRUE,height=400}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

mean(moody[moody$DOZES_OFF=='always',]$SCORE)
table(moody[moody$DOZES_OFF=='always',]$GRADE)

```

<br>

Great job!! You have made it this far. You are now familiar with the moody dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check1).

#### Snippet 9 {#check1-9}

Visualizing the grade w.r.t score for the moody dataset.

```{r,tut=TRUE,height=500}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")
colors<- c('red','blue','cyan','yellow','green')

boxplot(moody$SCORE~moody$GRADE, xlab="Grade",ylab="Score",col=colors, 
        main="Boxplot for student score and grade distribution",border="black")

```

### Moody Data Quiz

<a href = "https://ds1778.shinyapps.io/data_roulette_1/" target = "blank"> Quiz Time </a>

### Check yourself {#check1}
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

summary(moody)
```










## How to predict a good party?  Data puzzle {#party}

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv" download="Partyb.csv">Partyb.csv</a>

DataMaker has generated data about thousands of parties, some fun parties, others which were OK or simply boring.  Your goal is to discover secrets of a fun party. Is it music? Dancing? Does the host matter? Or who was present at a party? Maybe who was NOT present at the party?  All this data is stored in this data puzzle.

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv") #web load
# head(moody)
temp<-knitr::kable(
  party[sample(1:nrow(party),5), ], caption = 'Snippet of Party Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  party[sample(1:nrow(party),5), ], caption = 'Snippet of Party Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

![ Figiure 2: Distribution of party evaluation when HipHop music was played \@ref(check3-6)](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/data_puzzle/2-3.png)

### 🧙 Secret patterns embedded by the Data Maker (hint) 

When Vladimir is not at a party it is much more likely to be Fun. On the other hand with Angela…parties are almost always boring. She seems to be the life of a party!  Music plays a role as well, when HipHop is played and is catchy (people are dancing) than party rocks. Without any music, the party tends to be just OK. Alex is a good host, provided he plays classical music.  This must be an older crowd though?



### Practice Snippets 

#### Snippet 1: Get to know your data

```{r,tut=TRUE,height=400}
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv")

colnames(party)
nrow(party)
summary(party)
unique(party$Party)
unique(party$Music)
unique(party$Host)
unique(party$WasThere)
unique(party$WasNotThere)
table(party$Party)
table(party$Music)
table(party$Host)
table(party$WasThere)
table(party$WasNotThere)
colnames(party)
tapply(party$CaloriesDanc, party$Party, mean)
tapply(party$CaloriesDanc, party$Music, mean)
tapply(party$CaloriesDanc, party$Host, mean)
tapply(party$CaloriesDanc, party$WasThere, mean)
tapply(party$CaloriesDanc, party$WasNotThere, mean)

```

#### Snippet 2

**Q:** Did you know that a party is often boring when Angela is not there?

```{r,tut=TRUE,height=400}
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv")

table(party[party$WasNotThere=='Angela',]$Party)

```

#### Snippet 3

**Q:** What are the odds of the party being fun when Vladimir is not there? <br>
**A:** PosteriorOdds = 2.91 <br>
LikelihoodRatio = 3.83 <br>
Prior Odds = 0.76


```{r,tut=TRUE,height=400}
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv")

Prior<-nrow(party[party$Party =='Fun',])/nrow(party)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(party[party$Party=='Fun'& party$WasNotThere=='Vladimir',])/nrow(party[party$Party=='Fun',]),2)
TruePositive
FalsePositive<-round(nrow(party[party$Party!='Fun'& party$WasNotThere=='Vladimir',])/nrow(party[party$Party!='Fun',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior
```

#### Snippet 4

**Q:** Verify the hypothesis that there is more dancing at Fun parties than at Boring parties? <br>
**A:** Positive. We reject null hypothesis that there is same amount of dancing at Fun and Boring parties with p < 0.00001

```{r,tut=TRUE,ex="permutationtestfunction4",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction4",type="sample-code",height=400}
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv")

mean(party[party$Party=='Fun',]$CaloriesDanc)
mean(party[party$Party=='Boring',]$CaloriesDanc)

PermutationTestSecond::Permutation(party, "Party", "CaloriesDanc",10000, "Fun", "Boring")

```

#### Snippet 5

**Q:** What music is the most popular at Fun parties? <br>
**A:** HipHop

The following snippet allows us to find the most popular music, although the code just returns the frequency of music genres distribution. 

```{r,tut=TRUE,height=400}
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv")

table(party[party$Party=='Fun',]$Music)
```

#### Snippet 6 {#check3-6}

Bargraph of party distribution  for Music Hiphop.

```{r,tut=TRUE,height=500}
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv")
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

t<-table(party[party$Music=='HipHop',]$Party)


barplot(t, xlab="Type of music", ylab="Number of Parties", col=colors,
        main="Distribution of party evaluation when HipHop music was played", border="black")


```

<br>

You are now familiar with the Party dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check3). 


### Party Data Quiz

<a href = "https://ds1778.shinyapps.io/data_roulette_3/" target = "blank"> Quiz Time </a>

### Check yourself {#check3}
```{r,tut=TRUE,height=600}
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv")

summary(party)
```








## When election is truly local - data puzzle {#election}

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv" download="Voting1.csv">Voting1.csv</a>

In local elections in some small towns, candidates of three local parties: Royalists, KnowNothings and Anarchists are running for the office of the mayor. DataMaker has generated a survey of thousands of town residents and their political sympathies. Data of course can not be more local, leaving global concerns such as inflation or global warming to national or state office candidates.  

Here, the electorate cares  about  issues such as “should we allow leaflowers” (all, only electric, none?), what about CBD stores in town (none, just one, no restrictions), How about liquor (should the town be dry? Or hard liqueurs only). Speed limits? (none, 10mph etc) or even more extreme - the whole town being car-free, streets open only to bicycles and pedestrians?

Can we develop the profiles of voters for each of the parties? What does the  anarchist electorate  care about? Which party is leading among young people who do not want any speed limits in town?


```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
voting<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv") #web load
# head(moody)
temp<-knitr::kable(
  voting[sample(1:nrow(voting),5), ], caption = 'Snippet of Voting Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  voting[sample(1:nrow(voting),5), ], caption = 'Snippet of Voting Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

![ Figiure 3: Barplot for party vote distribution \@ref(check4-12)](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/data_puzzle/puzzle3.png)

### 🧙 Secret patterns embedded by the Data Maker (hint) 

In this local town elections where issues are very…local. Nobody cares here about wars, oil or tariffs with China. Issues of importance are leaf blowers (electric only? Not at all), mowers, SpeedLimit (none? 5mph, no cars at all?)CBD and Liquor stores in town? From a completely dry town to no restrictions whatsoever. 

The radicals who want no speed limits and no restrictions on leaf blowers as well as opening CBD stores anywhere you want, predictably mostly vote for Anarchists.  There is a local electorate strongly devoted to KnowNothings (this old party has resurrected itself in this small town).. There are voters who like Liquor stores to serve hard liquor only (real alcohol, no wimpy beers, wines) and voters who believe in very tight speed limits (below 5mph)!.  Seniors (Age >65) tend to vote Royalists. What does it mean to be a Royalist in a small town only? Good question. We guess the office of mayor is inherited and belongs to the local blue bloods?. 

### Practice Snippets 

#### Snippet 1: Get to know your data

```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

colnames(vote)
nrow(vote)
summary(vote)
unique(vote$LeafBlowers)
unique(vote$CBD)
unique(vote$GasMowers)
unique(vote$Party)
unique(vote$LiquerStores)
unique(vote$SpeedLimit)

table(vote$Party)
table(vote$SpeedLimit)
table(vote$LeafBlowers)
table(vote$GasMowers)

```

#### Snippet 2

**Q:**  Which party has the oldest constituents? <br>
**A:** Royalists


```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

tapply(vote$Age, vote$Party, mean)

```

#### Snippet 3

**Q:** How do voters who are against Gas Mowers vote? <br>
**A:** Royalists

```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

table(vote[vote$LeafBlowers=='None',]$Party)
```

#### Snippet 4

**Q:** How do KnowNothings voters vote on speed limits?

```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

table(vote[vote$Party=='KnowNothings',]$SpeedLimit)
```

#### Snippet 5

**Q:** What is the age of the oldest voter for KnowNothings? <br>
**A:** 100

```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

max(vote[vote$Party=='KnowNothings',]$Age)
```

#### Snippet 6

**Q:** Which party wins the most votes from supporters of no speed limits, no restrictions on CBD stores and Ban of Leaf Blowers?  <br>
**A:** Anarchists

```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

table(vote[vote$SpeedLimit =='NoLimits'&vote$CBD=='NoRestrictions'& vote$LeafBlowers=='None', ]$Party)

```

#### Snippet 7

**Q:**  Which party wins the most votes of supporters of HardLiquerOnly Liquor stores?

```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

table(vote[vote$LiquerStores=='HardLiquerOnly', ]$Party)
```

#### Snippet 8

**Q:**  What are the odds that a voter older than 65 will vote  for Royalists? <br>
**A:** Posterior Odds= 6.44 <br>
Likelihood ratio = 6.08 <br>
Prior Odds= 1.06

```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

Prior<-nrow(vote[vote$Party =='Royalists',])/nrow(vote)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(vote[vote$Party=='Royalists'& vote$Age>65,])/nrow(vote[vote$Party=='Royalists',]),2)
TruePositive
FalsePositive<-round(nrow(vote[vote$Party!='Royalists'& vote$Age>65,])/nrow(vote[vote$Party!='Royalists',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior

```

#### Snippet 9

**Q:** Verify the hypothesis that the average age of Anarchists voters is higher than the average age of KnowNothings voters? <br>
**A:** Positive. Reject of null hypothesis that average ages of Anarchists and KnowNothings voters are the same

```{r,tut=TRUE,ex="permutationtestfunction5",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction5",type="sample-code",height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

mean(vote[vote$Party=='Anarchists',]$Age) 
mean(vote[vote$Party=='KnowNothings',]$Age) 

Permutation(vote, "Party", "Age",10000, "Anarchists", "KnowNothings") 

```

#### Snippet 10

**Q:** What is the most frequent position of Anarchists on  the Speed Limit issue? <br>
**A:** “No limits” is the most frequent position of Anarchists on Speed Limit issue

```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

table(vote[vote$Party=='Anarchists',]$SpeedLimit)

```

#### Snippet 11

**Q:** Which party wins the  most votes  from  supporters of Electric Leaf Blowers? <br>
**A:** Royalists

```{r,tut=TRUE,height=400}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

table(vote[vote$LeafBlowers=='ElectricOnly',]$Party)

```

#### Snippet 12 {#check4-12}

Bar graph of Vote distribution among parties.

```{r,tut=TRUE,height=500}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

t<-table(vote$Party)


barplot(t,xlab="Party",ylab="Number of votes",col=colors, 
      main="Barplot for party vote distribution",border="black")

```

<br>

You are now familiar with the Election dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check4). 

### Election Data Quiz

<a href = "https://textbook.shinyapps.io/data_roulette_4/" target = "blank"> Quiz Time  </a>

### Check yourself {#check4}
```{r,tut=TRUE,height=600}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

summary(vote)
```







## Secrets of good sleep Data Puzzle {#sleep}

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv" download="SleepPrediction2.csv">SleepPrediction2.csv</a>

Who wouldn’t want to know the secrets of good sleep?  DataMaker has created a data set which may help to find these secrets. We store the number of exercise calories burnt during the day, the amount of wimpy tea a person has drunk (in ounces), hours spent on the computer and the quality of the preceding night’s sleep. 

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Sleep Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Sleep Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

![ Figiure 4: Boxplot for sleep and calories distribution \@ref(check5-6)](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/data_puzzle/4-3.png)


### 🧙 Secret patterns embedded by the Data Maker (hint) 

Insomnia (no sleep) occurs when the moon is full and there is a significant amount of exercise (above 300 Cal).  Even when there was little exercise the full moon has a very negative influence on sleep - “Little” is the quality of sleep with full moon and with less than 300 calories spent exercising. 

The secrets of deep sleep are scattered. Cold room and many cups of wimpy tea do the trick. Also, deep sleep comes when last night's sleep was shallow and a significant number of hours were spent on the computer!


### Practice Snippets 

#### Snippet 1: Get to know your data

```{r,tut=TRUE,height=400}
sleep<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv")

colnames(sleep)
nrow(sleep)
summary(sleep)
unique(sleep$Sleep)
unique(sleep$WimpyTea)
unique(sleep$Moon)
unique(sleep$LastSleep)
table(sleep$Sleep)
table(sleep$WimpyTea)
table(sleep$Moon)
table(sleep$LastSleep)

```

#### Snippet 2

**Q:** Is exercising more good for your sleep? <br>
**A:** So and so. You either have Little sleep or deep sleep, much less likely to have shallow sleep

```{r,tut=TRUE,height=400}
sleep<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv")

tapply(sleep$ExerciseCal, sleep$Sleep, mean)

```

#### Snippet 3

**Q:** What are the odds of Deep sleep when last day’s  sleep was Shallow? <br>
**A:** Posterior Odds= 15.27 (probability = 0.93!) <br>
Likelihood Ratio = 5.25 <br>
Prior Odds = 2.91


```{r,tut=TRUE,height=400}
sleep<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv")

Prior<-nrow(sleep[sleep$Sleep =='Deep',])/nrow(sleep)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(sleep[sleep$Sleep=='Deep'& sleep$LastSleep=='Shallow',])/nrow(sleep[sleep$Sleep=='Deep',]),2)
TruePositive
FalsePositive<-round(nrow(vote[sleep$Sleep!='Deep'& sleep$LastSleep=='Shallow',])/nrow(sleep[sleep$Sleep!='Deep',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior

```

#### Snippet 4

**Q:** Verify hypothesis that deep sleepers spend on average more time on the computer than Shallow sleepers? <br>
**A:** Negative. Fail to reject null hypotheses that means are the same.

```{r,tut=TRUE,ex="permutationtestfunction6",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction6",type="sample-code",height=400}

sleep<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv")

mean(sleep[sleep$Sleep=='Deep',]$OnComputer)
mean(sleep[sleep$Sleep=='Shallow',]$OnComputer)

Permutation(sleep, "Sleep", "OnComputer",10000, "Deep", "Shallow")

```

#### Snippet 5

**Q:** What is the highest Room temperature experienced by a  Deep sleeper? 

```{r,tut=TRUE,height=400}
sleep<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv")

max(sleep[sleep$Sleep=='Deep',]$RoomTemp)
```

#### Snippet 6 {#check5-6}

Boxplot showing calories exercising in function of sleep attribute.

```{r,tut=TRUE,height=500}
sleep<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv")
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

boxplot( sleep$ExerciseCal~sleep$Sleep, xlab=" Type of Sleep",ylab="Exercise calories",col=colors, 
        main="Boxplot for sleep and calories distribution",border="black")
```
<br>

You are now familiar with the Sleep dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check5). 



### Sleep Data Quiz

<a href = "https://textbook.shinyapps.io/data_roulette_5/" target = "blank"> Quiz Time  </a>

### Check yourself {#check5}

```{r,tut=TRUE,height=600}
sleep<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv")

summary(sleep)
```









## Let’s go to the movies:  Data Puzzle {#movie}

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv" download="Movies2022F-4.csv">Movies2022F-4.csv</a>

Using DataMaker  we have started with the imdb data set from Kaggle and embedded some patterns in it.  The original data set contains data about 12,800+ movies. We have expanded this data set by DataMaker’s opinions. Yes, only DataMaker can have an opinion on each of 12,800 movies! Can you predict which  movies does DataMaker love and which movies bore him so much that she quit?   What movies DataMaker passionately hates (hmm is DataMaker even passionate about anything at all?). 

When does DataMaker agree with the imdb score?

Can one predict an imdb score on the basis of a combination of DataMaker opinion (sort of super critic) and other attributes?


```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

![ Figiure 5: Boxplot for Genre and IMDB score distribution \@ref(check6-9)](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/data_puzzle/puzzle5.png)

### 🧙 Secret patterns embedded by the Data Maker (hint) 

The documentaries (History) have a tendency to have higher imdb scores. The high gross Comedies do not get appreciated by imdb. And there are also patterns which link high gross movies to genre and content rating. They are easy to discover. Try! 

### Practice Snippets 

#### Snippet 1: Get to know your data  <a href="https://drive.google.com/file/d/1iW3boknzx2REsQdwmEcfe46M_2s7iPSo/view?usp=sharing" target="blank">🔊</a>

```{r,tut=TRUE,height=500}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

colnames(movies)
nrow(movies)
summary(movies)
unique(movies$content)
unique(movies$genre)
unique(movies$Gross)
unique(movies$Budget)


table(movies$content)
table(movies$genre)
table(movies$Gross)
table(movies$Budget)

tapply(movies$imdb_score, movies$content, mean)
tapply(movies$imdb_score, movies$country, mean)
tapply(movies$imdb_score, movies$Gross, mean)
tapply(movies$imdb_score, movies$Budget, mean)
tapply(movies$imdb_score, movies$genre, mean)

```

#### Snippet 2

**Q:** What is the mean imdb of low budget comedies?

```{r,tut=TRUE,height=400}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

mean(movies[movies$Budget=='Low' & movies$genre=='Comedy', ]$imdb_score)
```

#### Snippet 3

**Q:** What is the lowest imdb score among high budget movies?

```{r,tut=TRUE,height=400}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

min(movies[movies$Budget=='High',]$imdb)
```

#### Snippet 4

**Q:**  How many low budget movies generated high gross income?

```{r,tut=TRUE,height=400}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

nrow(movies[movies$Budget=='Low' & movies$Gross =='High',])
```

#### Snippet 5

**Q:** What is the least frequent genre among UK movies?

```{r,tut=TRUE,height=400}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

table(movies[movies$country=='UK',]$genre, movies[movies$country=='UK',]$country)
```

#### Snippet 6

**Q:** Which content rating has the lowest average imdb score?

```{r,tut=TRUE,height=400}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

tapply(movies$imdb, movies$content, mean)
```

#### Snippet 7

**Q:** Movies from which country have the smallest average imdb score?

```{r,tut=TRUE,height=400}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

MA<-aggregate(movies$imdb_score, list(movies$country), mean)
colnames(MA)<-c("Country", "Mimdb")
MA<-MA[order(-MA$Mimdb), ]
MA[1,] 
```

#### Snippet 8

**Q:** What are the odds that a High Budget Movie will have High Gross Income? <br>
**A:** Prior Odds = 5.59 <br>
Likelihood  Ratio = 5.08 <br>
Prior Odds = 1.11


```{r,tut=TRUE,height=400}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

Prior<-nrow(market[movies$Gross =='High',])/nrow(movies)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(movies[movies$Gross=='High'& movies$Budget=='High',])/nrow(movies[movies$Gross=='High',]),2)
TruePositive
FalsePositive<-round(nrow(movies[movies$Gross!='High'& movies$Budget=='High',])/nrow(movies[movies$Gross!='High',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
```

#### Snippet 9 {#check6-9}

Histogram of imdb scores for different genres of the movies

```{r,tut=TRUE,height=500}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")
colors<- c('red','blue','cyan','yellow','green','brown') # Assigning different colors to bars

boxplot(movies$imdb_score~movies$genre, xlab=" Genre",ylab="IMDB score",col=colors, 
        main="Boxplot for Genre and IMDB score distribution",border="black")


```

<br>

You are now familiar with the Movies dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check6). 

### Movies Data Quiz

<a href = "https://textbook.shinyapps.io/data_roulette_6/" target = "blank"> Quiz Time  </a>

### Check yourself {#check6}

```{r,tut=TRUE,height=600}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

summary(movies)
```

<!--
### Secrets Revealed- Patterns in Movies data? 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MV12">Secrets Revealed</button>
<div id="MV12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/profhw4.pdf&embedded=true" width="100%" height="500px"></embed>
</div>



### Best Student's Submissions 2022

<button class="btn btn-primary" data-toggle="collapse" data-target="#hw41">Joshua Sze</button>
<button class="btn btn-primary" data-toggle="collapse" data-target="#hw42">Andrew Fasano</button>

<div id="hw41" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/student1hw4.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="hw42" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/studen2hw4.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
-->

## When canvas goes wild data puzzle {#canvas}

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv" download="Canvas1.csv">Canvas1.csv</a>

You are all familiar with Canvas, right?  This is where you look to see your grades for each assignment and exam.  This is where you see the scores.  However it seems that Canvas went a bit wild and unfair in this data set.One can still fail the class with the score of 82 (sounds familiar, yes, Professor Moody would do it, but Canvas?

How can one get a lower grade  with a higher score?

Yes, Canvas was instructed by someone and your goal is to discover the grading method. How to get an A, how to pass?  We know who that someone is… it is **DataMaker** of course. 

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Canvas Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Canvas Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

![ Figiure 6: Grade distribution for students who passed the homeoworks but failed exam \@ref(check7-8)](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/data_puzzle/6-3.png)

### 🧙 Secret patterns embedded by the Data Maker (hint) 

Canvas allows professors to build linear  formulas to provide weights for homeoworks, projects and exams and calculate the final score which is then mapped to grades, the usual; way, say A is over score of 90, B over score of 75 etc. This canvas method however has also a decision tree embedded addressing rare but troubling cases when a student’s score was very high on homoworks but low on the final exam. Even if the global weight of the final exam is small, say only 15%, the professor is concerned that if the score is very low on the final - say less than 20% of the maximum final score.  Even if the overall score is in A range should such a student get an A?

The method here is very cruel and would for sure be objected to by many students :-). No matter how high your final score is, if your final exam is low, you may actually fail the class!

Check out what is the weight of the final exam?  And what is the threshold of Fail, no matter what?  Is there such a threshold that if your final exam score falls under it, you fail no matter what?  


### Practice Snippets 

#### Snippet 1: Get to know your data

```{r,tut=TRUE,height=400}
grades<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")

colnames(grades)
nrow(grades)
summary(grades)
unique(grades$Grade)
table(grades$Grade)
tapply(grades$Homeworks, grades$Grade, mean)
tapply(grades$Exams, grades$Grade, mean)
tapply(grades$Score, grades$Grade, mean)
```

#### Snippet 2

**Q:**  What  is the distribution of possible grades when a student’s total scores is over 80?

```{r,tut=TRUE,height=400}
grades<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")

table(grades[grades$Score > 80,]$Grade)
```

#### Snippet 3

**Q:** Previous snippets showed that you can only get an A or an F with a score over 80. How can you get an F?  This snippet helps to answer this question.

```{r,tut=TRUE,height=400}
grades<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")

table(grades[grades$Score > 80 & grades$Exams >40,]$Grade)
```

#### Snippet 4

**Q:** What is the worst exam score of a student with final grade A? <br>
**A:** 25

```{r,tut=TRUE,height=400}
grades<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")

min(grades[grades$Grade =='A',]$Exams)
```

#### Snippet 5

**Q:** What are the odds of getting an A with an Exams score above 60? <br>
**A:** Posterior Odds = 0.45 <br>
Likelihood Ratio = 4.75 <br>
Prior Odds = 0.17


```{r,tut=TRUE,height=400}
grades<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")

Prior<-nrow(grades[grades$Grade =='A',])/nrow(grades)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(grades[grades$Grade=='A'& grades$Exams>60,])/nrow(grades[grades$Grade=='A',]),2)
TruePositive
FalsePostive<-round(nrow(grades[grades$Grade!='A'& grades$Exams<60,])/nrow(grades[grades$Grades!='A',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior

```

#### Snippet 6

**Q:** Verify Hypothesis that Mean exam score for B students is higher than mean exam score for C students.  What is the p-value? <br>
**A:** Negative. We fail to reject the null hypothesis with p=0.23

```{r,tut=TRUE,ex="permutationtestfunction7",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction7",type="sample-code",height=400}
grades<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")

mean(grades[grades$Grade=='B',]$Exams)
mean(grades[grades$Grade=='C',]$Exams)

Permutation(grades, "Grade", "Exams",10000, "C", "B") 

```

#### Snippet 7

**Q:** What is the chance of getting an A when you score less than 50 on exams? <br>
**A:** 0.093

```{r,tut=TRUE,height=400}
grades<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")

nrow(grades[grades$Grade =='A' & grades$Exams < 50,])/nrow(grades[grades$Exams < 50,])

```

#### Snippet 8 {#check7-8}

Boxplot of Exams Score w.r.t. Grades.

```{r,tut=TRUE,height=500}
grades<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

t <- table(grades[grades$Exams <50 & grades$Score >50,]$Grade)

barplot(t, xlab="Grades",ylab=" Number of students",col=colors, 
        main="Grade distribution for students who passed the homeoworks but failed exam",border="black")

```

<br>

You are now familiar with the Canvas dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check7). 



### Canvas Data Quiz

<a href = "https://textbook.shinyapps.io/data_roulette_7/" target = "blank"> Quiz Time  </a>

### Check yourself {#check7}

```{r,tut=TRUE,height=600}
grades<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")

summary(grades)
```



## Very local minimarket data puzzle {#minimarket}

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv" download="Minimarket.csv">HomeworkMarket2022.csv</a>

What items sell together? 

A small local minimarket chain (think Wawa at its early days) has a few locations in New Jersey and it sells beer, snacks, sweets. DataMaker provided the data set of several thousand of transactions in the minimarket storing what items were purchased, when they were purchased (weekday or weekend) at which location.  

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Minimarket Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Minimarket Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

![ Figiure 7: Distribution of snacks sales among Weekend buyers of Lager in New Brunswick \@ref(check8-5)](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/data_puzzle/puzzle7.png)

### 🧙 Secret patterns embedded by DataMaker

Boundless analytics (last section) has discovered the embedded patterns in the mini market data.  The golden pattern which was embedded was a subset of  Lager buyers on the weekends in New Brunswick.  These small subset of uses have a distinctly different distribution of snacks. Their taste for snacks is remarkably different among these transactions than the general distribution of snack sales over all transactions.  There are many other associations similar to this one - for example Cola and Popcorn buyers in Princeton. We call these subsets - slices. There are many slices in this data which imply different distributions of snacks, sweets, soft drinks, wines, beers, locations and even weekday/weekend timing of  the sale.

### Practice Snippets 

#### Snippet 1: Get to know your data

```{r,tut=TRUE,height=400}
market<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

colnames(market)
nrow(market)
summary(market)
unique(market$Day)
unique(market$Location)
unique(market$Beer)
unique(market$SoftDrinks)
unique(market$Sweets)
unique(market$Wine)
unique(market$Snacks)
table(market$Day)
table(market$Location)
table(market$Beer)
table(market$SoftDrinks)
table(market$Sweets)
table(market$Wine)
table(market$Snacks)

```

#### Snippet 2

**Q:**  What are  the odds that a customer in New Brunswick buys Lager on a weekend? <br>
**A:** Posterior Odds = 0.53 <br>
Likelihood Ratio = 1.08 <br>
Prior odds = 0.49


```{r,tut=TRUE,height=400}
market<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

Prior<-nrow(market[market$Beer =='Lager',])/nrow(market)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(market[market$Beer=='Lager'& market$Location=='New Brunswick' & market$Day =='Weekend',])/nrow(market[market$Beer=='Lager',]),2)
TruePositive
FalsePositive<-round(nrow(market[market$Beer!='Lager'& market$Location=='New Brunswick' & market$Day =='Weekend',])/nrow(market[market$Beer!='Lager',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior

```

#### Snippet 3

**Q:** What is the most frequent location of Lager purchases? <br>
**A:** Princeton is the most frequent location where Lager is sold

```{r,tut=TRUE,height=400}
market<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

table(market[market$Beer =='Lager',]$Location)

```

#### Snippet 4

**Q:** Is distribution of purchases of snacks among Weekend buyers of Lager in New Brunswick  different from base  distribution of snacks? <br>
**A:** yes, very different

```{r,tut=TRUE,height=400}
market<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

market$IN<-'Out_Slice'
market[market$Beer=='Lager' & market$Day=='Weekend' &  market$Location =='New Brunswick', ]$IN<-'In_Slice'
d<-table(market$Snacks,market$IN)
chisq.test(d)

```

#### Snippet 5 {#check8-5}

Bar plot to check market of snaks w.r.t. beer, day and location.

```{r,tut=TRUE,height=500}
market<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

t<-table(market[market$Beer=='Lager' &market$Day=='Weekend' &market$Location=='New Brunswick',]$Snacks)

barplot(t, xlab="Type of Snack",ylab="Sales",col=colors, 
        main="Distribution of snacks sales among Weekend buyers of Lager in New Brunswick",border="black")

```

<br>

You are now familiar with the MiniMarket dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check8). 

### MiniMarket Data Quiz

<a href = "https://ds1778.shinyapps.io/data_roulette_8/" target = "blank"> Quiz Time  </a>

### Check yourself {#check8}

```{r,tut=TRUE,height=600}
market<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

summary(market)
```
<br>


Now it's time for two real data sets - the airbnb data set and titanic sinking data set, both from Kaggle. These data sets have been cleaned before, this is why we do not have to spend our time on data wrangling!  


## Airbnb data puzzle 

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv" download="airbnb.csv">airbnb.csv</a>

The airbnb data set (Kaggle) stores around 30,000 plus data points about airbnb prices in NYC. We have modified the original set a little bit (we can’t stop!) adding the floor where the department is located to the existing attributes such as Room type, neighbourhood_group (boroughs), specific neighborhood and price.

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
airbnb<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv") #web load
# head(moody)
temp<-knitr::kable(
  airbnb[sample(1:nrow(airbnb),5), ], caption = 'Snippet of Airbnb Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  airbnb[sample(1:nrow(airbnb),5), ], caption = 'Snippet of Airbnb Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```


<br>

![ Figiure 8: Box plot for Price and neighbourhood distribution \@ref(check9-6)](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/data_puzzle/8-3.png)

### Practice Snippets 

#### Snippet 1: Get to know your data

```{r,tut=TRUE,height=400}
airbnb<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv")

nrow(airbnb)
summary(airbnb)
unique(airbnb$neighbourhood_group)
unique(airbnb$neighbourhood)
unique(airbnb$room_type)
unique(airbnb$floor)
table(airbnb$neighbourhood_group)
table(airbnb$neighbourhood)
table(airbnb$room_type)
table(airbnb$floor)
tapply(airbnb$price, airbnb$floor, mean)
tapply(airbnb$price, airbnb$neighbourhood, mean)
tapply(airbnb$price, airbnb$room_type, mean)

```

#### Snippet 2

**Q:** What is the price of the cheapest entire home/apt in Tribeca? <br>
**A:** $284


```{r,tut=TRUE,height=400}
airbnb<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv")

min(airbnb[airbnb$room_type=='Entire home/apt' &airbnb$neighbourhood=='Tribeca',]$price)

```

#### Snippet 3

**Q:** What is the lowest price of accommodation above the 10th floor in Manhattan? <br>
**A:** $184

```{r,tut=TRUE,height=400}
airbnb<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv")

min(airbnb[airbnb$floor >10 &airbnb$neighbourhood_group=='Manhattan',]$price)

```

#### Snippet 4

**Q:** What are the odds of finding a place for less than $200 in Tribeca? <br>
**A:** Posterior Odds = 0.09 <br>
Prior Odds = 0.86 <br>
Likelihood Ratio = 0.11


```{r,tut=TRUE,height=400}
airbnb<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv")

Prior<-nrow(airbnb[airbnb$price<200,])/nrow(airbnb)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
nrow(airbnb[airbnb$price<200& airbnb$neighbourhood=='Tribeca',])
nrow(airbnb[airbnb$neighbourhood=='Tribeca',])
TruePositive<-round(nrow(airbnb[airbnb$price<200& airbnb$neighbourhood=='Tribeca',])/nrow(airbnb[airbnb$price<200,]),5)
TruePositive
FalsePositive<-round(nrow(airbnb[airbnb$price>200& airbnb$neighbourhood=='Tribeca',])/nrow(airbnb[airbnb$price>200,]),5)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,4)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior

```

#### Snippet 5

**Q:** Verify hypothesis that West Village is more expensive than Upper East Side? <br>
**A:** Positive. Null hypothesis is rejected with the p value p < 0.0001

```{r,tut=TRUE,ex="permutationtestfunction8",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction8",type="sample-code",height=400}
airbnb<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv")

Permutation(airbnb, "neighbourhood", "price",10000, "West Village", "Upper East Side")

```

#### Snippet 6 {#check9-6}

Box plot to check price w.r.t. neighbourhood.

```{r,tut=TRUE,height=500}
airbnb<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv")
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

boxplot(airbnb$price ~ airbnb$neighbourhood_group, xlab="Location",ylab="Transactions",col=colors, 
        main="Box plot for Price and neighbourhood distribution",border="black")

```

<br>

You are now familiar with the Airbnb dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check9). 

### Airbnb Data Quiz

<a href = " https://textbook.shinyapps.io/data_roulette_9/?_ga=2.146438262.333337169.1654557226-2077060180.1654557226" target = "blank"> Quiz Time  </a>

### Check yourself {#check9}

```{r,tut=TRUE,height=600}
airbnb<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv")

summary(airbnb)
```






## Titanic data puzzle {#titanic}

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Titanic-train.csv" download="Titanic-train.csv">Titanic-train.csv</a>

The titanic data set (Kaggle) stores records of passengers of Titanic with attributes such as Survived, SibSp (family size), Fare, PClass (type of a cabin), Age etc.  Here is a sample of data

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
titanic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Titanic-train.csv") #web load
# head(moody)
temp<-knitr::kable(
  titanic[sample(1:nrow(titanic),5), ], caption = 'Snippet of Titanic Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  titanic[sample(1:nrow(titanic),5), ], caption = 'Snippet of Titanic Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```


<br>

![ Figiure 9: Age distribution among survivors of Titanic disaster \@ref(check10-7)](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/data_puzzle/puzzle9.png)

### Practice Snippets 

#### Snippet 1: Get to know your data

```{r,tut=TRUE,height=400}
titanic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Titanic-train.csv")

colnames(titanic)
nrow(titanic)
summary(titanic)
unique(titanic$Pclass)
unique(titanic$SibSp)
unique(titanic$Sex)
unique(titanic$Embarked)
unique(titanic$Survived)
table(titanic$Pclass)
table(titanic$SibSp)
table(titanic$Sex)
table(titanic$Embarked)
table(titanic$Survived)

```

#### Snippet 2

**Q:**  What are the odds of survival of single males on Titanic? <br>
**A:**  Posterior Odds = 0.196 <br>
Prior Odds = 0.62 <br>
Likelihood Ratio = 0.31



```{r,tut=TRUE,height=400}
airbnb<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Titanic-train.csv")

Prior<-nrow(titanic[titanic$Survived==1,])/nrow(titanic)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(titanic[titanic$Survived==1& titanic$Sex=='male' &titanic$SibSp==0,])/nrow(titanic[titanic$Survived==1,]),2)
TruePositive
FalsePositive<-round(nrow(titanic[titanic$Survived==0& titanic$Sex=='male'&titanic$SibSp==0,,])/nrow(titanic[titanic$Survived==0,]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,4)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior

```

#### Snippet 3

**Q:** Verify hypothesis that survivors paid on average more for the ticker than those who did not survive? <br>
**A:** Positive. Null hypothesis rejected with p < 0.0001

```{r,tut=TRUE,ex="permutationtestfunction9",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction9",type="sample-code",height=400}
titanic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Titanic-train.csv")

Permutation(titanic, "Survived", "Fare",10000, "1", "0")


```

#### Snippet 4

**Q:** What is the probability of survival for passengers who paid more than 100 pounds for a ticket?  How about those who paid less than 10 pounds? <br>
**A:**  0.73  for passengers who paid more than 100 pounds <br>
0.20 for passengers who paid less than 10 pounds <br>
0.38 for all passengers




```{r,tut=TRUE,height=400}
titanic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Titanic-train.csv")

nrow(titanic[titanic$Fare >100 & titanic$Survived ==1,])/nrow(titanic[titanic$Fare >100,])
nrow(titanic[titanic$Fare <10 & titanic$Survived ==1,])/nrow(titanic[titanic$Fare <10,])
nrow(titanic[titanic$Survived ==1,])/nrow(titanic)

```

#### Snippet 5

**Q:** What was the chance of survival for passengers who traveled at least in a group of 3? <br>
**A:** Just 10%!

```{r,tut=TRUE,height=400}
titanic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Titanic-train.csv")

table(titanic[titanic$SibSp>3,]$Survived)

```

#### Snippet 6

**Q:**  Did survival depend on the class of the cabin? <br>
**A:** Positive. Null hypothesis of independence rejected with p-value less than $e^{-16}$

```{r,tut=TRUE,height=400}
titanic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Titanic-train.csv")
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

chisq.test(titanic$Survived, titanic$Pclass)

```

#### Snippet 7 {#check10-7}

Fare distribution among survived and perished.

```{r,tut=TRUE,height=500}
titanic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Titanic-train.csv")
colors<- c('red','blue','cyan','yellow','green','brown','grey') # Assigning different colors to bars

hist(titanic[titanic$Survived ==0,]$Age, 100, xlab="Age",ylab="Survived",col=colors, 
        main="Age distribution among survivors of Titanic disaster",border="black")

```
<br>

You are now familiar with the Titanic dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check10). 


### Titanic Data Quiz

<a href = "https://dev7796.shinyapps.io/data_roulette_10/" target = "blank"> Quiz Time  </a>

### Check yourself {#check10}

```{r,tut=TRUE,height=600}
titanic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/airbnb.csv")

summary(titanic)
```






## Addiotional Reference 

<button class="btn btn-primary" data-toggle="collapse" data-target="#p12"> Prediction - Free Style </button> 
<div id="p12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1pA0bzMGr_Tu2CXsgtT9Ks5apHqxWh8l1XXBVvwWr6zc/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/FreeStyle.Rmd-->

# 🔖 Common Sense Judgement and Probability {#common}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

We will step away from coding for a moment.   In the class description we promise to address the million dollar question **“How not to be fooled by data?”**.  Let's dive into this important issue. We have already discussed powerful tools such as hypothesis testing, p-values and Bonferroni correction for multiple hypothesis traps. But even if you never want to write a line of code, you need to know about common traps which you may be fooled by in whatever you do. We need to be informed and educated citizens who can catch the fake inferences and fake discoveries whether we read them in the news or hear politicians falling for the traps. 

Daniel Kahnemann, the Nobel Prize winner in Economics is the author of the fascinating book “Think fast, think slow” and he identifies pitfalls of human relationships with numbers, frequencies.   We discuss Availability, Anchoring, Conjunctive fallacy, Narrative fallacy, Law of small numbers, Reverse to the mean and many other concepts in the attached power points. 

In the next section we will also discuss Bayesian theorem and Bayesian reasoning (with some coding handy) to finally come back to the paradoxes such as prosecutorial paradox, Simpson paradox and ecological fallacy in section 21.  

## Additional References

<button class="btn btn-primary" data-toggle="collapse" data-target="#common12">Common Sense Judgement and Probability</button>
<div id="common12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1IuqMFWih6WuUAH2gCztfUjDl2SL7OUV5y7VyDs6_EGo/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/common_sense.Rmd-->

# 🔖 Free Style: Prediction {#P1}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

What is a prediction model?

Prediction model is a set of rules which, given the values of independent variables (predictors) determine the value of predicted (dependent variable). Here are example of such rules

```If score > 80 and participation >0.6 then grade =’A’```

```If score >60 and score <70 and major=’Psychology’ and Ask_questions =’always’ then grade =’B’```

```If score <50 and score >40 and Doze_off =’always’ then grade = ‘F’```

By freestyle prediction we mean building a prediction model without the R library functions such as rpart and other machine learning packages.  In freestyle prediction one develops models from scratch, on the basis of plots as well as exploratory queries.  Freestyle prediction is important for two reasons:  First, building prediction models from scratch allows an aspiring data scientist to “feel the data”  - as opposed to often blind direct applications of these library functions, Second, even when one uses the prediction models based on library functions, the best models are often created by combining  of several such models. These combinations often arise from skillful subsetting of datasets and applying different models to different subsets. 

As our prediction challenge competitions indicate, the winning prediction models (the ones with the least error) are predominantly combinations of different models applied to different subsets of the data. Thus, freestyle prediction is almost always a part of the prediction model building. 
We start with showing an example of a simple freestyle prediction model.




## Example of a simple freestyle prediction model

```{r,tut=TRUE,height=400}

test<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")

summary(test)

myprediction<-test
decision <- rep('F',nrow(myprediction))
decision[myprediction$Score>40] <- 'D'
decision[myprediction$Score>60] <- 'C'
decision[myprediction$Score>70] <- 'B'
decision[myprediction$Score>80] <- 'A'
myprediction$Grade <-decision
error <- mean(test$Grade!= myprediction$Grade)
error
```

## How to build a freestyle (your own code) prediction model?

The key idea behind building freestyle prediction models is to subset data and select the most frequent value of the predicted variable as prediction.  Of course we are interested in finding highly discriminative subsets of data with one highly dominant (most frequent value), since such a very frequent value as prediction choice will lead to a small error. But how to find data subsets with such dominant most frequent values?  It is a bit of a trial and error process. As we show below in the snippet 2, it is a sequence of one line exploratory queries, which the programmer can rely on. Later, in the next section we show how the rpart() package generates such discriminative  subsets of data automatically, though recursive partitioning.


```{r,tut=TRUE,height=500}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")

# How do we build a freestyle prediction model?  Definitely start with plots like the boxplot from the section 5 (data exploration).  But then follow up with exploratory queries as in the recent quizzes. Examples here use table()  functon and look for situations when one grade is absoutely dominant. This would be your prediction. Thus, the goal is to slice the data using subsetting in such a way that for each slice you get a clear "winner grade". Then combine these subset rules into decision vector - just as we did in snippet 14.1.

# Below some examples of such exploratory queries with clear grade winners.

summary(moody)
table(moody$Grade)
table(moody[moody$Score>80,]$Grade)
table(moody[moody$Score>80 & moody$Major=='Psychology',]$Grade)
table(moody[moody$Score<40 & moody$Major=='Economics',]$Grade)
table(moody[moody$Score<40 & moody$Seniority=='Freshman',]$Grade)
```

## One-step crossvalidation

How do we know if our prediction model is any good?  After all, we may easily build a model which is close to perfect on the training data set but performs miserably on the new, testing data. This is a nightmare for every prediction model builder and it is called a Kaggle surprise. Kaggle surprise happens quite often during our prediction competitions when students build models which are overfitting the data and which give them a false feeling of great, low error just to do the opposite on the testing data and yield a miserably high error.

To avoid this or at least to protect one against it, cross validation is needed. We illustrate cross-validation in the next snippet. We split training data into the *real* training data and the testing data, which is the remaining part of our training data set. Thus we use part of the training data as testing data. We do it by randomly splitting our data set. Although we show here just one step of cross-validation, we should do it multiple times. This helps us to observe how our model behaves for different random subsets of training data and helps us to observe inconsistent results (high variance of error) - which is a warning sign of  future kaggle surprise.



```{r,tut=TRUE,height=900}
train<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")
summary(train)
#scramble the train frame
v<-sample(1:nrow(train))
v[1:5]
trainScrambled<-train[v, ]
#one step crossvalidation
trainSample<-trainScrambled[nrow(trainScrambled)-10:nrow(trainScrambled), ]
myprediction<-trainSample

#prediction model - free style
#How to test how good your model is?
#Crossvalidation:  Divide train data set into two disjoint subsets T (train) and train MINUS T, the complement of T. 
#You use T to derive your prediction model and the complement of T (train MINUS T) to validate (test it).
# We assume that you created prediction model looking just at the subset of training data T=trainScrambled[1:990,  ]. 
#Since for crossvalidation we train on a subset T of the training data set and validate (test) on the complement of T. 
#In this case T= trainScrambled[1:990,  ] and complement of T (to validate/test) is stored as trainSample.
#You can do it multiple times. And observe the error and its stability.
#You build your model using the decision vector.  Here is very SIMPLISTIC MODEL which is just illustration. Your model should have much better error and be more sophisticated. 

decision <- rep('F',nrow(myprediction))
decision[myprediction$Score>40] <- 'D'

decision[myprediction$Score>60] <- 'C'

decision[myprediction$Score>70] <- 'B'

decision[myprediction$Score>80 ] <- 'A'

myprediction$Grade <-decision
error <- mean(trainSample$Grade!= myprediction$Grade)
error   
```





We use selected data puzzles from section 4 in prediction challenges. Given a data puzzle (such as 4.1), we separate it into training data subset and testing data subset. The training data is given to students to build and cross-validate their prediction models. Then we use Kaggle to evaluate their models on the testing subset of the data puzzle. Each prediction challenge is structured as competition and Kaggle ranks students' models by prediction accuracy.  For categorical variables it is the **fraction of values** which are predicted correctly, for numerical variables it is **MSE (mean square error)**. 

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## General Structure of the Prediction Challenges

The submission will take place on Kaggle which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submissions. 

The datasets provided for each prediction challenge is as follows: 

- **Training Dataset** 

  - It is used for training and cross-validation purposes in the prediction challenge. This data has all the training attributes along and the values of the attribute wich is predicted (so called, Target attribute). 
  - Models for prediction are to be trained using this dataset only. 
  - Training data set is the set which is used when you build your prediction model - since this is the only data set which has all values of target attribute. 

- **Testing Dataset**

  - It is used for applying your prediction model to new data. You do it only when you are finished with building your prediction model.

  - Testing data set consists of all the attributes that were used for training, but it does not contain any values of the target attribute. 
  - It is disjoint with the training data set - it contains new data and it is missing the target variable. 
  

- **Submission Dataset**

  - After prediction using the “testing” dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g. ID or name,etc) and second the predicted attribute column. Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle. 

To read the datasets use the read.csv() function and for writing the dataset to the file, use the write.csv() function. Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle.

To avoid this, you can add the parameter, row.names = F in the write.csv() function. e.g. ```write.csv(*dataframe*,*fileaddress*,row.names = F)```.

<!--
## Challenge 1 - Freestyle prediction of grades in yet another MOODY data set

This is the next in the sequence of data puzzles about grading methods of the eccentric professor Moody. Professor Moody found out that his former grading methods were leaked to the student by treacherous TA and changed his grading methods (and the TA).

Unfortunately, again the data was leaked to the students (Professor Moody does not use passwords). It indicates that Professor Moody may be tougher on certain majors and also may apply different grading criteria for different student seniority levels

Can you build a prediction model which will mimic Moody's grading as closely as possible?

Attached  are three files : One <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv" download="M2022train.csv">M2022train.csv</a> with  original Professor Moody grading data and another, the <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022testS.csv" download="M2022testS.csv">M2022testS.csv</a> data with missing GRADE column.  Finally <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submissionS.csv" download="M2022submissionS.csv">M2022submissionS.csv</a> is the file you will submit to Kaggle of course after filling up the GRADE column.  

 Your job is to predict the grades in the testing file, adding to it GRADE attribute  with predicted grades. 

This test submission will be handled through Kaggle (just the error computation part)  and  Canvas just like for any assignments so far (Kaggle submission instructions coming). Kaggle will automatically calculate your prediction error.  In this case, of Professor Moody data, it will be a fraction of  grades which your prediction model have predicted incorrectly. 

**Data League:** https://data101.cs.rutgers.edu/?q=node/155 <br>
**Kaggle competition:** https://www.kaggle.com/competitions/predictive-challenge-1-2022/overview<br>
**Kaggle submission instructions:** https://data101.cs.rutgers.edu/?q=node/150 <br>
**Canvas HW9:** https://rutgers.instructure.com/courses/159918/assignments/1953810


## Challenge 2 - Same data but using rpart - decision tree

It is the same DATA as prediction challenge 1.  Just use **rpart()** this time.  Lets see if you can do better (certainly faster) with the rpart than with freestyle prediction. You have to use rpart, but you can use it as part of your prediction model and combine it with your model which you submitted for HW9.  We will talk about rpart in detail in recitations and lectures next week.

**FOR THIS PREDICTION CHALLENGE:**  Have to use rpart function (and predict of course). Have to use and show crossvalidation (use crossvalidate(). Explain in ppts how you used crossvalidation.

Use rpart contol functions - like minbucket and minsplit as well as different subsets of attributes - when corssvalidating. Make sure you explain in your ppts what "controls" have you tried and eventually used.
 
This test submission will be handled through Kaggle (just the error computation part)  and  Canvas just like for any assignments so far (Kaggle submission instructions coming). Kaggle will automatically calculate your prediction error.  In this case, of Professor Moody data, it will be a fraction of  grades which your prediction model have predicted incorrectly. 


**Data League:** https://data101.cs.rutgers.edu/?q=node/155 <br>
**Kaggle competition:** https://www.kaggle.com/competitions/predictive-challenge-2-2022/overview<br>
**Kaggle submission instructions:** https://data101.cs.rutgers.edu/?q=node/150 <br>
**Canvas HW10:** https://rutgers.instructure.com/courses/159918/assignments/1961012

-->

### Preparing submission.csv for Kaggle

```{r,tut=TRUE,height=500}
# Here you just need the test table (without grades) to apply your prediction model and calculate predicted grades. And submission data frame to fill it in with the predicted #grades

test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022testSNoGrade.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')

myprediction<-test
#Here is your model. I just show example of trivial prediction model
decision <- rep('F',nrow(myprediction))
decision[myprediction$Score>40] <- 'D'
decision[myprediction$Score>60] <- 'C'
decision[myprediction$Score>70] <- 'B'
decision[myprediction$Score>80] <- 'A'
#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-decision
submission
# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle

```

<br>

**Data League:** https://data101.cs.rutgers.edu/?q=node/155 <br>
**Kaggle competition:** https://www.kaggle.com/competitions/predictive-challenge-2-2022/overview<br>
**Kaggle submission instructions:** https://data101.cs.rutgers.edu/?q=node/150 <br>

## Additional Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#p12"> Prediction - Free Style </button> 
<div id="p12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1pA0bzMGr_Tu2CXsgtT9Ks5apHqxWh8l1XXBVvwWr6zc/edit?usp=sharing" width="100%" height="500px"></embed>
</div>


<!--chapter:end:chapters/Free_style.Rmd-->

# 🔖 Predictions with rpart {#prpart}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

## Introduction 

Decision trees are one of the most powerful and popular tools for classification and prediction. The reason decision trees are very popular is that they can generate rules which are easier to understand as compared to other models. They require much less computations for performing modeling and prediction. Both continuous/numerical and categorical variables are handled easily while creating the decision trees.


## Use of Rpart {#rpart}

Recursive Partitioning and Regression Tree `RPART` library is a collection of routines which implements a Decision Tree.The resulting model can be represented as a binary tree. For the purpose of illustration of rpart we will continue to use data puzzle 3.1 set - the Professor Moody data set. 


The library associated with this `RPART` is called `rpart`. Install this library using `install.packages("rpart")`.

Syntax for building the decision tree using rpart():

- `rpart( formula , method, data, control,...)`
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *method*: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use "anova" for regression, "class" for classification, etc.
  - *data*: here we provide the dataset on which we want to fit the decision tree on.
  - *control*: here we provide the control parameters for the decision tree. Explained more in detail in the section further in this chapter.
  
  
For more info on the rpart function visit [rpart documentation](https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart)

Lets look at an example on the Moody 2022 dataset.

- We will use the rpart() function with the following inputs:
  - prediction -> GRADE
  - predictors -> SCORE, DOZES_OFF, TEXTING_IN_CLASS, PARTICIPATION
  - data -> moody dataset
  - method -> "class" for classification.


### rpart()
```{r,tut=TRUE,height=300}
library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function.
rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody,method = "class")

```
We can see that the output of the rpart() function is the decision tree with details of, 

- node -> node number
- split -> split conditions/tests
- n -> number of records in either branch i.e. subset
- yval -> output value i.e. the target predicted value.
- yprob -> probability of obtaining a particular category as the predicted output.

Using the output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section \@ref(rpartpredict)

But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section.

## Visualize the Decision tree {#rpartplot}

To visualize and understand the rpart() tree output in the easiest way possible, we use a library called `rpart.plot`. The function `rpart.plot()` of the rpart.plot library is the function used to visualize decision trees.

*NOTE*: The online runnable code block does not support `rpart.plot` library and functions, thus the output of the following code examples are provided directly.

### rpart.plot()

```{r,tut=TRUE,height=500}
# First lets import the rpart library
library(rpart)

# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function.
rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody,method = "class")

# Now lets import the rpart.plot library to use the rpart.plot() function.
#library(rpart.plot)

# Use of the rpart.plot() function  to visualize the decision tree.
#rpart.plot(tree)
```
![Output Plot of *rpart.plot()* function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt.png)

We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about 

- the grade category.
- the outcome probability of each grade category.
- the records percentage  out of total records.

To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides [rpart.plot](https://www.rdocumentation.org/packages/rpart.plot/versions/3.0.9/topics/rpart.plot) and [Plotting with rpart.plot (PDF)](http://www.milbo.org/doc/prp.pdf)

---
**NOTE**: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment.
---

## Rpart Control {#rpartcontrol}

Now let's look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function.

- `rpart.control( *minsplit*, *minbucket*, *cp*,...)`
 - *minsplit*: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -> the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition.
 - *minbucket*: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -> the minimum number of observation in the terminal/leaf node of the trees must be 500 or above.  
 - *cp*: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by *cp*, will not be made in the tree.
 

For more information of the other arguments of the `rpart.control()` function visit [rpart.control](https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart.control)

Let look at few examples.

Suppose you want to set the control parameter minsplit=200. 

### rpart(): Minsplit = 200

```{r,tut=TRUE,height=500}
library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter minsplit=200
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minsplit = 200))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)
```
![Output tree plot of after setting minsplit=200 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt2.png)

### rpart(): Minsplit = 100

```{r,tut=TRUE,height=500}
library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter minsplit=100
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minsplit = 100))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)
```
![Output tree plot of after setting minsplit=100 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt8.png)

We can see from the output of `tree$splits` and the tree plot, that at each split the total amount of observations are above 200 and 100. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits.

Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters.

### rpart(): Minbucket = 100

```{r,tut=TRUE,height=500}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter Minbucket=100
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minbucket = 100))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)

```
![Output tree plot of after setting minbucket=100 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt3.png)

We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size.

### rpart(): Minbucket = 200

```{r,tut=TRUE,height=500}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter Minbucket=200
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minbucket = 200))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)

```
![Output tree plot of after setting minbucket=200 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt4.png)

We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 200. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size.

Lets now use the `cp` parameter and see its effect on the tree.

### rpart(): cp = 0.05

```{r}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter cp=0.2
tree <- rpart(GRADE ~ ., data = moody,method = "class",control=rpart.control(cp = 0.05))

tree

#library(rpart.plot)
#rpart.plot(tree)


```
![Output tree plot of after setting cp=0.05 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt6.png)


### rpart(): cp = 0.005

```{r}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter cp=0.005
tree <- rpart(GRADE ~ ., data = moody,method = "class",control=rpart.control(cp = 0.005))

tree

#library(rpart.plot)
#rpart.plot(tree)


```
![Output tree plot of after setting cp=0.005 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt7.png)
We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005.


## Cross Validation {#crossvalidation}

Overfitting takes place when you have a high accuracy on training dataset, but a low accuracy on the test dataset. But how do you know whether you are overfitting or not? Especially since you cannot determine accuracy on the test dataset? That is where cross-validation comes into play.

Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. The partition is defined by split ratio. If split ratio =0.7, 70% of the training dataset will be used for the actual training of your model (rpart or lm), and 30 % will be used for validation (or testing). The accuracy of this validation data is called cross-validation accuracy.

To know if you are overfitting or not, compare the training accuracy with the cross-validation accuracy. If your training accuracy is high, and cross-validation accuracy is low, that means you are overfitting.

- `cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*)`
  - *data*: The dataset on which cross validation is to be performed.
  - *tree*: The decision tree generated using rpart.
  - *n_iter*: Number of iterations.
  - *split_ratio*: The splitting ratio of the data into train data and validation data.
  - *method*: Method of the prediction. "class" for classification.

The way the function works is as follows:

- It randomly partitions your data into training and validation. 
- It then constructs the following two decision trees on training partition:
  -  The tree that you pass to the function.
  -  The tree is constructed on all attributes as predictors and with no control parameters.
-It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees.

The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting.

We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired.

The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant.

Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration.


Let's look at the cross_validate() function in action in the example below.

We will pass the tree with formula as `GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION`, and control parameter, with `minsplit=100`. 
And for cross_validate() function, we will use` n_iter=5, and split_raitio=0.7` 

---
**NOTE:** Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use 
---

```
install.packages("devtools") 
devtools::install_github("devanshagr/CrossValidation")
CrossValidation::cross_validate()
```

### cross_validate()

```{r,tut=TRUE,ex="crossvalidate",type="pre-exercise-code"}

cross_validate <- function(df, tree, n_iter, split_ratio, method = 'class')
{
  # training data frame df
  df <- as.data.frame(df)

  # mean_subset is a vector of accuracy values generated from the specified features in the tree object
  mean_subset <- c()

  # mean_all is a vector of accuracy values generated from all the available features in the data frame
  mean_all <- c()

  # control parameters for the decision tree
  contro = tree$control

  # the following snippet will create relations to generate decision trees
  # relation_all will create a decision tree with all the features
  # relation_subset will create a decision tree with only user-specified features in tree
  dep <- all.vars(terms(tree))[1]
  indep <- list()
  relation_all = as.formula(paste(dep, '.', sep = "~"))
  i <- 1
  while (i < length(all.vars(terms(tree)))) {
    indep[[i]] <- all.vars(terms(tree))[i + 1]
    i <- i + 1
  }
  b <- paste(indep, collapse = "+")
  relation_subset <- as.formula(paste(dep, b, sep = "~"))

  # creating train and test samples with the given split ratio
  # performing cross-validation n_iter times
  for (i in 1:n_iter) {
    sample <-
      sample.int(n = nrow(df),
                 size = floor(split_ratio * nrow(df)),
                 replace = F)
    train <- df[sample,]
    testing  <- df[-sample,]
    type = typeof(unlist(testing[dep]))

    # decision tree for regression if the method specified is "anova"
    if (method == 'anova') {
      first.tree <-
        rpart(
          relation_subset,
          data = train,
          control = contro,
          method = 'anova'
        )
      second.tree <- rpart(relation_all, data = train, method = 'anova')
      pred1.tree <- predict(first.tree, newdata = testing)
      pred2.tree <- predict(second.tree, newdata = testing)
      mean1 <- mean((as.numeric(pred1.tree) - testing[, dep]) ^ 2)
      mean2 <- mean((as.numeric(pred2.tree) - testing[, dep]) ^ 2)
      mean_subset <- c(mean_subset, mean1)
      mean_all <- c(mean_all, mean2)
    }

    # decision tree for classification
    # if the method specified is not "anova", then this block is executed
    # if the method is not specified by the user, the default option is to perform classification
    else{
      first.tree <-
        rpart(
          relation_subset,
          data = train,
          control = contro,
          method = 'class'
        )
      second.tree <- rpart(relation_all, data = train, method = 'class')
      pred1.tree <- predict(first.tree, newdata = testing, type = 'class')
      pred2.tree <-
        predict(second.tree, newdata = testing, type = 'class')
      mean1 <-
        mean(as.character(pred1.tree) == as.character(testing[, dep]))
      mean2 <-
        mean(as.character(pred2.tree) == as.character(testing[, dep]))
      mean_subset <- c(mean_subset, mean1)
      mean_all <- c(mean_all, mean2)
    }
  }

  # average_accuracy_subset is the average accuracy of n_iter iterations of cross-validation with user-specified features
  # average_acuracy_all is the average accuracy of n_iter iterations of cross-validation with all the available features
  # variance_accuracy_subset is the variance of accuracy of n_iter iterations of cross-validation with user-specified features
  # variance_accuracy_all is the variance of accuracy of n_iter iterations of cross-validation with all the available features
  cross_validation_stats <-
    list(
      "average_accuracy_subset" = mean(mean_subset, na.rm = T),
      "average_accuracy_all" = mean(mean_all, na.rm = T),
      "variance_accuracy_subset" = var(mean_subset, na.rm = T),
      "variance_accuracy_all" = var(mean_all, na.rm = T)
    )

  # creating a data frame of accuracy_subset and accuracy_all
  # accuracy_subset contains n_iter accuracy values on cross-validation with user-specified features
  # accuracy_all contains n_iter accuracy values on cross-validation with all the available features
  cross_validation_df <-
    data.frame(accuracy_subset = mean_subset, accuracy_all = mean_all)
  return(list(cross_validation_df, cross_validation_stats))
}
```


```{r,tut=TRUE, ex="crossvalidate",type="sample-code",height=500}
# First lets import the rpart library
library(rpart)
# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv',stringsAsFactors = T)
# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS, data = moody,method = "class",control = rpart.control(minsplit = 100))
tree
# Now lets predict the Grades of the Moody Dataset.
pred <- predict(tree, moody, type="class")
head(pred)
# Lets check the Training Accuracy
mean(moody$GRADE==pred)
# Lets us the cross_validate() function.
cross_validate(moody,tree,5,0.7)
```

You can see that the cross-validation accuracies for the tree that was passed (accuracy_subset) are fairly high and close to our training accuracy of 84%. This means we are not overfitting. Also observe that accuracy_subset and accuracy_all have the same values, which means that the only relevant attributes are score and participation, and adding more attributes doesn't make any difference to the tree. Finally, the values in accuracy_subset are reasonably close to each other, which mean low variance.


## Prediction using rpart. {#rpartpredict}

Now that we have seen the process to create a decision tree and also plot it, we will like to use the output tree to predict the required attribute.

From the moody example, we are trying to predict the grade of students. Lets look at the `predict()` function to predict the outcomes.

- `predict(*object*,*data*,*type*,...)`
  - *object*: the generated tree from the rpart function.
  - *data*: the data on which the prediction is to be performed.
  - *type*: the type of prediction required. One of "vector", "prob", "class" or "matrix".

Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset.

### predict()

```{r,tut=TRUE,height=500}
# First lets import the rpart library
library(rpart)

# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody ,method = "class")
tree

# Now lets predict the Grades of the Moody Dataset.
pred <- predict(tree, moody, type="class")
head(pred)
```

<!--
## Your Model with rpart

```{r,tut=TRUE,height=600}
#How to combine your freestyle prediction model with the rpart? 

#One way of doing it is to divide the data sets into two mutually exclusive subsets (which cover all data also).  How do you make these subsets?  Unfortunately there is no algorithm for this and it is more relying on how well is your model doing for different slices of the data.  

#In this example (similarly to snippet 16.7 where we combine two rpart models, we assume that initial split we decided on is based on SCORE. But instead of having two rpart models  (16.7), we will use our prediction  model from prediction challenge 1  for SCORE >50 and rpart for SCORE <=50.

#Lets assume that yourPrediction is our model from Prediction Challenge 1 (your entire code has to be applied here to the data set (moody, below)

moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

#rpartModel<-rpart(GRADE~., data=moody[moody$SCORE<=50,]);
#pred_rpartModel <- predict(rpartModel, newdata=moody[moody$SCORE<=50,], type="class")
#pred_yourModel <- yourPrediction[moody$SCORE<=50]
#myprediction<-moody

## Here we combine two models - our model from prediction 1 challenge and rpart.

#decision <- rep('F',nrow(myprediction))
#decision[myprediction$SCORE>50] <- pred_yourModel
#decision[myprediction$SCORE<=50] <-as.character(pred_rpartModel )
#myprediction$GRADE <-decision
#error <- mean(moody$GRADE!= myprediction$GRADE
#error
```

## Freestyle +  rpart: Combining rpart prediction models

```{r,tut=TRUE,height=600}

library(rpart)
# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')
model1<-rpart(GRADE~., data=moody[moody$SCORE>50,]);
model2<-rpart(GRADE~., data=moody[moody$SCORE<=50,]);
model1
model2
pred1 <- predict(model1, newdata=moody[moody$SCORE>50,], type="class")
pred2 <- predict(model2, newdata=moody[moody$SCORE<=50,], type="class")
myprediction<-moody
decision <- rep('F',nrow(myprediction))
decision[myprediction$SCORE>50] <- as.character(pred1)
decision[myprediction$SCORE<=50] <-as.character(pred2)
myprediction$GRADE <-decision
error <- mean(moody$GRADE!= myprediction$GRADE)
error

```
-->

## Combining multiple prediction models 

How to build highly predictive models? 

This is the million dollar question which many students always ask in the context of our Prediction Challenges (see the leaderboard for 2022). These usually consist of 4-5 prediction tasks and students who achieve the lowest cumulative error make it to the top of the leaderboard and are widely celebrated. What is the secret of building a competitive prediction model?  It is not blind application of machine learning library functions such as rpart(). Even with the great set up of parameter values and careful cross validation a singular model will usually not be very competitive. The top prediction models combine human ingenuity, knowledge of data with machine learning library functions. But how to combine different prediction models to build the “supermodel”:-)? . First - know your data, do some preliminary freestyle data exploration, make some plots, see how data is distributed. Possibly identify subsets of data which may behave very differently and may require different prediction models - either “hand made” or ML made. 

We will start by showing how to combine two different prediction models - applied to different partitions of the data set.  We assume that the partition is “given”. It is usually the result of preliminary data exploration and plotting. In the next section we show an elegant and generic method of combining arbitrary numbers of prediction models using rpart() function.

For now, let us assume that we have partitioned  the moody data set based on the attribute SCORE into two subsets: one with SCORE >50 and another with SCORE <=50. Furthermore we have trained separate rpart() prediction models for each of the two partitions. Now we want to combine these two models into the one, combined model and apply the combined model to the testing data set moodyTest.

The following snippet 16.6.1 shows how to do it.  Two models: model1 and model2 are trained  by running rpart() on two partitions of moody - *the training data set,  based on SCORE. Then we use predict() function by applying model1 to the partition of SCORE >50 and model2  for the subset defined by SCORE <=50 of the testing data set - moodyTest.  Finally the lines 11-14 built the  decision vector which combines predictions of models 1 and 2 into one prediction vector on the moodyTest. 

### Combining rpart prediction models 

```{r,tut=TRUE,height=600}
library(rpart) 

# Import dataset 

moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')
moodyTest<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv') 

# We need two sets here: training moody and testing moody (the full testing like Kaggle stores but does not give to students
model1<-rpart(GRADE~., data=moody[moody$SCORE>50,]); 
model2<-rpart(GRADE~., data=moody[moody$SCORE<=50,]); 
model1 
model2 

pred1 <- predict(model1, newdata=moodyTest[moodyTest$SCORE>50,], type="class") 
pred2 <- predict(model2, newdata=moodyTest[moodyTest$SCORE<=50,], type="class") 
myprediction<-moodyTest 

decision <- rep('F',nrow(myprediction)) 

decision[myprediction$SCORE>50] <- as.character(pred1) 
 
decision[myprediction$SCORE<=50] <-as.character(pred2) 
 
myprediction$GRADE <-decision 

error <- mean(moody$GRADE!= myprediction$GRADE) 
error

```

###  Combining multiple prediction models using rpart

We describe here an elegant method which will allow us to build a combined model in two (or more) phases. Let us start with two prediction models:  pred1 which is freestyle model and pred2 which is rpart() model.  We have faced this situation in our prediction challenges in the spring of 2022. Students were asked to create two prediction modes: one, which was their own code (freestyle prediction) and another - through application of rpart(). Turned out that top freestyle prediction models had lower error on the testing data than rpart().  The challenging task was to combine two models and make the best out of the two, hopefully getting a combined model which beats both freestyle and rpart() models. But how to build such a model?  In the previous section we just described the mechanics of combining two models - by splitting the data set into two disjoint partitions and applying each model to just one partition.  But how to find such partitions?  Fortunately we have rpart() to help us. 

We will demonstrate the proposed method using some pseudo-code and then illustrate it further with an executable snippet combining two specific prediction models. We will expand first the training (and testing) data with two additional, derived attributes. One for each prediction model. Call these attributes $model1$ and $model2$. Then use rpart() to find the best model which uses original attributes of the data set as well as these two new attributes. Therefore we just let rpart() decide what is the best use of these two new attributes. 

Let df_train be the training data set (data frame) and let df_test be the testing data frame.  Let pred_yourModel be the freestyle prediction function which  returns a decision vector according to a freestyle prediction model. For example 16.6.2 snippet shows such a very simplistic model for a moody data set, which assigns grades based on the disjoint intervals of SCORE attribute. 

```
df_train$model1<- pred_yourModel(df_train)
tree<-rpart(df_traing,...)
df_train$model2 <- predict(tree, test, type="class") 

Now, the training data set has two extra attributes: model1 and model2.

Finally we create a compound model by using the extended attribute set of moody.

Tree_combined <- rpart(F, data = df_train, method = "class")) 

F is of the form T~.  where T is the target attribute of df (the one we predict). We let rpart() use all attributes including the new ones: model1 and model2. 

Tree_combined will use both prediction models as attributes and depending on their information gain these two new attributes may play an important role. We can cross validate like before and estimate the error of this combined prediction model on the training data set. 

If we are satisfied with the combined model, we then repeat the same process on testing data.

df_test$model1<- pred_yourModel(df_test)
tree<-rpart()
df_test$model2 <- predict(tree, df_test, type="class") 
Now, the training data set has two extra attributes: model1 and model2.

And calculate final prediction using predict function: 

predict(Tree_combined, moody_test, type="class") 


The next snippet illustrates this process  for a moody data set.  Freestyle model is very simplistic:

decision <- rep('F',nrow(moody))
decision[moody$Score>40] <- 'D'
decision[moody$Score>60] <- 'C'
decision[moody$Score>70] <- 'B'
decision[moody$Score>80] <- 'A'
moody$model2 <-decision

```

This prediction model assigns grades solely on the basis of SCORE attribute: A’s for SCORE over 80, B’s for SCORE between 70 and 80, C’s for SCORE between 60 and 70, D’ s for SCORE between 40 and 60 and finally F for SCORE <40.

We combine this model with model1 which uses rpart(). 

Last two lines of the code show where model1 and model2 differ and how often do these two models differ (in almost 25% of the data set)

####  Combining two prediction models using rpart() for moody data set

```{r,tut=TRUE,height=800}
# First lets import the rpart library
library(rpart)
#install.packages('rpart.plot')
#library(rpart.plot)

# Use of the rpart.plot() function  to visualize the decision tree.


# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')
moodyTest<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv') 

#These are the same two sets - but I wanted to distinguish that one of them is training and another is testing.
#Maybe we can then use moody  and moodyTest as two partitions of moody, like in cross validation.

# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody ,method = "class")
tree

# Now let's predict the Grades of the Moody Dataset.
moody$model1<-rep('F',nrow(moody))
moody$model2<-rep('F',nrow(moody))
moody$model1<- predict(tree, moody, type="class")
decision <- rep('F',nrow(moody))
decision[moody$SCORE>40] <- 'D'
decision[moody$SCORE>60] <- 'C'
decision[moody$SCORE>70] <- 'B'
decision[moody$SCORE>80] <- 'A'
moody$model2 <-decision
colnames(moody)
moodyTest$model1<-rep('F',nrow(moody))
moodyTest$model2<-rep('F',nrow(moody))
moodyTest$model1<- predict(tree, moodyTest, type="class")
decision <- rep('F',nrow(moody))
decision[moodyTest$SCORE>40] <- 'D'
decision[moodyTest$SCORE>60] <- 'C'
decision[moodyTest$SCORE>70] <- 'B'
decision[moodyTest$SCORE>80] <- 'A'
moodyTest$model2 <-decision
colnames(moody)
tree_combined<-rpart(GRADE~., data=moody, method='class')
tree_combined
colnames(moodyTest)
#rpart.plot(tree_combined)
predict(tree_combined, moodyTest, type='class')
nrow(moody[moody$model1!=moody$model2,])
nrow(moody)
error<-mean(moodyTest$GRADE!=predict(tree_combined, moodyTest, type='class'))
error


```

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody_multi_model.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of combined models Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of combined models Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

![Output tree plot of after combining model1 and model2](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/multi_model.png)

Next snippet shows how to submit your prediction vector to Kaggle by creating your submission data frame. 

## Submission of your prediction vector

```{r,tut=TRUE,height=600}
library(rpart)
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022testSNoGrade.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')
train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv")

tree <- rpart(Grade ~ Major+Score+Seniority, data = train, method = "class",control=rpart.control(minbucket = 200))
tree

prediction <- predict(tree, test, type="class")

#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-prediction 

# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle
```

## Additional Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#dt12"> Decision trees </button> 
<div id="dt12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1krkp_icoYBVxISnCndr2BtOOZud1Xc6CCiuLDBUa7TU/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/Decision_trees.Rmd-->

# 🔖 Linear Regression {#lr}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

## Introduction 

How to build prediction models for numerical variables?

So far we have discussed prediction models for categorical target variables.  In order to predict numerical variables we often use linear regression.

<!--
Linear regression is a linear approach to modeling the relationship between a numerical response ($Y$) and one or more independent variables ($X_i$).

Usually in linear regression, models are used to predict only one scalar variable. But there are two subtype if these models:
- First when there is only one explanatory variable and one output variable. This type of linear regression model known as simple linear regression.
- Second, when there are multiple predictors, i.e. explanatory/dependent variables for the output variable. This type of linear regression model known as multiple linear regression.

![Linear models fitted to various different type of data spread. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. Credits: Wikipedia.](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling2/lmvariants.svg)
-->

## Linear regression using lm() function {#lm}

Syntax for building the regression model using the *lm()* function is as follows:

- `lm(formula, data, ...)`
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *data*: here we provide the dataset on which the linear regression model is to be trained.
  
For more info on the *lm()* function visit [lm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm)

Lets look at the example on the Moody dataset.

```{r,echo=FALSE}
moodyNUM<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv')
temp<-knitr::kable(
  head(moodyNUM, 10), caption = 'Snippet of Moody Num Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

Imagine that we do not know the weights of  midterm, project and final exam.  However we have the data from the previous semesters.  Can we find these weights out? The answer is yes - by using `linear regression`.

### How much do Midterm, Project and Final Exam count?

```{r,tut=TRUE,height=500}
moodyNUM<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv')
split<-0.7*nrow(moodyNUM)
split
moodyNUMTr<-moodyNUM[1:split,]
moodyNUMTr
moodyNUMTs<-moodyNUM[split:nrow(moodyNUM),]
#We use linear regression to #find out the weights of #Midterm, Project and Final #Exam in calculation of the #final class score. Each of #them are scored out of 100 and #the final class score is also #scored out of 100 as weighted #sum of Midterm, Project and #Final Exam scores.
train <- lm(ClassScore~.,  data=moodyNUMTr)
train
pred <- predict(train,newdata=moodyNUMTs)
mean((pred - moodyNUMTs$ClassScore)^2)
```

We can see that, 

- The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc.
- The predicted values are obtained from the predict function using the trained model and the test data.

## Calculating the Error using mse() {#mse}

As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test won't prove useful now in our numerical predictions scenario.

We don't want to eyeball every time we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique.

To do this we will use the Mean Squared Error(MSE).

- The MSE is a measure of the quality of an predictor/estimator
- It is always non-negative
- Values closer to zero are better.

The equation to calculate the MSE is as follows:

\begin{equation}
MSE=\frac{1}{n} \sum_{i=1}^{n}{(Y_i - \hat{Y_i})^2}
\\ \text{where $n$ is the number of data points, $Y_i$ are the observed value}\\ \text{and $\hat{Y_i}$ are the predicted values}
\end{equation}

To implement this, we will use the *mse()* function present in the Metrics Package, so remember to install the Metrics package and use `library(Metrics)` in the code for local use.

The syntax for *mse()* function is very simple:

- `mse(actual,predicted)`
  - *actual*: vector of the actual values of the attribute we want to predict.
  - *predicted*: vector of the predicted values obtained using our model.


## Cross Validate your prediction

```{r,tut=TRUE,height=600}
library(ModelMetrics)

train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")
#scramble the train frame
v<-sample(1:nrow(train))
v[1:5]
trainScrambled<-train[v, ]

#one step crossvalidation
n <- 100
trainSample<-trainScrambled[nrow(trainScrambled)-n:nrow(trainScrambled), ]
testSample <- trainScrambled[1:n,]

lm.tree <- lm(ClassScore~.,  data=trainSample)
lm.tree

pred <- predict(lm.tree,newdata=testSample)
pred

mse(testSample$ClassScore,pred)

  
```

We can see that,

- The summary of the lm model gives us information about the parameters of the model, the residuals and coefficients, etc.
- The predicted values are obtained from the predict function using the trained model and the test data. In comparison to the previous model we are using the cross validation technique to check if we have more accurate predictions, thus increasing the overall accuracy of the model.

## Submission with lm

```{r,tut=TRUE,height=600}
library(rpart)
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM_test.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')
train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")

tree <- lm(ClassScore~.,  data=train)
tree

prediction <- predict(tree, newdata=test)

#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-prediction 

# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle
```

## Additional Reference 

<button class="btn btn-primary" data-toggle="collapse" data-target="#lr12"> Linear Regression </button> 
<div id="lr12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1RuAidmZGDoTRYMjONLEsw3bfx_aQb-f3C0V4Jjk4Vzs/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/Linear_regression.Rmd-->


# Power Law Distribution  {#pld}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

Power law distributions have been known for a while but came to prominence with the advent of the world wide web where power law occurs quite frequently. 

Naseem Taleeb in his book Black Swan has termed Power Law as Extremistan  and Normal (Gaussian) distribution as Mediocristan.  The Mandelbrotian and the Gaussian. from Mandelbrodt and his pioneering work in the field of fractals and power distribution.

Figure 1 displays power law which reflects acute concentration and so-called long tail characteristics to virtual quantities such as web traffic, sales data, wealth data, youtube views, word frequency etc.  Gausian law (Bell curve) usually displays more physical quantities such as weight, height etc.  The latter concentrate heavily around mean and decline quickly (so called Gausian decline).  Long tail of power law distributions declines much slower and as we will see helps to explain much more wild deviation of wealth, sales etc.  


![Figure 19.1 Power Law](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Normalplot.png)

![Figure 19.2 Bell curve – Gaussian Distribution](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/predblog/Long_tail.png)

For example, take the height of men and women.  Average height is 1.67 meters. It is distributed normally around this mean height.  Assume that standard deviation of height is 10 cm.

Consider odds of being

<table>
<tr>
<td> 10 centimeters taller than the average (i.e., taller than 1.77 m, </td>
<td> 1 in 6.3 </td>
</tr>

<tr>
<td> 20 centimeters taller than the average (i.e., taller than 1.87 m, or 6 feet 2 </td>
<td> 1 in 44 </td>
</tr>

<tr>
<td> 30 centimeters taller than the average (i.e., taller than 1.97 m, or 6 feet 6): 1 in 740 </td>
<td> 1 in 740 </td>
</tr>

<tr>
<td> 40 centimeters taller than the average (i.e., taller than 2.07 m, or 6 feet </td>
<td> 1 in 32000 </td>
</tr>

<tr>
<td> 50 centimeters taller than the average (i.e., taller than 2.17 m, or 7 feet </td>
<td> 1 in 3,500,000 </td>
</tr>

<tr>
<td> 60 centimeters taller than the average (i.e., taller than 2.27 m, or 7 feet \n

70 centimeters taller than the average (i.e., taller than 2.37 m,
or 8 feet 9 </td>

<td> 1 in 1,000,000,0000 </td>
</tr>

<tr>
<td> 100 centimeters taller than the average (i.e., taller than 2.67 m,
or 8 feet 9 </td>
<td> 1 in 130,000,000,000,000,000,000,000 </td>
</tr>
</table>

<br />

\begin{equation}

P(X>x) = a * x ^{-c}

\end{equation}

Thus if one doubles x (the threshold) then the probability of exceeding x goes down $2^{-c}$ times. If exponent c=1, then x doubles the $P(X>2x) = ½ * P(X>x)$


The following snippet illustrates the radical differences between power law distribution and the normal distribution. 

We first generate the power law distribution with the exponent -1.  To this aim we first generate uniform distribution (runif() function does this) of real numbers between 0.001 and 1. We are also rounding these numbers up to 3 digits using round(). Power distribution is generated by one line of code 

\begin{equation}

Power < -v ^ {-1} 

\end{equation}

We generate 10000 data points which range from 1000 down to 1. The mean value of the power law distribution is 7. Now to compare the power law distribution and normal distributions we use rnorm() to create 10000 data points distributed normally around the same mean =7 and with standard deviation of 0.1.  We show histograms of both distributions to illustrate the different shapes, long tail shape of the power law distribution and the characteristic Bell curve of the Gaussian distribution. Then we follow by illustrating the main point: the rapid descending of the Gaussian distribution.  Just change the values in two last statements of the snippet:  length(normal[normal>7.1]) and length(power[power>7.1]). Notice that the number of values which are larger than 7.1 (one standard deviation from the mean) is roughly the same for both distributions: it is around 1500. But now observe what happens when we move x higher in  length(normal[normal>x]),  and length(power[power>x]).   The value of x=7.3 is the largest value when there are still 19 values left in length(normal[normal>x]), when we move x to 7.4, this number is already zero. Now compare it with the power law distribution.  For x =7.3 we get 1319 values. 

Try  x=8, 16,32, 64 and there are still a substantial number of values left. We get 140 values for power>64. Notice that each time we double x, we halve the number of values above x.

length(power[power>8])

length(power[power>16])

length(power[power>32])

length(power[power>64])
…
length(power[power>256])  still gets us 27 values (it is stochastic, so when you run it values may be slightly different.


You are encouraged to experiment with the exponent (-1) by increasing and decreasing it, as well as with standard deviation of the normal distribution (increase it and decrease it). Just remember to keep the mean of normal distribution the same as the mean of power law distribution for easier comparison.
The rapid descent of normal distribution is clearly visible, when all values are between 6.61 and 7.35. while power law distribution values range from 1000 to 1.

**Snippet 19.1:** Power Law snippet

```{r, tut=TRUE, height = 400}

v<-round(runif(10000, min = 0.001, max = 1), 3)
v[order(v)]
power<-v^(-1)
mean(power)
max(power)
min(power)
normal<-rnorm(10000,mean=7, sd=0.1)
hist(normal,50)
hist(power,50)
length(normal[normal>7.1])
length(power[power>7.1])

```


In general  the probability of exceeding the mean by number of sigmas (standard deviations) is

0 sigmas: 1 in 2 times

1 sigmas: 1 in 6.3 times

2 sigmas: 1 in 44 times

3 sigmas: 1 in 740 times

4 sigmas: 1 in 32,000 times

5 sigmas: 1 in 3,500,000 times

6 sigmas: 1 in 1,000,000,000 times

7 sigmas: 1 in 780,000,000,000 times

8 sigmas: 1 in 1,600,000,000,000,000 times

9 sigmas: 1 in 8,900,000,000,000,000,000 times

10 sigmas: 1 in 130,000,000,000,000,000,000,000 times

and, skipping a bit:

20 sigmas: 1 in 36,000,000,000,000,000,000,000,000,000,000,000
,000,000,000,000,000,000,000,000,000,000,000,0
00,000,000,000,000,000, 000 times

Soon, after about 22 sigmas, one hits a  “googol”, which is 1 with 100 zeros behind it

We find that the odds of encountering a millionaire
in Europe are as follows:

Richer than 1 million: 1 in 62.5

Richer than 2 million: 1 in 250

Richer than 4 million: 1 in 1,000

Richer than 8 million: 1 in 4,000

Richer than 16 million: 1 in 16,000

Richer than 32 million: 1 in 64,000

Richer than 320 million: 1 in 6,400,000


One of the most misunderstood aspects of a Gaussian is its fragility and vulnerability in the estimation of tail events. The odds of a 4 sigma move are twice that of a 4.15 sigma. The odds of a 20 sigma are a trillion times higher than those of a 21 sigma! It means that a small measurement error of the sigma will lead to a massive underestimation of the probability. We can be a trillion times wrong about some events

If wealth was distributed according to Gaussian

If Wealth Distribution was  following Gaussian Law

People with a net worth higher than €1 million: 1 in 63

Higher than €2 million: 1 in 127,000

Higher than €3 million: 1 in 14,000,000,000

Higher than €4 million: 1 in 886,000,000,000,000,000

Higher than €8 million:
1 in 16,000,000,000,000,000,000,000,000,000,000,000

Very egalitarian!


Other differences between Mediocristan vs Extremistan


Take a random sample of any two people from the U.S. population who jointly earn $1 million per annum. What is the most likely breakdown of their respective incomes? 

In Mediocristan, the most likely combination is half a million each.

In Extremistan, it would be between $50,000 and $950,000.

For  two authors to have  sold a total of a million copies of their books, the most likely  combination is 993,000 copies sold for one and 7,000 for the other. 

This is far more likely than that the books each sold 500,000 copies. 

For any large total, the breakdown will be more and more asymmetric.

Similarly for Height of two people if Total height of two people is fourteen feet, the most likely breakdown is seven feet each, not two feet and twelve feet, not even eight feet and six feet! 

Persons taller than eight feet are so rare that such a combination would be impossible



<!--chapter:end:chapters/Power_law.Rmd-->

# Prediction models from R library {#MLP}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

Believe it or not you are ready now to use pretty much any machine learning package from the extensive R library.  
In other words you can drive any car without knowing how the engine works. This you can find out by taking more advanced classes in Machine learning from computer science, statistics or machine learning departments.  

As per CRAN there are around 8,341 packages that are currently available.  Apart from CRAN, there are other repositories which contribute multiple packages. The simple straightforward syntax to install any of these machine learning packages is: **install.packages (“MLPackage”)**.

`Install Packages(‘MLPackage’)`

`Library(MLPackage)`

`MlPackage<-MLPackage(Formula, data=YOUR_TRAINING,…)`

`Predict(MLPackage, newdata=YOUR_TESTING…)`

`Error <- ……`

MLPackage can be rpart, Random Forests, naive Bayes, LDA, SVM, Neural Network and many others.

<!--
## Additional Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#ML12"> Prediction Loop </button> 
<div id="ML12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1n-7uFUUS40SwZO31rKxblplHtkAZMvQDXQzA3lY4PPU/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/Prediction_loop.Rmd-->

# Prediction Challenge {#pc}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

The following five snippets are designed to practice your prediction model building skills.  Each snippet uses one of the data puzzles from section \@ref(dp) - the party \@ref(party), sleep \@ref(sleep), voting \@ref(election), canvas \@ref(canvas) and finally the real data set - the titanic \@ref(titanic). 

We begin with an example created for a moody data set - snippet \@ref(onesixone).  We upload the training and testing data sets for the moody data puzzle. The task there is to predict the grade attribute. We begin with uploading the training and the testing data set (this one misses GRADE attribute). Students use the training data set to build the prediction model. In \@ref(onesixone), a simple rpart() model was built. Then this rpart() model is applied to the testing data set to create the  vector of predicted GRADE values. This constitutes the submission vector. Finally, the VERIFY function evaluates the error of the submission vector by comparing the predicted values with the real grades from the testing data set. It simulates what Kaggle does when students submit their submission file. The full testing data set is not available to students and is embedded inside the VERIFY() function. Students have access only to the training data set and to the testing data set WITHOUT the grade attribute. Just like in our prediction challenges on Kaggle. 

Snippets \@ref(onesixone)-\@ref(onesixfive) provide the training and testing data for  party, sleep, voting, canvass and the titanic data sets. Students can use these snippets to plug in their submission vectors, just as they submit them to Kaggle. 

**Example:**

Note: Student code in red.

```
# First lets import the rpart library
library(rpart)

# Import datasets (training and testing)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

moodyTest<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_test_students.csv')  [testing data set without target variable]
```

<span style="color: red;">
tree <- rpart(Grade ~ Major+Score+Seniority, data = train, method = "class",control=rpart.control(minbucket = 200)) </span>

<span style="color: red;">
submission<- predict(tree, test, type="class")
</span>

```
VERIFY(submission, moody_Test)
# will return the error of student Submission (accuracy or MSE) vs the hidden testing data set which will be part of VERIFY but hidden from the student. 
```


## Predict if the party will be fun? Boring? Just Ok? {#onesixone}

Description of the dataset: \@ref(party)

```{r,tut=TRUE,ex="challenge1",type="pre-exercise-code"}
verify <- function(data1) {
  data2 <- read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/party_test.csv')
  dataframe1<- data.frame(Party = data1)
  dataframe2 <- data2['Party']
  accuracy<-mean(dataframe1$Party == dataframe2$Party)
  return(accuracy)
}
```

```{r,tut=TRUE,ex="challenge1",type="sample-code",height=600}
#s import the rpart library
library(rpart)

# Import datasets (training and testing)
train<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/party_train.csv")
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/party_train_test.csv')

tree <- rpart(Party ~ ., data = train, method = "class")
tree
submission<- predict(tree, test, type="class")

verify(submission)
# will return the error of student Submission (accuracy or MSE) vs the hidden testing data set which will be part of VERIFY but hidden from the student. 



```




## Predict if sleep will be deep, shallow, little or not at all? 

Description of the dataset: \@ref(sleep)

```{r,tut=TRUE,ex="challenge2",type="pre-exercise-code"}
verify <- function(data1) {
  data2 <- read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/sleep_test.csv')
  dataframe1<- data.frame(Sleep = data1)
  dataframe2 <- data2['Sleep']
  accuracy<-mean(dataframe1$Sleep == dataframe2$Sleep)
  return(accuracy)
}
```

```{r,tut=TRUE,ex="challenge2",type="sample-code",height=600}
#s import the rpart library
library(rpart)

# Import datasets (training and testing)
train<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/sleep_train.csv")
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/sleep_train_test.csv')

tree <- rpart(Sleep ~ ., data = train, method = "class")
tree
submission<- predict(tree, test, type="class")

verify(submission)
# will return the error of student Submission (accuracy or MSE) vs the hidden testing data set which will be part of VERIFY but hidden from the student. 



```



## Predict if a local voter will vote for Anarchists, KnowNothings, or Royalists? 

Description of the dataset: \@ref(election)

```{r,tut=TRUE,ex="challenge3",type="pre-exercise-code"}
verify <- function(data1) {
  data2 <- read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/party_test.csv')
  dataframe1<- data.frame(Party = data1)
  dataframe2 <- data2['Party']
  accuracy<-mean(dataframe1$Party == dataframe2$Party)
  return(accuracy)
}
```

```{r,tut=TRUE,ex="challenge3",type="sample-code",height=600}
#s import the rpart library
library(rpart)

# Import datasets (training and testing)
train<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/party_train.csv")
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/party_train_test.csv')

tree <- rpart(Party ~ ., data = train, method = "class")
tree
submission<- predict(tree, test, type="class")

verify(submission)
# will return the error of student Submission (accuracy or MSE) vs the hidden testing data set which will be part of VERIFY but hidden from the student. 



```



## Predict student's grade 

Description of the dataset: \@ref(canvas)

```{r,tut=TRUE,ex="challenge4",type="pre-exercise-code"}
verify <- function(data1) {
  data2 <- read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Grade_test.csv')
  dataframe1<- data.frame(Grade = data1)
  dataframe2 <- data2['Grade']
  accuracy<-mean(dataframe1$Grade == dataframe2$Grade)
  return(accuracy)
}
```

```{r,tut=TRUE,ex="challenge4",type="sample-code",height=600}
#s import the rpart library
library(rpart)

# Import datasets (training and testing)
train<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Grade_train.csv")
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Grade_train_test.csv')

tree <- rpart(Grade ~ ., data = train, method = "class")
tree
submission<- predict(tree, test, type="class")

verify(submission)
# will return the error of student Submission (accuracy or MSE) vs the hidden testing data set which will be part of VERIFY but hidden from the student. 

```



## Predict Titanic passenger's survival {#onesixfive}

Description of the dataset: \@ref(titanic)

```{r,tut=TRUE,ex="challenge5",type="pre-exercise-code"}
verify <- function(data1) {
  data2 <- read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/titanic_test.csv')
  dataframe1<- data.frame(Survived = data1)
  dataframe2 <- data2['Survived']
  accuracy<-mean(dataframe1$Survived == dataframe2$Survived)
  return(accuracy)
}
```

```{r,tut=TRUE,ex="challenge5",type="sample-code",height=600}
#s import the rpart library
library(rpart)

# Import datasets (training and testing)
train<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/titanic_train.csv")
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/titanic_train_test.csv')

tree <- rpart(Survived ~ ., data = train, method = "class")
tree
submission<- predict(tree, test, type="class")

verify(submission)
# will return the error of student Submission (accuracy or MSE) vs the hidden testing data set which will be part of VERIFY but hidden from the student. 



```


<!--chapter:end:chapters/Prediction_challege_in_book.Rmd-->

# 🔖 How can data fool us? {#MAD}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction 

`How not to be fooled by data?`  In the description of this class we promised that data 101 will teach you this. Have we? I hope so. Please use our question roulette to test yourself. In this section we discuss Simpson Paradox, Prosecutorial and Ecological Fallacies.  Before we proceed with the paradoxes let us summarize what we have learned so far:

- `Beware of randomness` (Hypothesis testing, p-values, Multiple hypothesis testing)
- `Be an optimist` - use Bayesian reasoning. Remember about prior odds first!
- `Beware of extreme results` - Apply law of small numbers 
- `Remember we process information following availability` - we assign high frequency falsely to events which are just talked about very often 
- `Narrative fallacy - do not find false patterns` - “Stocks went down due to concerns about rising cost of living”

Here we will discuss the Simpson paradox as well as Prosecutorial and Ecological fallacies. 
Let us start with the Simpson paradox. Here is a  simple example of two basketball players, Aaron and Barry

The Table **16.1** shows Aaron’s and Barry’s  free throw point averages (FTP) over the 2021 and 2022 season respectively. Clearly for both seasons Barry  beats Aaron  in terms of FTP, in 2021 by 90% to 80% and in 2022 by 70% to 65%.  `Can Aaron  still beat Barry over both seasons - that is get a higher FTP over the sum of two seasons, 2021+2022?`  First answer which comes to mind  is absolutely not. `How can Aaaron beat Barry over 2021+2022 when   Barry beats him in each of the two seasons? `

**Table 16.1** 

<table>
<tr>
<td> season/player </td>
<td> Aaron</td>
<td> Barry</td>
</tr>


<tr>
<td> 2021 season</td>
<td> 80%</td>
<td> 90%</td>
</tr>

<tr>
<td> 2022 season</td>
<td> 65%</td>
<td> 70%</td>
</tr>

</table>


But table **16.2** explains that it is quite possible that  Aaron can beat Barry over 2021+ 2022. Indeed, since we do not know the absolute number of attempts at free throws, we can easily pick any number of attempts for each of them in any of the two seasons.

**Indeed** - here is the proof that Aaron can still beat Barry. If Barry made 100 attempts in 2021 and 20 attempts in 2022, while Barry made only 20 attempts in 2021 and 100 attempts in 2022, Aaron's overall FRP for both 2021+2022 will be higher than Barry’s. And this is Simposon’s paradox. 

**Table 16.2** 

<table>
<tr>
<td> season/player </td>
<td> Aaron</td>
<td> Barry</td>
</tr>


<tr>
<td> 2021 season</td>
<td> 80% out of 100</td>
<td> 90% out of 20</td>
</tr>

<tr>
<td> 2022 season</td>
<td> 65% out of 20</td>
<td> 70% out of 100</td>
</tr>

</table>

Indeed  Aaron’s FTP over 2021+2022 is  
\begin{equation}
\frac{80+13}{120} = \frac{93}{120}
\end{equation}
which is larger than Barry’s   

\begin{equation}
\frac{18+20}{120} = \frac{88}{120}
\end{equation}

More generally, trends in subsets of data may reverse themselves after aggregation. 

In fact we can have any number of seasons and have Barry  beat Aaron  in FTP in each and every season and Aaron still wins with better FTP over all seasons.   This is again simply because we do not know how many attempts each player made each season.  This applies to many real world situations such as graduate admissions for example (the famous Berkeley admission bias case). There  women may have a higher chance to be admitted than men in each single academic  department  and nevertheless, men  beat women in overall acceptance ratio.  This is again hard to comprehend at first but it is due to the fact that the absolute number of female and male applicants may be different for each department. 

**Is such reversal always possible? **

Let's look at the table below:

**Table 16.3** 

<table>
<tr>
<td> season/player </td>
<td> Aaron</td>
<td> Barry</td>
</tr>


<tr>
<td> 2021 season</td>
<td> 65%</td>
<td> 90%</td>
</tr>

<tr>
<td> 2022 season</td>
<td> 60%</td>
<td> 70%</td>
</tr>

</table>

`In this case the Simpson paradox is not possible. Why?`  Because Aaron’s highest FTP (65% in 2021 season is  lower than Barry’s lowest FTP in 2022). You can easily see that no matter what the absolute numbers of attempts in each season, Aaron can never beat Barry for 2021+2022. 
Thus the Simpson paradox was possible in this simple case only because Aaron’s highest FTP was higher than Barry’s lowest FTP. 

One also has to be careful with the Simpson paradox and not apply it to situations when both groups /individuals have the same absolute number of “attempts”.  For example, the Simpson paradox is not possible for students and their individual scores on homework's and exams.  If Barry  scores higher than  Aaron on each homework and on each  exam then Barry will always have a higher score overall than Aaron.  There is no  Simpsonian trend reversal. Every homework and every exam counts the same for all students. This is as if players always made the same number of free throw attempts.

### Ecological Paradox

Ecological paradox is kind of the reverse of the Simpson paradox. Let's assume that we consider net worth  for each member of groups A and B.   Even if average net worth of group A is higher than average net worth  of group B, it may be possible  that random individual member of the group B has higher net worth  than random individual member of the group A. Thus the order of aggregates may be reversed when we look at the level of individuals. 

For example as table **16.4** illustrates, the average net worth of Group A dominates the average net worth of Group B due to the presence of one wealthy individual.  However for 90% of pairs of individuals, group B members are more wealthy than Group A members.

**Table 16.3** 

<table>
<tr>
<td> Group A </td>
<td> Group B</td>
</tr>


<tr>
<td> $10,000,000</td>
<td> $210,000</td>
</tr>

<tr>
<td> $100,000</td>
<td> $290,000</td>
</tr>

<tr>
<td> $120,000</td>
<td> $220,000</td>
</tr>

<tr>
<td> $80,000</td>
<td> $210,000</td>
</tr>

<tr>
<td> $60,000</td>
<td> $270,000</td>
</tr>

<tr>
<td> $160,000</td>
<td> $210,000</td>
</tr>

<tr>
<td> $110,000</td>
<td> $240,000</td>
</tr>

<tr>
<td> $100,000</td>
<td> $210,000</td>
</tr>

<tr>
<td> $200,000</td>
<td> $240,000</td>
</tr>

</table>


For example, it is well known that Democrats win the richest states, while (until recently), the richest individuals vote republican. `How is it possible?`  Explanation is simple. Everyone’s vote counts the same and there are few very rich people. Very rich people may contribute more to the average wealth of the state (due to their extreme wealth), but there are just very few of them. 

**Do not be fooled  by aggregates!  **

Let's assume that a Democrat wins 70% of the vote and a Republican wins 30% of the vote in some state.    `Is it possible that, nevertheless, the  republican candidate wins all 19 counties out of 20 in the state?  ` 
This actually happens a lot when the population is heavily concentrated in a heavily  populated urban county which has the vast majority of voters living there. 



## Additional References


<button class="btn btn-primary" data-toggle="collapse" data-target="#Mt12">  How can data fool us? </button> 
<div id="Mt12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1vDCRffHzeSxka1sxQSUCnZvzaKIMmwPE7UO-5IefrM4/edit?usp=sharing" width="100%" height="500px"></embed>
</div>


<!--chapter:end:chapters/Mysteries_of_Aggregated_data.Rmd-->

# Boundless Analytics - Pre-discovery Tool {#dsaf}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

In this section we demonstrate application of Boundless Analytics - the tool developed by Tomasz Imielinski and his team at Rutgers (and supported by NSF subcontract of Center of Science of Information at Purdue University). Boundless Analytics calculates all significant bar graphs from the data set and allows to find data subsets (slices) which deviate the most from the whole data set in regard to frequency distribution of an attribute. Boundless performs an otherwise very tedious task of looking at all combinations of attribute value pairs to identify the “significant ones” - saving enormous amounts of work in preliminary exploration of data. 

We are using here the Minimarket  data puzzle \@ref(minimarket) describing customer transactions in the small chain of minimarkets in NJ. Data 101 students used Boundless Analytics to discover the most interesting subsets of this data set 


## Minimarket Data Set description

<a href="https://rutgers.zoom.us/rec/share/JIx-ZC0P5oXk5rnyOk_kZ8pF1XAC5rJnLGaIJGxxgLNmSzg6F-IRfUyNiQoKR9v0.MratfWX1STX2sYho">Zoom recording</a>

## Demo of Boundless Analytics 

<a href="https://rutgers.zoom.us/rec/share/bAr3uyPJA1vlGxbb6jVpvOHNFOuRgmj9TE4iSCcBn3_hS549xnqY9IePPsMgGE1i.F6P4omjuUebsQFl6">Zoom Recording</a>

## The Boundless Analytics web application 

**Boundless Analytics Interface:** http://209.97.156.178:8082/

(it is a soft login abc/abc will do)

**Objective: Nominate  the most interesting subset of  the Minimarket2022 data set**

Seems open ended, no? what is the "most interesting"?

- Chi-square value is a good measure. We explain it below.

- By swiping through possible plots (using Next), one can identify good candidates for the “interesting data subsets”) 

- These are plots where red and blue bars differ the most.  In other words we want to reject the null hypothesis of independence of red and blue distributions over the data slice and the complement of the data slice. The higher the chi-square is, the strongest is our rejection of independence of **red** and **blue** distributions. 

Therefore this task can be seen as chi-square hunt for the highest chi-square value  (use the  snippet **17.1** code after plugging in definition of a slice and the anchor attribute)

## Chi square hunt

```{r,tut=TRUE,height=600}

# Say, the Boundless analytics provides us with the slice:  Beer =='Lager' &  Day =='Weekend' and Snacks ='Crackers' and anchor attribute is Location.  You can calculate Chisq for this slice and the Location attribute to test if distribution of locations is affected if we limit ourselves only to transactions selling Lager and Crackers on Weekends?  

# The most interesting slice-anchor attribute combinations are the ones with the largest chisq test and lowest p-value. Nevertheless do not forget about multiple hypothesis correction - since we can on chi-square hunt here!

Minimarket<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

Minimarket$IN<-'Out_Slice'
Minimarket[Minimarket$Beer=='Lager' & Minimarket$Day=='Weekend' &  Minimarket$Snacks =='Crackers', ]$IN<-'In_Slice'
d<-table(Minimarket$Location, Minimarket$IN)
chisq.test(d)

```

<br> 

ATTACHED - the data set (same as on the Boundless Analytics interface)
<a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022-2.csv" download="HomeworkMarket2022-2.csv">HomeworkMarket2022-2.csv</a> 

**RESULTS: **

Here are two out of 250+ submissions.  The one with the highest chi-square of `600.15` is the slice  showing **weekend** buyers of **lager** in **New brunswick** but disproportionately more snacks (in particular Crackers). This was identified by nearly 20 students.

![](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling2/image.png)

<br> 

Here is another find by Eva Zhang showing disproportionately frequent **sales of Coca Cola** on **Weekdays** in **Princeton** for transactions which purchased Popcorn. The chi-square  value of this find is `205.31`, with  df=3.

![](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling2/image-2.png)

<!--chapter:end:chapters/22.Rmd-->

# 🔖 Best Works of 2022 {#b2022}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## DataBlog

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt12"> Ella Walmsley </button> 
<div id="bt12" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best1.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

<!--
## Prediction Challenge 1

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt15"> Upsham Naik </button> 

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt13"> Jeevanandan Ramasamy </button> 
<div id="bt15" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best2.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
<div id="bt13" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

## Prediction Challenge 2

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt14"> Jeevanandan Ramasamy </button> 
<div id="bt14" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best4.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

## Prediction Challenge 3

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt19"> Eva Zhang </button> 
<div id="bt19" class="collapse">
<embed src="https://docs.google.com/presentation/d/1ZKQa5-JGiQydxjOvLrE_SaX9eOnjP6jHdqJ4Dpyc-bw/edit?usp=sharing" width="100%" height="500px"></embed>
</div>
-->


## Boundless Analytics

<button class="btn btn-primary" data-toggle="collapse" data-target="#ba1"> Anastasiya Chuchkova </button> 
<button class="btn btn-primary" data-toggle="collapse" data-target="#ba2"> Shreya Tiwari </button> 
<button class="btn btn-primary" data-toggle="collapse" data-target="#ba3"> George Basta </button> 

<div id="ba1" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba1.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="ba2" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba2.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="ba3" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba3.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<br>

<button class="btn btn-primary" data-toggle="collapse" data-target="#ba5"> Paul Kotys </button> 
<button class="btn btn-primary" data-toggle="collapse" data-target="#ba6"> Selin Altimparmak </button> 

<div id="ba5" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba5.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="ba6" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba6.pptx&embedded=true" width="100%" height="500px"></embed>
</div>


<!--chapter:end:chapters/best_work_2022.Rmd-->

# Data League Leaderboard {#DLL}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Leaderboard'2022.csv")
# head(moody)
temp<-knitr::kable(
  moody, caption = 'Leaderboard 2022',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
**Honourable Mentions: **<br>
Upsham Naik, Joshua B. Sze, Kirtan Patel, Maria Xu, Devam Patel, Eva Zhang, Toshanraju Vysyaraju, Maanas Pimplikar, Jared Chiou, Nitya Narayanan, Shrish Vellore, Yousra Belgaid, Mitali Shroff, Michael Jucan, Jackie Hong, Arvin Sung, Eric Xuan, Eva Allred, Leah Ranavat, Nami Jain, Gautam Agarwal, Aditya Patil


<!--chapter:end:chapters/Leaderboard.Rmd-->

# example

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r}
library(DBI)
db <- dbConnect(
  MySQL(),
  host = "209.97.156.178",
  port =3307,
  user = "student", 
  password = "cs336", 
  dbname = "BarBeerDrinker"
)
knitr::opts_chunk$set(connection = db, max.print = 20)
tutorial::go_interactive(greedy = FALSE)
```

```{sql,connection = db}
select * from Penna limit 10;

```

<!--chapter:end:chapters/example.Rmd-->

