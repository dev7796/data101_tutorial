# Introduction {#intro}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->

<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->


<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->

<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(iris, 20), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->

<!-- You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015]. -->


The objective of this textbook is to provide you with the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. 

Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in a statistically convincing manner, perhaps in a colorful and visually appealing way.  Finally, you will be able to apply some basic machine learning methods to build, train and test prediction models.  All of this will be accomplished in a succinct and crisp way using a small subset of R instructions. 

It is an active textbook. It assumes no prior programming background.  We will teach you as little R as possible to achieve the goals of this book which are quite impressive. In fact you will be able to do a good chunk of work which data scientists do.  We will accomplish this goal through active snippets of executable code. These are examples of R code (**around 80 executable snippets of code**) embedded in  the textbook itself.  More importantly, you will be able to modify the code and execute the modified code without need to install any application on your machine.  This will allow you to understand the code in the book through “what if”  exploratory process. Thus, every code snippet is just an invitation to endless modifications. This is why we call this textbook - an active textbook.

Another unique aspect of this textbook is its reliance on data puzzles. These are synthetic data sets with embedded patterns and rules generated by our tool called **DataMaker**. We will present our data puzzles (dynamic list, may vary from year to year) in section 4 and follow by showing how to make plots of data.  Then we will proceed to freestyle data exploration.  This will allow us to learn  more about our data,  form the leads, and finally state our hypotheses.  We will follow up by an elementary introduction to hypothesis testing through a permutation test.  We will learn how to calculate p-values and how to use them to defend our findings against the  randomness trap.

We will use as few R functions as possible to achieve our goals. In fact we will demonstrate how using **less than ten R functions** we can perform quite sophisticated data exploration. In the appendix, we show many more useful commands of R which eventually you would have to use. However, our goal in this short textbook, is to present the shortest path to data analysis which will let you import the data, plot it, make some analysis yourself and use R-libraries to build machine learning models.  In this textbook and in this class we do not teach how to clean the data (data wrangling) and how to deal with a wide variety of data types. We also do not address complex data transformations such as multi-frame operations like merge function. We also do not explain how different machine learning methods work, we only show you how to use them. It is similar to teaching one how to drive a car without knowing how a car engine works. 

Sections **5.6** and **5.7** provide the lists of all concepts which we cover in our active textbook and all R functions which are needed.  Notice how small the set of R functions is.  It is important for programming novices to start small and also see how far this small set of functions can get you. 

Our **question roulette** allows self-testing on nearly 100 questions relevant to the material. Each question is answered, but students are encouraged first to answer questions themselves and only then follow it with checking the correct answer.











<!--chapter:end:chapters/intro.Rmd-->

# Setting Up R 

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

- **Important Instructions**
  - Installation of R is required before installing RStudio
    - "R” is a programming language, and,
    - “RStudio” is an Integrated Development Environment (IDE) which provides you a platform to code in R.

- __How to download and install R & RStudio?__

  - _Downloading and installing R._

    - For Windows Users.
      - Click on the link provided below or copy paste it on your favourite browser and go to the website.
          - [https://cran.r-project.org/bin/windows/base/](https://cran.r-project.org/bin/windows/base/)
      - Click on the link at top left where it says “Download R 4.0.3 for windows” or the latest at the time of your installation.
      - Open the downloaded file and follow the instructions as it is.

    - For MAC Users.
      - Click on the link provided below or copy paste it on your favourite browser and go to the website.
          - [https://cloud.r-project.org/bin/macosx/](https://cloud.r-project.org/bin/macosx/)
      - Under “Latest release”, click on “R-4.0.3.pkg” or the latest at the time of your installation.
      - Open the downloaded file and follow the instructions as it is.
      
 
  - _Downloading and installing RStudio._
  
    - For Windows Users.
      - Click on the link below or copy paste it in your favourite browser.
          - [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/)
      - Scroll down almost till the end of the web page until you find a section named “All Installers”.
      - Click on the download link beside “Windows 10/8/7” to download the windows version of RStudio.
      - Install RStudio by clicking on the downloaded file and following the instructions as it is.

    - For MAC Users.
      - Click on the link below or copy paste it in your favourite browser.
          - [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/)
      - Scroll down almost till the end of the web page until you find a section named “All Installers”.
      - Click on the link beside “macOS 10.13+” to start your download the MAC version of RStudio.
      - Install RStudio by clicking on the downloaded file and following the instructions as it is.


---

## Create New Project {#setting}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->

<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->


<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->

<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(iris, 20), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->

<!-- You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015]. -->

After installing R studio successfully the first step is to create a project R studio. 

- Step 1: Go to **File -> New Project**

![New Project](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/1_text.png)

- Step 2: Select **New Directory**

![New Directory](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/2_text.png)

- Step 3: Select **New Project**

![New Project](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/3_text.png)

- Step 4: Give your preferred directory name like **"Data101_Assignmnets"**

![Directory Name](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/4_text.png)

- Step 5: Click on Create Project and finally the R studio should look like

![Rstudio](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/4.5_text.png)


---

## How to upload a data set?
<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive()

```

 - To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon.

- There are two options while accessing the dataset from your local machine:
  1. To avoid giving long directory paths for accessing the dataset, one should use the command **getwd()** to get the current working directory and store the dataset in the same directory. 
  
![Getwd](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/5_text.png)

- To access the dataset stored in the same directory one can use the following: **read.csv("MOODY_DATA.csv")**.

![Store the moody dataset in the same directory](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/6_text.png)

  2. One can also store the dataset at a different location and can access it using the following command: (Suppose the dataset is stored inside the folder Data101_Tutorials on the desktop)
  
    - For Windows Users.
      - Example: read.csv("C:/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv")

    - For MAC Users.
      - Example: read.csv("/Users/Desktop/Data101_Tutorials/MOODY_DATA.csv")
      

**Note: **
The directory path given here is the current working directory hosted on *Github* where the dataset has been stored.
```{r, tut=TRUE}

# Read in the data
df <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

# Print out `df`
head(df)
```

---

## Saving your work

- To save your work go to **File -> Save**. It will ask you to give a name for your **.R file** and then click on Save.

![Save](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/7_text.png)

- After making modifications to your saved file, you will need to save the file again. 
If the name of the file on the top is in <span style="color: red;"> Red Color </span> indicates that the file have **unsaved** changes.

![Unsaved File](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/8_text.png)


- Go to **File -> Save** to save your .R file again. After saving the file the color of the file name i.e. **HW1.R** will again change back to **black**.

![Saved File](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/9_text.png)

**Note: ** You can create multiple files inside the same project such as for your each homework assignments

## General R References

https://www.w3schools.com/r/ <br>
https://cran.r-project.org/doc/contrib/Short-refcard.pdf <br>
https://www.amazon.com/Statistics-Engineers-Scientists-William-Navidi/dp/0073376337/ref=pd_lpo_3?pd_rd_i=0073376337&psc=1 <br>
https://data101.cs.rutgers.edu/laboratory/

## Textbook Concepts

- Hypothesis testing: \@ref(ztest)

- Difference of means hypothesis testing: \@ref(ztest)

- Null Hypothesis: \@ref(ztest)

- Alternative Hypothesis: \@ref(ztest)

- z-value: \@ref(ztest)

- critical value: \@ref(ztest)

- significance level: \@ref(ztest)

- p-value: \@ref(ztest)

- Bonferroni correction: \@ref(Mtest)

- Chi square test: \@ref(chitest)

- Independence: \@ref(chitest)

- Multiple Hypothesis testing: \@ref(Mtest)

- False Discovery Proportion: \@ref(Mtest)

- Contingency Matrix: \@ref(chitest)

- Bayesian Reasoning: \@ref(br)

- Prior odds: \@ref(br)

- Posterior odds: \@ref(br)

- Likelihood ratio: \@ref(br)

- False positive: \@ref(br)

- True positive: \@ref(br)

- Crossvalidation: \@ref(crossvalidation)

- Decision trees: \@ref(prpart)

- Linear regression: \@ref(lr)

- Recursive partitioning: \@ref(lr)

- MSE: \@ref(lr)

- Prediction accuracy: \@ref(lr)

- Training: \@ref(lr)

- Testing: \@ref(lr)


## R functions used in this class

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

- **Elementary instructions: ** c() \@ref(vector),  mean() \@ref(mean),   nrow() \@ref(nrow), rep(), sd() \@ref(sd), cut() \@ref(cut)

- **Plots: **  plot() \@ref(scartterplot), barplot() \@ref(barplot), boxplot() \@ref(boxplot)  mosaicplot() \@ref(mosaicplot)

- **Data Transformations: ** subset() \@ref(subset), tapply() \@ref(tapply),  table() \@ref(table), aggregate() 

- **Library functions: ** chisq.test() \@ref(chitest), pnorm() \@ref(permutaion), Permutation() \@ref(permutaion), rpart() \@ref(prpart), predict() \@ref(rpartpredict), lm() \@ref(lm), crossvalidation() \@ref(crossvalidation)

- **Parameters of rpart: ** minsplit \@ref(rpartcontrol), minbucket \@ref(rpartcontrol), cp \@ref(rpartcontrol)


<!--
## Data sets

### Moody

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv" download="moody2022_new.csv">moody2022_new.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

summary(moody)
```

### Movies

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv" download="Movies2022F-4.csv">Movies2022F-4.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

summary(movies)
```

### Traffic

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv" download="Traffic2022.csv">Traffic2022.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
traffic<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')
# head(moody)
temp<-knitr::kable(
  traffic[sample(1:nrow(traffic),5), ], caption = 'Snippet of Traffic Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
traffic<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')

summary(traffic)
```

### Hindex

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv" download="Hindex.csv">Hindex.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
hindex<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv") #web load
# head(moody)
temp<-knitr::kable(
  hindex[sample(1:nrow(hindex),5), ], caption = 'Snippet of Hindex Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
hindex <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv")

summary(hindex)
```


### Prediction 1 Dataset

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv" download="M2022train.csv">M2022train.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Snippet of Moody Predicition 1 dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv")

summary(moody)
```

### Midterm, Project and Final Exam distribution in Prof. Moody class 

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv" download="MoodyNUM.csv">MoodyNUM.csv</a>

**Assumptions:** Midterm, Project and Final Exam are all out of 100

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Midterm, Project and Final Exam distribution in Prof. Moody class',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")

summary(moody)
```


### Minimarket

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv" download="HomeworkMarket2022.csv">HomeworkMarket2022.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")
# head(moody)
temp<-knitr::kable(
  moody[sample(1:nrow(moody),5), ], caption = 'Minimarket dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
```{r,tut=TRUE,height=600}
minimarket<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

summary(minimarket)
```

-->





<!--chapter:end:chapters/Setting_up_R.Rmd-->

# 🔖 Data puzzles

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

```{r setup, include=FALSE}
library(shiny)
library(shinythemes)
library(shinyjs)
library(shinymanager)
library(googlesheets4)
gs4_deauth()
```

Data Puzzles are synthetically generated datasets with some embedded patterns.  Patterns have various forms from relationships between attributes to rules of the form “if condition then value” between specific attribute-value pairs.  These patterns are stochastic and embedded in datasets using **DataMaker** -  our  Data Puzzle Generation Tool.

We use data puzzles extensively in the class assignments.  These range from data exploration and plotting through  hypothesis testing to prediction and machine learning.  After the assignment is completed we reveal the data secrets - the patterns which were embedded by DataMaker.  Students do not have to find exactly the embedded patterns, often they find related patterns which makes the “game” even more fun. 

 In the following  we provide the list of data puzzles along with the underlying data sets.  Using DataMaker we change the patterns and even data sets from academic year to academic year..  We can also provide data puzzles of different levels of difficulty from the  one star (easy) to five star (most difficult)  ones. 



## Strange grading methods of Professor Moody Data Puzzle

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv" download="moody2022_new.csv">moody2022_new.csv</a>

How to get a good grade in Professor Moody’s class?  

Professor Moody does not give final grades just on the basis of your total score alone. Our data shows that two students with the same total score may get widely varying final grades. Can you believe that you can even fail his class with a score as high as 82%? This is outrageous, isn’t it? 

DataMaker has generated  thousands of tuples  which in addition to the total score and final grade also store bizarre information about student behaviors in the class - do they often doze off? Does a student text a lot?  Does s/he ask a lot of questions?    Does it help if you ask a lot of questions? Does it hurt if you doze off a lot?

**Comment:** There is a series of Professor Moody’s puzzles which we have used over the years. We have used different attributes including student’s major, , seniority, class participation etc. 



```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv") #web load
# head(moody)
temp<-knitr::kable(
  head(moody, 5), caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  head(moody, 5), caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>

Great job!! You have made it this far. You are now familiar with the moody dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check1). 

### Moody Data Quiz

<a href = "https://ds1778.shinyapps.io/data_roulette_1/" target = "blank"> Test Yourself </a>

### Check yourself {#check1}
```{r,tut=TRUE,height=600}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")

summary(moody)
```

<!--
### Secrets Revealed- Patterns in Professor Moody's data? 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MDP12">My Patterns</button>
<div id="MDP12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/HW3-Moody-My-patterns.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
<br>
Many student solutions falsely attribute higher grades to higher values of participation attribute.. 

This is a classic example of a hidden variable described in the reference attached below. The truth is that participation attribute value impacts the score attribute value. Generally, the higher the participation in Moody’s class, the higher the score. But it is the score attribute which has a direct impact on the grade. Thus, it is the score which is the real “hidden variable” impacting the final grade.

Thus, the score already reflects participation. Professor Moody seemed to look only at texting and dozing off attributes in grade determination (see the power points above with the explanation) 

Compare with examples of hidden variables in the following reference about correlation and causation. 

https://www.stewartmath.com/precalc_7e_dp/precalc_7e_dp6.html 

-->
<!--
### Best Student's Submissions 2022

<button class="btn btn-primary" data-toggle="collapse" data-target="#PPT2021">Lauretta Martin</button>
<button class="btn btn-primary" data-toggle="collapse" data-target="#PPT2020">Sanjaya Budhathoki</button>
<button class="btn btn-primary" data-toggle="collapse" data-target="#PPT2019">Sandhya Senthilkumar</button>

<div id="PPT2021" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/LM-HW3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="PPT2020" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/SB-HW3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="PPT2019" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/HW3-SS.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
-->


## Magic Coin: Heads or Tails? Data Puzzle

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Tosses2022.csv" download="Tosses2022.csv">Tosses2022.csv</a>

Your goal is to discover secrets of a magic coin. Ordinary coins when tossed a large number of times  result equally often in Heads and in Tails.  The magic coin  though is not an ordinary coin.  Your job is to discover when the magic coin is most biased one way or another.  The data consists of thousands of tosses of the magic coin including where the toss took place, who tossed it, who reported the result as well as the time of the day.

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
toss<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Tosses2022.csv") #web load
# head(moody)
temp<-knitr::kable(
  toss[sample(1:nrow(toss),5), ], caption = 'Snippet of Magic Coin Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  toss[sample(1:nrow(toss),5), ], caption = 'Snippet of Magic Coin Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

You are now familiar with the Magic Coin dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check2). 

### Magic Coin Data Quiz

<a href = "https://ds1778.shinyapps.io/data_roulette_2/" target = "blank"> Test Yourself </a>

### Check yourself {#check2}
```{r,tut=TRUE,height=600}
toss<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Tosses2022.csv")

summary(toss)
```

## How to predict a good party?  Data puzzle

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv" download="Partyb.csv">Partyb.csv</a>

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv") #web load
# head(moody)
temp<-knitr::kable(
  party[sample(1:nrow(party),5), ], caption = 'Snippet of Party Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  party[sample(1:nrow(party),5), ], caption = 'Snippet of Party Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>

DataMaker has generated data about thousands of parties, some fun parties, others which were OK or simply boring.  Your goal is to discover secrets of a fun party. Is it music? Dancing? Does the host matter? Or who was present at a party? Maybe who was NOT present at the party?  All this data is stored in this data puzzle.

You are now familiar with the Party dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check3). 

### Party Data Quiz

<a href = "https://ds1778.shinyapps.io/data_roulette_3/" target = "blank"> Test Yourself </a>

### Check yourself {#check3}
```{r,tut=TRUE,height=600}
party<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Partyb.csv")

summary(party)
```

## When election is truly local - data puzzle

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv" download="Voting1.csv">Voting1.csv</a>

In local elections in some small towns, candidates of three local parties: Royalists, KnowNothings and Anarchists are running for the office of the mayor. DataMaker has generated a survey of thousands of town residents and their political sympathies. Data of course can not be more local, leaving global concerns such as inflation or global warming to national or state office candidates.  

Here, the electorate cares  about  issues such as “should we allow leaflowers” (all, only electric, none?), what about CBD stores in town (none, just one, no restrictions), How about liquor (should the town be dry? Or hard liqueurs only). Speed limits? (none, 10mph etc) or even more extreme - the whole town being car-free, streets open only to bicycles and pedestrians?

Can we develop the profiles of voters for each of the parties? What does the  anarchist electorate  care about? Which party is leading among young people who do not want any speed limits in town?


```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
voting<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv") #web load
# head(moody)
temp<-knitr::kable(
  voting[sample(1:nrow(voting),5), ], caption = 'Snippet of Voting Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  voting[sample(1:nrow(voting),5), ], caption = 'Snippet of Voting Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

You are now familiar with the Election dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check4). 

### Election Data Quiz

<a href = "https://textbook.shinyapps.io/data_roulette_4/" target = "blank"> Test Yourself </a>

### Check yourself {#check4}
```{r,tut=TRUE,height=600}
vote<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Voting1.csv")

summary(vote)
```

## Secrets of good sleep Data Puzzle 

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv" download="SleepPrediction2.csv">SleepPrediction2.csv</a>

Who wouldn’t want to know the secrets of good sleep?  DataMaker has created a data set which may help to find these secrets. We store the number of exercise calories burnt during the day, the amount of wimpy tea a person has drunk (in ounces), hours spent on the computer and the quality of the preceding night’s sleep. 

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Sleep Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Sleep Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<br>

You are now familiar with the Sleep dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check5). 

### Sleep Data Quiz

<a href = "https://textbook.shinyapps.io/data_roulette_5/" target = "blank"> Test Yourself </a>

### Check yourself {#check5}

```{r,tut=TRUE,height=600}
sleep<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/SleepPrediction2.csv")

summary(sleep)
```

## Let’s go to the movies:  Data Puzzle 

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoviesDataMaker2.csv" download="MoviesDataMaker2.csv">MoviesDataMaker2.csv</a>

Using DataMaker  we have started with the imdb data set from Kaggle and embedded some patterns in it.  The original data set contains data about 12,800+ movies. We have expanded this data set by DataMaker’s opinions. Yes, only DataMaker can have an opinion on each of 12,800 movies! Can you predict which  movies does DataMaker love and which movies bore him so much that she quit?   What movies DataMaker passionately hates (hmm is DataMaker even passionate about anything at all?). 

When does DataMaker agree with the imdb score?

Can one predict an imdb score on the basis of a combination of DataMaker opinion (sort of super critic) and other attributes?


```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoviesDataMaker2.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Movies Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>

You are now familiar with the Movies dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check6). 

### Movies Data Quiz

<a href = "https://textbook.shinyapps.io/data_roulette_6/" target = "blank"> Test Yourself </a>

### Check yourself {#check6}

```{r,tut=TRUE,height=600}
movie<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoviesDataMaker2.csv")

summary(movie)
```

<!--
### Secrets Revealed- Patterns in Movies data? 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MV12">Secrets Revealed</button>
<div id="MV12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/profhw4.pdf&embedded=true" width="100%" height="500px"></embed>
</div>



### Best Student's Submissions 2022

<button class="btn btn-primary" data-toggle="collapse" data-target="#hw41">Joshua Sze</button>
<button class="btn btn-primary" data-toggle="collapse" data-target="#hw42">Andrew Fasano</button>

<div id="hw41" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/student1hw4.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="hw42" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/studen2hw4.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
-->

## When canvas goes wild data puzzle 

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv" download="Canvas1.csv">Canvas1.csv</a>

You are all familiar with Canvas, right?  This is where you look to see your grades for each assignment and exam.  This is where you see the scores.  However it seems that Canvas went a bit wild and unfair in this data set.One can still fail the class with the score of 82 (sounds familiar, yes, Professor Moody would do it, but Canvas?

How can one get a lower grade  with a higher score?

Yes, Canvas was instructed by someone and your goal is to discover the grading method. How to get an A, how to pass?  We know who that someone is… it is **DataMaker** of course. 

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Canvas Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Canvas Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>

You are now familiar with the Canvas dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check7). 

### Canvas Data Quiz

<a href = "https://textbook.shinyapps.io/data_roulette_7/" target = "blank"> Test Yourself </a>

### Check yourself {#check7}

```{r,tut=TRUE,height=600}
canvas<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Canvas1.csv")

summary(canvas)
```



## Very local minimarket data puzzle 

Download: <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv" download="Minimarket.csv">HomeworkMarket2022.csv</a>

What items sell together? 

A small local minimarket chain (think Wawa at its early days) has a few locations in New Jersey and it sells beer, snacks, sweets. DataMaker provided the data set of several thousand of transactions in the minimarket storing what items were purchased, when they were purchased (weekday or weekend) at which location.  

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv") #web load
# head(moody)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Minimarket Dataset',
  booktabs = TRUE
)
temp<-knitr::kable(
  movies[sample(1:nrow(movies),5), ], caption = 'Snippet of Minimarket Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>

You are now familiar with the MiniMarket dataset and it's time to give your understanding a test. Please click the link below to get to the quiz and come back here to cross-check your answer in the snippet \@ref(check8). 

### MiniMarket Data Quiz

<a href = "https://ds1778.shinyapps.io/data_roulette_8/" target = "blank"> Test Yourself </a>

### Check yourself {#check8}

```{r,tut=TRUE,height=600}
minimarket<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

summary(minimarket)
```

<!--
### What were the secret associations  between  items in the minimarket?

<button class="btn btn-primary" data-toggle="collapse" data-target="#MV12">Secrets Revealed</button>
<div id="MV12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ecrets-revealed-prediction-minimarket.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

## Virus Data Puzzle

Whose computer  is in most danger of being infected with a virus?  Let's assume that obvious protections have been secured: antivirus software is installed and an uncrackable password is in place. 

Anything else?  But how about your personal characteristics - are you an optimist or a pessimist?  Do you have lots of facebook friends?  Is your instagram followed?  Massively?  Do you spend all your time online? Are you an outdoorsy type ? Do you have a dog? 

## Good mood Data Puzzle

What are the secrets of DataMaker’s good mood?  Now we fully admit that DataMaker is moody!  Data is not surprisingly very nerdy - amount of free disk space, number of processes, CPU utilization, number of active connections.

## Airbnb data puzzle

## Predicting grades in Professor Moody's class

### How did I cook the Professor Moody Prediction challenge data? 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MV13">Secrets Revealed</button>
<div id="MV13" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/Secrets-revealed-prediction-challenge.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
-->

## Addiotional Reference 

<button class="btn btn-primary" data-toggle="collapse" data-target="#p12"> Prediction - Free Style </button> 
<div id="p12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1pA0bzMGr_Tu2CXsgtT9Ks5apHqxWh8l1XXBVvwWr6zc/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/FreeStyle.Rmd-->

# 🔖 Plots {#plots}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

When you import your data to R studio one of the first things you do is plot. Data visualization is a key components of data analysis. Before we talk about plots, we introduce some very basis data structures in R: vectors, data frames and tables.  These are introduced below in the form of code snippets that you can run and modify. 

**Then we are ready to plot!**

## Vector {#vector}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive( greedy = FALSE)

```
- A vector is simply a list of items that are of the same type.

<!-- To combine the list of items to a vector, use the **c()** function and separate the items by a comma.
- All of the items inside a vector to are of the same type like numerical or categorical. -->

### Snippet 1

Lets look at example of creating a vector:

```{r,tut=TRUE,height=300}

#Lets create 3 vectors with title, author and year.
color <- c('Red','Blue','Yellow','Green')

#Lets look at how the created vectors look.
color
```

### Snippet 2

Create a vector with numerical values in a sequence, use the **:** operator:

```{r,tut=TRUE,height=300}

#Lets create a vectors with numerical sequence.
year <- 2018:2022

#Lets look at how the created vectors look.
year

```
<!--
<br />

- To find out how many items a vector has, use the **length()** function:

```{r,tut=TRUE,height=300}

#Lets create a vectors with categorical values
author <- c('Foreman', 'John', 'Said')

# You can access the vector items by referring to its index number inside brackets []. The first item has index 1, the second item has index 2, and so on:
author[1]

#Lets look at the size of a vectors.
length(author)

```

<br />

- To sort items in a vector alphabetically or numerically, use the **sort()** function and to change the value of a specific item, refer to the **index number**:

```{r,tut=TRUE,height=300}

#Lets create a vectors with categorical values
title <- c('Data Smart','Orientalism','False Impressions','Making Software')

#sorting titles
sort(title)

# change the title 'Data Smart' to 'Smart Data'
title[1] <- "Smart Data"

#Lets look at how the updated vectors look.
title 
```

-->

---

## Data Frames

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive( greedy = FALSE)

```
- Data Frames are data displayed in a format as a table.
<!--
- Data Frames can have different types of data inside it. While the first column can be character, the second and third can be numeric or logical. 

- Following are the characteristics of a data frame.
  - The column names should be non-empty.
  - The row names should be unique.
  - The data stored in a data frame can be of numeric, factor or character type.
  - Each column should contain same number of data items.

Use the **data.frame()** function to create a data frame:-->

### Snippet 1

Populating the dataframe:

```{r,tut=TRUE,height=300}

# Load the dataset into the moody variable
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

# Now lets view the dataframe moody with just 5-6 tuples
head(moody)
```

### Snippet 2 

Get the summary of the dataframe:

```{r,tut=TRUE,height=300}

# Load the dataset into the moody variable
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

# Use the summary() function to summarize the data from a Data Frame:
summary(moody)
```
<!--
- Just like vectors, you can access specific data **(Slicing)** in dataframes using brackets. 
- But now, instead of just using one indexing vector, we use two indexing vectors: one for the rows and one for the columns. -->

### Snippet 3 

Use the notation **data[rows, columns]**, which selects the subsets of rows and columns:

```{r,tut=TRUE,height=500}

# Load the dataset into the moody variable
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

# Return row 1
moody[1, ]

# Return column 5
moody[, 5]

# Rows 1:5 and column 2
moody[1:5, 2]

# Give me rows 1-3 and columns 2 and 4 of moody
moody[1:3, c(2:4)]
```

---

## Table {#table}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive( greedy = FALSE)

```

- **table()** displays frequency distribution of its arguments. 

<!--
- Tables (i.e. frequency distribution table) can be created using **table()** along with some of its variations. 
- To use **table()**, simply add in the variables you want to tabulate separated by a comma. 
- You must reference the variable using **dataset$variable**. 
- By default, missing values are excluded from the counts; if you want a count for these missing values you must specify the argument **useNA=“ifany”** or **useNA=“always”**. -->
### Snippet 1

The below examples show how to use this function:

```{r, tut=TRUE,height=400}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load

#lets make a table for the grades of students and counts of students for each Grade. 
grades <- table(moody$grade)

#lets see the above frequency distrbuted tables
grades
```

### Code Review
#### What would R say?

```{r, tut=TRUE,height=400}
moody <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

table(moody[moody$questions!='always',]$grade)
#What will R say?

# A. error
# B. distribution of grades for students who always ask questions
# C. distribution of grades for students who do not always ask questions 
```

#### What would R say?

```{r, tut=TRUE,height=400}
moody <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

table(moody[moody$questions==('always','never'),]$grade)
#What will R say?

# A. error.
# B. distribution of grades for students who always or never ask questions.  
# C. distribution of grades for students who do not ask questions always or never. 
```

<br />

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
# head(moody)

temp<-knitr::kable(
  head(moody, 5), caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>

**Now we are ready to plot.**

We will introduce several basic plots such as scatter plot, bar plot, boxplot and mosaic plot.  How do we know which plot to apply?  It depends on whether the variables to be plotted are categorical or  numerical.   Below we show a simple table which can serve as a guide which plot to use depending on types of variables to be plotted. 

<table>
<tr>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
NUM x NUM 
</td>
<td>
scatter plot
</td>
</tr>

<tr>
<td>
CAT x CAT
</td>
<td>
mosaic plot
</td>
</tr>

<tr>
<td>
CAT x NUM 
</td>
<td>
box plot
</td>
</tr>

<tr>
<td> NUM
</td>
<td> box plot, histogram
</td>
</tr>

<tr>
<td> CAT
</td>
<td> bargraph
</td>
</tr>
</table>


<!-- --- -->

<!-- ### Topics visited in this sub-chapter -->

<!-- * Scatter Plot -->
<!-- * Barplot -->
<!-- * Boxplot -->
<!-- * Mosaic Plot -->

## Scatter Plot {#scartterplot}

- Scatter Plot are used to plot two numerical variables.
- Hence it is used when both the labels are numerical values.


Lets look at example of scatter plot using Moody.

```{r,tut=TRUE,height=700}
# Let's look at a 2 attribute scatter plot.
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
plot(moody$participation,moody$score,ylab="score",xlab="participation",main=" Participation vs Score",col="red")


```

---

## Bar Plot {#barplot}

- A bar plot are used to plot a categorical variable. 
- This rectangle height is proportional to the value of the variable in the vector.
<!--
- Barplots are also used to graphically represent the distribution of a categorical variable, after converting the categorical vector into a table(i.e. frequency distribution table)
- In a bar plot, you can also give different colors to each bar. -->



```{r, tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#lets make a table for the grades of students and counts of students for each Grade. 

t<-table(moody$grade)

#once we have the table lets create a barplot for it.

barplot(t,xlab="Grade",ylab="Number of Students",col=colors, 
        main="Barplot for student grade distribution",border="black")
```


---

##  Box Plot {#boxplot}

- A boxplot is used to display a numerical variable.
- A boxplot shows the distribution of data in a dataset. 

![Boxplot](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/plots/boxplot1.png)
<br />

- A boxplot shows the following things:
  - Minimum
  - Maximum
  - Median
  - First quartile
  - Third quartile
  - Outliers
  
<!--
- You can create a single boxplot using just a vector or a multiple boxplot using a formula.
- When you write a formula, you should use the Tilde (~) operator. This column name on the left side of this operator goes on the y axis and the column name on the right side of this operator goes on the x axis.-->



```{r,tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#Suppose you want to find the distribution of students score per Grade. We use box plot for getting that. 
boxplot(score~grade,data=moody,xlab="Grade",ylab="Score", main="Boxplot of grade vs score",col=colors,border="black")

# the circles represent outliers.
```


<!-- ## 4. Histogram -->

<!-- Refer Slide 15. -->

<!-- ```{r} -->

<!-- #Suppose you want to find the frequecy/distribution of cars with mileage in particular range. We use histogram for this.  -->

<!-- hist(automobile$`city-mpg`,xlim = c(0,100),xlab = 'milage', main = "Histogram of Car milage",col=colors,border="black") -->

<!-- # You can Change column range using breaks. -->

<!-- ``` -->


<!-- For more detail,reference and example refer Slides -->



---
##  Mosaic Plot {#mosaicplot}

- Mosaic plot is used to visualize two categorical variables.

<!--
- Mosaic plot is a graphical method for visualizing data from two or more qualitative variables.
- The length of the rectangles in the mosaic plot represents the frequency of that particular value.
- The width and length of the mosaic plot can be used to interpret the frequencies of the elements.
- For example, if you want to plot the number of individuals per letter grade using a smartphone, you want to look at a mosaic plot. -->


```{r,tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#suppose you want to find numbers of students with a particular grade based on their texting habits. Use Mosiac-plot.

mosaicplot(moody$grade~moody$texting,xlab = 'Grade',ylab = 'Texting habit', main = "Mosiac of grade vs texing habit in class",col=colors,border="black")


```

## Additional References

<button class="btn btn-primary" data-toggle="collapse" data-target="#plots12">Plots</button>
<div id="plots12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1L7ml_mwV7ms3eZ2qbznGgNdarhTfrSP6PWq-RxNUgQM/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

https://www.datamentor.io/r-programming/plot-function/

<!--
```
{r child="./chapters/datatransformation.Rmd"}
```
 
 -->

<!--chapter:end:chapters/plots.Rmd-->

# 🔖 Data Transformation

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

Very often data is transformed before it is visualized. In this section we review basic transformation techniques. Before we do this, we review a few basic functions which are going to be very useful when transforming data. 

## Basic Functions

### mean() {#mean}

- **mean()** function is used to find the average of values in a numerical vector.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#Lets look at the mean of score column.
mean(moody$score)
```


### length()

- **length()** function is used to get the number of elements in any vector

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#Lets look at the length of the grade column 
length(moody$grade)
```

### max()

- **max()** function is used to get the maximum value in a numerical vector.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#lets look at the maximum value of the score in the score column
max(moody$score)
```

### min()

- **min()** function is used to get the minimum value in a numerical vector

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#Lets look at the minimum value of score in the score column.
min(moody$score)

```

### sd() {#sd}

- **sd()** function is used to find the standard deviation of numerical vector

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")

#Lets look at the standard deviation of score column
sd(moody$score)
```

---

Now we are ready to introduce basic data transformation techniques such as slicing and dicing.  Slicing, otherwise known as **subsetting**, allows the selection of data frame subsets. These subsets are defined by boolean conditions built from Attribute op value pairs where op is one of the arithmetic operators such as =, !=, < etc.  For example (Score >70)& (Grade ==’A’) refers to a subset of a data frame describing students who scored more than 70 points and got an A.  

Dicing refers to eliminating some of the attributes from a data frame  - it is vertical slicing - which results in a more “narrow” frame.
Finally we can also expand our data frame with new, so called derived, attributes. This is a very useful operation in data analysis since it allows so-called **“feature engineering”**. These new user-defined features can lead to totally new insights into the data. 

## Subset {#subset}

### Snippet 1- example of subset function {#nrow}

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
#Subset of rows
moody_never_smartphone<-subset(moody,ON_SMARTPHONE=="never")
nrow(moody)
nrow(moody_never_smartphone)

```

### Snippet 2- example of  subset  function

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
#Subset of rows
moody1<-subset(moody,ON_SMARTPHONE=="never")
# You can see only student never on smartphone are in the subset.
table(moody1$ON_SMARTPHONE) 

```

### Snippet 3- subset as subframe 

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
#Alternate way to subset.
moody2<-moody[moody$ON_SMARTPHONE=="never", ]
# You can see a similar table as above.
table(moody2$ON_SMARTPHONE) 

```

### Snippet 4- subsetting columns

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
colnames(moody)
#subset of columns
moody3<-subset(moody, select = -c(1))
ncol(moody3)
# You can see the number of columns has been reduced by 1, due to sub-setting without column 1
ncol(moody3)

```

### Snippet 5- sub-setting rows and columns

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
#Subset of Rows and Columns
moody1<-subset(moody, select = c(2:4), ON_SMARTPHONE == "never")
colnames(moody1)
#Notice that only 3 columns are remaining
dim(moody1)

```

### Code Review
#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")
moody[moody$SCORE>=90,3]
# What will R say?


# A. Get subset of all columns which contains students who scored more than equal to 90
# B. error
# C. get all score values which are more than equal to 90
# D. get subset of only the grades of students with score greater than equal to 90


```

#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

moody[moody$SCORE>=80.0 & moody$GRADE =='B',] 
# What will R say?

# A. subset of moody data frame who got B grade.
# B. error.
# C. subset of moody data frame with score greater than 80.
# D. subset of moody data frame with score more than 80 and got B grade.


```

---

One of the most important R instructions is **tapply**. It allows parallel execution of an aggregate function for different values of a categorical variable.

## tapply {#tapply}

- **tapply()** computes a measure (mean, median, min, max, etc..) or a function for each factor variable in a vector. It is a very useful function that lets you create a subset of a vector and then apply some functions to each of the subset.
- tapply(numerical, categorical, aggregagte function)

### Snippet 1-  Example of tapply followed by barplot

```{r,height=700}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")


# To apply tapply() on SCORE factored on ON_SMARTPHONE

moody1<-tapply(moody$SCORE,moody$ON_SMARTPHONE,mean)
moody1 # We can see it calculated mean value of the score by students with respect to their use of phone in class.

barplot(moody1,col = "cyan",xlab = "Labels", ylab = "mean_val",main = "tapply() example 1",las = 2, cex.names = 0.75)#plot

```
<!--
### Snippet 2 (*)

```{r,height=700}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

#Lets factor the grades on on_smartphone as well as grade category.
moody2<-tapply(moody$GRADE,list(moody$ON_SMARTPHONE,moody$GRADE),length)
# We can see it calculated count of the grade of student with respect to their in-class smartphone usage  and grade category.
moody2
barplot(moody2,col=c("red","cyan","orange","blue"),main = "tapply() example 2",beside = TRUE,legend=rownames(moody2))
```
-->
### Code Review
#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

tapply(moody, GRADE, SCORE, min)
# What will R say?

# A. minimum score for each grade
# B. minimum grade for each score
# C. minimum grade only 
# D. Error.


```
<!--
#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

tapply(moody$GRADE, list(moody$ON_SMARTPHONE,moody$GRADE), length)
# What will R say?

# A. Total no. of students grade for each values of on_smartphone attribute
# B. mean value of on_smartphone attribute for each grade
# C. mean category of on_smartphone only 
# D. error.
```
-->
---

## Derived Attribute

R allows creating new data frame attributes (columns) “on the fly”.  These are new vectors, which are often defined as functions of existing attributes. Hence, the name - derived  attributes. 

Derived attributes will play an important role  in data exploration as well as in building prediction models. Very often, derived attributes allow discovery of important patterns in data. Similarly, derived attributes may be more predictive than original attributes in the imported data sets.

The term feature engineering is often used in machine learning to describe creation of derived attributes.

### Snippet 1  - Making  new categorical attribute. 
 
The  line 4 initializes the new attribute PF (Pass/Fail) to "Pass".  The line 5 replaces "Pass" by “Fail”  for students who received F. This new attribute, PF, will allow exploratory analysis to find “How to pass Professor Moody’s class”. The answer to this question may be different than then answer to “How to get a good grade in Professor Moody’s class”.


```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

# Cut Example using breaks - Cutting data using defined vector. 
moody$PF<-'Pass'
moody[moody$GRADE=='F',]$PF<-'Fail'

# lets see our added column PF
moody

```



### Cut 

- **cut()** function  divides the range of x into intervals. Provides ability to label intervals as well. It plays important role in defining derived attributes from attributes which are numerical.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

# Cut Example using breaks - Cutting data using defined vector. 
score1 <- cut(moody$SCORE,breaks=c(0,50,100),labels=c("F","P"))
table(score1)

```

### Code Review
#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

cut(moody$SCORE, breaks=c(0,25,70,100),labels=c("low", "medium", "high"))
#What would R say?

# A. 5 intervals of attribute score
# B. 3 intervals (0,25) (25,70) (75,100)
# C. 3 categorical values "low", "medium" and "high" for different score intervals
# D. 3 separate datasets with similar score values

```

#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

output<-cut(moody$SCORE, 5)
summary(output)
#What would R say?

# A. 5 intervals of attribute score of unequal count of elements
# B. 5 intervals of attribute score of equal count of elements
# C. 5 categorical values for different score intervals
# D. 5 separate dataset with similar score values

```

#### What would R say?

```{r}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MOODY-2019.csv")

output<-cut(moody$ASKS_QUESTIONS, 2)
summary(output)
#What would R say?

# A. 2 intervals of attribute ask_questions of unequal count of elements in each interval
# B. 2 intervals of attribute ask_questions of equal count of elements in each interval
# C. 2 categorical values for different ask_questions intervals
# D. Error.

```

#### More complex  example of defining derived attributes

The next snippet  illustrates defining a new numerical attribute, $adjustedScore of a student in the Moody data frame.  

Score is adjusted by the value of participation attribute in the following way:   

- If participation is larger than 0.5 - a bonus proportional to participation * 10 is added to the score.  

- If participation is smaller than 0.5, a penalty of 1-participation) * 10 is subtracted from the score. 

In this way, for someone with very small participation, the 10 point penalty will be imposed (10 points subtracted from the score). Conversely,  someone with perfect participation (1.0) will receive a 10 point bonus. 

##### Snippet 1

```{r,height=700}

moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2020b.csv")


moody$conditional <-0
moody[moody$participation<0.50, ]$conditional <- moody[moody$participation<0.50, ]$score -10*(1-moody[moody$participation<0.50, ]$participation)
moody[moody$participation>=0.50, ]$conditional <- moody[moody$participation>=0.50, ]$score +10*moody[moody$participation>=0.50, ]$participation

# print the column names
colnames(moody)

# lets look at the conditional attribute 
head(moody)

#subset the moody dataset rows = 1 to 10 and cols = 1,5
moody[1:10, c(1,5)]

#subset the moody dataset rows = 1 to 10 and cols = 1,5,6
moody[1:10, c(1,5,6)]

# print summary of inidividual columns
summary(moody$score)
summary(moody$conditional)

# Plotting the conditional attribute using boxplot
boxplot(moody$conditional,col = c("red"),main="Complex Example")

# Plotting the score attribute using boxplot
boxplot(moody$score,col = c("blue"),main="Complex Example")

```

## Additional references

<button class="btn btn-primary" data-toggle="collapse" data-target="#subset12">Data Transformation</button>
<div id="subset12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1--AeLFVrESqyWO7iNiIBRW7pX-VSYtBco1o8bOpOatE/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/tapply_subsetting.Rmd-->

# 🔖 Hypothesis Testing {#ztest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

Randomness is the biggest enemy of data scientists. How to distinguish what's real from what's random? This is the goal of hypothesis testing.  We will introduce hypothesis testing through the permutation test. To illustrate the permutation test, let us start with a simple example of a dataset storing information about traffic in Lincoln and Holland tunnels. 

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
traffic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv") #web load
# head(moody)
temp<-knitr::kable(
  traffic[sample(1:nrow(traffic),10), ], caption = 'Snippet of Traffic Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br> 

We observe that Lincoln traffic is higher than Holland tunnel traffic by calculating average traffic volume per minute for each of the tunnels using the provided data.

We conclude with 68.54 mean volume per minute for Lincoln and 67.71  mean volume per minute for the Holland tunnel. This seems to indicate that Lincoln traffic is higher than Holland traffic.  But is it?  Or is it just random deviation? Perhaps if we took more measurements, the trend would be reversed? This is where the permutation test comes handy. First, let us  talk about the null hypothesis and the alternative hypothesis.


**Null hypothesis** for the Lincoln-Holland tunnel observation  is that,  not surprisingly,  there is no difference in traffic  between the two tunnels.   

The **alternative hypothesis** states that Lincoln tunnel is more busy than Holland tunnel.

Does observed data (observed traffic difference) provide us with enough evidence to *reject null hypothesis* and in fact support the alternative  hypothesis?  To answer this question we need to decide whether the observed result is reasonably likely to come up randomly under the condition that NULL hypothesis holds.  

How likely it is  that observed difference (D=0.83) comes *randomly*?
Permutation test helps us to estimate the chance that D=0.83 will come up randomly under the condition that traffic in Holland and Lincoln tunnels is equal.

In each permutation of the permutation test we  randomly scramble the traffic table once. Permutation test is run many times, typically 10,000, even 100,000 times, and each permutation simulates a random process by simply reassigning the traffic volume values randomly between tunnels. The numbers of traffic measurements in Holland and Lincoln  tunnels respectively remain the same. Existing values are scrambled  though - breaking any relationship between volume numbers and tunnel names. Each permutation is like rolling a dice. How often will this  random process produce the result which is at least as extreme as D=0.83 that we have observed? The less often it happens the more likely it is that what we have observed is NOT random. For example, if we can get our observed result only 3 times in 1000 rolls of a dice (permutation test) it means that with probability of 99.7% our observed result cannot be random.

Permutation test provides a palpable experience of randomness. Just roll the dice many times and see how often you can get the observed result or more. If you can get D>0.83 relatively often (above what is called significance level usually it is at least 5% of the time), then you cannot reject the null hypothesis. In other words the conclusion that your observation appeared RANDOMLY. Otherwise, we can conclude that observation was not random - and we reject the null hypothesis.

Notice, that every time we run the permutation test function we may get slightly different p-values. This is because permutations are random. The more times we run a permutation test, the closer it will approximate the “real” p-value.  Snippet 6.1 shows permutation test results for the Traffic data set. 

Another test which is often used for difference of means hypothesis testing is the z-test.  It is described very well in the attached link to the Khan Academy lecture.  Here we run z-test function in one of the following snippets.

## Snippet 1: Permutation test {#permutaion}


The following snippet \@ref(permutaion) shows the code for hypothesis test of difference of means.

Is the mean traffic (VOLUME_PER_MINUTE)  in the Holland tunnel bigger than  mean  traffic (VOLUME_PER_MINUTE)  in the Lincoln? 

Do this in your R studio, since we cannot install our package in data camp service we are using to run the code snippets

```{r,tut=TRUE,ex="permutationtestfunction",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,tut=TRUE,ex="permutationtestfunction",type="sample-code",height=500}
#install.packages("devtools")
#devtools::install_github("devanshagr/PermutationTestSecond")

#PermutationTestSecond::Permutation(d, "Cat", "Val",10000, "GroupA", "GroupB")
traffic<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv")
Permutation(traffic, "TUNNEL", "VOLUME_PER_MINUTE",1000,"Holland", "Lincoln")
 
#The Permutation function returns the absolute value of the difference. So the red line is the absolute value of the observed difference. You will see a histogram having a normal distribution with a red showing the observed difference.
```

## Snippet 2: z-test {#ztests}

**Null Hypothesis** - Traffic in Holland tunnel is the same as traffic in Lincoln tunnel.

**Alternative Hypothesis** - Traffic in the Holland Tunnel is larger than traffic in the Lincoln  tunnel.

In the snippet \@ref(ztests)  we end up calculating the p-value which leads to rejection of Null hypothesis (good news for data scientist, bad for the sceptic).  Indeed, p-value is less than the significance level  of 5%.
This means, that under null hypothesis it is extremely unlikely (less than 5% chance) to see the result which is at least as big as the  observed difference of means.

```{r,tut=TRUE,ex="ztestfunction1",type="pre-exercise-code"}

z_test <- function(data,col1,col2,sub1,sub2) {
  data <- as.data.frame(data)
  V1<-data[,col1]
  V2<-data[,col2]
  #data clean and subset, either
  lincoln.data <- subset(data, V1 == sub1)
  holland.data <- subset(data, V1 == sub2)
  
  #traffic at lincoln
  lincoln.traffic <- lincoln.data[,col2]
  #traffic at holland
  holland.traffic <- holland.data[,col2]
  
  # standard deviation of two samples.
  sd.lincoln <- sd(lincoln.traffic)
  sd.holland <- sd(holland.traffic)
  
  #length of lincoln and holland
  len_lincoln <- length(lincoln.traffic)
  len_holland <- length(holland.traffic)
  len_lincoln
  len_holland
  
  #standard deviation of difference traffic
  sd.lin.hol <- sqrt(sd.lincoln^2/len_lincoln + sd.holland^2/len_holland)
  sd.lin.hol
  
  #means of two samples
  mean.lincoln <- mean(lincoln.traffic)
  mean.holland <- mean(holland.traffic)
  mean.lincoln
  mean.holland
  
  #z score
  zeta <- (mean.lincoln - mean.holland)/sd.lin.hol
  print(paste(zeta," is the z-value"))
  
  #plot red line
  plot(x=seq(from = -5, to= 5, by=0.1),y=dnorm(seq(from = -5, to= 5,  by=0.1),mean=0),type='l',xlab = 'mean difference',  ylab='possibility')
  abline(v=zeta, col='red')
  
  #get p
  p = 1-pnorm(zeta)
  print(paste(p, " is the p-value"))
}
```

```{r,tut=TRUE,ex="ztestfunction1",type="sample-code",height=500}

TRAFFIC<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')

z_test(TRAFFIC,"TUNNEL", "VOLUME_PER_MINUTE","Lincoln", "Holland")

```


## Snippet 3: Make your own data and see how p-value changes

For students familiar with basic descriptive statistics (mean, standard deviation)we build a synthetic data set ourselves and see how difference of means and difference of standard deviations affects the p-value. We will build our two distributions ourselves - varying the means and standard deviations. We will use **rnorm()** to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. 

Build the data frame with two attributes: Cat and Val, using rnorm() function. 
Our null hypothesis is that Group A and Group B  have identical mean(Val).

The alternative hypothesis is that the mean(Val) for Group B is higher than mean(Val) for Group A.
We will change the mean and standard deviation of the data distributions for Group A and Group B and see how these changes affect the p-value. We will first use a permutation test and a single-step permutation test (just to illustrate what happens each single step when we run a permutation test). Then we finish off with the z-test. 

### Permuation test

**Exercise** - How p-value is affected by difference of means and standard deviations

We will build our two distributions ourseleves - varying the means and standard deviations.  We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use permutation test (can be z-test as well) to test difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values.

Build the data frame with two attributes: **Cat** and **Val**, using **rnorm()** function

```{r,tut=TRUE,ex="permutationtestfunction1",type="pre-exercise-code"}
Permutation <- function(df1,c1,c2,n,w1,w2){
  df <- as.data.frame(df1)
  D_null<-c()
  V1<-df[,c1]
  V2<-df[,c2]
  sub.value1 <- df[df[, c1] == w1, c2]
  sub.value2 <- df[df[, c1] == w2, c2]
  D <-  abs(mean(sub.value2, na.rm=TRUE) - mean(sub.value1, na.rm=TRUE))
  m=length(V1)
  l=length(V1[V1==w2])
  for(jj in 1:n){
    null <- rep(w1,length(V1))
    null[sample(m,l)] <- w2
    nf <- data.frame(Key=null, Value=V2)
    names(nf) <- c("Key","Value")
    w1_null <- nf[nf$Key == w1,2]
    w2_null <- nf[nf$Key == w2,2]
    D_null <- c(D_null,mean(w2_null, na.rm=TRUE) - mean(w1_null, na.rm=TRUE))
  }
  myhist<-hist(D_null, prob=TRUE)
  multiplier <- myhist$counts / myhist$density
  mydensity <- density(D_null, adjust=2)
  mydensity$y <- mydensity$y * multiplier[1]
  plot(myhist)
  lines(mydensity, col='blue')
  abline(v=D, col='red')
  M<-mean(D_null>D)
  return(M)
}
```

```{r,ex="permutationtestfunction1", tut=TRUE,type="sample-code",height=600}
Val1<-rnorm(10,mean=25, sd=10)
Val2<-rnorm(10,mean=30, sd=10)
 
Cat1<-rep("GroupA",10)  # for example GroupA can be Holland Tunnel
Cat2<-rep("GroupB",10)  # for example Group B will be Lincoln Tunnel

Cat1
Cat2

#The rep command will repeat, the variables will be of type character and will contain 10 values each.

Cat<-c(Cat1,Cat2) # A variable with first 10 values GroupA and next 10 values GroupB
Cat

Val<-c(Val1,Val2)
Val

d<-data.frame(Cat,Val)
d

Permutation(d, "Cat", "Val",1000,"GroupA", "GroupB")

Observed_Difference<-mean(d[d$Cat=='GroupB',2])-mean(d[d$Cat=='GroupA',2])
Observed_Difference

#This will calculate the mean of the second column (having 10 random values for each group), and the mean of groupB values is subtracted from the mean of groupA values, which will give you the value of the difference of the mean.
 
 #Try changing mean and sd values. When you run this you will see that the difference is sometimes negative #or sometimes positive.
```

### One permutation at a time

```{r,tut=TRUE,height=400}
traffic<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Traffic2022.csv')

ranNum <- sample(1:nrow(traffic),nrow(traffic))
ranNum[1:5]

VOLUME_PER_MINUTE<-traffic$VOLUME_PER_MINUTE[ranNum]
TUNNEL<-traffic$TUNNEL

Permuted_traffic<-data.frame(TUNNEL, VOLUME_PER_MINUTE)

mean(traffic[traffic$TUNNEL=='Lincoln', ]$VOLUME_PER_MINUTE) -mean(traffic[traffic$TUNNEL=='Holland', ]$VOLUME_PER_MINUTE)

mean(Permuted_traffic[Permuted_traffic$TUNNEL=='Lincoln', ]$VOLUME_PER_MINUTE)-mean(Permuted_traffic[Permuted_traffic$TUNNEL=='Holland', ]$VOLUME_PER_MINUTE)
```


### z-test

How p-value is affected by difference of means and standard deviations.

We will build two distributions ourselves - varying the means and standard deviations. We will use rnorm() to generate normal distributions with given means and standard deviations. Then we will use a permutation test (can be a z-test as well) to test the difference of means for these two synthetic distributions. See for yourself the impact means and standard deviations have on p-values. You can do it by changing values of mean and standard deviation in the rnorm() function.

Clearly the further apart the mean values are - the lower the p-value. But how do standard deviations affect the p-value?  See for yourself.

Build the data frame with two attributes: Cat and Val, using rnorm() function

```{r,tut=TRUE,ex="ztestfunction",type="pre-exercise-code"}

z_test <- function(data,col1,col2,sub1,sub2) {
  data <- as.data.frame(data)
  V1<-data[,col1]
  V2<-data[,col2]
  #data clean and subset, either
  lincoln.data <- subset(data, V1 == sub1)
  holland.data <- subset(data, V1 == sub2)
  
  #traffic at lincoln
  lincoln.traffic <- lincoln.data[,col2]
  #traffic at holland
  holland.traffic <- holland.data[,col2]
  
  # standard deviation of two samples.
  sd.lincoln <- sd(lincoln.traffic)
  sd.holland <- sd(holland.traffic)
  
  #length of lincoln and holland
  len_lincoln <- length(lincoln.traffic)
  len_holland <- length(holland.traffic)
  len_lincoln
  len_holland
  
  #standard deviation of difference traffic
  sd.lin.hol <- sqrt(sd.lincoln^2/len_lincoln + sd.holland^2/len_holland)
  sd.lin.hol
  
  #means of two samples
  mean.lincoln <- mean(lincoln.traffic)
  mean.holland <- mean(holland.traffic)
  mean.lincoln
  mean.holland
  
  #z score
  zeta <- (mean.lincoln - mean.holland)/sd.lin.hol
  print(paste(zeta," is the z-value"))
  
  #plot red line
  plot(x=seq(from = -5, to= 5, by=0.1),y=dnorm(seq(from = -5, to= 5,  by=0.1),mean=0),type='l',xlab = 'mean difference',  ylab='possibility')
  abline(v=zeta, col='red')
  
  #get p
  p = 1-pnorm(zeta)
  print(paste(p, " is the p-value"))
}
```


```{r,tut=TRUE,ex="ztestfunction",height=700}
Val1<-rnorm(10,mean=25, sd=10)
Val2<-rnorm(10,mean=35, sd=10)
Cat1<-rep("GroupA",10)  
Cat2<-rep("GroupB",10)  
Cat<-c(Cat1,Cat2) 
Val<-c(Val1,Val2)

d<-data.frame(Cat,Val)
Observed_Difference<-mean(d[d$Cat=='GroupB',2])-mean(d[d$Cat=='GroupA',2])
Observed_Difference

z_test(d,"Cat", "Val","GroupB", "GroupA")
```




## Additional References

<button class="btn btn-primary" data-toggle="collapse" data-target="#HPT12">Hypothesis Testing</button> 
<button class="btn btn-primary" data-toggle="collapse" data-target="#PT12">Permutation Test</button> 

<button class="btn btn-primary" data-toggle="collapse" data-target="#KH12">Khan Academy Video</button>

<div id="HPT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/17zX82imn7S3_r9Uls3pqInIwdPx1NIQW-O22ynSa9Sg/edit?usp=sharing" width="100%" height="500px"></embed>
</div>
<div id="PT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1Dat8r1wUbNzFCqbjxj_jbH8T11BFwmOJOxK5T_mbnUE/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<div id="KH12" class="collapse">https://www.khanacademy.org/math/statistics-probability/significance-tests-confidence-intervals-two-samples/comparing-two-means/v/hypothesis-test-for-difference-of-means
</div>


https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/p-value/ <br>
http://www.z-table.com/ <br>
https://www.statisticshowto.com/probability-and-statistics/z-score/ <br>
https://sixsigmastudyguide.com/z-scores-z-table-z-transformations/

<!--chapter:end:chapters/hypothesis_testing.Rmd-->

# 🔖 Test of Independence {#chitest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

We would like to test if a student's final grade in Professor Moody’s class  depends  on the student's major.  The null hypothesis in this case is the hypothesis of independence.  Independence means that the distribution of final grades is the same no matter what the major is. The alternative hypothesis is that the distribution of final grades  changes from major to major, rejecting the null hypothesis of independence. 

Notice that we do not specify how the grades depend on students' majors.  Do CS students get better grades than Psychology  majors? Do Economics majors get lower grades than Statistics majors?  We do not care about this. We are only testing here whether there is a relationship between Major and the final grade distribution.

We will describe the permutation test which scrambles our data randomly in such a way that any relationship between grades and major  (if it ever existed) is broken. We will run a permutation test a large number of times - possibly tens of thousands of times. Then we will determine how likely it is to randomly obtain the observed result.  But what is the observed result? It is a bit more complex than the difference of means which were observing the difference of means hypothesis testing. 

The observed result in our case is calculated by so called chi-square statistic which is calculated on the contingency table,  ```table(moody$Grade, moody$Major)```

To explain it, let us first define two tables: The observed contingency table and the expected contingency table. It is calculated by table() function.


**OBSERVED CONTINGENCY TABLE**
<table>
<tr>
<td>Grade/Major</td>
<td>CS</td>
<td>Economics</td>
<td>Psychology</td>
<td>Statistics</td>
</tr>

<tr>
<td>A</td>
<td>46</td>
<td>54</td>
<td>69</td>
<td>42</td>
</tr>

<tr>
<td>B</td>
<td>46</td>
<td>12</td>
<td>2</td>
<td>35</td>
</tr>

<tr>
<td>C</td>
<td>51</td>
<td>33</td>
<td>30</td>
<td>34</td>
</tr>

<tr>
<td>D</td>
<td>41</td>
<td>37</td>
<td>29</td>
<td>34</td>
</tr>

<tr>
<td>F</td>
<td>108</td>
<td>99</td>
<td>99</td>
<td>99</td>
</tr>

</table>


The second table we need is called the expected table. This is the hypothetical table of the relationship between grade and major,  assuming grade and major are completely independent.  Such a table  would be the result of  the same distribution of grades for each of the majors.  Notice that we 1000 students in the data set the expected table (i.e. table which have grades completely independent from majors) would  have the same distribution of grades for each major that over all students - which is shown by the **TOTAL** column.


**EXPECTED CONTINGENCY TABLE**
<table>
<tr>
<td>Grade/Major</td>
<td>CS</td>
<td>Economics</td>
<td>Psychology</td>
<td>Statistics</td>
</tr>

<tr>
<td>A</td>
<td>61</td>
<td>50</td>
<td>48</td>
<td>51</td>
</tr>

<tr>
<td>B</td>
<td>28</td>
<td>22</td>
<td>22</td>
<td>23</td>
</tr>

<tr>
<td>C</td>
<td>43</td>
<td>35</td>
<td>34</td>
<td>36</td>
</tr>

<tr>
<td>D</td>
<td>41</td>
<td>33</td>
<td>32</td>
<td>34</td>
</tr>

<tr>
<td>F</td>
<td>118</td>
<td>96</td>
<td>93</td>
<td>99</td>
</tr>

<tr>
<td>TOTAL</td>
<td>292</td>
<td>236</td>
<td>229</td>
<td>244</td>
<td>1000</td>
</tr>
</table>

We kept fractions - although these are number of students - therefore would have to be rounded up to integers 

The chi-square formula calculates the **“distance”** between the observed contingency table and the expected contingency table.

\begin{equation}

\sum \frac{(O_i - E_i)^2}{E_i}\\


\text{where:}\\
\text{O = observed values}\\
\text{E = expected values}\\

\end{equation}

For the two tables above the, 

\begin{equation}

\sum \frac{(O_i - E_i)^2}{E_i}  = 60.03\\

\end{equation}

To evaluate how far off is this observed result assuming that Grades are independent from Major, we run a permutation test which scrambles Grades and Majors randomly and every time computes the chi-square formula with the new observed table (the expected table is always the same).  Then, we see how many times out of, say 10,000 iterations of permutation test we obtain a result larger than the observed result of 60.03? This is the p-value. 

Permutation test for independence hypothesis gives us again a better feeling about the impact of randomness and whether the observed chi-square result for “similarity” of grade distributions for different majors can be obtained randomly. 
In the following snippet we run the chisq test which is based on the so-called chi square distribution. Here we simply show you a function which can calculate p-value, just like the z-test function does.  The explanation of the chi-square test is provided in attached link to the excellent Khan Academy video.

Permutation tests in both cases of difference of means and independence hypotheses give a better intuitive sense of how we answer the question - can the observed result be obtained randomly? 

Notice that the independence test is looking globally at two vectors and whether one depends on another. If we wanted to be more specific and know if psychology majors are more likely to get an A than CS majors, we can frame this as a difference of means hypothesis. Testing this hypothesis will be using the difference of means of frequencies of A’s among CS majors and psychology majors.  This could be done again by permutation test in section 8 or the z-test. 



## Snippet 1

```{r,tut=TRUE,height=300}
Expected <-matrix(c(200,420,180, 40,120,40), nrow=3, ncol=2)
Observed<-matrix(c(200,420,180,35,120,45), nrow=3, ncol=2)
Expected
Observed
chisq.test(Observed)
```

## Snippet 2

```{r,tut=TRUE,ex="chisquarefunction",type="pre-exercise-code"}
library(dplyr)
options(warn=-1)
chi_test <- function(data,col1,col2,iter) {
  
  df <- data.frame(data)
  vals<- unique(df[[col2]])
  no_rows <- nrow(df)
  dt <- table(df[[col1]], df[[col2]])
  res <- chisq.test(dt)
  real_ans <- res$statistic
  p_value <- res$p.value
  ans_vec <- vector()
  for (x in 1:iter){

    new_data <- sample(x=vals,size=no_rows,replace = TRUE)

    dt_new <- table(df[[col1]], new_data)

    res_new <- chisq.test(dt_new)

    ans_vec <- append(ans_vec,res_new$statistic)
  }
  hist(ans_vec,main="Permutation Test for Chi-Square",xlab="Chi-Square Values",breaks = 100)
  print(real_ans)
  abline(v=real_ans,col="blue",lwd=2)
  return (p_value)
}

```


```{r,tut=TRUE,ex="chisquarefunction",type="sample-code",height=500}

d<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")
head(d)

chi_test(d,"Major","Grade",1000)

```

## Snippet 3

```{r,tut=TRUE,height=300}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv")
moody$IN<-'Out_Slice'
moody[moody$DOZES_OFF=='never' & moody$TEXTING_IN_CLASS=='always', ]$IN<-'In_Slice'
d<-table(moody$GRADE, moody$IN)
d
chisq.test(d)
```

## Snippet 4


```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")
data<-table(movies$content, movies$genre)
chisq.test(data)
```

## Additional Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#CS12">Chi Square</button> 

<button class="btn btn-primary" data-toggle="collapse" data-target="#KH13">Khan Academy Video</button>

<div id="CS12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1h-h2S5lW6ReFwdeJKNflPpE0iCjoS08T0mpgSLiFv88/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<div id="KH13" class="collapse">https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-goodness-fit/v/chi-square-statistic
</div>

<!--chapter:end:chapters/Chisquare.Rmd-->

# 🔖 Multiple Hypothesis Testing {#Mtest}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

We often consider multiple possible hypotheses in our search for discovery  to find one with lowest possible p-value. Consciously or subconsciously we are engaging, what is often called,  p-value hunting. We have to be very careful! We may “discover” what is simply random even if we correctly calculate p-value and compare it with the significance level. It is very important to learn about multiple hypothesis traps very early in the process of learning data science.  

For example assume that we are looking for associations between sales of individual items in a supermarket.  Does bread sell with butter? Does coffee sell with spring water?  There is an exponential number of possible combinations (N choose 2 to be exact, where N is the number of items).  For each such pair we perform hypothesis testing. If one test is performed at the 5% significance level and the corresponding null hypothesis is true, there is only a 5% chance of incorrectly rejecting the null hypothesis. However, if 100 tests are each conducted at the 5% significance level and all corresponding null hypotheses are true, the expected number of incorrect rejections (also known as false positives or Type I errors) is 5. If the tests are statistically independent from each other, the probability of at least one incorrect rejection is approximately 99.4%.  Thus, we will almost surely find one false positive! In other words we will be fooled by data. 

Bonferroni correction is a method to counteract the multiple hypothesis (often called multiple comparison problem. Make it harder to reject null hypotheses by dividing the significance level by number of hypotheses. The Bonferroni correction compensates for that increase by testing each individual hypothesis at a significance level of **α / m**  where  m is the number of hypotheses. For example, if a trial is testing me = 20 hypotheses with a desired α = 0.05, then the Bonferroni correction would test each individual hypothesis at

\begin{equation}
\alpha = \frac{0.05} {20} 
       = 0.0025
\end{equation}

Thus, there is a very simple remedy for multiple hypothesis traps. Just divide the significance level by the number of (potential) hypotheses tested.  This will make it harder, often much much harder to reject the null hypothesis and yell Eureka! Critics say that in fact Bonferroni correction is too conservative and too “pro-null”  and tough on alternative hypotheses to be acceptable.  

The unwanted side effect of Bonferroni correction is that  we may  fail to reject the null hypothesis too often. Bonferroni correction makes discovery sometimes too hard, making data scientists too conservative and accepting null hypothesis when they should be rejected.  It may also be the case that even Bonferroni correction will not protect us, as we will show in our example below.  But at least we will be much less likely to  make fools of ourselves coming with false discoveries leading potentially to very wrong business decisions. 

There are other less conservative methods of correcting for multiple hypotheses - such as the Benjamini-Hochberg method described in the attached slides.

The \@ref(Snippet1) describes the data set based on a hypothetical temperature readings in various municipalities of New Jersey over summer.  Is one city experiencing higher average temperatures than another?  Can we find such a pair of cities?   This is the ultimate p-value hunt. Let's compare townshiships  pair by pair, until we find a pair with sufficiently large differences of mean temperatures and sufficiently low p-value.  Careful!  You may come up with false discovery if you do not correct for multiple hypotheses! 

\@ref(Snippet1) shows several permutation tests for different pairs of townships and difference of means of temperatures hypothesis test. Two of four pairs show p–values less than customary significance level of 5%.  Should we then reject the null hypothesis and conclude that indeed Ocean Grove is warmer than New Brunswick and that New Brunswick is warmer than Holmdel?  Indeed, both pairs result in p-values significantly lower than 5%.  If we incorrectly disregard the number of hypotheses considered, we may come to wrong conclusions supporting these two alternative hypotheses.  But there are around 20 townships in the Temp data set.  Thus there are around 200 possible hypotheses (200 pairs of townships) which we may consider in our p-value hunt. If we apply Bonferroni correction for N=200, the significance level will be 200 times lower, instead of 5%, it will be 0.025%.  None of the two hypotheses (Ocean Grove vs New Brunswick and New Brunswick vs Holmdel) meets the new significance level. Indeed in both cases p-values are significantly larger than 0.025%.  Thus, for none of the four pairs we can reject null hypotheses. 

Now we can disclose that we have created our Temp data set completely randomly - assigning random temperatures between 50 and 100 degrees to each township.  Thus, without Bonferroni coefficient we would be fooled by data, not once, but twice in our four tests. We would find a trend when it does not exist - it is simply random deviation.  

It turns out however that even Bonferroni correction is not sufficient to protect us against incorrectly rejecting null hypothesis. Indeed, for Red Bank and Holmdel, we conclude that Red Band is warmer than Holmdel with  p-value of 0.01%!  (see the last permutation test in the snippet 1). This p-value falls even below significance level adjusted with Bonferroni correction (0.025%).  It only shows that dealing with multiple hypotheses is a risky adventure. We may end up being fooled by data even when we apply Bonferroni correction. But at least we are less likely to fall into the trap of multiple hypotheses when we apply Bonferroni Correction.

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
hindex<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv") #web load
# head(moody)
temp<-knitr::kable(
  hindex[sample(1:nrow(hindex),10), ], caption = 'Snippet of Hindex Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

## Snippet 1 - Benjamini-Hochberg Algorithm {#Snippet1}

```{r,tut=TRUE,height=450}
p<-sort(round(runif(100, min=0, max=0.05), 4))
p
p<-p+0.0003
p
#implement Benjamini-Hochberg formula
q<-rep(0.05,100)
q
r=c(1:100)
q<-round(q*r/100,4)
temp<-p<q
#Select p-values which correspond to discoveries (reject NULL)
maxindex<-max(which(temp=='TRUE'))
p[1:maxindex]
```

## Snippet 2

Happiness Index synthetic data set which is used in my slides for multiple hypotheses testing

- How to order by aggregate?

- First make a data frame out of tapply? Use  aggregate  and list functions.

```{r,tut=TRUE,height=450}
Hindex <-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Hindex.csv") #web load

Hindex<-aggregate(Hindex$HAPPINESS, list(Hindex$COUNTRY), mean)
colnames(Hindex)<- c("Country","AverageH")
#renames columns of the Hindex data frame
colnames(Hindex)

Hindex[order(Hindex$AverageH),]
```

## Additional References 

<button class="btn btn-primary" data-toggle="collapse" data-target="#MPT12"> Multiple Hypothesis Testing</button> 
<div id="MPT12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1dCbhnuGMsXYJEltQXUfPhl8dCTAqJWCZ0xI-0IhnxAc/edit?usp=sharing" width="100%" height="500px"></embed>
</div>


https://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/

<!--chapter:end:chapters/Multiple_Hypothesis.Rmd-->

# Code Review: Exploratory Queries in R {#cr}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

Using simple one-line “programs” it is possible to learn a lot  about your data. We call these single lines - exploratory queries. You do not even need to plot. You can familiarize yourself with the dataset and satisfy your curiosity by combining **tapply()**, **table()** and aggregates such as **mean()**. This combination of simple instructions is very powerful. And you can do it in one line. Below we present many examples over multiple data sets. 

Writing these one line exploratory queries is fast and can precede hypothesis testing. Just remember about multiple hypothesis correction. 


## Movies Dataset Example 
### Snippet 1: What is the mean imdb of low budget comedies?


```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

mean(movies[movies$Budget=='Low' & movies$genre=='Comedy', ]$imdb_score)

```

### Snippet 2: What is the standard deviation of imdb score of  high gross Family movies?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

sd(movies[movies$Gross=='High' & movies$genre =='Family',]$imdb)

```

### Snippet 3: What is the lowest imdb score among high budget movies?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

min(movies[movies$Budget=='High',]$imdb)

```

### Snippet 4: How many low budget movies generated high gross income?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

nrow(movies[movies$Budget=='Low' & movies$Gross =='High',])

```

### Snippet 5: What is the imdb score of the first non-US movie in the movies data frame?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

#You can use this simple command to quickly find out
head(movies[movies$country!='USA', ]$imdb_score)

```

### Snippet 6: What is the least frequent genre among UK movies?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

#You can use this code to find out
table(movies[movies$country=='UK',]$genre, movies[movies$country=='UK',]$country)

```

### Snippet 7: Which content rating has the lowest average imdb score?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

#You can use this code to find out
tapply(movies$imdb, movies$content, mean)

```

### Snippet 8: Movies from which country have the smallest average imdb score?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

#Better compute it, since there are too many countries for visual inspection
MA<-aggregate(movies$imdb_score, list(movies$country), mean)
colnames(MA)<-c("Country", "Mimdb")
MA<-MA[order(-MA$Mimdb), ]
MA[1,]
```

### Snippet 9: What is the least frequent genre in movies data frame?

```{r,tut=TRUE,height=300}
movies<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Movies2022F-4.csv")

z<-table(movies$genre)
sort(z,decreasing=FALSE)[1]
```

### Snippet 10: z value = 2.4, whats the p-value?

```{r,tut=TRUE,height=300}

1-pnorm(2.4)

```

## Census Dataset Example

### Snippet 11: For the individual over 50, which profession has the highest average capital gain?
```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

age_greater_than_49 = subset(census_data, census_data$AGE >= 50)
aged_capitalgains = tapply(age_greater_than_49$CAPITALGAINS, age_greater_than_49$PROFESSION, mean)
aged_capitalgains

```

### Snippet 12: Which profession has the highest average capital gains; Sales or Tech-support?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example12_data = tapply(census_data$CAPITALGAIN, census_data$PROFESSION, mean)
example12_data
```

### Snippet 13: What is the most frequent profession of people with less than 10 years od of education?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example13_data = table(subset(census_data, YEARS <= 10)$PROFESSION)
example13_data
```

### Snippet 14: What is the minimum number of years of education for people with the Exec-managerial specialty?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example14_data = min(subset(census_data, PROFESSION == "Exec-managerial")$YEARS)
example14_data
```

### Snippet 15: What is the most frequent degree for natives of the United States?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example15_data = table(subset(census_data, NATIVE == "United-States")$EDUCATION)
example15_data
```

### Snippet 16: What is the least frequent degree for people with at least 12 years of education?

```{r,tut=TRUE,height=300}
census_data<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/CensusData.csv")

example16_data = table(subset(census_data, YEARS >= 12)$EDUCATION)
example16_data
```


<!--chapter:end:chapters/code_review.Rmd-->

# 🔖 Common Sense Judgement and Probability {#common}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

We will step away from coding for a moment.   In the class description we promise to address the million dollar question **“How not to be fooled by data?”**.  Let's dive into this important issue. We have already discussed powerful tools such as hypothesis testing, p-values and Bonferroni correction for multiple hypothesis traps. But even if you never want to write a line of code, you need to know about common traps which you may be fooled by in whatever you do. We need to be informed and educated citizens who can catch the fake inferences and fake discoveries whether we read them in the news or hear politicians falling for the traps. 

Daniel Kahnemann, the Nobel Prize winner in Economics is the author of the fascinating book “Think fast, think slow” and he identifies pitfalls of human relationships with numbers, frequencies.   We discuss Availability, Anchoring, Conjunctive fallacy, Narrative fallacy, Law of small numbers, Reverse to the mean and many other concepts in the attached power points. 

In the next section we will also discuss Bayesian theorem and Bayesian reasoning (with some coding handy) to finally come back to the paradoxes such as prosecutorial paradox, Simpson paradox and ecological fallacy in section 21.  

## Additional References

<button class="btn btn-primary" data-toggle="collapse" data-target="#common12">Common Sense Judgement and Probability</button>
<div id="common12" class="collapse">    
<embed src="https://docs.google.com/presentation/d/1IuqMFWih6WuUAH2gCztfUjDl2SL7OUV5y7VyDs6_EGo/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/common_sense.Rmd-->

# 🔖 Bayesian Reasoning {#br}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

Bayesian Reasoning and Bayesian theorem are fundamental instruments to use both in data science as well as in real, everyday life. They are definitely part of data literacy and should be widely taught, especially among future doctors, lawyers and politicians.  In this section we will explain why Bayesian reasoning is so important and also teach the most simple and intuitive formulation of Bayesian theorem - the  Odds formulation.

Bayesian theorem is a calming tool  - the chances of bad things happening are lower than expected!  This is why Bayes helps  pessimists.  Take a situation at a doctor's office when a patient learns that a medical test for some potentially serious condition came positive. The doctor believes the test and the test is almost 100% accurate. Should the patient despair?  Not so fast. Bayes theorem allows the patient to ask the doctor some important questions. 

In bayesian reasoning we distinguish between two concepts = observation and belief.  Belief is something unknown, Observation is known. We use observation to modify the odds (probability) of the  belief from prior odds (before we learned about observation) and the posterior odds (after we learned about observation). For example a patient taking a covid test is concerned about having covid. But s/he does not know whether they have covid. Thus “having covid” is a belief.  Test result is an observation (positive or negative).  Given the prior odds of covid (say 1:100), and positive covid test what are the posterior odds of covid?  Bayesian theorem tells us how to compute posterior odds from prior odds, given the observation.
w
Odds formulation of Bayesian theorem states;

\begin{equation}

\text{POSTERIOR ODDS = LIKELIHOOD RATIO *  PRIOR ODDS} \\
               

\textbf{Prior odds}   \text{-  odds for the belief before observation (evidence)}\\

\textbf{Likelihood ratio}  \text{-  effect of observation, evidence. Can be larger or smaller than 1!!}\\

\textbf{Posterior odds} \text{-  New odds with observation(evidence) taken under consideration.}\\

\end{equation}

Let B a belief and O be an observation, then

\begin{equation}

\textbf{Prior odds} - \frac{P(B)}{P(\sim B)}\\

\textbf{Likelihood ratio}  - \frac{P(O|B)}{P(O|\sim B)}\\

\textbf{Posterior odds} - \frac{P(B|O)}{P(\sim B|O)}
\end{equation}

Let's discuss the multiplier – likelihood ratio in more detail.  It is the red colored part of the bayes Theorem:

\begin{equation}

\frac{P(B|O)}{P(\sim B|O)} =	\frac{P(O|B)}{P(O|\sim B)}  *   \frac{P(B)}{P(\sim B)} \\

\text{The red colored ratio is the ratio of true positive and false positive,}\\

\text{P(O|B) – True positive} \\
\text{P(O|$\sim$ B) – False positive}

\end{equation}

True positive is the conditional probability of seeing the observation given that our  belief is true. In our medical example it is the probability of testing positive for covid, given that in fact we have covid. 

False positive, on the other hand, is the probability of observation under condition that the belief is not true. For example in our case that covid test comes positive even when we do not have covid. 

In real life False positives are often overlooked. And this is the critical question we should ask the doctor or health professional who administers any test. What is the false positive of this test?   Since this is what we divide the true positive by. Even if the true positive is 99.9% (almost sure), if the false positive is, say 20% - the likelihood ratio is around 5.  In such a situation, a positive test increases the odds of having covid just 5 times. If prior odds of covid are 1:100, the posterior odds of covide after such a positive test are just 5:100, still minimal!.  Even if a false positive was 10%, the likelihood ratio of 10, would increase odds of covid 10 fold, to just 1:10 and false positive of 5%, would result in a likelihood ratio of 20 - still leading to higher odds of NOT having covid than having it!  

This is why false positives are so critical. IBut the main question that Bayes teaches us to ask is what are the prior odds.  Since if prior odds are very small (we are testing a really rare condition) then the likelihood ratio would have to be really large to make posterior odds significant.  For example if prior odds are one in a million, we need a likelihood ratio of more than half a million to actually make posterior odds better than proverbial fifty - fifty.

Hence to main questions we should ask our doctor upon hearing that the test results are positive are:

**What are the prior odds of the disease?**

**What is the false positive of the test (since we assume that the true positive of the test would usually be close to 100%)?**

In the following snippets we show how to calculate the posterior odds, while being tested for a disease and then closer to our data puzzles, how to calculate the posterior odds of getting an A in class, when scoring more than 85%.In all these situations we begin with identifying what is belief (the unknown), what is the observation (the known) and we use the snippets by plugging in some assumed values of prior odds, as well as true positives and false positives.


## Snippet 1: Covid Odds after positive Home Test. 

```{r,tut=TRUE,height=600}
#Belief = "Have Covid"
#Observation = Covid Test
#How much the probability of having covid increases upon positive COVID-test?
#We use the odds formulation of Bayesian Theorem
# we begin with prior odds of having Covid:  P(Covid)/(1-P(Covid)
PriorHaveCovid<-0.01
PriorCovidOdds<-PriorHaveCovid/(1-PriorHaveCovid)
PriorCovidOdds
#True positive:  Probability of having positive Covid test when having covid  = P(PositiveCovidTest|HaveCovid)
TruePositive<-0.99
#False positive = Probability of having positive Covid test when not having covid = P(PostiveCovidTest/DoNotHaveCovid)
FalsePositive<-0.001
LikelihoodRatio<-TruePositive/FalsePositive
PosteriorCovidOdds<-LikelihoodRatio*PriorCovidOdds
PosteriorHaveCovid<- PosteriorCovidOdds/(1+PosteriorCovidOdds)
PosteriorHaveCovid
```

## Snippet 2: What are the odds that an 'F' student is a freshman?

```{r,tut=TRUE,height=600}
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv')
#Belief - Student is a freshman
#Observation - Failed the class
Prior<-nrow(moody[moody$Seniority =='Freshman',])/nrow(moody)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(moody[moody$Grade=='F' & moody$Seniority=='Freshman',])/nrow(
  moody[moody$Seniority =='Freshman',]),2)
TruePositive
FalsePositive<-round(nrow(moody[moody$Grade=='F'& moody$Seniority !='Freshman',])/nrow(moody[moody$Seniority !='Freshman',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
round(Posterior,2)
```


## Snippet 3: What are the odds that a 'A' student with the score less than 80 is a psychology major?

```{r,tut=TRUE,height=600}
#Belief - what we do not know. #Is a student a psychology #major?
#Observation = what we do #know. They got an A and less #than 80 in score

moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv')
Prior<-nrow(moody[moody$Major =='Psychology',])/nrow(moody)
Prior
PriorOdds<-round(Prior/(1-Prior),2)
PriorOdds
TruePositive<-round(nrow(moody[moody$Score <80 & moody$Grade=='A'& moody$Major=='Psychology',])/nrow(moody[moody$Major=='Psychology',]),2)
TruePositive
FalsePositive<-round(nrow(moody[moody$Score <80 & moody$Grade=='A'& moody$Major!='Psychology',])/nrow(moody[moody$Major!='Psychology',]),2)
FalsePositive
LikelihoodRatio<-round(TruePositive/FalsePositive,2)
LikelihoodRatio
PosteriorOdds <-LikelihoodRatio * PriorOdds
PosteriorOdds
Posterior <-PosteriorOdds/(1+PosteriorOdds)
Posterior
```

## Additinal Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#BR12">Bayesian Reasoning</button> 
<div id="BR12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1gwI6y7yyqi8GWdPuS9dZlXsZs-1r1TsumQb-PL4Hu5s/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/bayesian_reasoning.Rmd-->

# 🔖 Free Style: Prediction {#P1}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```


What is a prediction model?

Prediction model is a set of rules which, given the values of independent variables (predictors) determine the value of predicted (dependent variable). Here are example of such rules

```If score > 80 and participation >0.6 then grade =’A’```

```If score >60 and score <70 and major=’Psychology’ and Ask_questions =’always’ then grade =’B’```

```If score <50 and score >40 and Doze_off =’always’ then grade = ‘F’```

By freestyle prediction we mean building a prediction model without the R library functions such as rpart and other machine learning packages.  In freestyle prediction one develops models from scratch, on the basis of plots as well as exploratory queries.  Freestyle prediction is important for two reasons:  First, building prediction models from scratch allows an aspiring data scientist to “feel the data”  - as opposed to often blind direct applications of these library functions, Second, even when one uses the prediction models based on library functions, the best models are often created by combining  of several such models. These combinations often arise from skillful subsetting of datasets and applying different models to different subsets. 

As our prediction challenge competitions indicate, the winning prediction models (the ones with the least error) are predominantly combinations of different models applied to different subsets of the data. Thus, freestyle prediction is almost always a part of the prediction model building. 
We start with showing an example of a simple freestyle prediction model.




## Snippet 1: Example of a simple freestyle prediction model

```{r,tut=TRUE,height=400}

test<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")

summary(test)

myprediction<-test
decision <- rep('F',nrow(myprediction))
decision[myprediction$Score>40] <- 'D'
decision[myprediction$Score>60] <- 'C'
decision[myprediction$Score>70] <- 'B'
decision[myprediction$Score>80] <- 'A'
myprediction$Grade <-decision
error <- mean(test$Grade!= myprediction$Grade)
error
```

## Snippet 2: How to build a freestyle (your own code) prediction model?

The key idea behind building freestyle prediction models is to subset data and select the most frequent value of the predicted variable as prediction.  Of course we are interested in finding highly discriminative subsets of data with one highly dominant (most frequent value), since such a very frequent value as prediction choice will lead to a small error. But how to find data subsets with such dominant most frequent values?  It is a bit of a trial and error process. As we show below in the snippet 2, it is a sequence of one line exploratory queries, which the programmer can rely on. Later, in the next section we show how the rpart() package generates such discriminative  subsets of data automatically, though recursive partitioning.


```{r,tut=TRUE,height=500}
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")

# How do we build a freestyle prediction model?  Definitely start with plots like the boxplot from the section 5 (data exploration).  But then follow up with exploratory queries as in the recent quizzes. Examples here use table()  functon and look for situations when one grade is absoutely dominant. This would be your prediction. Thus, the goal is to slice the data using subsetting in such a way that for each slice you get a clear "winner grade". Then combine these subset rules into decision vector - just as we did in snippet 14.1.

# Below some examples of such exploratory queries with clear grade winners.

summary(moody)
table(moody$Grade)
table(moody[moody$Score>80,]$Grade)
table(moody[moody$Score>80 & moody$Major=='Psychology',]$Grade)
table(moody[moody$Score<40 & moody$Major=='Economics',]$Grade)
table(moody[moody$Score<40 & moody$Seniority=='Freshman',]$Grade)
```

## Snippet 3: One-step crossvalidation

How do we know if our prediction model is any good?  After all, we may easily build a model which is close to perfect on the training data set but performs miserably on the new, testing data. This is a nightmare for every prediction model builder and it is called a Kaggle surprise. Kaggle surprise happens quite often during our prediction competitions when students build models which are overfitting the data and which give them a false feeling of great, low error just to do the opposite on the testing data and yield a miserably high error.

To avoid this or at least to protect one against it, cross validation is needed. We illustrate cross-validation in the next snippet. We split training data into the *real* training data and the testing data, which is the remaining part of our training data set. Thus we use part of the training data as testing data. We do it by randomly splitting our data set. Although we show here just one step of cross-validation, we should do it multiple times. This helps us to observe how our model behaves for different random subsets of training data and helps us to observe inconsistent results (high variance of error) - which is a warning sign of  future kaggle surprise.



```{r,tut=TRUE,height=900}
train<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyMarch2022b.csv")
summary(train)
#scramble the train frame
v<-sample(1:nrow(train))
v[1:5]
trainScrambled<-train[v, ]
#one step crossvalidation
trainSample<-trainScrambled[nrow(trainScrambled)-10:nrow(trainScrambled), ]
myprediction<-trainSample

#prediction model - free style
#How to test how good your model is?
#Crossvalidation:  Divide train data set into two disjoint subsets T (train) and train MINUS T, the complement of T. 
#You use T to derive your prediction model and the complement of T (train MINUS T) to validate (test it).
# We assume that you created prediction model looking just at the subset of training data T=trainScrambled[1:990,  ]. 
#Since for crossvalidation we train on a subset T of the training data set and validate (test) on the complement of T. 
#In this case T= trainScrambled[1:990,  ] and complement of T (to validate/test) is stored as trainSample.
#You can do it multiple times. And observe the error and its stability.
#You build your model using the decision vector.  Here is very SIMPLISTIC MODEL which is just illustration. Your model should have much better error and be more sophisticated. 

decision <- rep('F',nrow(myprediction))
decision[myprediction$Score>40] <- 'D'

decision[myprediction$Score>60] <- 'C'

decision[myprediction$Score>70] <- 'B'

decision[myprediction$Score>80 ] <- 'A'

myprediction$Grade <-decision
error <- mean(trainSample$Grade!= myprediction$Grade)
error   
```





We use selected data puzzles from section 4 in prediction challenges. Given a data puzzle (such as 4.1), we separate it into training data subset and testing data subset. The training data is given to students to build and cross-validate their prediction models. Then we use Kaggle to evaluate their models on the testing subset of the data puzzle. Each prediction challenge is structured as competition and Kaggle ranks students' models by prediction accuracy.  For categorical variables it is the **fraction of values** which are predicted correctly, for numerical variables it is **MSE (mean square error)**. 

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## General Structure of the Prediction Challenges

The submission will take place on Kaggle which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submissions. 

The datasets provided for each prediction challenge is as follows: 

- **Training Dataset** 

  - It is used for training and cross-validation purposes in the prediction challenge. This data has all the training attributes along and the values of the attribute wich is predicted (so called, Target attribute). 
  - Models for prediction are to be trained using this dataset only. 
  - Training data set is the set which is used when you build your prediction model - since this is the only data set which has all values of target attribute. 

- **Testing Dataset**

  - It is used for applying your prediction model to new data. You do it only when you are finished with building your prediction model.

  - Testing data set consists of all the attributes that were used for training, but it does not contain any values of the target attribute. 
  - It is disjoint with the training data set - it contains new data and it is missing the target variable. 
  

- **Submission Dataset**

  - After prediction using the “testing” dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g. ID or name,etc) and second the predicted attribute column. Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle. 

To read the datasets use the read.csv() function and for writing the dataset to the file, use the write.csv() function. Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle.

To avoid this, you can add the parameter, row.names = F in the write.csv() function. e.g. ```write.csv(*dataframe*,*fileaddress*,row.names = F)```.

<!--
## Challenge 1 - Freestyle prediction of grades in yet another MOODY data set

This is the next in the sequence of data puzzles about grading methods of the eccentric professor Moody. Professor Moody found out that his former grading methods were leaked to the student by treacherous TA and changed his grading methods (and the TA).

Unfortunately, again the data was leaked to the students (Professor Moody does not use passwords). It indicates that Professor Moody may be tougher on certain majors and also may apply different grading criteria for different student seniority levels

Can you build a prediction model which will mimic Moody's grading as closely as possible?

Attached  are three files : One <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv" download="M2022train.csv">M2022train.csv</a> with  original Professor Moody grading data and another, the <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022testS.csv" download="M2022testS.csv">M2022testS.csv</a> data with missing GRADE column.  Finally <a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submissionS.csv" download="M2022submissionS.csv">M2022submissionS.csv</a> is the file you will submit to Kaggle of course after filling up the GRADE column.  

 Your job is to predict the grades in the testing file, adding to it GRADE attribute  with predicted grades. 

This test submission will be handled through Kaggle (just the error computation part)  and  Canvas just like for any assignments so far (Kaggle submission instructions coming). Kaggle will automatically calculate your prediction error.  In this case, of Professor Moody data, it will be a fraction of  grades which your prediction model have predicted incorrectly. 

**Data League:** https://data101.cs.rutgers.edu/?q=node/155 <br>
**Kaggle competition:** https://www.kaggle.com/competitions/predictive-challenge-1-2022/overview<br>
**Kaggle submission instructions:** https://data101.cs.rutgers.edu/?q=node/150 <br>
**Canvas HW9:** https://rutgers.instructure.com/courses/159918/assignments/1953810


## Challenge 2 - Same data but using rpart - decision tree

It is the same DATA as prediction challenge 1.  Just use **rpart()** this time.  Lets see if you can do better (certainly faster) with the rpart than with freestyle prediction. You have to use rpart, but you can use it as part of your prediction model and combine it with your model which you submitted for HW9.  We will talk about rpart in detail in recitations and lectures next week.

**FOR THIS PREDICTION CHALLENGE:**  Have to use rpart function (and predict of course). Have to use and show crossvalidation (use crossvalidate(). Explain in ppts how you used crossvalidation.

Use rpart contol functions - like minbucket and minsplit as well as different subsets of attributes - when corssvalidating. Make sure you explain in your ppts what "controls" have you tried and eventually used.
 
This test submission will be handled through Kaggle (just the error computation part)  and  Canvas just like for any assignments so far (Kaggle submission instructions coming). Kaggle will automatically calculate your prediction error.  In this case, of Professor Moody data, it will be a fraction of  grades which your prediction model have predicted incorrectly. 


**Data League:** https://data101.cs.rutgers.edu/?q=node/155 <br>
**Kaggle competition:** https://www.kaggle.com/competitions/predictive-challenge-2-2022/overview<br>
**Kaggle submission instructions:** https://data101.cs.rutgers.edu/?q=node/150 <br>
**Canvas HW10:** https://rutgers.instructure.com/courses/159918/assignments/1961012

-->

### Snippet 4: Preparing submission.csv for Kaggle

```{r,tut=TRUE,height=500}
# Here you just need the test table (without grades) to apply your prediction model and calculate predicted grades. And submission data frame to fill it in with the predicted #grades

test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022testSNoGrade.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')

myprediction<-test
#Here is your model. I just show example of trivial prediction model
decision <- rep('F',nrow(myprediction))
decision[myprediction$Score>40] <- 'D'
decision[myprediction$Score>60] <- 'C'
decision[myprediction$Score>70] <- 'B'
decision[myprediction$Score>80] <- 'A'
#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-decision
submission
# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle

```

<br>

**Data League:** https://data101.cs.rutgers.edu/?q=node/155 <br>
**Kaggle competition:** https://www.kaggle.com/competitions/predictive-challenge-2-2022/overview<br>
**Kaggle submission instructions:** https://data101.cs.rutgers.edu/?q=node/150 <br>

## Additional Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#p12"> Prediction - Free Style </button> 
<div id="p12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1pA0bzMGr_Tu2CXsgtT9Ks5apHqxWh8l1XXBVvwWr6zc/edit?usp=sharing" width="100%" height="500px"></embed>
</div>


<!--chapter:end:chapters/Free_style.Rmd-->

# 🔖 Predictions with rpart {#prpart}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

## Introduction 

Decision trees are one of the most powerful and popular tools for classification and prediction. The reason decision trees are very popular is that they can generate rules which are easier to understand as compared to other models. They require much less computations for performing modeling and prediction. Both continuous/numerical and categorical variables are handled easily while creating the decision trees.


## Use of Rpart {#rpart}

Recursive Partitioning and Regression Tree `RPART` library is a collection of routines which implements a Decision Tree.The resulting model can be represented as a binary tree. For the purpose of illustration of rpart we will continue to use data puzzle 3.1 set - the Professor Moody data set. 


The library associated with this `RPART` is called `rpart`. Install this library using `install.packages("rpart")`.

Syntax for building the decision tree using rpart():

- `rpart( formula , method, data, control,...)`
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *method*: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use "anova" for regression, "class" for classification, etc.
  - *data*: here we provide the dataset on which we want to fit the decision tree on.
  - *control*: here we provide the control parameters for the decision tree. Explained more in detail in the section further in this chapter.
  
  
For more info on the rpart function visit [rpart documentation](https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart)

Lets look at an example on the Moody 2022 dataset.

- We will use the rpart() function with the following inputs:
  - prediction -> GRADE
  - predictors -> SCORE, DOZES_OFF, TEXTING_IN_CLASS, PARTICIPATION
  - data -> moody dataset
  - method -> "class" for classification.


### Snippet 1
```{r,tut=TRUE,height=300}
library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function.
rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody,method = "class")

```
We can see that the output of the rpart() function is the decision tree with details of, 

- node -> node number
- split -> split conditions/tests
- n -> number of records in either branch i.e. subset
- yval -> output value i.e. the target predicted value.
- yprob -> probability of obtaining a particular category as the predicted output.

Using the output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section \@ref(rpartpredict)

But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section.

## Visualize the Decision tree {#rpartplot}

To visualize and understand the rpart() tree output in the easiest way possible, we use a library called `rpart.plot`. The function `rpart.plot()` of the rpart.plot library is the function used to visualize decision trees.

*NOTE*: The online runnable code block does not support `rpart.plot` library and functions, thus the output of the following code examples are provided directly.

### Snippet 2
```{r,tut=TRUE,height=500}
# First lets import the rpart library
library(rpart)

# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function.
rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody,method = "class")

# Now lets import the rpart.plot library to use the rpart.plot() function.
#library(rpart.plot)

# Use of the rpart.plot() function  to visualize the decision tree.
#rpart.plot(tree)
```
![Output Plot of *rpart.plot()* function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt.png)

We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about 

- the grade category.
- the outcome probability of each grade category.
- the records percentage  out of total records.

To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides [rpart.plot](https://www.rdocumentation.org/packages/rpart.plot/versions/3.0.9/topics/rpart.plot) and [Plotting with rpart.plot (PDF)](http://www.milbo.org/doc/prp.pdf)

---
**NOTE**: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment.
---

## Rpart Control {#rpartcontrol}

Now let's look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function.

- `rpart.control( *minsplit*, *minbucket*, *cp*,...)`
 - *minsplit*: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -> the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition.
 - *minbucket*: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -> the minimum number of observation in the terminal/leaf node of the trees must be 500 or above.  
 - *cp*: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by *cp*, will not be made in the tree.
 

For more information of the other arguments of the `rpart.control()` function visit [rpart.control](https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart.control)

Let look at few examples.

Suppose you want to set the control parameter minsplit=200. 

### Snippet 3: Minsplit = 200

```{r,tut=TRUE,height=500}
library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter minsplit=200
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minsplit = 200))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)
```
![Output tree plot of after setting minsplit=200 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt2.png)

### Snippet 4: Minsplit = 100

```{r,tut=TRUE,height=500}
library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter minsplit=100
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minsplit = 100))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)
```
![Output tree plot of after setting minsplit=100 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt8.png)

We can see from the output of `tree$splits` and the tree plot, that at each split the total amount of observations are above 200 and 100. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits.

Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters.

### Snippet 5: Minbucket = 100

```{r,tut=TRUE,height=500}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter Minbucket=100
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minbucket = 100))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)

```
![Output tree plot of after setting minbucket=100 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt3.png)

We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size.

### Snippet 6: Minbucket = 200

```{r,tut=TRUE,height=500}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter Minbucket=200
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody, method = "class",control=rpart.control(minbucket = 200))

tree

#library(rpart.plot)
#rpart.plot(tree,extra = 2)

```
![Output tree plot of after setting minbucket=200 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt4.png)

We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 200. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size.

Lets now use the `cp` parameter and see its effect on the tree.

### Snippet 7: cp = 0.05

```{r}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter cp=0.2
tree <- rpart(GRADE ~ ., data = moody,method = "class",control=rpart.control(cp = 0.05))

tree

#library(rpart.plot)
#rpart.plot(tree)


```
![Output tree plot of after setting cp=0.05 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt6.png)


### Snippet 8: cp = 0.005

```{r}

library(rpart)
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function with the control parameter cp=0.005
tree <- rpart(GRADE ~ ., data = moody,method = "class",control=rpart.control(cp = 0.005))

tree

#library(rpart.plot)
#rpart.plot(tree)


```
![Output tree plot of after setting cp=0.005 in rpart.control() function](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling/2022dt7.png)
We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005.


## Cross Validation {#crossvalidation}

Overfitting takes place when you have a high accuracy on training dataset, but a low accuracy on the test dataset. But how do you know whether you are overfitting or not? Especially since you cannot determine accuracy on the test dataset? That is where cross-validation comes into play.

Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. The partition is defined by split ratio. If split ratio =0.7, 70% of the training dataset will be used for the actual training of your model (rpart or lm), and 30 % will be used for validation (or testing). The accuracy of this validation data is called cross-validation accuracy.

To know if you are overfitting or not, compare the training accuracy with the cross-validation accuracy. If your training accuracy is high, and cross-validation accuracy is low, that means you are overfitting.

- `cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*)`
  - *data*: The dataset on which cross validation is to be performed.
  - *tree*: The decision tree generated using rpart.
  - *n_iter*: Number of iterations.
  - *split_ratio*: The splitting ratio of the data into train data and validation data.
  - *method*: Method of the prediction. "class" for classification.

The way the function works is as follows:

- It randomly partitions your data into training and validation. 
- It then constructs the following two decision trees on training partition:
  -  The tree that you pass to the function.
  -  The tree is constructed on all attributes as predictors and with no control parameters.
-It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees.

The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting.

We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired.

The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant.

Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration.


Let's look at the cross_validate() function in action in the example below.

We will pass the tree with formula as `GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION`, and control parameter, with `minsplit=100`. 
And for cross_validate() function, we will use` n_iter=5, and split_raitio=0.7` 

---
**NOTE:** Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use 
---

```
install.packages("devtools") 
devtools::install_github("devanshagr/CrossValidation")
CrossValidation::cross_validate()
```

### Snippet 9

```{r,tut=TRUE,ex="crossvalidate",type="pre-exercise-code"}

cross_validate <- function(df, tree, n_iter, split_ratio, method = 'class')
{
  # training data frame df
  df <- as.data.frame(df)

  # mean_subset is a vector of accuracy values generated from the specified features in the tree object
  mean_subset <- c()

  # mean_all is a vector of accuracy values generated from all the available features in the data frame
  mean_all <- c()

  # control parameters for the decision tree
  contro = tree$control

  # the following snippet will create relations to generate decision trees
  # relation_all will create a decision tree with all the features
  # relation_subset will create a decision tree with only user-specified features in tree
  dep <- all.vars(terms(tree))[1]
  indep <- list()
  relation_all = as.formula(paste(dep, '.', sep = "~"))
  i <- 1
  while (i < length(all.vars(terms(tree)))) {
    indep[[i]] <- all.vars(terms(tree))[i + 1]
    i <- i + 1
  }
  b <- paste(indep, collapse = "+")
  relation_subset <- as.formula(paste(dep, b, sep = "~"))

  # creating train and test samples with the given split ratio
  # performing cross-validation n_iter times
  for (i in 1:n_iter) {
    sample <-
      sample.int(n = nrow(df),
                 size = floor(split_ratio * nrow(df)),
                 replace = F)
    train <- df[sample,]
    testing  <- df[-sample,]
    type = typeof(unlist(testing[dep]))

    # decision tree for regression if the method specified is "anova"
    if (method == 'anova') {
      first.tree <-
        rpart(
          relation_subset,
          data = train,
          control = contro,
          method = 'anova'
        )
      second.tree <- rpart(relation_all, data = train, method = 'anova')
      pred1.tree <- predict(first.tree, newdata = testing)
      pred2.tree <- predict(second.tree, newdata = testing)
      mean1 <- mean((as.numeric(pred1.tree) - testing[, dep]) ^ 2)
      mean2 <- mean((as.numeric(pred2.tree) - testing[, dep]) ^ 2)
      mean_subset <- c(mean_subset, mean1)
      mean_all <- c(mean_all, mean2)
    }

    # decision tree for classification
    # if the method specified is not "anova", then this block is executed
    # if the method is not specified by the user, the default option is to perform classification
    else{
      first.tree <-
        rpart(
          relation_subset,
          data = train,
          control = contro,
          method = 'class'
        )
      second.tree <- rpart(relation_all, data = train, method = 'class')
      pred1.tree <- predict(first.tree, newdata = testing, type = 'class')
      pred2.tree <-
        predict(second.tree, newdata = testing, type = 'class')
      mean1 <-
        mean(as.character(pred1.tree) == as.character(testing[, dep]))
      mean2 <-
        mean(as.character(pred2.tree) == as.character(testing[, dep]))
      mean_subset <- c(mean_subset, mean1)
      mean_all <- c(mean_all, mean2)
    }
  }

  # average_accuracy_subset is the average accuracy of n_iter iterations of cross-validation with user-specified features
  # average_acuracy_all is the average accuracy of n_iter iterations of cross-validation with all the available features
  # variance_accuracy_subset is the variance of accuracy of n_iter iterations of cross-validation with user-specified features
  # variance_accuracy_all is the variance of accuracy of n_iter iterations of cross-validation with all the available features
  cross_validation_stats <-
    list(
      "average_accuracy_subset" = mean(mean_subset, na.rm = T),
      "average_accuracy_all" = mean(mean_all, na.rm = T),
      "variance_accuracy_subset" = var(mean_subset, na.rm = T),
      "variance_accuracy_all" = var(mean_all, na.rm = T)
    )

  # creating a data frame of accuracy_subset and accuracy_all
  # accuracy_subset contains n_iter accuracy values on cross-validation with user-specified features
  # accuracy_all contains n_iter accuracy values on cross-validation with all the available features
  cross_validation_df <-
    data.frame(accuracy_subset = mean_subset, accuracy_all = mean_all)
  return(list(cross_validation_df, cross_validation_stats))
}
```


```{r,tut=TRUE, ex="crossvalidate",type="sample-code",height=500}
# First lets import the rpart library
library(rpart)
# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv',stringsAsFactors = T)
# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS, data = moody,method = "class",control = rpart.control(minsplit = 100))
tree
# Now lets predict the Grades of the Moody Dataset.
pred <- predict(tree, moody, type="class")
head(pred)
# Lets check the Training Accuracy
mean(moody$GRADE==pred)
# Lets us the cross_validate() function.
cross_validate(moody,tree,5,0.7)
```

You can see that the cross-validation accuracies for the tree that was passed (accuracy_subset) are fairly high and close to our training accuracy of 84%. This means we are not overfitting. Also observe that accuracy_subset and accuracy_all have the same values, which means that the only relevant attributes are score and participation, and adding more attributes doesn't make any difference to the tree. Finally, the values in accuracy_subset are reasonably close to each other, which mean low variance.


## Prediction using rpart. {#rpartpredict}

Now that we have seen the process to create a decision tree and also plot it, we will like to use the output tree to predict the required attribute.

From the moody example, we are trying to predict the grade of students. Lets look at the `predict()` function to predict the outcomes.

- `predict(*object*,*data*,*type*,...)`
  - *object*: the generated tree from the rpart function.
  - *data*: the data on which the prediction is to be performed.
  - *type*: the type of prediction required. One of "vector", "prob", "class" or "matrix".

Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset.

### Snippet 10

```{r,tut=TRUE,height=500}
# First lets import the rpart library
library(rpart)

# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+DOZES_OFF+TEXTING_IN_CLASS+PARTICIPATION, data = moody ,method = "class")
tree

# Now lets predict the Grades of the Moody Dataset.
pred <- predict(tree, moody, type="class")
head(pred)
```

## Snippet 11: Your Model with rpart

```{r,tut=TRUE,height=600}
#How to combine your freestyle prediction model with the rpart? 

#One way of doing it is to divide the data sets into two mutually exclusive subsets (which cover all data also).  How do you make these subsets?  Unfortunately there is no algorithm for this and it is more relying on how well is your model doing for different slices of the data.  

#In this example (similarly to snippet 16.7 where we combine two rpart models, we assume that initial split we decided on is based on SCORE. But instead of having two rpart models  (16.7), we will use our prediction  model from prediction challenge 1  for SCORE >50 and rpart for SCORE <=50.

#Lets assume that yourPrediction is our model from Prediction Challenge 1 (your entire code has to be applied here to the data set (moody, below)

moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')

#rpartModel<-rpart(GRADE~., data=moody[moody$SCORE<=50,]);
#pred_rpartModel <- predict(rpartModel, newdata=moody[moody$SCORE<=50,], type="class")
#pred_yourModel <- yourPrediction[moody$SCORE<=50]
#myprediction<-moody

## Here we combine two models - our model from prediction 1 challenge and rpart.

#decision <- rep('F',nrow(myprediction))
#decision[myprediction$SCORE>50] <- pred_yourModel
#decision[myprediction$SCORE<=50] <-as.character(pred_rpartModel )
#myprediction$GRADE <-decision
#error <- mean(moody$GRADE!= myprediction$GRADE
#error
```

## Snippet 12: Freestyle +  rpart: Combining rpart prediction models

```{r,tut=TRUE,height=600}

library(rpart)
# Import dataset
moody<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/moody2022_new.csv')
model1<-rpart(GRADE~., data=moody[moody$SCORE>50,]);
model2<-rpart(GRADE~., data=moody[moody$SCORE<=50,]);
model1
model2
pred1 <- predict(model1, newdata=moody[moody$SCORE>50,], type="class")
pred2 <- predict(model2, newdata=moody[moody$SCORE<=50,], type="class")
myprediction<-moody
decision <- rep('F',nrow(myprediction))
decision[myprediction$SCORE>50] <- as.character(pred1)
decision[myprediction$SCORE<=50] <-as.character(pred2)
myprediction$GRADE <-decision
error <- mean(moody$GRADE!= myprediction$GRADE)
error

```

## Snippet 13: Submission with rpart

```{r,tut=TRUE,height=600}
library(rpart)
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022testSNoGrade.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')
train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022train.csv")

tree <- rpart(Grade ~ Major+Score+Seniority, data = train, method = "class",control=rpart.control(minbucket = 200))
tree

prediction <- predict(tree, test, type="class")

#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-prediction 

# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle
```

## Additional Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#dt12"> Prediction with rpart </button> 
<div id="dt12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1Yw11iu0FnUbiNKrwEKyWHxZ1oXUVG0APfI9acEbB91o/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/Decision_trees.Rmd-->

# 🔖 Linear Regression {#lr}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

## Introduction 

How to build prediction models for numerical variables?

So far we have discussed prediction models for categorical target variables.  In order to predict numerical variables we often use linear regression.

<!--
Linear regression is a linear approach to modeling the relationship between a numerical response ($Y$) and one or more independent variables ($X_i$).

Usually in linear regression, models are used to predict only one scalar variable. But there are two subtype if these models:
- First when there is only one explanatory variable and one output variable. This type of linear regression model known as simple linear regression.
- Second, when there are multiple predictors, i.e. explanatory/dependent variables for the output variable. This type of linear regression model known as multiple linear regression.

![Linear models fitted to various different type of data spread. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. Credits: Wikipedia.](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling2/lmvariants.svg)
-->

## Linear regression using lm() function {#lm}

Syntax for building the regression model using the *lm()* function is as follows:

- `lm(formula, data, ...)`
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *data*: here we provide the dataset on which the linear regression model is to be trained.
  
For more info on the *lm()* function visit [lm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm)

Lets look at the example on the Moody dataset.

```{r,echo=FALSE}
moodyNUM<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv')
temp<-knitr::kable(
  head(moodyNUM, 10), caption = 'Snippet of Moody Num Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

Imagine that we do not know the weights of  midterm, project and final exam.  However we have the data from the previous semesters.  Can we find these weights out? The answer is yes - by using `linear regression`.

### Snippet 1: How much do Midterm, Project and Final Exam count?

```{r,tut=TRUE,height=500}
moodyNUM<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv')
split<-0.7*nrow(moodyNUM)
split
moodyNUMTr<-moodyNUM[1:split,]
moodyNUMTr
moodyNUMTs<-moodyNUM[split:nrow(moodyNUM),]
#We use linear regression to #find out the weights of #Midterm, Project and Final #Exam in calculation of the #final class score. Each of #them are scored out of 100 and #the final class score is also #scored out of 100 as weighted #sum of Midterm, Project and #Final Exam scores.
train <- lm(ClassScore~.,  data=moodyNUMTr)
train
pred <- predict(train,newdata=moodyNUMTs)
mean((pred - moodyNUMTs$ClassScore)^2)
```

We can see that, 

- The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc.
- The predicted values are obtained from the predict function using the trained model and the test data.

## Calculating the Error using mse() {#mse}

As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test won't prove useful now in our numerical predictions scenario.

We don't want to eyeball every time we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique.

To do this we will use the Mean Squared Error(MSE).

- The MSE is a measure of the quality of an predictor/estimator
- It is always non-negative
- Values closer to zero are better.

The equation to calculate the MSE is as follows:

\begin{equation}
MSE=\frac{1}{n} \sum_{i=1}^{n}{(Y_i - \hat{Y_i})^2}
\\ \text{where $n$ is the number of data points, $Y_i$ are the observed value}\\ \text{and $\hat{Y_i}$ are the predicted values}
\end{equation}

To implement this, we will use the *mse()* function present in the Metrics Package, so remember to install the Metrics package and use `library(Metrics)` in the code for local use.

The syntax for *mse()* function is very simple:

- `mse(actual,predicted)`
  - *actual*: vector of the actual values of the attribute we want to predict.
  - *predicted*: vector of the predicted values obtained using our model.


## Snippet 2: Cross Validate your prediction

```{r,tut=TRUE,height=600}
library(ModelMetrics)

train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")
#scramble the train frame
v<-sample(1:nrow(train))
v[1:5]
trainScrambled<-train[v, ]

#one step crossvalidation
n <- 100
trainSample<-trainScrambled[nrow(trainScrambled)-n:nrow(trainScrambled), ]
testSample <- trainScrambled[1:n,]

lm.tree <- lm(ClassScore~.,  data=trainSample)
lm.tree

pred <- predict(lm.tree,newdata=testSample)
pred

mse(testSample$ClassScore,pred)

  
```

We can see that,

- The summary of the lm model gives us information about the parameters of the model, the residuals and coefficients, etc.
- The predicted values are obtained from the predict function using the trained model and the test data. In comparison to the previous model we are using the cross validation technique to check if we have more accurate predictions, thus increasing the overall accuracy of the model.

## Snippet 3: Submission with lm

```{r,tut=TRUE,height=600}
library(rpart)
test<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM_test.csv')
submission<-read.csv('https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/M2022submission.csv')
train <- read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/MoodyNUM.csv")

tree <- lm(ClassScore~.,  data=train)
tree

prediction <- predict(tree, newdata=test)

#Now make your submission file - it will have the IDs and now the predicted grades
submission$Grade<-prediction 

# use write.csv(submission, 'submission.csv', row.names=FALSE) to store submission as csv file on your machine and subsequently submit it on Kaggle
```

## Additional Reference 

<button class="btn btn-primary" data-toggle="collapse" data-target="#lr12"> Linear Regression </button> 
<div id="lr12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1RuAidmZGDoTRYMjONLEsw3bfx_aQb-f3C0V4Jjk4Vzs/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/Linear_regression.Rmd-->

# 🔖 Machine Learning-Prediction Loop {#MLP}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

Believe it or not you are ready now to use pretty much any machine learning package from the extensive R library.  
In other words you can drive any car without knowing how the engine works. This you can find out by taking more advanced classes in Machine learning from computer science, statistics or machine learning departments.  

As per CRAN there are around 8,341 packages that are currently available.  Apart from CRAN, there are other repositories which contribute multiple packages. The simple straightforward syntax to install any of these machine learning packages is: **install.packages (“MLPackage”)**.

`Install Packages(‘MLPackage’)`

`Library(MLPackage)`

`MlPackage<-MLPackage(Formula, data=YOUR_TRAINING,…)`

`Predict(MLPackage, newdata=YOUR_TESTING…)`

`Error <- ……`

MLPackage can be rpart, Random Forests, naive Bayes, LDA, SVM, Neural Network and many others.


## Additional Reference

<button class="btn btn-primary" data-toggle="collapse" data-target="#ML12"> Prediction Loop </button> 
<div id="ML12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1n-7uFUUS40SwZO31rKxblplHtkAZMvQDXQzA3lY4PPU/edit?usp=sharing" width="100%" height="500px"></embed>
</div>

<!--chapter:end:chapters/Prediction_loop.Rmd-->

# 🔖 How can data fool us? {#MAD}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction 

`How not to be fooled by data?`  In the description of this class we promised that data 101 will teach you this. Have we? I hope so. Please use our question roulette to test yourself. In this section we discuss Simpson Paradox, Prosecutorial and Ecological Fallacies.  Before we proceed with the paradoxes let us summarize what we have learned so far:

- `Beware of randomness` (Hypothesis testing, p-values, Multiple hypothesis testing)
- `Be an optimist` - use Bayesian reasoning. Remember about prior odds first!
- `Beware of extreme results` - Apply law of small numbers 
- `Remember we process information following availability` - we assign high frequency falsely to events which are just talked about very often 
- `Narrative fallacy - do not find false patterns` - “Stocks went down due to concerns about rising cost of living”

Here we will discuss the Simpson paradox as well as Prosecutorial and Ecological fallacies. 
Let us start with the Simpson paradox. Here is a  simple example of two basketball players, Aaron and Barry

The Table **16.1** shows Aaron’s and Barry’s  free throw point averages (FTP) over the 2021 and 2022 season respectively. Clearly for both seasons Barry  beats Aaron  in terms of FTP, in 2021 by 90% to 80% and in 2022 by 70% to 65%.  `Can Aaron  still beat Barry over both seasons - that is get a higher FTP over the sum of two seasons, 2021+2022?`  First answer which comes to mind  is absolutely not. `How can Aaaron beat Barry over 2021+2022 when   Barry beats him in each of the two seasons? `

**Table 16.1** 

<table>
<tr>
<td> season/player </td>
<td> Aaron</td>
<td> Barry</td>
</tr>


<tr>
<td> 2021 season</td>
<td> 80%</td>
<td> 90%</td>
</tr>

<tr>
<td> 2022 season</td>
<td> 65%</td>
<td> 70%</td>
</tr>

</table>


But table **16.2** explains that it is quite possible that  Aaron can beat Barry over 2021+ 2022. Indeed, since we do not know the absolute number of attempts at free throws, we can easily pick any number of attempts for each of them in any of the two seasons.

**Indeed** - here is the proof that Aaron can still beat Barry. If Barry made 100 attempts in 2021 and 20 attempts in 2022, while Barry made only 20 attempts in 2021 and 100 attempts in 2022, Aaron's overall FRP for both 2021+2022 will be higher than Barry’s. And this is Simposon’s paradox. 

**Table 16.2** 

<table>
<tr>
<td> season/player </td>
<td> Aaron</td>
<td> Barry</td>
</tr>


<tr>
<td> 2021 season</td>
<td> 80% out of 100</td>
<td> 90% out of 20</td>
</tr>

<tr>
<td> 2022 season</td>
<td> 65% out of 20</td>
<td> 70% out of 100</td>
</tr>

</table>

Indeed  Aaron’s FTP over 2021+2022 is  
\begin{equation}
\frac{80+13}{120} = \frac{93}{120}
\end{equation}
which is larger than Barry’s   

\begin{equation}
\frac{18+20}{120} = \frac{88}{120}
\end{equation}

More generally, trends in subsets of data may reverse themselves after aggregation. 

In fact we can have any number of seasons and have Barry  beat Aaron  in FTP in each and every season and Aaron still wins with better FTP over all seasons.   This is again simply because we do not know how many attempts each player made each season.  This applies to many real world situations such as graduate admissions for example (the famous Berkeley admission bias case). There  women may have a higher chance to be admitted than men in each single academic  department  and nevertheless, men  beat women in overall acceptance ratio.  This is again hard to comprehend at first but it is due to the fact that the absolute number of female and male applicants may be different for each department. 

**Is such reversal always possible? **

Let's look at the table below:

**Table 16.3** 

<table>
<tr>
<td> season/player </td>
<td> Aaron</td>
<td> Barry</td>
</tr>


<tr>
<td> 2021 season</td>
<td> 65%</td>
<td> 90%</td>
</tr>

<tr>
<td> 2022 season</td>
<td> 60%</td>
<td> 70%</td>
</tr>

</table>

`In this case the Simpson paradox is not possible. Why?`  Because Aaron’s highest FTP (65% in 2021 season is  lower than Barry’s lowest FTP in 2022). You can easily see that no matter what the absolute numbers of attempts in each season, Aaron can never beat Barry for 2021+2022. 
Thus the Simpson paradox was possible in this simple case only because Aaron’s highest FTP was higher than Barry’s lowest FTP. 

One also has to be careful with the Simpson paradox and not apply it to situations when both groups /individuals have the same absolute number of “attempts”.  For example, the Simpson paradox is not possible for students and their individual scores on homework's and exams.  If Barry  scores higher than  Aaron on each homework and on each  exam then Barry will always have a higher score overall than Aaron.  There is no  Simpsonian trend reversal. Every homework and every exam counts the same for all students. This is as if players always made the same number of free throw attempts.

### Ecological Paradox

Ecological paradox is kind of the reverse of the Simpson paradox. Let's assume that we consider net worth  for each member of groups A and B.   Even if average net worth of group A is higher than average net worth  of group B, it may be possible  that random individual member of the group B has higher net worth  than random individual member of the group A. Thus the order of aggregates may be reversed when we look at the level of individuals. 

For example as table **16.4** illustrates, the average net worth of Group A dominates the average net worth of Group B due to the presence of one wealthy individual.  However for 90% of pairs of individuals, group B members are more wealthy than Group A members.

**Table 16.3** 

<table>
<tr>
<td> Group A </td>
<td> Group B</td>
</tr>


<tr>
<td> $10,000,000</td>
<td> $210,000</td>
</tr>

<tr>
<td> $100,000</td>
<td> $290,000</td>
</tr>

<tr>
<td> $120,000</td>
<td> $220,000</td>
</tr>

<tr>
<td> $80,000</td>
<td> $210,000</td>
</tr>

<tr>
<td> $60,000</td>
<td> $270,000</td>
</tr>

<tr>
<td> $160,000</td>
<td> $210,000</td>
</tr>

<tr>
<td> $110,000</td>
<td> $240,000</td>
</tr>

<tr>
<td> $100,000</td>
<td> $210,000</td>
</tr>

<tr>
<td> $200,000</td>
<td> $240,000</td>
</tr>

</table>


For example, it is well known that Democrats win the richest states, while (until recently), the richest individuals vote republican. `How is it possible?`  Explanation is simple. Everyone’s vote counts the same and there are few very rich people. Very rich people may contribute more to the average wealth of the state (due to their extreme wealth), but there are just very few of them. 

**Do not be fooled  by aggregates!  **

Let's assume that a Democrat wins 70% of the vote and a Republican wins 30% of the vote in some state.    `Is it possible that, nevertheless, the  republican candidate wins all 19 counties out of 20 in the state?  ` 
This actually happens a lot when the population is heavily concentrated in a heavily  populated urban county which has the vast majority of voters living there. 



## Additional References


<button class="btn btn-primary" data-toggle="collapse" data-target="#Mt12">  How can data fool us? </button> 
<div id="Mt12" class="collapse">
<embed src="https://docs.google.com/presentation/d/1vDCRffHzeSxka1sxQSUCnZvzaKIMmwPE7UO-5IefrM4/edit?usp=sharing" width="100%" height="500px"></embed>
</div>


<!--chapter:end:chapters/Mysteries_of_Aggregated_data.Rmd-->

# Boundless Analytics - Pre-discovery Tool {#dsaf}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## Introduction

In this section we demonstrate application of Boundless Analytics - the tool developed by **Tomasz Imielinski** and his team at Rutgers (and supported by NSF subcontract of Center of Science of Information at Purdue University). Boundless Analytics calculates all significant bar graphs from the data set and allows to find data subsets (slices) which deviate the most from the whole data set in regard to frequency distribution of an attribute. Boundless performs an otherwise very tedious task of looking at all combinations of attribute value pairs to identify the “significant ones” - saving enormous amounts of work in preliminary exploration of data. 

We are using here the Minimarket  data puzzle **3.8** describing customer transactions in the small chain of minimarkets in NJ. Data 101 students used Boundless Analytics to discover the most interesting subsets of this data set 


## Minimarket Data Set description

<a href="https://rutgers.zoom.us/rec/share/JIx-ZC0P5oXk5rnyOk_kZ8pF1XAC5rJnLGaIJGxxgLNmSzg6F-IRfUyNiQoKR9v0.MratfWX1STX2sYho">Zoom recording</a>

## Demo of Boundless Analytics 

<a href="https://rutgers.zoom.us/rec/share/bAr3uyPJA1vlGxbb6jVpvOHNFOuRgmj9TE4iSCcBn3_hS549xnqY9IePPsMgGE1i.F6P4omjuUebsQFl6">Zoom Recording</a>

## The Boundless Analytics web application 

**Boundless Analytics Interface:** http://209.97.156.178:8082/

(it is a soft login abc/abc will do)

**Objective: Nominate  the most interesting subset of  the Minimarket2022 data set**

Seems open ended, no? what is the "most interesting"?

- Chi-square value is a good measure. We explain it below.

- By swiping through possible plots (using Next), one can identify good candidates for the “interesting data subsets”) 

- These are plots where red and blue bars differ the most.  In other words we want to reject the null hypothesis of independence of red and blue distributions over the data slice and the complement of the data slice. The higher the chi-square is, the strongest is our rejection of independence of **red** and **blue** distributions. 

Therefore this task can be seen as chi-square hunt for the highest chi-square value  (use the  snippet **17.1** code after plugging in definition of a slice and the anchor attribute)

## Snippet 1: Chi square hunt

```{r,tut=TRUE,height=600}

# Say, the Boundless analytics provides us with the slice:  Beer =='Lager' &  Day =='Weekend' and Snacks ='Crackers' and anchor attribute is Location.  You can calculate Chisq for this slice and the Location attribute to test if distribution of locations is affected if we limit ourselves only to transactions selling Lager and Crackers on Weekends?  

# The most interesting slice-anchor attribute combinations are the ones with the largest chisq test and lowest p-value. Nevertheless do not forget about multiple hypothesis correction - since we can on chi-square hunt here!

Minimarket<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022.csv")

Minimarket$IN<-'Out_Slice'
Minimarket[Minimarket$Beer=='Lager' & Minimarket$Day=='Weekend' &  Minimarket$Snacks =='Crackers', ]$IN<-'In_Slice'
d<-table(Minimarket$Location, Minimarket$IN)
chisq.test(d)

```

<br> 

ATTACHED - the data set (same as on the Boundless Analytics interface)
<a href="https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/HomeworkMarket2022-2.csv" download="HomeworkMarket2022-2.csv">HomeworkMarket2022-2.csv</a> 

**RESULTS: **

Here are two out of 250+ submissions.  The one with the highest chi-square of `600.15` is the slice  showing **weekend** buyers of **lager** in **New brunswick** but disproportionately more snacks (in particular Crackers). This was identified by nearly 20 students.

![](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling2/image.png)

<br> 

Here is another find by Eva Zhang showing disproportionately frequent **sales of Coca Cola** on **Weekdays** in **Princeton** for transactions which purchased Popcorn. The chi-square  value of this find is `205.31`, with  df=3.

![](https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/img/modeling2/image-2.png)

<!--chapter:end:chapters/22.Rmd-->

# 🔖 Best Works of 2022 {#b2022}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"> </script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

## DataBlog

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt12"> Ella Walmsley </button> 
<div id="bt12" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best1.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

<!--
## Prediction Challenge 1

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt15"> Upsham Naik </button> 

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt13"> Jeevanandan Ramasamy </button> 
<div id="bt15" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best2.pptx&embedded=true" width="100%" height="500px"></embed>
</div>
<div id="bt13" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

## Prediction Challenge 2

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt14"> Jeevanandan Ramasamy </button> 
<div id="bt14" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/best4.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

## Prediction Challenge 3

<button class="btn btn-primary" data-toggle="collapse" data-target="#bt19"> Eva Zhang </button> 
<div id="bt19" class="collapse">
<embed src="https://docs.google.com/presentation/d/1ZKQa5-JGiQydxjOvLrE_SaX9eOnjP6jHdqJ4Dpyc-bw/edit?usp=sharing" width="100%" height="500px"></embed>
</div>
-->


## Boundless Analytics

<button class="btn btn-primary" data-toggle="collapse" data-target="#ba1"> Anastasiya Chuchkova </button> 
<button class="btn btn-primary" data-toggle="collapse" data-target="#ba2"> Shreya Tiwari </button> 
<button class="btn btn-primary" data-toggle="collapse" data-target="#ba3"> George Basta </button> 

<div id="ba1" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba1.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="ba2" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba2.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="ba3" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba3.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<br>

<button class="btn btn-primary" data-toggle="collapse" data-target="#ba5"> Paul Kotys </button> 
<button class="btn btn-primary" data-toggle="collapse" data-target="#ba6"> Selin Altimparmak </button> 

<div id="ba5" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba5.pptx&embedded=true" width="100%" height="500px"></embed>
</div>

<div id="ba6" class="collapse">
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/pred/ba6.pptx&embedded=true" width="100%" height="500px"></embed>
</div>


<!--chapter:end:chapters/best_work_2022.Rmd-->

# Data League Leaderboard {#DLL}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy = FALSE)

```

```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/dev7796/data101_tutorial/main/files/dataset/Leaderboard'2022.csv")
# head(moody)
temp<-knitr::kable(
  moody, caption = 'Leaderboard 2022',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```
<br>
**Honourable Mentions: **<br>
Upsham Naik, Joshua B. Sze, Kirtan Patel, Maria Xu, Devam Patel, Eva Zhang, Toshanraju Vysyaraju, Maanas Pimplikar, Jared Chiou, Nitya Narayanan, Shrish Vellore, Yousra Belgaid, Mitali Shroff, Michael Jucan, Jackie Hong, Arvin Sung, Eric Xuan, Eva Allred, Leah Ranavat, Nami Jain, Gautam Agarwal, Aditya Patil


<!--chapter:end:chapters/Leaderboard.Rmd-->

